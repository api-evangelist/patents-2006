---

title: Systems and methods for tracking the eye
abstract: Disclosed are systems and methods for tracking the eye. In one embodiment, a method for tracking the eye includes acquiring stereo images of the eye using multiple sensors, isolating internal features of the eye in the stereo images acquired from the multiple sensors, and determining an eye gaze direction relative to the isolated internal features.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07747068&OS=07747068&RS=07747068
owner: 
number: 07747068
owner_city: 
owner_country: 
publication_date: 20060120
---
The invention described herein may be manufactured used and licensed by or for the United States Government for governmental purposes without the payment of any royalty.

Oculometers are used to measure the eye gaze direction as well as the fixation duration and dual eye binocular convergence point. Such oculometers have many potential applications in the medical scientific engineering manufacturing military and entertainment domains. Example applications include use of an oculometer as a tool for the medical diagnosis of ocular functions as an aid to the paraplegic handicapped for the measurement of ocular functions and workload in human factors studies as a measure of subject training as a tool for fatigue monitoring as part of an electronic safety net to detect performance degradation due to pilot incapacitation in piloted and tele operated vehicles as a component of an electronic intelligent pilot vehicle interface used for adaptive aiding in piloted and tele operated vehicles for task scan analysis including measuring situation awareness for human operator control of machines and interaction with computer games and for advertisement and usability analysis. Oculometers can be designed for use with head mounted video displays such as those that have been developed for virtual reality stereographic displays monocular or binocular vision helmet mounted displays and night vision goggles. These displays are used in piloted helicopters vehicles and control stations for teleoperated robotics.

Prior art oculometers typically comprise a light source that illuminates the eye to be tracked and a single light sensor that captures rays of light that are reflected from the eye. Although such oculometers provide an indication of eye position and therefore gaze direction the use of a single light sensor presents various potential limitations or drawbacks. For example a single sensor may not receive the rays reflected off of the cornea or eye interior in cases in which the user s gaze is fixed upon an object positioned at an extreme angle relative to the forward looking direction e.g. when the wearer is gazing laterally . As another example if the single sensor is used to collect image data that is used to locate features in the eye of interest in three dimensional space multiple images may need to be captured over time thereby increasing processing time and potentially introducing error due to eye movement over the duration in which the image data is captured.

Disclosed are systems and methods for tracking the eye. In one embodiment a method for tracking the eye includes acquiring stereo images of the eye using multiple sensors isolating internal features of the eye in the stereo images acquired from the multiple sensors and determining an eye gaze direction relative to the isolated internal features.

Disclosed herein are systems and methods for tracking an eye. More particularly disclosed are eye tracking systems and methods that employ multiple light sensors that provide image data that is processed to determine at least a gaze direction of an eye.

Referring now to the drawings in which like numerals indicate corresponding parts throughout the several views illustrates an eye of a user e.g. a wearer of an eye tracking mechanism . Positioned on opposite sides e.g. opposed lateral sides of the eye are optical systems and light sensors . The optical systems each comprise one or more lenses and or mirrors that are used to direct and focus light that reflects off of the eye on their associated light sensors . The light sensors receive the reflected light i.e. light rays which is used to determine the gaze direction of the eye. More particularly the light sensors generate two dimensional stereo images of the eye that can be used to calculate the optical axis and visual axis of the eye. Although the optical systems and the light sensors are shown and have been described as being positioned on opposite sides of the eye other configurations are possible. One such configuration preferred for use with this invention consists of sensors with light collecting optics that are positioned on opposite sides of the central axis of sight for the optical device. As shall be elaborated the optical device consists of a light source optical systems to collect the light reflected from the eye and sensors matched to the optical systems to convert the light images to electrical outputs. In the preferred embodiment the light collecting optics for the sensors are positioned so as to collect stereo images i.e. slightly displaced two dimensional images of the eye as much as possible. Moreover although two light sensors are shown and have been described a greater number of light sensors could be used if desired for example sensors with optics positioned about the sides of the eye to cover the full range of viewing directions.

Referring to illustrated is a first embodiment of an eye tracking system that can be used to determine the gaze direction of an eye of a user . The tracking system comprises several components that are similar in configuration and or function to components disclosed in U.S. Pat. No. 6 120 461 which is hereby incorporated by reference in its entirety into the present disclosure.

As is shown in the tracking system includes an optical device that is mounted on a headset that is donned by the user . In addition the tracking system includes an opto electronic device an image processor and a visual axis calculation system . The optical device comprises a light source and two light sensors with associated optical systems and opposed light collecting optics. The light source is configured to under the control of the image processor shine light on an eye of the user . By way of example the light source comprises a display or one or more light emitting diodes LEDs . In cases in which the light source comprises multiple light elements e.g. pixels of a display or individual LEDs in an LED array the elements of the light source can be sequentially illuminated by a control signal generated by the image processor and delivered to the light source via control line . The light sensors detect the light reflected from the user s eye via the optical systems from the corresponding light collecting optics and output analog image signals to the opto electronic device .

The opto electronic device receives the image signals generated by the sensors isolates specular reflections created by the light source determines image coordinates of the reflection points on the corneal surface and outputs signals to the image processor . The image processor isolates various pertinent features i.e. internal features of the eye s anatomy from images captured by the sensors for instance captured simultaneously in an instant in time or captured sequentially over a finite period of time. In cases in which images are captured simultaneously different colored light elements can optionally be used to illuminate the eye so that the origin of reflected light can be determined i.e. from the light s wavelength . By way of example features of the eye that are isolated by the image processor include the apparent pupil and the inner structures of the eye including the sphincteral pattern of the iris and the retinal capillary network. Once the eye features are identified their locations i.e. coordinates within the two dimensional images are determined by the image processor and those locations are output as digital signals to the visual axis calculation system .

Operation from this point depends upon the particular configuration of the visual axis calculation system . Generally speaking however the visual axis calculation system receives the data output by the image processor and uses that data to calculate the optical axis of the eye from which the visual axis and the gaze direction can be determined. More particularly the visual axis calculation system runs several processes or routines that compute the visual line of sight. This information can then be output from the visual axis calculation system and used for example by a digital computer to determine various information such as the workspace line of sight eye fixations gaze points and the binocular convergence point for a dual eye system. As is described in the following the visual axis calculation system may comprise one or more of a digital processor a stereo reconstructor and opthalmometric ray tracer that are used to determine the visual axis and gaze direction.

Referring now to illustrated is an example embodiment of the opto electronic device shown in . The illustrated embodiment has particular advantages for processing images from multiple sources or a source with multiple elements under sequential control of the processor . As is indicated in the opto electronic device can comprise a photo amplifier array a comparator array an encoder and latch and an image acquiring array . The photo amplifier array amplifies the analog signals received from the sensors and outputs amplified signals to the comparator array and the image acquiring array . The comparator array isolates the amplifier element s that first respond s to the illuminating element of the light source and provides an electrical input to the encoder and latch . The encoder and latch is controlled for example by the image processor to output an electrical signal for each light source to the visual axis calculation system via the image processor . The electrical signal contains the address of the first responding amplifier and thereby maps the location of the specular reflection of the light source in the photo amplifier array . In turn the encoder and latch output causes the output of the amplifier element to be suppressed in the photo amplifier array during the image acquisition period resulting in an image of the diffused reflections from the inner surfaces of the eye. The image acquiring array outputs signals to the image processor for instance to a charge coupled device CCD array or a random access memory RAM cache of the image.

While the above hardware has been described as an example the functions that hardware provides can alternatively be performed in a pre image processing stage in which the image data collected in the photo amplifier array for a given light source is thresholded in the comparator array to locate the specular reflection. The array address of the reflection point can then be output to the image processor while the reflection image is masked from the array before being added to the image acquiring array. The image acquiring array is outputted to the image processor for each sensor at the end of the source activation cycle and reset by the image processor before the cycle is repeated.

In the illustrated embodiment the image processing device is embodied as a stack of two dimensional very large scale integrated VLSI circuit arrays made of conductive metal oxide silicon CMOS wafers. Each array in the stack comprises identical digital processing elements that are matched between arrays by data bus lines for image processing. The arrays are controlled by a central processing unit CPU with common address and control bus lines . The CPU operates with a clock instruction decoder register arithmetic and logic unit and access to a memory cache with a stored program for stack control. The initial array of the stack is the digital memory cache that stores the digitized images. The remaining arrays of the stack comprise digital memory comparators counters accumulators and replicating elements. In operation the CPU writes numerical values to the array elements in the stack operating the arrays en masse through the data and control bus lines .

The array elements of the stack perform conventional digital functions 1 the memory elements perform store shift invert Boolean logic OR function and readout 2 the accumulators store add and readout 3 the replicating elements input output and perform stack control 4 the counters count in and shift out and 5 the comparators store reference values and output whether the input is greater equal or lower in value. The inputs to the stack are the digital memory cache and the pupil image moments . The output of the stack is a memory mapped list of the image features e.g. spectral reflection point and structural eye features and their image positions.

The accumulated image is transferred to the digital memory cache from the CCD array . This may be accomplished by means of clocked serial bucket brigade downloaded to an analog to digital converter with an analog input and a digital output. In that process the output is read as a serial digital input by the very large scale integrated VLSI circuit . The circuit can for example comprise a video moment generator chip that is used to compute the image moments of the pupil and is made up of CMOS elements. In such a case the chip comprises a digital comparator for thresholding and counters shifters adders registers and a programmable logic array PLA . The digital comparator separates the serial digital input for the pupil image intensity from that for the iris and the sclera. The digital elements perform recursive moment computations on the resulting binary image under the control of the PLA. At the end of the serial transfer the circuit outputs the elliptical moments of the pupil image such as the centroid the principal and minor axes and deviations from these axes.

In preparation for feature processing the pupil image is isolated by the stack . The CPU reads the pupil image moments from the VLSI circuit following transfer of the digitized image to the digital memory cache . The CPU isolates the portion of the image containing the pupil by computing a template from the elliptical moments and masking the image array cache. This is done by for example writing ones into the elements of a memory array which match the pupil s template and zeros into the remaining elements. The CPU then uses an accumulator array to multiply the elements of the template array with the matched elements of the image array and writes the products en masse to produce a masked memory array.

Following isolation of the pupil image the apparent pupil can be parameterized as a centroid and elliptical axes. Such parameterization can be used for reference by a stereo reconstructor see in matching features across stereo images. The apparent pupil is used to segment the image into two separate regions the apparent iris and the retinal fundus. From those two regions the stack can abstract key image features of the subject eye including the sphincteral cusps of the iris and the capillary network of the retinal fundus. With reference to vascular capillary network radiates from the optic disk at the back of the retina where the optic nerve second cranial nerve enters the eyeball. The portion of the retinal fundus that is visible in the image is determined by the diameter and orientation of the pupil . For reference the figure shows the sclera the cornea and the pupil centroid as they appear in an image of the eye. Also shown is an overlaid grid of corneal reflection points that are not part of the image but that correspond to the table of reflection points for the sources that are generated by the image processor from the latch outputs. The analysis of the vascular network is performed through a series of consecutive image processing steps in which 1 the pupil image is enhanced with histogram equalization 2 the vascular network is enhanced with spatial gradient templates for edges and lines 3 the network is segmented by binary thresholding and 4 the junction points of the network segments are isolated as key features.

The stack can further isolate the cusps of the pigmentary pattern on the sphincteral muscles of the iris the opaque contractile diaphragm performed by the pupil. The sphincteral muscles contract or dilate the iris with changes in ambient light and accommodation. While the pattern changes with contraction the general shape defined by the cusps remains invariant and unique to the individual. The stack isolates these key features through a series of consecutive processing steps the first of which comprising isolation of the iris image from that of the eye image. The remaining steps are similar to the vascular network analysis 1 the iris image is enhanced with histogram equalization 2 the outline of the pigmented portion of the iris is enhanced with spatial gradient templates for edges and lines 3 the outline is segmented by binary thresholding and 4 the cusp points of the outline are isolated.

The stack isolates the iris image by intensity thresholding and binary equalization of the digitized eye image with removal of the pupil image. The intensity thresholding and binary equalization are performed by computing a threshold reference value with the histogram technique and then using a replicator array to write the reference value to a comparator array. The gray scale intensities of the digital memory cache are then compared en masse under control of the CPU . In this process zeros are written to a memory array for intensities that are equal to or less than the threshold and ones for those that are greater than the threshold.

The stack computes the image moments from the thresholded memory array. The CPU writes the moment coefficients to the elements of an accumulator array as a function of the coordinates of the element in the array and the moment orders. The accumulator array repeatedly adds en masse the thresholded gray scale intensities according to the corresponding moment coefficients and the accumulated sums are written to the memory array. The sums are accumulated as the contents of the memory array are shifted along the columns or rows . This continues as the contents are shifted along the row or column following writing the summed column or row results to the memory array. The CPU reads the array total as the value of the image moment. This action is repeated for all combinations of image moments of second order or less 0 0 1 0 0 1 2 0 1 1 and 0 2 . The elliptical central moments comprising the centroid principal and minor axes and deviations from the axes of an elliptic representation are then computed from these values. Finally the image of interest is isolated by masking the image memory array with the thresholded memory array.

In some embodiments the stack computes an intensity histogram for an isolated image by first loading a comparator array with reference intensity levels and then shifting the masked image array past the comparator while counting the occurrences of the levels. More specifically the CPU writes reference values into the columns or rows of a comparator array with a replicator array where the reference values are the same for each column or row and the values are indexed from zero to the highest intensity level. The contents of the masked memory plane are then shifted along the rows or columns of the array in a toroidal manner while a counter array counts the comparator s equal conditions. The counted values in the counter array are transferred to a memory array and then summed in an accumulator as the memory array values are shifted along the columns or rows . Finally the CPU reads the histogram distribution from the bottom row or column of the accumulator.

The stack can enhance the intensity contrast of the isolated image with histogram equalization. The CPU computes a histogram equalization mapping from the intensity histogram. Reference values are written into the columns or rows of a comparator array with a replicator array where the reference values are the same for each column or row and the values are indexed from zero to the highest intensity level. The corresponding mapping function values for the histogram equalization are written to the elements of an accumulating array. The mapping function values are then written en masse to an enhanced memory array for the comparator s equal condition as the contents of the masked memory array and the enhanced memory array are shifted in step along the rows or columns of the array in a toroidal manner.

The stack can further enhance the line structure in the isolated image with a spatial gradient template. In this process the CPU first zeros a destination accumulator array and then performs a sequence of template matching processes on the enhanced image array. The template matching process first zeros a matching accumulator array and then writes template weight values to an accumulator array. The enhanced image array is multiplied en masse by the template weights and the products are added to the matching accumulator array. The enhanced image array is then shifted in turn left right down up and along each diagonal and the above template weighting process is repeated for each shift. The absolute values of the contents for the template matching are then added to the destination accumulator array. This template matching operation is repeated for each template in the orthogonal set of line matching templates. Finally the contents of the destination accumulator are written to a line enhanced memory array.

The stack segments the enhanced line structure e.g. either a retinal capillary network or a pigmentary outline by binary thresholding. An intensity histogram is first computed for the edge enhanced image and a selected threshold value is then written as a reference value to the elements of a comparator array. The edge enhanced image is compared en masse to the threshold value. A zero or one is written to a memory array depending upon whether the intensity of an element of the image is less than or equal to the threshold value or if it is greater than the threshold value.

The stack isolates the junction points of the line segments by counting with an accumulator array the number of neighboring pixels that are one for each one pixel in the segmented image. For the junctions of the capillary network the count is compared to a threshold of three with a comparator array a zero is written to an isolation memory array for a count that is less than the threshold and one is written for a count that is equal to or greater than the threshold. For the sphincteral cusps on the iris the count is compared to a threshold of two a one is written to an isolation memory array for a count less than the threshold and zero is written for a count that is equal to or greater than the threshold.

The CPU stores the image coordinates of the junction points of the network segments in cache memory. This is done by reading the isolation array with the CPU as the elements are shifted by rows or columns and columns or rows until all elements are read and storing the row and column addresses of those elements that are ones. Finally at completion of the image feature isolation the CPU outputs to a memory mapped area the pupil moments and the image coordinates of the retinal network segment junctions and the iris sphincteral cusps along with the image locations of the specular reflection points. This process is repeated for all sensor images collected during the source element activation cycle. It is noted that although the tasks described above in relation to are described as being performed by specific hardware these tasks could alternatively be performed by a CPU using an appropriate algorithm.

Important to the successful application of this technique is the proper parametric representation of the corneal surface . The corneal surface may be calculated as a spherical section with center in the coordinate system of the optical device holding the sources and sensors with the memory mapped image locations of the surface reflection points such as and . In an alternative embodiment an ellipsoidal fit is determined for the corneal surface with orientation axes because the corneal surface is more basilar with a slightly ellipsoidal shape than spherical. In a further alternative embodiment using geometric modeling a grid of triangular meshes of basilar segments is used to define the corneal surface.

The ray tracer then uses the computed model of the corneal surface to trace the images of the eye features isolated by the image processor back to the inner structure sources. With further reference to the ray tracer computes the traces of the refracted rays for the pupil center back from the sensors to the locations of the apparent pupil images on the corneal surface . From the locations of these apparent images the ray tracer then computes the traces of the transmitted rays back to the pupil center which lies in a plane defined by the iris and along the optical axis at a distance d from the corneal center . These back traced rays identified by traces are the reverse of the light rays refracted by the surface of the corneal surface . The true rays originate from the inner structure of the eye and are transmitted to the surface points e.g. points where they are refracted toward the sensors 

The distance d is determined during the initial calibration for the pupil and conceptually defines a feature sphere . The feature sphere is a mathematical construction that is centered at the center of the cornea idealized itself as a sphere and has the location of the inner structure feature on its surface. Thus a ray traced back for a single sensor will intersect the surface of the feature sphere at the location of the feature itself. The location of the ray intersection is calculated from the computed radius of the corneal model and that of the feature sphere the radius of which is determined through the calibration process. The pupil center lies on the sphere at the point at which the back traced transmitted rays intersect the sphere. Since the pupil is positioned in front of the corneal center a correct solution for the ray tracing occurs with a negative value for the vector dot product formed from the directional cosines of the back traced transmitted rays and the outer directed surface normal from the corneal center to the sphere at the ray intersection point.

This technique of opthalmometric ray tracing may further be applied to determine the positions of the capillary junctions of the retinal fundus and the cusps of the sphincteral pattern on the iris. In such a case the ray tracer computes the locations of these structural features using the memory mapped eye image locations from the image processor . The light rays from the features are transmitted through the eye and refracted by the cornea to reach the image plane of the sensor for processing by the accumulator of the image processor . The locations of the refraction points are computed from the image coordinates of the feature the locations and orientations of the corresponding sensors and the geometry of the corneal surface. The directional cosines for the transmitted rays are computed from the corneal index of refraction and Snell s law relating the angle of incidence to the angle of refraction. In turn the location of each feature is computed from the intersection of the back traced transmitted ray with the corresponding feature sphere. The location of the feature is the point at which the back traced refraction rays intersect the sphere.

Since the iris is positioned in front of the corneal center a correct solution for the ray tracing of the cusp features occurs with a negative value for the vector dot product formed from the directional cosines of the back traced ray and the outer directed surface normal from the corneal center to the cusp feature sphere at the ray intersection point.

Unlike the iris the retinal fundus lies behind the corneal center. Therefore a correct solution for the locations of the capillary junctions occurs for a positive vector dot product. The computations for the features of the retinal network include transmission through the crystalline lens and the internal vitreous body. The indexes of refraction for the aqueous humor and vitreous body are the same at 1.33 the index for the crystalline lens at 1.42 is nearly the same as that for the cornea at 1.38. The fundus capillaries are on the retina behind the crystalline lens within the eye and the corresponding images acquired by the sensors depend upon the visual accommodation distance. However the distribution of the fundus capillary junctions about the visual axis remains invariant of lens accommodation and this relation is used in the computations.

The ray tracer computes the optical origin and the optical and median axes by best fit to a three dimensional model of the computed values. These are for the corneal optical center and axes the pupil optical center and the optical locations of the key features for the capillary network of the retinal fundus and the sphincteral pattern of the iris. The pupil optical center and the optical locations of the capillary network and the sphincteral pattern can be computed using the opthalmometric ray tracing method as above. Separate sets of these values are computed for each of the stereo images and the ray tracer uses a best fit approach to match feature locations across images. In turn the averages of the locations of the matched features are used to best fit a three dimensional model for computations of the optical origin and the optical and median axes. The ray tracer computes the center and directional cosines for the optical and median axes of the eye in the coordinate system of the optical device . The roll of the eye is computed in an orthogonal set of median axes that are perpendicular to the optical axis to complete the specification of the optical orientation.

The ray tracer produces an accurate description of the optical axes and center in the display coordinate system. This is the output of the ray tracer to the digital processor for computation of the visual axis. However the visual axis extends from the first Gaussian nodal point of the eye normally near the center of curvature of the corneal anterior surface and is commonly perpendicular to the cornea it is generally directed 5 degrees inwards to the optical axis and 2 to 3 degrees upward. The visual axis is defined by an origin point in the optical axes coordinate system and three directional cosines determined in an initial calibration process. These parameters defining the visual axis are transformed into coordinates for the optical device from the relations for the optical axes. The result is an accurate specification of the visual axis.

Referring back to the ray tracer computes the rays and for the pupil centroid from the apparent images at and respectively. However these rays are identified by the stereo reconstructor as corresponding to the pupil centroid and the physical location of the feature is computed from the intersection of the rays and . Equivalently the ray tracer can compute the back traced rays for the inner features of the iris cusp and retinal capillary junctions as matched across images to determine the inner structure physical location of the feature from the intersection of the matched rays. Notably in extreme viewing cases in which the user is looking so far to the side of the optical device that the pupil image is acquired by only one sensor this ray tracing technique i.e. back tracing across matched images is not possible. In such cases the ray tracing technique described in relation to can be used to compute the location of the feature from the intersection of the back traced ray with the corneal centered feature sphere determined from calibration.

The ray tracer computes the origin and orientation of the optical axis and outputs this information to the digital processor which in turn calculates and outputs the origin and directional cosines of the visual axis. The advantage of this process over that described in relation to is that by using the stereo reconstructor feature location may be computed from the intersection of the multiple rays and is therefore more accurate. Since the features have been identified in the stereo reconstruction process the model fitting used to determine the optical axes has been replaced by a direct computational equation with the feature locations as predetermined variables. Furthermore stereo reconstruction improves the determination of the feature spheres in situations in which only a single sensor image can be acquired such as for lateral vision. Moreover the calibration process is improved since a more accurate determination of the radii of the feature spheres can be made after the inner structure features have been located using the stereo reconstruction rather than as part of the calibration process.

Irrespective of the particular configuration of the tracking system used to determine the optical and visual axes data obtained from the tracking system can be further processed to make further determinations. For example if a separate tracking system is used for each eye of the user the binocular convergence point of visual gaze may be determined. To accomplish this the visual axes are used as inputs and their closest point of intersection is computed to determine the convergence point.

Furthermore as is described in U.S. Pat. No. 6 120 461 the output of the tracking system can be input into a separate digital computer or equivalent task specific digital processor for control of display cues and other task relevant elements. In such a case the digital computer can use the various visual parameters provided by the tracking system to compute the visual gaze point with regard to task workspace coordinates from an embedded knowledge of the workspace geometry and the head location and orientation in that space determined from a head position and orientation sensor. The computer can use those determined gaze points to determine control signals for the task relevant elements including a display driver with output of display cue parameters. In such an implementation an initial calibration process is performed under control of the digital computer to determine the relation between the optical parameters derived by the apparatus and the visual lines of sight. In one embodiment these parameters comprise the image locations of the source specular points and the apparent pupil centers and elliptical axes isolated for the images. In other embodiments the parameters comprise the origins and directional cosines of the optical axes. In the calibration process the digital computer receives as input the optical features e.g. the origins and directional cosines of the optical axes of sight in lieu of that for the visual lines of sight as the user looks at visual cues presented under computer control. The computer determines the relation between these features and the visual lines of sight and the relating values are stored in the digital processor of the eye tracking system by the computer. This result may be in the form of an artificial neural network relating the optical features such as the source specular points and the apparent pupil centers and elliptical axes or the origin and directional cosines of the optical line of sight to the location and direction of the visual axis of the eye for each apparatus of the invention.

In the image acquisition process multiple light sources can be activated one at a time in a sequential cycle by setting a source count to zero and then incrementing the count to activate the source. During the activation of a source the images acquired by the sensors are accumulated and in some embodiments processed one at a time by setting the sensor image count and then incrementing the count until all images have been accumulated. In such an image accumulation process the sensor images are first threshold limited and then masked to remove the source specular points the image coordinates of which can be stored for reference. The cycle is continued until all sources have been activated. At that time the accumulated images can be output for the source activation cycle.

Referring next to block the system then isolates internal features of the eye from the accumulated images. In this process the images can be accessed one at a time by first setting an image count to zero and then incrementing the count in similar manner to that described above. By way of example the apparent pupil and its centroid and elliptical axes can first be isolated followed by iris and retinal fundus images. Next the image features comprising the apparent iris cusps and the retinal capillary junctions can then be isolated and finally the image coordinates of these image features can be abstracted and stored. This process is repeated for all accumulated images and the image coordinates of the apparent features can be output in a set of files indexed by image.

With reference to decision block flow from this point depends upon whether stereo matching is to be performed. This determination can depend upon the system design. Specifically whether stereo matching is or is not performed can depend upon whether the system includes or does not include the stereo reconstructor . Alternatively or additionally this determination can depend upon the image data that is acquired in block . Specifically even in embodiments in which the system comprises the stereo reconstructor stereo matching may not be possible if the user s gaze is so extreme that the pupil image is only acquired by one sensor. In such a case no internal features of the eye can be matched across images from separate sensors.

If stereo matching is not performed either due to the system design or lack of sufficient image data flow continues down to decision block described below. If stereo matching is performed however the isolated features across the stereo images are matched as indicated in block . In this process the image files can be accessed one at a time first by setting an image count to zero and then incrementing the count to access the corresponding file. The features in the file are then located relative to the pupil image centroid and elliptical axes for that file. This process is repeated until all image files are processed. The apparent features across the images can be matched by a closest fit of the relative locations. At this point a file of apparent features annotated by matching identifiers can be output.

Referring next to block ray tracing is performed to locate inner features of the eye by computing feature locations from intersections of stereo matched rays see discussion of . Important to this process is the input of the list of source specular points with image coordinates indexed by source and sensor and the files of image apparent features with image coordinates and matching identifiers described above. The system i.e. ray tracer first parameterizes the corneal anterior surface from the coordinates of the source specular points and the locations of the sources and sensors in the optical device . The system then locates the inner structure within the eye for the apparent features. In some embodiments the features are processed one at a time first by setting a feature count to zero and then incrementing the count to access the images coordinates of the feature in all images. For each image the refracted ray from the apparent feature is back traced to the inner structure of the eye.

With reference back to decision block if stereo matching is not performed again due to system design or lack of image data flow continues to decision block at which it is determined whether ray tracing is to be performed. This determination can depend upon the system design. Specifically whether ray tracing is or is not performed can depend upon whether the system includes or does not include the ray tracer or . If no ray tracing is to be performed flow continues to block described below. If on the other hand ray tracing is to be performed flow continues to block at which the feature locations are computed from the intersections of the back traced ray with the corresponding feature spheres separately for all images. The location of the feature is the average of the locations with the closest fit across images. This process is repeated for all features and the inner structure coordinates of the features can then be output.

If no feature matching is performed decision block and no ray tracing is performed decision block flow continues to block at which the image locations of the various apparent features isolated by the system block are tabulated.

Referring now to block once ray tracing has been performed by matching rays block or feature spheres block or if no ray tracing was to be performed block the gaze direction is next computed. The manner in which this process is performed depends upon whether ray tracing was performed. If so the inner feature coordinates are used to compute the origin and directional cosines of the optical axes and in turn compute the visual axis and gaze direction from relations derived in calibration and the gaze history. If on the other hand no such ray tracing is performed the gaze direction is estimated using the isolated apparent image features.

The feature set used in the gaze computations can in some embodiments be automatically and dynamically selected depending upon the quantity of the features successfully isolated. For example a lateral gaze by the user so far to the side that the pupil and inner structure falls outside of the acquisition range of the sensors can force the system to depend upon apparent image features. A more centered gaze will place more of the features within acquisition range of a sensor and the ray tracing coupled with the feature spheres can provide the feature set. Finally a forward gaze will place the full set of eye features within acquisition by the stereo sensors and ray tracing coupled with matching rays can be used to provide the feature set. Of course the range of flexibility will depend upon the optical design for the instrument.

As can be appreciated from the foregoing disclosure the described systems and methods enable computation of gaze with increased accuracy. The increased accuracy is due to an improvement in the opthalmometric determination of the locations of the features of the inner structures of the eye including the pupil center and the pigmentary cusp patterns on the sphincteral muscles of the iris. The improved determination is possible because the structural features are located by the intersections of the multiple ray tracings from the opposing stereo images a procedure which replaces the less accurate determination of a single back traced ray intersection with a corneal centered feature sphere derived in calibration for the single sensor design the locations of the apparent images of the structure features used in the ray tracing are determined by the intercepts of the image projections with the corneal surface a procedure replacing the less accurate derivation of the apparent image location by interpolating among multiple source corneal reflections and direct computational equation with the feature locations as predetermined variables replaces the model fitting used previously to determine the optical axes.

The aforementioned advantages are made possible by the use of a stereo reconstruction procedure using multiple light sensors with which the apparent images of the features are correlated across images. The accuracy of the calibration process is improved for the determination of the corneal centered feature spheres which are used in gaze computations when only a single sensor image can be acquired such as for lateral vision. Because multiple light sensors are used in the procedure relatively few strategically placed light sources may be used instead of an extensive array of light sources as in prior systems and methods. Moreover multiple sensor designs enable a wider coverage of the eye movement including extreme lateral angles whereas present commercial designs are often limited in the extent of coverage to a central field of view. In contrast to existing designs the disclosed system allows wider if not full coverage while taking advantage of the stereo image properties for increased accuracy.

Various computer logic has been described above. This logic can be stored on any computer readable medium for use by or in connection with any computer related system or method. In the context of this disclosure a computer readable medium is an electronic magnetic optical or other physical device or means that contains or stores a computer program or routine for use by or in connection with a computer related system or method. Logic can be embodied in any computer readable medium for use by or in connection with an instruction execution system apparatus or device such as a computer based system processor containing system or other system that can fetch the instructions from the instruction execution system apparatus or device and execute the instructions.


---

title: Distributed database
abstract: A technique for storing a plurality of data records in a database. Client processes link to the database to access the data records of the database. Each client process has a specified scope. The scope defines to which of the data records within the database each of the client processes has access.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07761485&OS=07761485&RS=07761485
owner: Zeugma Systems Inc.
number: 07761485
owner_city: Richmond
owner_country: CA
publication_date: 20061025
---
This disclosure relates generally to software and in particular but not exclusively relates to databases.

Since database merely indexes data buffers or records to internal keys the knowledge and complexity required to run higher level queries on database is pushed onto application developers of database client . Furthermore since the internal keys themselves are not part of the useful data stored by database client but rather independently generated values used simply for retrieving records or data buffers the internal keys consume additional memory resources within database .

In an alternative conventional database system database itself may contain knowledge of the internal representation of the data buffers or records it stores to perform its own complex queries and indexing. This alternative embodiment pushes the complexities of indexing and queries onto the database developer however does so at the expense of performance by adding a layer of abstraction between the records stored and the database clients accessing the records.

Embodiments of a system and method for distributed database are described herein. In the following description numerous specific details are set forth to provide a thorough understanding of the embodiments. One skilled in the relevant art will recognize however that the techniques described herein can be practiced without one or more of the specific details or with other methods components materials etc. In other instances well known structures materials or operations are not shown or described in detail to avoid obscuring certain aspects.

Reference throughout this specification to one embodiment or an embodiment means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Thus the appearances of the phrases in one embodiment or in an embodiment in various places throughout this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics may be combined in any suitable manner in one or more embodiments.

In the illustrated embodiment network service element is implemented using an Advanced Telecommunication and Computing Architecture ATCA chassis. Mesh interconnect may provide cross connectivity between traffic and compute modules and with the ATCA backplane. The illustrated configuration includes four compute modules and 10 traffic modules with one of the compute modules being provisioned to provide operations administration maintenance and provisioning functionality OAMP functions. As depicted by interconnection mesh each module is communicatively coupled with every other module under the control of fabric switching operations performed by each module s fabric switch. In one embodiment mesh interconnect provides a 10 Gbps connection between each pair of modules with an aggregate bandwidth of 280 Gbps.

In the illustrated embodiments network service element is implemented using a distributed architecture wherein various processor and memory resources are distributed across multiple modules. To scale a system one simply adds another module e.g. blade . The system is further enabled to dynamically allocate processor tasks and to automatically perform fail over operations in response to a module failure or the like. Furthermore under an ATCA implementation modules may be hot swapped without taking the system down thus supporting dynamic scaling. Although embodiments of the distributed database disclosed here are described in connection with ATCA architecture and mesh interconnect it should be appreciated that other hardware architecture and configurations may used and further that the software components of the distributed database may be installed and executed on other hardware systems.

HAL abstracts the underlying hardware resources to the software layers above and may include various device drivers a kernel software buffers or the like. Runtime layer is used to maintain dynamic state information for the modules of network service node which may be in a state of flux during operation. For example routing demons may execute in runtime layer to setup and tear down route changes to receive and process open shortest path first OSPF protocol packets or service other dynamic change requests coming up from HAL .

Management layer services application programming interface API calls from interface layer and translates the calls into data typically to be stored into a provisioning database or occasionally into a runtime database . The APIs are published into interface layer via a management layer API MLAPI which may provide a variety of APIs for accessing the databases. For example the MLAPI may publish five APIs into interface layer including a set API a get API a get multiple API a create API and a remove API. Management layer typically facilities the provisioning of static attributes assigned to the modules of network service node . For example static attributes may include port assignments the existence or lack thereof of a module in a slot power settings a registry of applications executing on each module and the like.

Finally interface layer provides an access layer to enable a user e.g. network administrator or other Information Technology IT technician to interface with network service element and the lower layers of layered software stack . For example the user may invoke any of the APIs published by the MLAPI using a command line interface CLI to get e.g. retrieve one or more records stored in provisioning database or runtime database create a new record remove e.g. delete an existing record therefrom or set an attribute of an object existing in lower layers of layered software stack . In other cases the interface layer may enable the user to push user data files e.g. extensible markup language XML files etc. down to the lower layers using various converters.

As mentioned interface layer enables a user to push in data files from external sources. Data files may be XML files compiled C object compiled C objects compiled C objects compiled Java objects or otherwise. As a data file is pushed down to management layer layered software stack may convert data file into a serializable object . A serializable object SO is a software object that lends itself well to serialization and which is typically a complex of linked memory structures. As SO is pushed further down to runtime layer SO may be converted into a flat structure . Flat structure typically is a fixed length contiguous memory structure which may be quickly and easy manipulated in memory and therefore well suited for the high speed dynamic environment of runtime layer .

Provisioning database may be used to store provisioning data for setting static or semi static attributes of network service element while runtime database may be used to store runtime data arriving on datapaths rising up from HAL . In one embodiment provisioning database may convert SO into variable length compressed flat memory structures prior to storing SO while runtime database may simply store flat structure as a fixed length uncompressed flat structure. Since runtime layer manages high speed dynamically changing events it is reasonable to tradeoff memory consumption e.g. fixed length uncompress structures in exchange for low latency high speed access to runtime database . In contrast management layer typically manages static or semi static attributes therefore compressed variable length structures are advantages even at the expense of incurring some processing overhead related to accessing variable length structures.

In one embodiment replication clients reside on each processing node and link to a corresponding shared memory region . Replication clients passively and independently inspect the data records stored within their associated shared memory region and determine whether data records are designated for replication into other shared memory regions . A designation of replication may include a designation for replication to other processing nodes within a given processing module a designation for replication to other processing modules or otherwise. In one embodiment if a replication client determines that a particular data record is designated for replication to another shared memory region the data record is passed onto the appropriate replication client for coping into that replication client s linked shared memory region .

Each ADS may link to the same or a different subset of data records within shared memory region . The subset of data records to which each ADS links delineates the scope of that ADS and its corresponding client process. symbolically illustrates how different client processes may have different scopes within distributed database . As previously mentioned distributed database may represent the collection of all data records stored in all shared memory regions and process memory regions distributed across processing modules . In the illustrated example client process CP has limited access to a subset of the data records falling within its corresponding scope S. Similarly client process CP has limited access to a subset of data records falling within its corresponding scope S. Scopes S and S may overlap such that some data records fall within the scope of both client processes CP and CP. Correspondingly there may be other data records that fall outside of the scopes of both client processes CP and CP. In one embodiment data records that fall outside of scope of client processes CP and CP are not only inaccessible to client processes CP and CP but hidden therefrom as well.

Returning to each ADS may use a different type of abstract data structure to organize reference and access data records falling within its scope. ADS may also organize and link to local data records e.g. data record maintained in process memory regions . For example ADS A represents a hierarchical tree structure. The hierarchical tree structure includes a collection of organized nodes e.g. nodes N N each including a pointer or key s for identifying and accessing its corresponding data records or . ADS B represents a hash table structure. The hash table structure includes hash values indexed to nodes e.g. nodes N N which in turn include pointers or keys for identifying and accessing corresponding data records or . ADS C represents a link list structure. The link list structure includes linked nodes e.g. nodes N N each including a pointer or key s for identifying and accessing corresponding data records or .

Other known organizational or hierarchical structures may be implemented as well. The particular ADS selected for use with each client process may vary depending upon the purpose and tasks associated with each client process. By enabling each client process to select the organizational structure by which it links to data records or each client process can have its own unique perspective on the very same shared memory region . Even client processes with substantially overlapping scopes may view shared memory region in a very different manner that is uniquely tailored for its execution purpose. Furthermore although illustrates each client process CP CP and CP as including only a single ADS it should be appreciated that each client process may include multiple different ADS to provide primary and secondary access mechanisms into shared memory region .

The illustrated embodiment of record header may include a number header fields including a global identifier ID and a replication field . In one embodiment global ID is a different value for each data record maintained within distributed database which uniquely identifies a particular data record from all other data records . The illustrated embodiment of replication field contains data indicating the extent or lack thereof to which data record should be replicated throughout distributed database by replication clients . Replication field provides a mechanism to partition data records into subsets to enable selective replication throughout distributed database .

For example replication field may include a two bit field capable of encoding four replication choices. Replication field may indicate that data record is to remain local to a client process within its process memory region e.g. 00 remain local to a processing node but replicated into one or more shared memories within a given processing node e.g. 01 be replicated to specified processing modules within network service element e.g. 10 or be fully replicated to all processing modules within network service element . In the event replication field designates partial replication e.g. 10 then replication field may include additional space for designating to which processing modules data record is to be replicated. In one embodiment replication is carried out independently by replication clients while the client processes set replication field to indicate their replication preference.

User data is the payload portion of data record for storing the useful data itself. User data may contain one or more subfields e.g. F to FN which include declared variables with set field values e.g. variable Sally s phone number field value 1 604 247 XXXX .

The illustrated embodiment of pointer fields include reserved fields at the bottom of base portion for storing pointers or links to one or more add on portions . Unused fields within pointer fields may be initialized to a NULL value and subsequently replaced with a pointer to a new add on portion as they are added. Pointer fields provide a mechanism to extend the amount of user data that may be stored in a single data record while at the same time using fixed size memory structures. By fixing the size of base portion and each add on portion these memory structures can be quickly and easily manipulated and copied. Accordingly the illustrated embodiment of data record is well suited for use in runtime database where speed is premium. However since data record is extendable all be it in increments equal to each add on portion a minimum useful size may be selected for base portion for efficient memory use. In this manner the extendable format of data record is also well suited for use in provisioning database where memory is a premium.

Returning to not only may each client process maintain its own organizational structure for data records but it may also reference data records using different keys. As previously mentioned each node e.g. nodes N includes a key s for accessing its associated data record in shared memory region . In one embodiment this key may include either global ID or one or more field values associated with subfields F FN. Accordingly client process CP may link to one of data records by referencing field values associated with subfields F and F while client process CP links to the same data record by referencing field values associated with subfields F and FN while client process CP links to the same data record by referencing global ID . Of course in order to uniquely identify a particular data record within shared memory region the combination of subfields referenced within user data should uniquely identify that data record from all other data records. As mentioned above each client process CP CP or CP may use multiple ADS for accessing shared memory region . In one embodiment one or more of the client processes may include a primary ADS that references data records with one set of field values and a secondary ADS that references data records with another set of field values.

The order in which some or all of the process blocks appear in each process should not be deemed limiting. Rather one of ordinary skill in the art having the benefit of the present disclosure will understand that some of the process blocks may be executed in a variety of orders not illustrated. In a process block client process CP modifies data record A in shared memory region . After modifying data record A client process CP updates event ordered list process block . In one embodiment event ordered list is a link list data structure maintained in shared memory region to track the chronological order in which each data records are modified within shared memory region . Each time a particular data record is updated its corresponding version element is bumped to the head of event ordered list and a version number associated with the version element assumes a value of the current version number incremented by one. If the same data record is modified multiple times in a row then its version element remains at the head of event ordered list but its version number is incremented by one. Of course version numbers may wrap back to zero after reaching a maximum value. Accordingly the version element at the head of event ordered list corresponds to data record A since data record A is the most recently changed record.

In a process block an update agent inspects event ordered list and determines whether any changes have occurred to data records within shared memory region since its last inspection. Update agent is an entity that resides within process memory region A of client process CP. Update agent may inspect event ordered list on a periodic basis or in response to a prompt from client process CP. It should be appreciated that each client process may include its own update agent to maintain coherency with shared memory region .

In one embodiment update agent determines that a change has occurred since its last inspection by comparing the highest version number within event ordered list against its shared version number . Update agent maintains shared version number as a record of the last version number up to which it has inspected. If no changes have occurred since the last inspection decision block then update agent does nothing process block . However if the highest version number within event ordered list is greater than shared version number then process continues to a process block .

In process block update agent bumps version numbers within version data structure . Version data structure maintains version information for client process CP. Version data structure is a tree structure of nodes where each node corresponds to a node within ADS A. When update agent bumps version numbers it only bumps the version number of the node associated with data record A and then adjusts the MAX numbers of above nodes to reflect this new version number . For example if data record A corresponds to node then version number of node would be increased to 10 and the MAX number of node would also be increased to 10 as well.

In a process block a version walk agent walks version data structure to determine if any changes have occurred. In one embodiment version walk agent is maintained in process memory region A of client process CP. Version walk agent determines whether any changes have occurred decision block by walking version data structure and comparing version numbers against a local version number . If local version number falls within the MIN MAX range of a branch within version data structure then version walk agent will walk that branch to determine which node has a version number greater than local version number . Once the updated node is identified version walk agent executes a callback process block to notify client process CP to re execute appropriate code to reflect the changed data record A. Finally in a process block version walk agent increments local version number . Returning to decision block if version walk agent determines no changes have occurred then no action is taken in a process block .

It should be appreciated that update agent and version walk agent may be implemented as independent execution threads within client process CP. Therefore their operations may be executed in parallel along with operation of client process CP.

In one embodiment database maintains a directory structure e.g. database tree structure to organize internal links to all data records within shared memory region . The directory structure may include a database directory node e.g. database directory node associated with each data record which maintains a pointer to its associated data record as well as other internal data. Accordingly updating pointer may include updating database directory node .

In one embodiment if the write is to replace or modify an existing data record the database directory will have a pre existing pointer referencing the existing data record . Once the client process has written into chunk database directory node is updated with the new pointer referencing chunk and the memory consumed by existing data record is released and reclaimed.

In one embodiment if the client process is inserting a new data record a buffer or memory chunk may be allocated by the client process populated and then passed into database . This provides for a zero copy insertion since insertion is effected by updating pointers and a one copy write when the allocated memory chunk is populated.

In one embodiment database may implement a one copy read as follows. First the database allocates a chunk of shared memory region and copies an original data record into the allocated chunk to create a copied data record. At the same time database places a lock on the original data record so it cannot be modified. Next database passes a pointer to the copied data record. The one copy read protects the original data record from the client process since the client process only receives access to the copied data record.

DB interface links to the various other subcomponents of software architecture and operates as the entry point for a client process to gain access into distributed database . Shared memory object enables one or more client processes to use the resources of shared memory region . Shared memory object assigns and or frees chunks of shared memory to the requesting client process and may perform miscellaneous memory management tasks. Shared memory object obtains these chunks of shared memory from shared chunk object which pre allocated fixed sized chunks for future use. Since multiple concurrent accesses to shared memory object could result in memory thrashing shared chunk object pre allocates fixed sized chunks of memory for better performance.

DB tree object links to a tree structure maintained in shared memory region . This tree structure is used to internally organize data records keyed to global IDs . Nodes are added and removed to this tree structure as client processes add and remove data records from shared memory region .

DB delta object is the object in process memory region that provides access to event ordered list maintained in shared memory region . DB delta object performs updates and applies changes to process memory region .

DB master object is in charge of administering the local shared memory region of distributed database cleaning up event ordered list and freeing any resources when a process client disconnects. Each client process instantiates its own DB master object however only one such object operates as the master at a given time.

DB client object maintains client information for every client process that links to shared memory region . This client information includes the current version number up to which each client process has read and updated itself. If the instant client process is the master and therefore its DB master object is active then DB master object will refer to DB client object to determine up to what version number all client process have read and therefore up to which version number DB master object can perform cleanup duties.

ADT interface performs the actual updates on the local ADS performs read write remove operations on ADS and bumps version numbers on version data structure . In one embodiment ADT interface includes the functionality of update agent . DB update object is a thread class that is in charge of reading event ordered list via invoking DB delta object determines the current shared version number of the client process and calls DB delta object to update by passing ADT interface into DB delta object . One of ADT tree ADT hash or ADT link list may be linked to ADT interface at a time and represent ADS A B or C respectively. ADT walker object performs the version walk described above in connection with version walk agent . It should be appreciated that software architecture is only one possible embodiment of a software architecture capable of implementing the functionality described above.

In accordance with architecture aspects of some embodiments the aforementioned functions may be facilitated by various processing and storage resources hosted by associated line cards and the like which are mounted in a common chassis. As shown in from a datapath perspective the hardware architecture of one embodiment of network service node can be decomposed into three entities traffic modules TM compute modules CM and the chassis . In one embodiment a CM can be further re defined to be an OAMP module based on its slot index within chassis . OAMP modules are a functional superset of CMs adding operations administration maintenance and provisioning functionality collectively referred to as OAMP card function or OAMP CF .

As illustrated in the embodiments herein chassis comprises an Advanced Telecommunication and Computing Architecture ATCA or AdvancedTCA chassis. The ATCA Chassis provides physical connectivity between the modules via a passive backplane including a full mesh interconnect . It is noted that the ATCA environment depicted herein is merely illustrative of one modular board environment in which the principles and teachings of the embodiments of the invention described herein may be applied. In general similar configurations may be deployed for other standardized and proprietary board environments including but not limited to blade server environments.

The ATCA 3.0 base specification approved Dec. 30 2002 which is being carried out by the PCI Industrial Computer Manufacturers Group PICMG defines the physical and electrical characteristics of an off the shelf modular chassis based on switch fabric connections between hot swappable modules or blades. As used herein the terms board blade and card are interchangeable. This specification defines the frame rack and shelf chassis form factors core backplane fabric connectivity power cooling management interfaces and the electromechanical specification of the ATCA compliant boards.

An exemplary architecture for a compute module is shown in . In one embodiment a single compute module physical architecture is employed for both Compute Blades and OAMP CF s. More particularly under architecture a corresponding module may be deployed to support both Compute and OAMP functionality.

Compute module employs four multiple processor compute nodes . In general each of compute nodes functions as multiple processor resources with each processor resource being associated with a logical processor. Accordingly such processor resources may be implemented using separate processors or processor chips employing multiple processor cores. For example in the illustrated embodiment of each of compute nodes is implemented via an associated symmetric multi core processor. Exemplary multi core processors that may be implemented include but are not limited to Broadcom and devices. Each of the compute nodes is enabled to communicate with other compute nodes via an appropriate interface e.g. bus or serial based interfaces . For the Broadcom and devices this interface comprises a Hyper Transport HT interface. Other native standard or proprietary interfaces between processors may also be employed.

As further depicted in architecture each compute nodes is allocated various memory resources including respective RAM . Under various implementations each of compute nodes may also be allocated an external cache or may provide one or more levels of cache on chip. In one embodiment the RAM comprises ECC Error Correction Code RAM. In one embodiment each compute node employs a NUMA Non Uniform Memory Access cache coherency scheme. Other cache coherency schemes such as MESI Modified Exclusive Shared Invalidated may also be implemented for other embodiments.

Each Compute module includes a means for interfacing with ATCA mesh interconnect . In the illustrated embodiment of this is facilitated by a Backplane Fabric Switch . Meanwhile a field programmable gate array FPGA containing appropriate programmed logic is used as an intermediary component to enable each of compute nodes to access backplane fabric switch using native interfaces for each of the compute nodes and the fabric switch. In the illustrated embodiment the interface between each of compute nodes and the FPGA comprises an SPI System Packet Interface 4.2 interface while the interface between the FPGA and backplane fabric switch comprises a Broadcom HiGig interface. It is noted that these interfaces are merely exemplary and that other interface may be employed depending on the native interfaces of the various blade components.

In addition to local RAM e.g. RAM the compute node associated with the OAMP function depicted in as Compute Node is provided with local SRAM and a non volatile store depicted as Compact flash . The non volatile store is used to store persistent data used for the OAMP function such as provisioning information and logs. In Compute modules that do not support the OAMP function each compute node is provided with local RAM and a local cache.

In the embodiment illustrated in compute module is provisioned as an OAMP blade. In one configuration as shown one of the compute nodes is employed for performing OAMP functions e.g. compute node while the other three compute nodes e.g. compute nodes perform normal compute functions associated with compute blades as described in further detail below. When a compute module is provisioned as a compute blade each of compute nodes is available for performing the compute functions described herein.

PHY block and Ethernet MAC block respectively perform layer Physical and layer Data Link functions which are well known in the art. In general the PHY and Ethernet MAC functions may be implemented in hardware via separate components or a single component or may be implemented in a combination of hardware and software via an embedded processor or the like.

One of the operations performed by a traffic module is packet identification classification. As discussed above a multi level classification hierarchy scheme is implemented for this purpose. Typically a first level of classification such as a 5 Tuple signature classification scheme is performed by the traffic blade s NPU . Additional classification operations in the classification hierarchy may be required to fully classify a packet e.g. identify an application flow type . In general these higher level classification operations may be performed by the traffic blade s host processor and or a processor on a compute blade depending on the particular classification.

NPU includes various interfaces for communicating with other board components. These include an Ethernet MAC interface a memory controller not shown to access RAM Ethernet and PCI interfaces to communicate with host processor and an XGMII interface. SERDES interface provides the interface between XGMII interface signals and HiGig signals thus enabling NPU to communicate with backplane fabric switch . NPU may also provide additional interfaces to interface with other components such as an SRAM Static Random Access Memory interface unit to interface with off chip SRAM both not shown .

Similarly host processor includes various interfaces for communicating with other board components. These include the aforementioned Ethernet and PCI interfaces to communicate with NPU a memory controller on chip or off chip not shown to access RAM and a pair of SPI 4.2 interfaces. FPGA is employed to as an interface between the SPI 4.2 interface signals and the HiGig interface signals.

Typically NPUs are designed for performing particular tasks in a very efficient manner. These tasks include packet forwarding and packet classification among other tasks related to packet processing. To support such functionality NPU executes corresponding NPU software . This software is shown in dashed outline to indicate that the software may be stored persist on a given traffic blade e.g. in a flash device or the like or may be downloaded from an external to the traffic blade store during initialization operations as described below. During run time execution NPU software is loaded into internal SRAM provided by NPU .

Host processor is employed for various purposes including lower level in the hierarchy packet classification gathering and correlation of flow statistics and application of traffic profiles. Host processor may also be employed for other purposes. In general host processor will comprise a general purpose processor or the like and may include one or more compute cores as illustrated in one embodiment a two core processor is used . As with NPU the functionality performed by host processor is effected via execution of corresponding software e.g. machine code and or virtual machine byte code which is depicted as host software . As before this software may already reside on a traffic blade or be loaded during blade initialization.

In one embodiment host processor is responsible for initializing and configuring NPU . Under one initialization scheme host processor performs network booting via the DHCP or BOOTP protocol. During the network boot process an operating system is loaded into RAM and is booted. The host processor then configures and initializes NPU via the PCI interface. Once initialized NPU may execute NPU software on a run time basis without the need or use of an operating system.

The processes explained above are described in terms of computer software and hardware. The techniques described may constitute machine executable instructions embodied within a machine e.g. computer readable medium that when executed by a machine will cause the machine to perform the operations described. Additionally the processes may be embodied within hardware such as an application specific integrated circuit ASIC or the like.

A machine accessible medium includes any mechanism that provides i.e. stores information in a form accessible by a machine e.g. a computer network device personal digital assistant manufacturing tool any device with a set of one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. .

The above description of illustrated embodiments of the invention including what is described in the Abstract is not intended to be exhaustive or to limit the invention to the precise forms disclosed. While specific embodiments of and examples for the invention are described herein for illustrative purposes various modifications are possible within the scope of the invention as those skilled in the relevant art will recognize.

These modifications can be made to the invention in light of the above detailed description. The terms used in the following claims should not be construed to limit the invention to the specific embodiments disclosed in the specification. Rather the scope of the invention is to be determined entirely by the following claims which are to be construed in accordance with established doctrines of claim interpretation.


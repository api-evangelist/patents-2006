---

title: Clickable snippets in audio/video search results
abstract: Search results are provided in a format that allows users to efficiently determine whether audio or video documents identified from a search query actually contain the words in the query. This is achieved by returning snippets of text around query term matches and allowing the user to play a segment of the audio signal by selecting a word in the snippet. In other embodiments, markers are placed on a timeline that represents the duration of the audio signal. Each marker represents a query term match and when selected causes the audio signal to begin to play near the temporal location represented by the marker.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07680853&OS=07680853&RS=07680853
owner: Microsoft Corporation
number: 07680853
owner_city: Redmond
owner_country: US
publication_date: 20060410
---
Currently search engines are used to search large networks such as the internet for textual documents that are relevant to a user. Typically the user types in one or more query words and the search engine accesses an inverted index to locate the network documents that contain the words.

Some search engines are available that allow the user to search for audio documents or video documents that contain audio signals. In order to perform this search function the search engine first performs speech recognition on the audio signal to form a string of words represented by the audio signal. The string of words is then indexed.

Because speech recognition is imperfect the documents identified by the search engine may not actually contain the search query terms.

The discussion above is merely provided for general background information and is not intended to be used as an aid in determining the scope of the claimed subject matter.

Search results are provided in a format that allows users to efficiently determine whether audio or video documents identified from a search query actually contain the words in the query. This is achieved by returning snippets of text around query term matches and allowing the user to play a segment of the audio signal by selecting a word in the snippet. In other embodiments markers are placed on a timeline that represents the duration of the audio signal. Each marker represents a query term match and when selected causes the audio signal to begin to play near the temporal location represented by the marker.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter. The claimed subject matter is not limited to implementations that solve any or all disadvantages noted in the background.

Embodiments are operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with various embodiments include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputers mainframe computers telephony systems distributed computing environments that include any of the above systems or devices and the like.

Embodiments may be described in the general context of computer executable instructions such as program modules being executed by a computer. Generally program modules include routines programs objects components data structures etc. that perform particular tasks or implement particular abstract data types. Some embodiments are designed to be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment program modules are located in both local and remote computer storage media including memory storage devices.

With reference to an exemplary system for implementing some embodiments includes a general purpose computing device in the form of a computer . Components of computer may include but are not limited to a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnect PCI bus also known as Mezzanine bus.

Computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer and includes both volatile and nonvolatile media removable and non removable media. By way of example and not limitation computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.

The system memory includes computer storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules that are immediately accessible to and or presently being operated on by processing unit . By way of example and not limitation illustrates operating system application programs other program modules and program data .

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example only illustrates a hard disk drive that reads from or writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from or writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from or writes to a removable nonvolatile optical disk such as a CD ROM or other optical media. Other removable non removable volatile nonvolatile computer storage media that can be used in the exemplary operating environment include but are not limited to magnetic tape cassettes flash memory cards digital versatile disks digital video tape solid state RAM solid state ROM and the like. The hard disk drive is typically connected to the system bus through a non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

The drives and their associated computer storage media discussed above and illustrated in FIG. provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing operating system application programs other program modules and program data . Note that these components can either be the same as or different from operating system application programs other program modules and program data . Operating system application programs other program modules and program data are given different numbers here to illustrate that at a minimum they are different copies.

A user may enter commands and information into the computer through input devices such as a keyboard a microphone and a pointing device such as a mouse trackball or touch pad. Other input devices not shown may include a joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . In addition to the monitor computers may also include other peripheral output devices such as speakers and printer which may be connected through an output peripheral interface .

The computer is operated in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a hand held device a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or other appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. By way of example and not limitation illustrates remote application programs as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

Memory is implemented as non volatile electronic memory such as random access memory RAM with a battery back up module not shown such that information stored in memory is not lost when the general power to mobile device is shut down. A portion of memory is preferably allocated as addressable memory for program execution while another portion of memory is preferably used for storage such as to simulate storage on a disk drive.

Memory includes an operating system application programs as well as an object store . During operation operating system is preferably executed by processor from memory . Operating system in one preferred embodiment is a WINDOWS CE brand operating system commercially available from Microsoft Corporation. Operating system is preferably designed for mobile devices and implements database features that can be utilized by applications through a set of exposed application programming interfaces and methods. The objects in object store are maintained by applications and operating system at least partially in response to calls to the exposed application programming interfaces and methods.

Communication interface represents numerous devices and technologies that allow mobile device to send and receive information. The devices include wired and wireless modems satellite receivers and broadcast tuners to name a few. Mobile device can also be directly connected to a computer to exchange data therewith. In such cases communication interface can be an infrared transceiver or a serial or parallel communication connection all of which are capable of transmitting streaming information.

Input output components include a variety of input devices such as a touch sensitive screen buttons rollers and a microphone as well as a variety of output devices including an audio generator a vibrating device and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition other input output devices may be attached to or found with mobile device .

Embodiments provide search results in a format that allows users to efficiently determine whether audio or video documents identified from a search query actually contain the words in the query. In particular embodiments allow users to quickly identify and play segments of the audio video document that are likely to include the words in the query without having to listen to the entire audio signal and without having to read through the entire text recognized from the audio signal. This is achieved by returning snippets of text around query term matches and allowing the user to play a segment of the audio signal by selecting a word in the snippet. In other embodiments markers are placed on a timeline that represents the duration of the audio signal. Each marker represents a query term match and when selected causes the audio signal to begin to play near the temporal location represented by the marker.

In step of spoken documents of are received. In general a spoken document is a collection of speech signals that are related to each other in some manner. For example speech signals that occur at a meeting speech signals associated with a lecture or speech signals associated with a multimedia document such as a movie or a multimedia presentation. To form spoken documents some embodiments separate the speech content from other content in a multimedia document. For example the speech content may be removed from a movie to separate it from the video and musical content of the movie. When the spoken document represents only the speech content of a multimedia document a mapping may be stored that links the spoken document to the multimedia document. This allows a path to the multimedia document to be returned in the search results. In other cases there is text meta data title abstract author description that comes with a given spoken document.

Spoken documents may be stored so that all of the documents can be processed at the same time or individual documents may be received and processed separately. In other embodiments each document is received in a streaming manner and is indexed without having to store the spoken document.

Each of the spoken documents is provided to a speech recognizer which uses an acoustic model and a language model to decode each spoken document into an N best recognition lattice of possible text sequences at step of . Each N best recognition lattice includes a most likely sequence of words that is associated with a path through the lattice as well as alternative sequences of words along other paths through the lattice. For at least some words in the lattice there is an alternative word or words that span the same temporal period in the audio signal.

Typically speech recognizer performs the recognition based on a sequence of feature vectors that represent the speech in the spoken documents. Under one embodiment the feature vectors used by the speech recognizer are formed by first converting an analog speech signal into digital values using an analog to digital converter. In several embodiments the analog to digital converter samples the analog signal at 16 kHz and 16 bits per sample thereby creating 32 kilobytes of speech data per second. These digital values are provided to a frame constructor which in one embodiment groups the values into 25 millisecond frames that start 10 milliseconds apart. The frames of data created by the frame constructor are provided to a feature extractor which extracts a feature from each frame.

Examples of feature extraction modules include modules for performing Linear Predictive Coding LPC LPC derived cepstrum Perceptive Linear Prediction PLP Auditory model feature extraction and Mel Frequency Cepstrum Coefficients MFCC feature extraction. Note that embodiments are not limited to these feature extraction modules and that other modules may be used within the context of disclosed embodiments.

The steps needed to form the feature vectors can be performed entirely by speech recognizer or some or all of the steps may be performed when generating spoken documents . Thus spoken documents may be stored as analog signals digital signals frames of digital signals or feature vectors.

During recognition each feature vector is applied to acoustic model which provides a probability of each of a set of phonetic units given an input feature vector. The acoustic probability of a word is determined by combining the probabilities of the phonetic units that form the word. In addition each word receives a language model score from language model that indicates the probability of a word or a sequence of words appearing in a particular language. Using the scores provided by acoustic model and language model speech recognizer is able to include a word score for each word in recognition lattice . Note that in most embodiments speech recognizer prunes unlikely word sequences from lattice based on the word scores.

At step word hypotheses merging simplifies each recognition lattice by combining multiple occurrences of the same word that cover the same time frame in the lattice. For example if two paths through the lattice both have the word Microsoft designated for the same time period these two paths are merged at the word Microsoft so that there is only one occurrence for the word Microsoft at that time period. This reduces the amount of information that must be indexed. During this merging the scores for the word hypotheses that are merged are summed to form a new score for the merged word in the lattice. This produces merged lattices .

At step an indexer forms inverted index and transcript index from merged lattices . Inverted index is formed by providing an entry for every word in merged lattices . Each entry contains the word and information identifying each occurrence of the word in merged lattices . For each occurrence this information includes a document identifier that identifies the spoken document a representation of the start and end times for the word in the audio signal and the word score associated with that occurrence of the word. Transcript index is formed by selecting the most likely path through each merged lattice based on the word scores. For each path an entry is formed for each word slot along the path. Each entry includes a document identifier a start and end time for the slot and the word that is found in the slot. Preferably transcript index is sorted by document and then start and end times so that sequences of words along the most likely path can be easily retrieved from transcript index .

In another embodiment spoken documents include closed captioning. Closed captioning is a textual representation of the audio content of the spoken documents. In many cases close captioning is only roughly aligned with the audio content in the spoken document. For instance an entire text sentence in the closed captioning will be aligned with the beginning of the audio segment that contains that sentence.

To produce a more refined alignment the closed captioning is time aligned to the spoken document by time alignment unit . Such forced alignment is well known in the art and involves using a speech recognizer to align the acoustic feature vectors with the acoustic states of the phonetic units identified in the closed captioning. The alignment that provides the highest probability for the phonetic units in the closed captioning is used as the final alignment.

Once the acoustic signal of spoken documents has been aligned to the closed captioning the start and end times of the words in the closed captioning can be used to form inverted index and transcript index . In most cases a single alignment is identified so that inverted index and transcript index are both formed from a single path instead of a lattice of paths.

After the inverted index and the transcript index have been formed step of is complete and the indices can now be used to perform searching. provides a block diagram of elements used to perform searches and display search results and the elements of are discussed below in connection with the process of .

At step of a search user interface of is generated. An example of such a search user interface is shown in and includes a query box into which a user may type a query and a search button which the user may select to submit their query. In the embodiment shown in the user interface is shown within a network browser window and is transmitted across a network such as the Internet as a page written in a markup language such as HTML or XML.

At step search query is received. At step the words in the search query are used by a compound search query formation unit to form a compound search query . Compound search query includes a set of query terms connected by various operators to indicate relationships between the query terms. A query term can have one or more words. For query terms that include more than one word the words are tied together by an operator to indicate their relationship within the query term. For example words that must be found next to each other can be grouped within quotation marks and words that must be separated by fewer than n words can be tied together by a w n operator.

In one embodiment each word in the query is placed as a separate query term connected by a logical OR operator to other query terms in compound search query . In addition each combination of word subsequences in the search query is placed as a separate search term in the compound query. In further embodiments pairs of words in the search query are connected by an operator that indicates proximity between the words to form additional query terms. For example the proximity operator may indicate that the two words are to appear within two words of each other in order for there to be a match.

At step compound query of is used with a term confidence level by a search unit to search inverted index . Term confidence level indicates the combined word score that must be assigned to a term in inverted index in order for the term to be considered a match for a search term in compound search query . Under one embodiment when search terms have multiple words each word must have a word score that exceeds the word score associated with the term confidence level. In other embodiments the word scores for such multiple word search terms are averaged and the average must exceed the word score associated with the confidence level in order for there to be a match. In further embodiments word scores for multiple word search terms are increased before being compared to the term confidence level. This increase in the word score is based on the observation that the search query provides additional language model information that is not captured in language model .

The search performed by search unit may include any known search strategies for searching an inverted index . For each occurrence of a search term in the inverted index that exceeds the word score associated with the term confidence level search unit retrieves a document id for the document in which the term is found the start and end times of the search term in the document and the word score for the search term. These values are stored with the search term as search matches .

At step the search matches are ordered by a match ordering unit to form ordered matches . Under one embodiment the matches are ordered first based on document ID and then based on temporal location within a document.

Under one embodiment the search results provided to the user include textual snippets that include words around the matching search term where all the words in the snippet are selectable links that when selected by the user initiate playback of the corresponding audio or video file from the time point in the file corresponding to the selected word. By providing such snippets embodiments allow the user to quickly verify that the spoken document actually contains the words represented in the snippet by allowing the user to quickly identify and play the segment of the audio signal that the search system has indicated contains the search term.

The formation of such snippets for the search results is shown at step in and is performed by snippet generation of . A flow diagram for forming such snippets is shown in .

In step of the ordered matches for one of the documents are selected at step . At step the first search match is selected. For search matches that are based on proximity search terms such as one word within five words of another word the words in the document between the two query words are retrieved at step . These words are retrieved using transcript index . In particular beginning with the ending time of the first query word in the search term every word in transcript index between that time period and the starting time period of the other query word in the search term is retrieved from the transcript index .

After step the method of begins to add words to the beginning of the snippet by looking for words that precede the first word of the search match along the most likely path through the recognition lattice. This is done at step by searching transcript index for the word in the document that has an end time that matches the start time of the first word in the search match. Note that the search match is based on the inverted index which is formed from the recognition lattice and the other words in the snippet are determined from the transcript index which is based on only the most likely path through the lattice. As a result the only words in the snippet that may not be from the most likely path through the lattice will be those that form part of a search match.

If there is a word that precedes the first word of the search match the word its starting and end time points and its word score are retrieved from transcript index . The retrieved word score is then compared to a threshold at step . This threshold is used to ensure that only words with a high enough word score are returned in the snippet to help reduce confusion that might otherwise occur if a large amount of erroneous words are returned in the snippet. Note that this threshold does not have to be the same as term confidence level .

If the word score of the preceding word exceeds the threshold the word is added to the snippet in step by placing it before the first word of the search match.

The current snippet and the previous snippet if any are then evaluated at step to determine if the two snippets should be merged together. If there is no previous snippet no merger is possible and the process continues at step . If there is a previous snippet the ending time point for the last word in the previous snippet is compared to the starting time point for the first word in the current snippet at step . If the two time points match the two snippets are merged together at step .

If the current snippet is not to be merged with the previous snippet the number of words added to the snippet before the first word in the search term is evaluated at step to determine if the appropriate length for the portion of the snippet before the search term has been reached. If the appropriate length has not been reached the process returns to step to select the word in the transcript index that precedes the last word added to the snippet along the most likely path through the lattice. Steps and are then repeated for the new word.

If a word score for a word from the transcript index is below the threshold at step or if the current snippet is merged with the previous snippet at step or if the number of words added to the snippet before the search term equals the desired number of words at step the process continues at step where the transcript index is searched for the first word after the last word in the search match. This search involves looking for the term in the transcript index that has a starting time that matches the ending time of the last word in the search match.

At step the word score for the identified word is compared to the threshold to determine if it exceeds the threshold or not. If the word score does not exceed the threshold the word is not added to the snippet. If the word score exceeds the threshold the word is added to the snippet at step by appending it to the end of the snippet.

At step the number of words after the last word in the search match are counted to determine if the end portion of the snippet is a desired length. If the end portion is the desired length the process returns to step and selects the next word in the transcript index. Steps and are then repeated. When a word from the transcript index has a word score that is less than the threshold at step or the desired length for the segment of the snippet after the last word in the search match is reached at step the snippet is complete and it is added to the snippet list at step .

At step the process determines if there are more search matches for the current document. If there are more search matches the next search match is selected by returning to step and the process repeats for the new search match. When there are no further search matches for the current document the process ends at step .

After the snippets have been formed at step the process of continues at step where a timeline generation unit forms timeline data that graphically represents the time points of the spoken document where search matches have been found. Specifically data is generated for positioning markers along a graphical representation of the time span of the spoken document which in one embodiment takes the form of a timeline. A process for forming such timeline data is shown in the flow diagram of .

At step of a color is assigned to each search term in the compound search query and at step the dimensions position and color of the timeline are set.

At step the duration of the spoken document is determined. At step a search match in the spoken document is selected. For the selected search match a ratio of the starting time point of the search match to the duration of the spoken document is determined to identify a position along the timeline for a marker representing the search match at step . In particular the location of the marker along the timeline is set by multiplying the ratio of the time point of the search match to the duration of the spoken document by the length of the timeline that is to be displayed.

At step the word score for the search match is used to set the height or brightness of the marker on the timeline. In particular the marker is made taller for search matches that have a higher word score and or is made brighter for search matches that have a higher word score.

After the height and or brightness of the marker has been set at step the method of determines if there are more search matches at step . If there are more search matches the next search match is selected at step and steps and are performed for the new search match. When there are no further search matches at step timeline generation unit passes timeline data to user interface generation unit at step .

At step of user interface generation unit identifies segments of the spoken document to play for each word in the snippets and each marker. The starting time point for the segment can be selected by simply selecting starting time point for the word. Alternatively the starting point for the segment can be selected so that the segment includes words before the selected word or marker. This provides the user with more context to make a better determination of whether the spoken document actually includes the selected term. The end of the segment can also be determined and in some embodiments is simply set to the end of the spoken document.

At step of user interface generation unit generates a search results user interface that includes the snippet information and the timeline data formed at steps and . Under one embodiment interface generation unit generates the user interface as a markup language page that may be transmitted across a network if necessary and displayed in an application such as a network browser.

During the generation of the user interface generation unit creates links to the segments of the spoken document identified for each word in the snippets and each marker on the timeline at step . By selecting the words or the markers the user will be able to activate the links and thereby play the segment of the spoken document.

Under one embodiment user interface generation includes a selectable item to allow the user to view less sure matches. When a view less sure matches input is received at step of the input is used to lower the term confidence level at step . The view less sure matches input can include a document ID and compound search query . Providing the compound search query within the view less sure matches input gives search unit access to the compound search query without requiring search unit to store the query. If the document ID is not provided search unit uses the lower term confidence level and the compound search query to search the inverted index and perform steps and . Because the term confidence level is lower during this search different search results will be obtained.

If the document ID is provided search unit can limit the new search to just the document represented by document ID . In such cases the snippets and timelines displayed for other documents remain unchanged and the snippets and timelines for the document represented by document ID are updated with the new search results formed with the lower term confidence level.

In one embodiment user interface generation forms a user interface so it has the appearance of the user interface of .

In the search results user interface of the search query is shown in a query box . Below the query box is the compound search query which shows each of the search terms used to perform the search separated by a logical OR .

The search results user interface of also includes two document results areas and . Document result area includes a document title document meta data which includes the date of the document snippets timeline keywords button and view less sure matches link .

Document results area includes document title metadata snippets timeline keywords button and view less sure matches link .

Within snippets and search terms are shown bolded while words that are not search terms are shown in regular font. The snippets are grouped together in a paragraph but are separated from each other by an ellipse.

Snippet in snippets is for the search term microsoft . . . research which matches when ever Microsoft is followed by research with no more than five words between the two search words. Snippet is a merged snippet in which a snippet for the search term Microsoft has been merged with a snippet for the search term Microsoft research .

Timeline has a start time and an end time and includes markers and . Each marker corresponds to the temporal location of a search term in the spoken document. Each of the markers has a different height representing a different word score for each search term match.

Keywords buttons and of generate a show keywords command as shown in . Upon receiving the show keywords command at step of the keywords that correspond to the markers on the timeline are added to the user interface at step by UI generation to form a new search results user interface . For example if keywords button is selected by the user a new search results user interface as shown in is generated in which keywords that correspond to the markers in timeline are shown. For example keyword entry corresponds to marker and keyword entry corresponds to marker . Each keyword entry includes the search terms associated with the marker such as search terms the time point in the spoken document at which the search term is found such as time point and the word score for the search term in the spoken document such as word score .

Returning to each of the words in snippets and may be selected by clicking on the term or the marker using a pointing device or by tabbing to the term or the marker and pressing enter on a keyboard. If a term or a marker is selected the selected timeline marker input or the selected term input are received by a media player as shown in and as shown in at step .

Media player uses the information in marker input or term input to locate the spoken document and the starting point within document . Based on the starting point and the document media player plays the spoken document through a media player user interface at step . An example of a media player interface is shown in as media player which includes a video display area a play button a stop button and a volume control . Those skilled in the art will recognize that the media player user interface could include additional control graphics.

By providing search results as snippets in which the user can click on individual words to begin playback of segments of the spoken document embodiments make it easier for users to validate that the spoken documents actually contain the words identified in the snippets.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.


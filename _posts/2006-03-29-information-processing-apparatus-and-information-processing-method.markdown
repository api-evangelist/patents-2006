---

title: Information processing apparatus and information processing method
abstract: A configuration for measuring the position and the orientation specific to a mixed reality is separated from an application  to operate independently, and a shared memory  or process-to-process communication is used for data communication between them. This allows to modify configuration associated with the measurement of the position and the orientation such as the type of the sensor or an alignment method without modifying or recompiling the application . This facilitates providing the mixed reality system and supporting system modification.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07558403&OS=07558403&RS=07558403
owner: Canon Kabushiki Kaisha
number: 07558403
owner_city: Tokyo
owner_country: JP
publication_date: 20060329
---
The present invention relates to an information processing apparatus and information processing method and particularly relates to an information processing apparatus and information processing method for acquiring and generating information such as a position and orientation and an image required to provide mixed reality.

Conventionally as software for realizing a system providing Mixed Reality MR mixed reality system AR toolkit MR platform or the like has been proposed. For details about AR toolkit please refer to H. Kato M. Billinghurst I. Poupyrev K. Imamoto K. Tachibana. Virtual Object Manipulation on a Table Top AR Environment. In Proceedings of ISAR 2000 Oct. 5 6 2000. For details about MR platform please refer to Uchiyama Takemoto Yamamoto Tamura MR System Koutiku Kiban MR Platform No Kaihatsu Development of MR System Construction Platform MR Platform Nihon Virtual Reality Gakkai Dai 6 Kai Taikai Ronbunshu Proceedings of 6th Annual Conference of the Virtual Reality Society of Japan pp. 457 460 September 2001.

These provide as software library programs for acquiring the position and the orientation information or the like of an observer or an object required to construct the mixed reality system. Therefore developers of the mixed reality system first incorporate the software as a library into a system having a required hardware configuration. Next by developing an application using the library the desired mixed reality application can be easily constructed.

However to use the software library disclosed in the above references it has been needed to compile it with an application incorporating it. Therefore if the type of an alignment function included in the provided library a sensor or the like is modified it has been needed to rewrite and recompile the source code of the application program in order to use the modified function.

To operate the mixed reality system processes are needed such as determining the combination of the type of the position and orientation sensor and a registration scheme calibrating a camera and the sensor in advance editing a data to be used. Since the combination of the sensor and the registration scheme has been conventionally managed by users the users have been required to understand the combination which operates correctly and it was a laborious task. Furthermore while the calibrations of the camera sensor and the like have been independently performed by their dedicated tools the order of using tools was fixed or each calibration data created by the respective tools had interdependency. For this reason with understanding each tool data relations and the situation users have been required to perform the calibrations.

In view of these problems in the prior art it is an object of the present invention to provide an information processing apparatus and information processing method for providing a mixed reality system and facilitating to accommodate system modification.

According to an aspect of the present invention there is provided an information processing method comprising an image information acquisition step of acquiring an image from an image pickup unit for picking up physical space a sensor information acquisition step of acquiring a sensor output for measuring at least one of the position and the orientation of a subject in the physical space a calculation step of calculating at least one of the position or the orientation of the subject using at least one of the image of the physical space and the sensor output an output step of supplying the image of the physical space and the at least one of the calculated position and the orientation of the subject to an external application for generating a virtual object image to be registered with the physical space image to display a superimposed image using the at least one of the position and the orientation and a presentation step of presenting a user interface causing a user to set parameters used in the calculation step wherein the presentation step determines parameter options the user can set based on a previously stored parameter information for dependency on other parameters and a possible combination with other parameters and presents a user interface according to the determination.

According to another aspect of the present invention there is provided an information processing apparatus comprising an image information acquisition unit adapted to acquire an image from an image pickup unit for picking up physical space a sensor information acquisition unit adapted to acquire a sensor output for measuring at least one of the position and the orientation of a subject in the physical space a calculation unit adapted to calculate at least one of the position and the orientation of the subject using at least one of the image of the physical space and the sensor output an output unit adapted to supply the image of the physical space and the at least one of the calculated position and the orientation of the subject to an external application for generating a virtual object image to be registered with the physical space image to display a superimposed image using the at least one of the position and the orientation and a presentation unit adapted to present a user interface causing a user to set parameters used in the calculation unit wherein the presentation unit determines parameter options the user can set based on a previously stored parameter information for dependency on other parameters and a possible combination with other parameters and presents a user interface according to the determination.

According to the present invention this configuration allows to provide a mixed reality system and facilitate to accommodate system modification.

Other features and advantages of the present invention will be apparent from the following description taken in conjunction with the accompanying drawings in which like reference characters designate the same or similar parts throughout the figures thereof.

The embodiments described below separate a position and orientation measurement portion specific to a mixed reality MR system and an application using the measured position and orientation and configure both as execute form modules. Data transfer between the modules is characterized by using a shared memory or process to process communication. This configuration eliminates the need for modifying the application program when the position and orientation measurement portion is modified and can eliminate modification of a source code and recompile required in the prior art. The term MR as used herein is intended to include Augmented Reality AR .

The mixed reality information generation device as an example of an information processing apparatus according to a first embodiment of the present invention will be described below with reference to the drawings. The term mixed reality information means measurement information required to provide the mixed reality system and includes at least information about the position and or orientation of a subject and further includes a real image and mask information as required.

In the figure an image pickup unit is constituted by an analog camera a digital camera or the like comprising an interface such as IEEE1394 or USB and outputs picked up image information . The image pickup unit is composed of two cameras such as a HMD attached to a device mounted on user s head and picks up physical space images for right and left eyes. Although not explicitly shown in the drawings and the following description it is intended that the process occurs in two channels for left and right eyes.

A sensing unit is a magnetic sensor gyro sensor acceleration sensor clinometer or the like for measuring one or both of the position and the orientation of at least one subject and outputs measurement information .

A mask information generation unit generates mask information for retrieving particular information from image information input from the image pickup unit . A position and orientation information calculation unit calculates one or both of the position and the orientation of the subject using one or both of the measurement information from the sensing unit and the image information from the image pickup unit and outputs them as position and orientation information .

A setting unit performs setting for operation initial information and the like of devices constituting the mixed reality information generation device hereinafter also referred to as an MRP Configuration . A memory retains image information from the image pickup unit mask information from the mask information generation unit and position and orientation information from the position and orientation information calculation unit . An application is an application program using mixed reality information output from the mixed reality information generation device. Although the application is not included in the mixed reality information generation device it is explicitly shown for the purpose of explanation.

In the following description the mask information generation unit and the position and orientation information calculation unit are collectively referred to as a mixed reality information generation unit or engine or MRP Engine .

Image pickup unit picks up an image according to setting information e.g. aperture shutter speed tint or the like which are set by the setting unit and outputs the picked up image as the image information to the mask information generation unit position and orientation information calculation unit and the memory . The sensing unit operates according to setting information such as the type of sensor used by the sensing unit and information required for the sensor to operate which are set by the setting unit and outputs the measurement result as the measurement information to the position and orientation information calculation unit .

The mask information generation unit receives the image information from the image pickup unit and operates according to setting information which is set by the setting unit . As an example a region which is estimated to be flesh color is extracted from the image and is output as mask information a bitmap in which the flesh color region is 1 and the others are 0 . If user s hands are included in the picked up range of the image pickup unit such mask information will be a mask extracting regions of hands.

However instead of this the mask information may be generated using edge information or depth information obtained from parallax images picked up by the image pickup unit and instead of user s hands included in the image information the mask information may be generated by extracting any object.

According to setting information which is set by the setting unit the position and orientation information calculation unit calculates the position and orientation information of the subject from the image information from the image pickup unit and the measurement information from the sensing unit and records it in the memory . Accordingly the image information and the corresponding mask information and position and orientation information are stored in the memory . The application accesses to the memory as a shared memory. Next the application acquires the position and orientation information required for generating a virtual environment CG virtual object the image information for combining with the virtual environment and the mask information and operates. The mask information is information for preventing experient s hands from being hidden by virtual objects when combined.

In the figure a display displays information of data being processed by the application program each type of message menu and the like and is constituted by a CRT Cathode Ray Tube LCD Liquid Crystal Display or the like. A CRTC as a Video RAM VRAM display controller controls screen display to the display . A keyboard and a pointing device are used to input characters point to an icon or a button or the like on GUI Graphical User Interface . A CPU is responsible for controlling entire computer device.

The ROM Read Only Memory stores programs executed by the CPU parameters and the like. A RAM Random Access Memory is used as a work area during execution of the CPU a save area during error processing or the like.

A hard disk drive HDD and a removable media drive RMD function as external storage devices. The removable media drive is a device for read write or readout of a removable recording medium. The removable media drive may be a removable HDD as well as a flexible disk drive optical disk drive magneto optical disk drive or memory card reader.

OS an application program such as a browser data library or the like is stored in one or more of the ROM the HDD and a recording medium of the RMD according to their use. These application programs include a program causing a general purpose computer to function as the mixed reality information generation device according to the present embodiment or an error processing program as described below. The application can also be executed on the computer device. In this case the shared memory can be provided as a portion of the RAM .

An expansion slot is a slot for mounting an expansion card conforming to for example PCI Peripheral Component Interconnect bus standard. Various expansion boards such as a video capture board for connecting the image pickup unit to capture image information a sound board a graphic board for connecting a HMD and GPIB board can be mounted on the expansion slot .

A network interface is an interface for connecting the computer device to a computer network. A bus consists of an address bus a data bus and a control bus and connects between each unit described above. In addition to the network interface the computer device has a serial interface such as RS 232C RS 422 USB Universal Serial Bus and IEEE1394 and a parallel interface such as IEEE1284 so that the computer device can be connected to a sensor included in the sensing unit as well as an external device such as a modem or a printer.

Such a general purpose computer device is connected to a router including dial up router over the network interface or a modem TA or the like via a serial interface as required. Using OS and driver software required to control these devices it is possible to communicate with other devices on a computer network or a public telephone network.

The mixed reality information generation unit in may be provided in the form of software for example with the CPU executing a control program or may be provided using a dedicated hardware. With the CPU displaying a GUI on the display in response to an operation of the keyboard and the pointing device and storing the content which is input and set via the GUI in a portion of the RAM the setting unit may provided. The CPU controls each unit using the stored setting values.

This system employs a simple mock up of an automobile a mock up that is made within a range that can be seen from driver s seat . A real image picked up by HMD embedded camera and an image of a instrument panel created with CG are combined and displayed on the HMD including a magnetic sensor as a sensing unit and the camera as an image pickup unit attached to an observer. This process allows to evaluate a design inside the automobile as if riding on and touching a real automobile.

A subject in the system is the HMD attached to the observer and more precisely the camera embedded in the HMD. By determining the position and the orientation of the camera creating and combining an image of a virtual object using this information the composite image in which the real image and the virtual object image are correctly aligned can be presented to the observer.

In the real image of the road which can be seen at the position corresponding to a windshield is provided by projecting a video image or the like previously recorded separately from the image pickup unit on a screen placed behind.

An image recognition unit detects position marker e.g. a region having a predetermined color or form from the image information . A position and orientation measurement unit transforms the measurement information from the sensing unit to a 4 4 matrix representing the position and the orientation in the standard coordinate system. The position and orientation correction unit corrects the measurement information according to marker information output from the image recognition unit and outputs it as the position and orientation information .

The operation is described when a magnetic sensor e.g. 3SPCACE FASTRAK from Polhemus Flock of Birds from Ascension Technology Corporation etc. is used as the sensing unit . Assume that the measured values from such a 6 degrees of freedom sensor are directly used directly as the position and the orientation of the subject in this case the camera embedded in the HMD to draw and combine a virtual object. In this case sensor error may cause the positions of the real image and a virtual object calculated based on the measured values to be substantially misaligned and the reality of the composite image may be compromised. For example if a magnetic sensor is used as the sensor for measuring the position and orientation the error is larger and the amount of misalignment is increased if metal exists near the measurement unit HMD since the measured value is affected by surrounding magnetic fields.

In order to correct such an error of the sensor measured value and increase the accuracy of the alignment the marker existing in the physical space or placed in the physical space is recognized at the image recognition unit . The position and orientation correction unit determines the corrected position and orientation measurement information using the recognized marker information.

An operation of the image recognition unit will be described where a region of a particular color assumed to be red placed in known coordinates is used as the marker.

The image recognition unit calculates a characteristic amount Iusing the following formula representing the ratio of red color of pixel color in the image picked up by the image pickup unit 2 formula 1 

If the characteristic amount Iexceeds a predetermined threshold it is determined that the pixel in question is within the marker region. This is applied to all pixels in the picked up image and detected characteristic regions are labeled respectively. Considering the center of gravity position of the labeled region as a two dimensional position of the marker the marker position x y in the observed image coordinate system is calculated. However if there is a red color object other than a marker in the picked up image or if camera noise occurs it may be accidentally determined to be a marker. Therefore the calculated marker position x y is considered as a candidate marker position and the determination of the marker is performed at the position and orientation correction unit .

First at step S the position and the orientation of the image pickup unit is acquired from the position and orientation measurement unit and a camera s viewing transformation matrix Mis determined from relative relation with the origin point on the standard coordinate system. Next at step S the two dimensional position x y of each marker in observed coordinates is determined. This calculation uses the three dimensional position of each marker on the standard coordinate system obtained by the image recognition unit an identical perspective transformation matrix of a known camera and the viewing transformation matrix Ms determined at step S. This two dimensional position x y is a predicted value obtained from transforming the three dimensional position of each marker determined from the position and the orientation of an indication unit which is the output of the sensing unit into the observed image coordinate system.

Next at step S the distance on a two dimensional plane between the candidate marker position x y detected at the image recognition unit and the predicted value x y in the observed image coordinate system from the sensing unit is calculated. It is determined that the candidate marker position nearest to each predicted value is the two dimensional marker position corresponding to the predicted value and it is set as the two dimensional position x y of the marker obtained from the marker detection.

At step S a correction matrix Ma for eliminating the misalignment of the predicted value and the detected value is calculated. This Ma is a correction matrix for correcting the output value from the sensing unit in three dimensional space so that each predicted marker position x y matches each marker position on the picked up image.

For example this correction matrix Ma is calculated as follows. First a marker on the picked up plane is identified and is assumed to be p and let p be the position of a marker on the picked up plane calculated from the output value of the sensing unit . Let v and v be a vector starting at the origin C of the observed coordinate system and ending at p and p respectively. The transformation in the observed coordinate system that is a three dimensional space such that p and p overlap on the two dimensional picked up image is performed as follows. First v and v are respectively transformed to unit vectors. Setting rotation axis as the vector u u u u determined by the outer product of the two unit vectors the output of the sensing unit is rotated and corrected by between the two vectors determined by the inner product of v and v . That is this rotation transformation will be the correction matrix Ma. The correction matrix Ma is represented as formula 2 below 

Finally at step S the correction matrix Ma determined at step S is multiplied by the camera s viewing transformation matrix Ms determined at step S to determine the corrected position and orientation Ma position and orientation information on the standard coordinate system. The relation is expressed as Formula 3 

If the marker can not be detected on the picked up image the correction process ends and the position and orientation obtained from the output of the sensing unit is the final position and orientation information . By drawing a virtual object using the position and orientation Ma the virtual object can be displayed at the intended position and the position and orientation of the subject in this case HMD can be more precisely determined than where only the measurement of the sensing unit is used.

In the foregoing description the output of the sensing unit in the present embodiment the magnetic sensor is corrected by detecting a marker. However if the marker can always be seen such an approach using both the sensor and the marker hybrid approach does not have to be used. That is the position and the orientation of a subject may be determined based on only the marker or if the output accuracy of the sensor is sufficiently high the output of the sensor may be used directly as the position and the orientation of the subject without the correction process using the marker.

As described above the position and orientation information together with the image information and the mask information is retained in the shared memory as an example of the means for communicating the mixed reality information to the application.

In the figure portions except for the application are a software module group and setting files for providing the mixed reality information generation device. As shown in the application communicates with mixed reality information generation device via an API Application Programming Interface provided by the mixed reality information generation unit MRP Engine .

The software module group and the setting files for providing the mixed reality information generation device include setting unit MRP Configuration and setting files a system configuration file a camera internal parameter file a camera external parameter file a marker file . Tools for calibrating the camera and the sensor HMD Parameter Tool Camera Parameter Tool 6 degrees of freedom sensor tool 6DOF Sensor Tool Fixed Camera Tool marker edit tools Marker Tools and the like are also included.

Setting value for each type of parameters which is input from the setting unit is stored in the system configuration file and the HMD Parameter Tool 6DOF Sensor Tool HMD Parameter Tool reference the setting values stored in the system configuration file and operate. In the marker file information of the marker absolute position such as the center of gravity position and vertices coordinates placed in the physical space and characteristic information such as color and shape to be recognized by a image processing are set by the marker edit tool . This marker information is used to identify the marker at the position and orientation information calculation unit in the MRP Engine or more particularly at the image recognition unit .

The camera internal parameters such as focal length lens distortion coefficient center of the image aspect ratio of the image and the camera external parameters rotation matrix and translation vector determining the position and the direction of the camera in the space are respectively registered and edited by the HMD Parameter Tool and the 6 degrees of freedom sensor tool . These camera parameters are used to calculate each type of matrix the position and orientation described above for the MRP Engine or the like.

From this GUI the MRP Engine can be invoked or other modules such as reference numerals in can be invoked. Particularly the invocation can be indicated by selecting at a left window a process module desired to be invoked and clicking an invocation button with a mouse.

Since multiple processes may not be able to access the image pickup unit or the sensing unit simultaneously depending on their configuration exclusion control of accesses to the image pickup unit or the sensing unit is made. In addition the setting unit can perform the following 

Although any method and format can be used for description of the setting file a general purpose XML format is used here.

In conventional MR platforms entire setting and data which is output from each module having interdependency have been managed by a user. For this reason human error such as wrong corresponding files has often caused systems to operate wrong. In the mixed reality information generation device according to the present embodiment with respect to dependency with other parameters and possible combinations with other parameters parameter information is previously stored in for example the setting file . Based on the parameter information the setting unit determines options of parameters that user can set. By displaying only user interface for setting the determined parameters efficiency of the entire setting process can be increased and human error such as a combination of wrong setting data can be reduced.

In the present embodiment without limitation the parameter information includes at least one of the following 

From this setting screen input port setting and device selection specifying capture image size and timing of lens distortion correction process and the like can be made. For lens distortion setting it can be specified whether image information passed to the application i.e. written into the shared memory in is corrected or correction is made on drawing. If distortion correction is made the application can freely process the image information. If not since the application corrects distortion and directly writes into a flame buffer used in drawing it becomes difficult for the application to freely process the image information .

Here the setting of the internal camera parameters of a pair of left and right cameras provided at the positions near user s view and the setting of the relative position relationship of the left and right cameras can be made. To start the settings a measurement button is pushed and the tool for measuring the camera parameters the Camera Parameter Tool or the HMD Parameter Tool in is started. At this time the setting unit acquires the information of the camera the image pickup unit has whether the number of cameras is one or stereo pair from for example the system configuration file and starts an appropriate tool. In this case since the cameras are stereo camera a tool for stereo camera HMD Parameter Tool is automatically invoked. If the number of camera is one the Camera Parameter Tool is invoked.

In this way an appropriate tool is invoked via the setting unit depending on the state of the system while the measured data is automatically reflected to the system configuration file . Therefore user s human error associated with management of tools and setting files can be reduced. If the camera is exactly the same as the previously used camera or the data exists which is measured under the same condition the data also can be used. Functions such as saving the measured data with its name calling up and reusing previously saved data organizing saved data are also provided through a known GUI for file saving and reading. By presetting a typical data to the configuration file as initialization the application can be executed without measurement. This can advantageously reduce the read time of setup when the system is used for the first time.

Here the state of the sensing unit connected to the MRP Engine is detected and the list of alignment methods is displayed which are operable in combination with a sensor used by the sensing unit or solely. The displayed content is associated with the type of the sensor and registered in advance on the system configuration file .

By having a user select the alignment method from the displayed candidates the range in which the user can participate can be limited and it can be prevented to select a wrong alignment method. Although the system is configured to automatically determine the type of a connected sensor and the like a user may also identify sensors to input the type of the sensor. If there is a desired alignment method instead of the type of a sensor the system may be configured to select the desired alignment method from a list of alignment methods by setting to an alignment method priority mode. Once the alignment method is selected depending on the alignment method a sensor or its candidates to be connected to the system and a screen for prompting connection to the system and its setting are displayed. Information of the type of the alignment method the type of the corresponding sensor and the like can also be registered in advance. According to the information a user can easily provide the desired system configuration setting and the like.

Corresponding to selecting the combination of the 6 degrees of freedom sensor and the marker as the alignment method from the setting screen in 6 degrees of freedom sensor and error correction by marker are displayed as setting items within camera alignment in the window . shows an example of a setting screen for an initial calibration data displayed on selecting the item of 6 degrees of freedom sensor from these items. Also it is assumed that a magnetic sensor is used as the 6 degrees of freedom sensor.

In the setting screen in a setting screen for a transformation matrix between the camera coordinate system and the magnetic sensor coordinate system Local Transform and a transformation matrix between the magnetic sensor coordinate system and the standard coordinate system World Transform is displayed.

Values of these matrices are values measured by a measurement tool invoked when a measurement button is clicked by a mouse. As is the case with other setting values a default value is set in advance. Also a measured value can be saved to reuse it later and an edit such as delete or rename can be made.

A reference numeral denote a display region of color difference space for example but not limited to CbCr of YCbCr color model . Ellipses in the region respectively indicate the color ranges of corresponding markers. Five ellipses in show the situation that dot markers of five colors are set. If the number of colors used as the marker is increases the number of the ellipses increases accordingly. The size and the position of the ellipses can be changed by a user using those user interfaces that is typically adopted in drawing applications. Therefore the user can intuitively set and recognize the region of color recognized as the marker and the relationship with other markers.

On selecting one of the ellipses in the region the marker corresponding to it is displayed in the tree in a manner that is different from the other markers. On selecting a marker in the tree the corresponding ellipse is displayed in a manner that is different from the other ellipse or only the corresponding ellipse is displayed. Displaying in this manner has an effect that the relationship between a recognized range and a marker color can be intuitively presented to a user and is easy to understand.

A through view window displays an image in which a marker is picked up by a camera. The recognition result window displays a situation that the image recognition unit extracts a marker from image information displayed in the through view window according to the information set by the marker tool. A rectangular region in the through view window and a rectangular region in the recognition result window are the same region. Only color information within the rectangular region may be displayed in the region so that color recognition range of the marker ellipse is set to surround it. This allows to set the color region more correctly.

Alternatively only a recognition result of a particular marker may be displayed on the recognition result window . Further when a marker picked up within the through view window is displayed on the recognition result window color of the marker can be modified to color having higher saturation or the like e.g. color having the highest saturation within the corresponding ellipse and displayed. This allows to present the recognition result to a user in an easy to understand way. These displays may be made with respect to a moving picture or only a scene still in a moving picture. By adjusting color recognition range of the marker in the state of static image the range can be set in more detail.

Alternatively coordinates of any one of the three vertices may be set to constant values and two dimensional coordinates in the plane in which the triangle marker exists may be specified. This allows to facilitate input of coordinates. In this case using the definition in world coordinate system of the plane defined by the vertices of the triangle marker the setting is made after transforming to three dimensional coordinates in the world coordinate system.

As described above according to the present embodiment a configuration for acquiring mixed reality information required to configure a mixed reality system is separated from a configuration utilizing the mixed reality information and the information is communicated between them using a shared memory process to process communication or the like. In this configuration if a modification occurs relating only to acquisition of the mixed reality information such as a modification of the sensor or the alignment method only the configuration for acquiring the mixed reality information has to be changed thus various aspects can be flexibly supported.

The setting unit for managing setting items required to acquire the mixed reality information is provided. This allows to present necessary and appropriate setting items to the user according to a hardware configuration and a setting that the user desires and prevent the inappropriate combination of the type of sensor and the alignment method from being set.

In the first embodiment the configuration is assumed in which a real image is passed as the image information to the application and the real image and CG are combined at the application side to display on the HMD. In the present embodiment the application performs only CG generation and passes the data to the mixed reality information generation device. At the mixed reality information generation device the real image and the CG are combined. It is apparent that the object of the present invention can be achieved in this configuration.

In the figure reference numeral is a sensing unit such as a magnetic or optical sensor reference numeral denotes an image pickup unit such as a camera reference numeral denotes a CG generation unit such as a personal computer or PDA in which applications operate. Reference numeral denotes a mixed reality information generation device and reference numeral denotes a monitor HMD for displaying the output of the mixed reality information generation device. The image pickup unit is also embedded in the HMD in the present embodiment. In such a configuration the sensing unit measures the position and the orientation of the image pickup unit and the image pickup unit acquires an image in the physical space and output it to the mixed reality information generation device . The mixed reality information generation device calculates the position and the orientation of the subject camera from the real image and the sensor information with high accuracy and outputs it to the CG generation device. An application stored in the CG generation device generates a CG image according to the position and orientation information and outputs it to the mixed reality information generation device . Further in the mixed reality information generation device mask information is generated indicating a region in which the CG image is not desired to be displayed the region in which the observer s hand is displayed and the like within the image from the image pickup unit . This mask information the image from the image pickup unit and the CG image from the CG generation unit are combined and output to the monitor .

If two pairs of the image pickup and the monitor are provided for right and left eyes the images can be displayed to the observer as stereo images and a stereo HMD can be configured. Further the number of the pair of the image pickup unit and the monitor may be three or more. Further the data that the CG generation device generates may be not only CG data but also text information.

It is apparent that the object of the present invention can also be achieved by providing a storage medium or a recording medium recording program code for software providing the functions of the embodiments described above to a system or a device and reading and executing the program code stored in the storage medium by a computer or CPU or MPU in the system or the device.

In this case the program code itself read from the storage medium would provide the functions of the embodiments described above and the storage medium storing the program code would constitute the present invention. In addition to the case where the functions of the embodiments described above are provided by executing the program code read by the computer it is apparent that the present invention also includes the case where operating system OS or the like running on the computer performs a part or all of actual processes based on instructions of the program code and the functions of the embodiments described above are provided by the processes.

Further it is apparent that the present invention also includes the case where after a program code read from the storage medium is written into a memory provided in a function expansion card inserted into the computer or a function expansion unit connected to the computer a CPU or the like provided in the function expansion card or the function expansion unit performs a part or all of actual processes based on the instruction of the program code and the functions of the embodiments described above are provided by the processes.

If the present invention is applied to the above storage medium a program code corresponding to the flow chart described above is stored in the storage medium.

As many apparently widely different embodiments of the present invention can be made without departing from the spirit and scope thereof it is to be understood that the invention is not limited to the specific embodiments thereof except as defined in the appended claims.

This application claims the benefit of Japanese Patent Application No. 2005 106793 filed on Apr. 1 2005 which is hereby incorporated by reference herein its entirety.


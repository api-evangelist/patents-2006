---

title: Synthetic full copies of data and dynamic bulk-to-brick transformation
abstract: Multi-dimensional surrogation systems and methods are provided that generate at least one up-to-date data surrogate using information of data and numerous data changes received from at least one data source, including a production system. Embodiments described further perform bulk-to-brick transformation of the up-to-date data surrogate. Brick-level data, or item data, is further indexed and stored in an application-aware manner. The stored item data is easily accessible by many data management applications for integrated item search and recovery functions, audit functions, monitoring and supervision functions, legal discovery functions, compliance functions, archival functions, backup functions, disaster recovery functions, and more. Bulk-to-brick transformation and access of the stored item data occur off of the production system, thus contributing no performance degradation to the production system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08918366&OS=08918366&RS=08918366
owner: Mimosa Systems, Inc.
number: 08918366
owner_city: San Francisco
owner_country: US
publication_date: 20061002
---
This application is a continuation in part of U.S. patent application Ser. No. 11 500 809 filed Aug. 7 2006 now U.S. Pat. No. 8 161 318 which is a continuation in part of U.S. patent application Ser. No. 11 211 056 filed Aug. 23 2005 now U.S. Pat. No. 7 778 976 which claims the benefit of U.S. Patent Application No. 60 650 556 filed Feb. 7 2005.

This application is related to the following United States Patent Applications which were each filed on Aug. 7 2006 Ser. No. 11 500 864 now U.S. Pat. No. 7 870 416 Ser. No. 11 500 805 now U.S. Pat. No. 7 657 780 Ser. No. 11 500 806 now U.S. Pat. No. 7 917 475 and Ser. No. 11 500 821 now U.S. Pat. No. 8 275 749.

The disclosure herein relates generally to data protection archival data management and information management.

Data servers host critical production data in their storage systems. The storage systems are usually required to provide a level of data availability and service availability. Data and service are usually required to be resilient to a variety of failures which could range from media failures to data center failures. Typically this requirement is addressed in part by a range of data protection schemes that may include tape based backup of all or some of the production data.

In addition there is typically a need for other servers to concurrently access this same critical production data. These applications include data protection applications site replication applications search applications discovery applications analysis applications and monitoring and supervision applications. This need has been addressed by a range of data management schemes including setting up a specialized analysis server with a replica of the critical production data. Typical data protection and management schemes have some well known limitations. For example in some cases direct access to the server could result in instability and performance affecting loads on the production servers. Other limitations are related to the serial and offline nature of traditional tape storage which makes access to backed up data time consuming and inefficient.

Regardless of the type of backup storage used some of the most significant limitations of conventional data protection and management stem from the characteristics of the dense data stored by the production system. is a block diagram of a prior art system that illustrates some these limitations. System includes a production system and a utility system. The production system includes one or more production servers and production databases storing large amounts of dense production data. Dense data is typically stored and transferred in large quantities and is usually in a hard to read format that is not amenable to manipulation by applications or entities other than the production system or applications specifically designed to interface with the production system. Dense data is also referred to as bulk data. On the other hand item data includes typically smaller data items in a variety of application formats. An example of item data is an Adobe Acrobat file or an email message but there are many other examples. Item data is also referred to as brick data.

An example of a production system includes a messaging system such as Microsoft Exchange . In the case of Exchange client applications that access Exchange servers through item interface application programming interfaces APIs include application programs also referred to as applications such as Outlook . When a user wishes to access an item such as an individual email using Outlook the protocol used includes one of a messaging application programming interface protocol MAPI protocol Post Office Protocol version 3 POP3 Internet message access protocol IMAP or IMAP4 or others. This type of access is appropriate at the item level but is extremely slow for accessing large numbers of items or performing transfer search or audit functions directed to items stored in bulk or in dense data format on the productions server.

For performing backup functions the production system includes a backup interface to the production server and database as well as backup applications. The backup applications are used by a utility system to perform a bulk backup also referred to as bulk transfer or bulk copy of the entire production database file. The transferred production database file is stored on the utility system or elsewhere but typically off of the production system as a bulk backup. In order to restore the production database file in the case of failure the backup applications are used to transfer the bulk backup to the production server.

If data at the item level is required to be accessed from a bulk backup for example to recover a particular lost email the bulk backup must be transferred to the production server or another location where the backed up production database can be accessed using the item interface APIs. This is extremely slow inefficient and error prone.

To address the problem of access to back up items the convention utility system may also perform a separate brick backup by using the protocols previously mentioned to access the bulk production data through the item interface APIs MAPI SMTP POP3 etc. . Because this is again very slow it is typically done on a very limited basis. For example only executives in an enterprise might have their messaging communications protected by brick backup. Brick backup involves accessing the production database directly using MAPI or SMTP for example to retrieve item data. This is a slow process that loads the server and may affect server performance. When an item in the brick backup is required to be accessed or restored it is accessed using the item interface APIs and protocols previously listed.

Accessing the production database separately for both bulk backups and brick backups increases the load on the production system and may negatively impact performance. Also maintaining two sets of backup data bulk and brick that are not reconciled is error prone and may not satisfy various compliance requirements. Further in conventional systems there is no mechanism for individual enterprise users to find and restore their own lost or deleted data. This increases workload for information technology IT personnel.

Conventional utility systems are not able to completely backup production data while at the same time allowing efficient access to that data in a usable item format. Some existing applications are designed specifically to perform functions such as auditing and legal discovery but these typically read data off of the production system negatively impacting its performance. Other existing applications painfully and slowly build archives of brick items extracted from the production database using MAPI or SMTP.

There is an increasingly urgent need for the ability to completely backup production data while at the same time allowing efficient access to that data in a usable item format. For example enterprises are increasingly required to preserve and provide access to production data for auditing purposes monitoring purposes legal discover purposes and other purposes.

Each publication and patent application mentioned in this specification is herein incorporated by reference in its entirety to the same extent as if each individual publication or patent application was specifically and individually indicated to be incorporated by reference.

Multi dimensional data surrogation and corresponding systems and methods are described herein. Embodiments described herein perform shadowing of production server databases including creation of synthetic fulls by retro fitting log shipping to systems including database systems enterprise database systems or other systems that do not have log shipping capabilities. For example the shadowing described herein can be used to integrate log shipping capability with non relational databases or databases of file system data.

Shadowing maintains an off host copy of up to date enterprise production data for purposes that include one or more of protection archival and analysis. Shadowing optionally leverages lower level mechanisms such as backup replication snapshots or continuous data protection CDP to construct an aggregate system and method for making near real time production data available to applications in a manner that is non disruptive to the production host while at the same time being trusted scalable and extensible. In an embodiment shadowing includes receiving a copy of original data from the production system including an initial copy of a production database. Delta data is received from the production system in multiple instances. The delta data includes information of changes to the original data. An updated version of the copy is generated and maintained by applying the delta data as the delta data is received. In an embodiment the delta data includes log files but embodiments are not so limited. The delta data includes data of an incremental difference or alternatively of a differential difference between the original data at different instances.

Embodiments of data surrogation enable a host of open ended data management applications while minimizing data movement latencies and post processing. Embodiments provide protection of data while storing the data in such a way as to be easily located and accessed. Application aware one pass protection and data management is described including production server database shadowing using log shipping for creation of synthetic full copies also referred to herein as synthetic fulls of the database and efficient dynamic transformation of the copied data from bulk form to brick form. The transformation is dynamic in that the bulk to brick process can be run on a regular ongoing basis to transform the shadowed database. The shadowing process keeps a shadowed copy of the production database up to date on a near real time basis because log files can be applied as soon as they are generated by the production system. This results in an accessible accurate archive that reflects changes to production data in near real time.

There are many advantages provided by the embodiments described herein as compared to prior systems that do not inherently include log shipping. For example when performing synthetic fulls any corruption is catalyzed right away. This is in contrast to typical systems with disc based or tape based backup. In typical system full copies of the database and incremental updates to the database in the form of log files are saved. In the case of a production server failure the log files must typically all be applied at once. If a corrupted file is encountered or anything causes the process to fail it is not possible to access either the primary production server or the back up data.

Another advantage provided by embodiments described herein is the use of less storage space. Significantly less storage space is used to store log files because in contrast to prior systems that merely store log files the log files are consumed as they are generated according to various intervals schedules events etc.

Storage of the application aware brick data is also described including facilitating easy access to the brick data in desired end user formats through friendly user interfaces for self service access including individual user access auditor access monitoring and supervision access and more. The application aware brick data is accessible in an embodiment using a pre existing messaging application such as Microsoft Outlook while custom applications can be written in other embodiments. In an embodiment the brick data transformed from production bulk data is extracted from the bulk data and stored as indexed items in an indexed item repository while associated metadata is extracted and stored in another database such as a SQL server database.

Embodiments of the invention include components of a utility system that provide complete information management for production data. Information management integrates item search and recovery functions audit functions monitoring and supervision functions legal discovery functions compliance functions archival functions backup functions and disaster recovery functions. Information management enables all of these functions and more by performing one pass protection and data management that includes transforming unstructured or semi structured data in bulk format to application aware item format and storing it in an accessible manner. The one pass protection and data management methods are performed off of the production system or host system so that no performance degradation results.

The system includes a production system and utility system. An example of a production server is any server usually referred to as an enterprise server but embodiments are not so limited. For example a Microsoft Exchange server is used as one example of a production server. The production system includes high level item interface APIs for facilitating accessing items in the production database also referred to as a database file by client applications also referred to as clients . Clients include any client device or application that provides end user access to production or enterprise servers. An example of a client is Microsoft Outlook but the embodiments described herein are not so limited. The clients communicate with the item interface APIs using appropriate protocols. In the case of a production server that is a messaging server these protocols include a messaging application programming interface protocol MAPI protocol Post Office Protocol version 3 POP3 Internet message access protocol IMAP or IMAP4 or others.

The production system further includes one or more low level backup interfaces and corresponding backup applications for allowing users to copy some or all of the data in the production database in bulk form for backup purposes or other purposes.

The system also includes a utility system. The utility system shadows production data after it is produced as described further herein. Shadowing results in a near real time accurate copy of the production database being readily available at all times. The near real time accurate copy of the database is used in various embodiments for many purposes ranging from disaster recovery in the case of complete failure of the production database to creation of an indexed object repository including easily accessible brick level item level archive of the production database.

Shadowing includes transferring the database file from the production system as well as transferring log files on a regular basis from the production system. A capture component transfers the database file either entirely or incrementally from the production system to a shadow repository of the utility system. An apply component applies the log files on a near continuous basis to keep the transferred database file up to date. An extract component to be described further below dynamically extracts items from the shadow repository. The extract component in an embodiment traverses the structure of the database file to identify and extract items and metadata. An indexing component indexes each item so that later searches of the indexed item repository are efficient and intelligent. The frequency with which the extract component operates on the shadow repository is configurable. For example the extract component is configurable to operate every 30 minutes after a production database is initially captured and logs are initially applied. The frequency could be greater or less and could be set depending on the frequency with which new log files are available from the production system for example. Alternatively the extract component can operate on an event driven basis such as whenever a new log file or a certain number of new log files are available. The result of the dynamic apply and extract processes is an easily accessible accurate archive reflecting changes to the production database in near real time. The extract component an also be invoked on demand.

In various embodiments described for convenience herein with reference to a Microsoft Exchange example the extract and indexing components include many features that optimize performance. These features include single instancing items such that identical items are not stored more than once thus conserving storage space. These features further include detection of changes to items including covert changes and detection of moved items. Embodiments described herein provide a complete solution for storing data in an application aware manner for archival replication disaster recovery etc. that also offers verifiable non repudiation verifiable chain of custody and more.

As shown in the indexed object repository is accessible to a host of data management applications through various APIs or through web access. Applications functions accessing the indexed object repository include but are not limited to individual self service to item data monitoring and supervision legal discovery compliance disaster recovery archival auditing and life cycle management.

Throughout the disclosure where a database is shown or described one or more corresponding servers are implied even if not shown or described. For example a production database implies a production server and a utility database implies a utility server. In various embodiments described herein the utility server is a near line server including the data surrogation or shadowing methods and apparatus described and claimed herein. Embodiments of the data surrogation or shadowing methods and apparatus described include products available from Mimosa Systems Inc. of Santa Clara Calif. including the NearPoint for Microsoft Exchange Server Disaster Recovery Option. Embodiments of the data surrogation or shadowing methods and apparatus include an add on module that integrates with a near line server. In an embodiment the near line server is a NearPoint server available from Mimosa Systems.

Shadowing generates shadow data that provides a relationship between the production data on the enterprise production system and the data on the utility system. The utility system stores the shadow data in a shadow database also referred to as a shadow repository. The utility system can optionally leverage near line storage to reduce costs.

In an embodiment shadowing is a method that maintains a relatively up to date copy of production enterprise data in a data surrogate which in this case includes the shadow database. This data may be optionally translated into multiple alternate formats and augmented with metadata.

The production and or utility systems can be single computers or they may be clustered replicated and or distributed systems. The production and or utility systems can be in the same data center or they can be remote. In an embodiment the primary connectivity between the production system and the utility system is through a local area network LAN a metropolitan area network MAN or a wide area network WAN . An optional storage area network SAN can be used for the data access and data movement.

As referred to herein clients and servers can be any type and or combination of processor based devices. Reference to a system and or a server in the singular tense may include multiple instances of that system or server. Couplings between various components of the system embodiments described herein can include wireless couplings wired couplings hybrid wired wireless couplings and other network coupling types as appropriate to the host system configuration. The network components and or couplings between system components can include any of a type number and or combination of networks and the corresponding network components including but not limited to a wide area network WAN local area networks LAN metropolitan area network MANs proprietary network backend network and the Internet to name a few. Use herein of terms like transport interconnect or network is inclusive of a conventional Ethernet a Storage Area Network SAN and or other type of network. The protocols may be inclusive of Transmission Control Protocol TCP Internet Protocol IP TCP IP and layered protocols Internet Small Computer System Interface SCSI iSCSI Fibre Channel InfiniBand HyperTransport HT Virtual Interface VI Remote Direct Memory Access RDMA and a range of other protocols.

A utility system includes a shadow repository as previously described. The shadow repository includes shadow data that is received from one or more of the messaging databases. A capture component obtains a copy of production data and an application or apply component keeps the shadow data up to date as further described below.

The capture component is configured to reduce disruption of production system operations. The capture component is able to capture the production data in a scalable and high performance manner securely and reliably. The data captured may be referred to variously herein as data production data the production database etc. In general the data captured is a production database file that includes one or more of application data databases storage groups mailbox data and server data.

The capture component supplies data to the shadow repository to keep the shadow copy as up to date as possible with high efficiency and low cost. The capture component can include backup snapshots replication and continuous data protection CDP methods but is not so limited. Various capture components configured for use in an embodiment are described in detail below.

The apply component is intrinsic to a data type in an embodiment. In an alternative embodiment the apply component is retrofitted to work with the particular data type. Typically enterprise applications reside on relational databases. Relatively more capable databases such as Oracle DB2 and Microsoft SQL Server offer log shipping mechanisms that facilitate direct re use for application. However relatively less capable databases and or other semi structured or unstructured data do not include log shipping capabilities. Microsoft Exchange Server is an example of an enterprise server that resides on a database that does not support log shipping. The shadowing described herein provides log shipping capability in support of the shadowing of databases and or other semi structured or unstructured data.

An extraction or extract component of an embodiment optionally transforms data formats from a relatively dense application format to a format that is directly usable by data management applications. The extract component provides high performance scalable lossless flexible and extensible data transformational capabilities. The extraction capabilities described herein are provided by systems such as the Microsoft Exchange Server. For example the Microsoft Exchange Server provides a MAPI and protocol that is relatively difficult to deploy on a remote utility or management server and generally does not meet the performance and scalability requirements of management applications.

An indexed object repository IOR includes extracted or transformed data objects in an object database and metadata related to the objects in a metadata database also referred to as a metadata repository or metabase . As used herein object denotes a data item in an application aware format. An example of an object stored in the object database is an email message body but there are many other examples.

An optional filter provides the data management applications with an API or Web Service capability for tuning or parameterizing the extract process.

An optional indexing mechanism operates on the data and metadata in the indexed object repository looking for patterns and relationships. When the indexing mechanism finds relevant information it enhances the metadata with this new information. Optionally the indexing mechanism may be guided by a data management application through the filter.

In an embodiment data management applications have API or Web Service access to the aggregate data as it is being semantically indexed. For examples the data management applications can get proactive notifications and callbacks when relevant additional data or metadata has been added to the indexed object repository. In an embodiment the utility system is actively involved in influencing guiding participating in or extending the function of the production servers. Applications that are part of the utility system can become active or passive participants in the production server workflow through positive or negative feedback loops and augmentation of the production server function to solve existing pain points or improve productivity through value additions.

The embodiment of includes a configuration with three messaging servers and one near line server. Other deployment variations are possible including a variable number of homogeneous or heterogeneous production servers and a complex near line server that may be clustered distributed part of a grid or virtualized. Although shows three messaging servers it is possible to provide equivalent services to multiple arbitrary homogeneous heterogeneous servers. Although shows a single near line server it may in actuality be clustered distributed replicated virtualized and may straddle multiple machines or sites.

Embodiments of a shadowing method are described herein with reference to an example host system. The shadowing is described in the context of providing log shipping of the application component for a Microsoft Exchange Server as an example but the shadowing described herein is not limited to the Microsoft Exchange Server.

An embodiment can optionally include cleansing and or repairing of the full copy data prior to the extraction when the capture component does not provide application consistent data. In embodiments to be further described below log files logs and are shown as an example are shipped from the production system as they are generated and are applied to the full copy to keep it up to date as a shadow copy of the production database.

The capture component of shadowing is configured to use one or more data capture capabilities that can include backup snapshots replication and or continuous data protection. is a block diagram of backup capture used in shadowing under an embodiment. The backup capture uses the backup APIs provided by the application running on the production system. In this example the production system is Microsoft Exchange Server but is not so limited. The utility system is configured to obtain occasional full backups and frequent incremental or differential backups. Both these mechanisms typically run on a default or administrator configured schedule. There are other enhancements or variations that include the ability to detect that new log files have been generated on the production system and pulling a copy over dynamic log shipping or mechanisms for tailing the log files as they are being written on the production system.

For snapshots that are crash consistent additional mechanisms can be used to validate the snapshots for consistency and perhaps repeat the process until a reasonably consistent copy is available . The additional mechanisms can cleanse and or repair the data in order to make it ready for application.

The capture of production data using replication includes use of replication techniques that capture every relevant write at the source e.g. the production system and propagate the captured writes to the target e.g. the utility system to be applied to the copy of the data to bring it up to date. This replication can be synchronous asynchronous or a quasi synchronous hybrid. The production and utility systems may be single computers or they may be clustered replicated or distributed. As in the case of snapshot capture additional mechanisms can be used to validate the snapshots for consistency and cleanse and or repair the data in order to make it ready for application.

The update of logs can optionally include an extract or bulk to brick transformation operation as further described herein. The extract operation is performed on the data resulting from the log apply operation to transform the resulting data from dense application format to one or more target formats for subsequent consumption by various data management applications.

An application communicates with the production database which in the case of Microsoft Exchange is called an Exchange database or EDB . When the application detects a change to the database it performs write ahead logging to a log file. This includes appending information to the log file which is much faster than traversing the database structure and updating the database each time there is a change. The information appended to the log file reflects the particular change made to data in the database.

A lazy writer takes all of the logged but not committed changes to the database and writes them to disc. One reason to use these log files is if the system suddenly crashes the system can replay the log files when it comes back up thus recovering all the lost data. Write ahead logging is usually used for database systems but other systems may have different ways of handling changes to data.

Another way of using log files in database systems is for creating a mirror database to provide a backup in the event of server loss or site loss. This is referred to variously as log shipping log apply or synthetic fulls. Any of these terms imply various methods that take incremental changes to a production server and apply them to a database copy on a utility server to bring the copy up to date. Log shipping is not supported by some systems including Microsoft Exchange . The inability to support log shipping introduces significant limitations on data backup operations data archiving operations and data discovery operations. For example conventionally third party applications designed to provide data backup data archiving and data discovery operations to Microsoft Exchange or other systems without log shipping capabilities go into the EDB and obtain the bulk version of the database. If such an application repeatedly obtains the bulk database without applying the log files many databases and many log files are accumulated which becomes very cumbersome. Then in order to restore data back to Exchange all of the accumulated log files must be applied to the EDB at the time of restoration. This makes the recovery time objective RTO of such conventional third party applications very long.

Performing shadowing with synthetic fulls as described herein allows the log files to be consumed as they are generated resulting in an improved RTO. In addition because a copy of the current EDB including applied log files is available extraction and transformation to brick form according to embodiments to be described becomes possible.

System further includes a utility system with a shadow repository and an IOR according to an embodiment. Initially the production database is copied from the production system to the shadow database on the utility system. In addition log files are shipped from the production system to the shadow repository as they are generated. The shadow repository in an embodiment also store STM files. STM files are files in a well known format for multi media typically emails.

In an embodiment each time a log file is generated it is received by the utility system and applied to the shadow database according to a retrofitted log shipping operation. Alternatively the log files can be batched before applying. Data in the shadow database is extracted to the indexed object repository in an application aware manner and stored in such a way as to be easily located and accessed even by data management applications external to the utility system.

In an embodiment an EDB header is made to point to a particular log file number as a starting log file number and the engine is run. The engine goes through each log file and checks for integrity for example by checking the checksums. The engine begins applying transactions from the log files into the shadow database. The engine moves through the log files in sequence applying each log file. For example log files are shown in . When the engine finishes applying the last log file log file the database enters a recovered state which indicates that the data is ready to be restored to the production database. In the recovered state no more log files can be applied to the database. This state is referred to as clean shutdown state in Microsoft Exchange . This behavior is an artifact from when tape was the dominant backup storage medium. For example if backups are stored to tape and retrieved from tape there should never be a need to apply log files more than once. Thus after a one time application of log files the EDB automatically enters a state in which no more logs can be applied. Conventionally when the production database is backed up it is transferred in backed up state which is the state in which log files can be applied. This state is referred to as dirty shutdown state in Microsoft Exchange .

According to an embodiment in order to apply log files at any time the EDB is allowed to go into clean shutdown state after the last log file for example log file . Then the EDB header is modified to indicate that it is in dirty shutdown state. When the utility system is ready to apply a new set of log files the EDB will be in dirty shutdown state and the engine will be able to apply the log files. This is referred to as toggling the dirty bit s in the appropriate header field of the EDB. The EDB and EDB header are specific to certain embodiments and are not meant to be limiting. In various embodiments other systems may use different databases in which there are headers or other structural metadata that can be manipulated to achieve the result of allowing application of log files using the database engine as described. The engine may be any recovery engine employed to recover a database including application of changes made to the database but not yet applied to the database.

If it is not the first time the process has been run an incremental copy is acquired. In order to obtain the incremental copy it is determined whether there are sufficient un applied logs present. If sufficient un applied logs are not present the process waits for sufficient logs. In one embodiment this includes going back to the initial starting point. If there are sufficient un applied logs it is determined whether the logs are in sequence. If the logs are not in sequence they cannot be applied and a full copy of the database is obtained. Alternatively the production system is accessed specifically to acquire the missing log files. Logs must be in sequence because of their nature as multiple transactions that may have interdependencies. In a manner that is analogous to the area of microprocessor instructions for example database transactions can be committed or uncommitted.

If there are sufficient log files the appropriate EDB headers are updated. In practice there are multiple EDBs so there are multiple EDB headers. The headers are updated to reference the first log file that has not been applied. The database recovery engine in this case the ESE is invoked. The engine is used to replicate the EDB by applying the log files. The replicated EDB is used for later transformation from bulk to brick also referred to as extraction as performed by the extract component according to embodiments to be later described.

The EDB headers are updated to indicate the backed up state or dirty shutdown state and the process returns to the starting point.

Whether or not log files are retained by the shadowing process and how long log files are retained depends on whether the log files include any uncommitted transactions. As previously mentioned each log file could include several transactions and several of the transaction could be outstanding. At some point there is a begin transaction and at another point there is a corresponding end transaction. When a begin transaction is encountered by the shadowing process it is bracketed. The brackets are closed when the corresponding end transaction is encountered. All of the transactions between the begin transaction and a later end transaction are saved until it is confirmed that every transaction in the bracketed chain completed successfully. If every transaction did not complete successfully all of the transactions in the chain are rolled back. Retention of the appropriate log files facilitates rollback. Accordingly the log files are accumulated and as they are applied a check is made for outstanding transactions. If there are no outstanding transactions associated with a log file the log file is deleted. If there are outstanding transactions associated with the log file the log file is saved.

A copy of the production data is received by an embodiment of the utility system. Initially a baseline copy of the entire production database file is received and stored in a shadow repository. As delta data is generated by the production system the delta data is received by the utility system. Delta data is any data that records changes made to the database file. In an embodiment the delta data is one or more log files. In an embodiment the log files are shipped to a near line server of the utility system from a remote Exchange Server. In an embodiment the frequency of log shipping is pre defined by a schedule but the frequency could be determined otherwise such as by an administrator through a data management application or the log shipping may be event driven.

The delta data is applied to the copy using the recovery engine. In systems such as Exchange that do not have log shipping capability after logs are applied the state of the database being operated on is changed to disallow the further application of log files. In an embodiment the copy is prevented from entering this state by stalling the recovery engine. When additional log files are ready to be applied the recovery engine is unstalled and the additional log files are applied.

The process illustrated in is described in relationship to Microsoft Exchange . However the process is applicable to other messaging and collaboration servers. The process is also extensible to generic applications that use structured semi structured or unstructured data. Though this example shows a production database or server it is possible to provide equivalent services to multiple homogeneous or heterogeneous databases or servers. Similarly though this example describes a single shadow database which in an embodiment includes a near line server in various embodiments the shadow database may be clustered distributed replicated virtualized and may straddle multiple machines or sites.

The extract component of an embodiment performs a raw read method and a smart extract method. The raw read method systematically reads the B trees including the data structures the data tables and the contents. Because the B tree is actually referencing a number of tables e.g. a mailbox table a folder table a message table and an attachment table it is possible to do a finite number in one case four according to the tree structure of lookups to reach leaf items. Finding items includes making branch decisions at nodes that are not leaf nodes until the leaf node is found.

The smart extract method extracts useful items and metadata from a raw structure read out by the raw read method. In an embodiment the same the same hierarchy found in the database structure is replicated in a directory structure or filing system. In an embodiment the directory structure is in the shadow repository not shown but embodiments are not so limited. In the directory structure the items for example folders mailboxes email bodies email headers email attachments etc. are placed at leaf nodes. Any application can go into the directory structure and look at the items. In an embodiment the extract component creates a directory for each Exchange server encountered and a directory for each storage group etc. down to leaf items. In this way the dense data of the database structure is exposed in a file system that can be traversed and searched by any application without performance degrading access to Exchange . Actual items leaf items may be in any format such as HTML Adobe Acrobat etc.

Optionally extracted items are placed in a composite container such as a ZIP file or a CAB file for storage.

Items extracted from the production database in an embodiment are stored in one of many storage locations according to a tiered storage scheme. A location can infer a type of storage medium and or an actual location. In an embodiment the tiered storage scheme includes pre determined criteria for placing items in tiers according to content age importance sender or receiver and many other criteria. An embodiment includes an algorithm for dynamically determining an appropriate tier for each item to be stored in.

The extract component includes decompression algorithms for decompressing items that are compressed for example by Outlook or Exchange . The extract component efficiently extracts the data. In an embodiment the first time an EDB is encountered for the bulk to brick transformation the structure is parsed and multiple extract threads run each of which takes a mailbox so that multiple mailboxes are extracted in parallel.

The directory structure that is created by the extract or bulk to brick process is traversed by the indexing component to perform full indexing. The indexing component looks for message types message content etc. The indexing component creates indices to allow full searching of items. For example a search could be performed for occurrences of the word Tabasco . As another example a search could be performed for users discussing a bug . Such searches are easily performed by the data surrogation system whereas they would be very expensive to perform using Exchange .

The indexed items are stored in an indexed object repository and the metadata associated with the indexed items is stored in a metabase. The metadata in the production database file indicates the database structure including locations of data tables are in the database file. There is metadata about the items metadata about indicate such things as which folder is in which mailbox and so on. There is also metadata about metadata. In an embodiment the metabase include a SQL server and a SQL database.

The words of the item that remain after filtering are then used to add to an index lookup table. If a word has been encountered before it is already included in the table. In this case a pointer to the new location is included in the table. If the word has not been encountered before it is not in the table so the word and its associated pointer are both added to the table. In this way an index of all of the words in the shadow database is built up.

Embodiments of the data surrogation system include various optimizations. One optimization is called delta extraction. In an embodiment the raw read process compares last modified dates at each node of the B trees. Delta extraction includes algorithms that determine last modified times of mailboxes folders messages and attachments. If the last modified time is not different from the previous last modified time the read does not proceed lower in the B tree. This greatly reduces extraction time after a production database has initially been completely extracted. For example an initial extraction might take 6 hours while subsequent extractions performed every 30 minutes might take 5 10 minutes. Delta extraction may not handle various situations such as a user moving an item from one folder to another. Other optimizations perform more detailed examinations of items to detect changes such as moves and deletions. These further optimizations are described below. In an embodiment the extract process is a two part process for increased efficiency but embodiments are not so limited. The first part includes delta extraction which runs very fast. The second part runs out of band and catches changes such as moves and deletions.

Another optimization is called single instancing or de duping. Every time a bulk to brick extract process is performed the extract component is looking at a historical view of the production database file which is constantly changing over time. However many of the items messages or attachments etc. may have not changed since the last bulk to brick process. According to an embodiment the extract component determines whether the item has already been stored on the utility system during a previous bulk to brick process. If so the item does not need to be stored again using more storage space. Instead a pointer to the existing message is stored.

Another optimization is called delta instancing according to which only changes to an item are saved rather than an entire changed item. As an example suppose a message with an attachment is sent from Peter to Roy. Roy makes changes to the attachment and sends it to Rick who sends the attachment on perhaps with or without changes etc. When the extract component operates on the database file it learns there are four instances of the attachment document. In an embodiment cryptographic checksums of the instances are compared. A checksum associated with an item changes if changes are made to an item. In an embodiment if single instancing reveals that there are four different versions of the attachment document a diff is performed to find the differences. If differences between the original version and a later version are relatively small the original version is stored and only the differences are stored for the later version.

Another consideration when detecting changes is that all of the changes should be stored efficiently for compliance purposes. Older versions should not be overwritten so that an accurate historical picture can be recreated. In an embodiment when a message is first extracted the message and attachment in total is cryptographically hashed which uniquely identifies it as a string. This checksum or hash is also called the aggregate message checksum. Later when the message is retrieved for a requesting user another checksum is generated and compared with the checksum computed at storage time to see whether they match. If they do not match the requested item has been touched in the meantime.

Returning to the point in the process when the message to be stored is originally hashed after the aggregate checksum is generated the message is decomposed into a message header message body and 0 or more attachments. All the individual items of the aggregate message are examined to determine whether they have been seen before including comparing checksums or hashes. The hashes are stored in a table that associates each hash with a storage location of an item. This constitutes a hash table mapping hashes to storage locations. In an embodiment the hash table is kept as metadata in the metabase but embodiments are not so limited.

Storing cryptographic hashes for items provides tamper detection because if a user accesses an item at all the hash will change. In addition cryptographic hashes facilitate detecting moved items. If a message has been moved from one location to another location it initially appears as a new item even though it is not. Embodiments include delta detection regarding which messages have disappeared since the last time and which have appeared since the last time. Embodiments also perform a cryptographic hash to determine if a message is really new. Embodiments also check an item s entry ID which is a MAPI property. The entry ID does not change no matter where an item is moved to or how many times it is moved.

When different messages with attachments are encountered in the extract process the attachments may be the same. When the messages are decomposed the checksums of the attachments are checked. Whenever a previously stored attachment is encountered a pointer is stored that points to the previously stored attachment.

In an embodiment a reference count for each item is also kept. Each time an attachment is encountered for example the reference count is incremented. Each time an attachment is deleted for example because a message was deleted the reference is count decremented. In an embodiment the reference count is kept as metadata in the metabase but embodiments are not so limited.

Structural metadata of the database file is enumerated. In the examples given an Exchange database structure was cited but embodiments are applicable to any database structure. The structural metadata indicates how the database is arranged timestamp information etc. Item metadata is then enumerated. Folders in the database structure are traversed to locate changed folders. This includes checking timestamps. If a folder is not changed the contents of the folder have not changed so the process does not go into that folder. Leaf items in under changed folders are enumerated and each leaf folder is diffed to determine what changes occurred since the last time the extract process was run. Items are extracted along with their metadata. The items are decomposed for example into message body message header attachment and so on. The decomposed items are stored along with the appropriate information in the hash table previously described. The process is then at an end. But the process is re run typically automatically on a regular basis such as every 30 minutes . This keeps the indexed object repository populated with very current data that is accessible as an archive a backup etc. Alternatively the extract or bulk to brick process could be run on demand by an administrator as desired.

A production system includes a Microsoft Exchange server and database. In other embodiments various production servers and databases may be present in place of Exchange . The Exchange server is presenting a logical view to a user at a user node at time T. The user node is any access device such as a personal computer PC . The user sees the logical view of Exchange through a client application such as Outlook . For the logical view the Exchange server has many mailboxes. A user can typically access only the user s own mailbox or mailboxes. The user mailbox has multiple folders which in turn can include notes calendar items messages task etc. And folder can also have other folders. The leaf nodes are items such as messages.

This logical view is in contrast to the physical view of the Exchange server which includes EDBs STMs and logs contained within multiple storage groups in a mailbox store. When data is extracted in bulk it is in the physical view whereas data extracted in brick is in the logical view. As described herein one pass protection and data management allows complete archiving in brick form without slow separate brick extraction. In addition the brick level or logical view archive is readily accessible to any data management applications.

Referring to the user node at time T an Outlook screen is displayed with a messaging view. A list of folders arranged hierarchically is on the left of the messaging view. A list of message items is on the right of the messaging view. This view is just one example of a view presented by an application. At T Outlook is talking to Exchange probably using MAPI also possibly POP3 or other messaging protocols . At T each time the user clicks on a folder they see a view that is being rendered by Exchange using MAPI.

However there is a special folder in the list on the left created by Mimosa and called the Mimosa archive this could be renamed . When the user clicks on the Mimosa folder a different protocol than MAPI is used. In an embodiment the folder invokes a web service that is talking to the Mimosa utility system. Transparently to the user there is no communication with Exchange after the Mimosa folder is clicked. This technology is referred to as folder home page .

In an embodiment the folder home page capability allows the user to access the indexed object repository using the Outlook messaging view. Clicking on the Mimosa folder invokes a web link that accesses the Mimosa utility system which includes an indexed object repository on a Mimosa NearPoint server and an SQL server and SQL database including metadata as previously described.

The user node at time T after clicking the Mimosa folder displays what looks like an Outlook messaging view but the right side information is rendered by Mimosa . For example the right side of the display could identify information to be entered for a search request. When the user makes a request whether it is a search request or a browse request the request is passed to the SQL database for identification of the user s folders attachments messages etc. using the stored metadata. The virtual view of that metadata is presented to the user so they can see their mailboxes folders attachments and so on. If the user clicks on an attachment and indicates a desire to look at it the message or attachment is found reconstituted and sent back. The user can operate on the returned item in any of the usual ways such as render a message forward the message delete the message etc.

In one embodiment clicking on the Mimosa folder renders a quick search view. In the quick search view the user can enter a search term and hit enter. The search term is rendered by the web service for example as SOAP XML HTTP etc. and analyzed by Mimosa to determine what view should be presented. Mimosa performs a database search for the items associated with the search term. The returned search results are then rendered on the right side of the Outlook screen.

What the user is looking for may be stored in any one of multiple tiers of storage. For example according to an information life cycle management function of an embodiment items are intelligently stored off line for example stored remotely on tape due to various factors including age. Depending on what the item is or who the reader is the item may be retrieved quickly or not so quickly. If the user searches for a message that is stored off line the metadata such as the message header can be quickly returned for viewing but not the message body. The user may not need to see more than the metadata in many circumstances. However if the user wants to see the off line data the user can select one or more off line messages and direct them to be retrieved. A process retrieves the requested messages from tape and returns them asynchronously. For example an hour or so after the request the user receives a .pst file with all of the requested messages.

Multiple layers of data management are made easy using embodiments described herein. For example more senior auditors may need to document the behavior of lower level auditors. Lower level auditors might do simple pattern matching. There may be many lower level auditors.

The work of the lower level auditors can be passed to a senior auditor for review. The enterprise may also need to tailor the audit results to verify that the auditors are actually doing their job. For example an auditor might be expected to look at X messages day. Being able to prove that this happened is often required for compliance with laws or regulations. Embodiments enable this and many other types of custom auditing tracking and reporting. The auditor or user is able to see their own logical view of the data. In an example of a messaging production system the logical view may be presented by Exchange but the data is archived managed and accessed by Mimosa .

The utility system further includes a framework multiple handlers and queues for example a notification queue and a task queue are shown . The utility system further includes a workflow. In an embodiment the utility system receives a request. Examples of a request include a timer being activated or a user or administrator making a request. The request manifests itself as a notification which is placed in the notification queue. The framework grabs the notification from the notification queue and looks it up in the workflow to determine how to handle the particular notification. The framework looks up the workflow and then calls the appropriate handler depending on what it learned from the workflow. The framework places the notification in the task queue. The handler takes the notification from the task queue and proceeds to handle it as appropriate.

The framework determines whether the request has been successfully handled and determines what to do next. The framework looks to the workflow to get the next notification and call the next handler and the process continues. This architecture allows hot code load . For example in an embodiment the utility system software code including the code related to the data surrogation capabilities described herein is written in the form of handlers. This is advantageous especially in the situation of a system in the field because the system can be easily updated by simply installing one or more new handlers. If there are any issues with a new handler the new handler can be discarded in favor of the handler it was meant to replace.

Many variations of the multi dimensional surrogation including bulk to brick transformation of structured semi structured and unstructured data are contemplated to be within the scope of the claimed invention. In various embodiments the bulk to brick transformation is dynamic in contrast to occurring according to a pre defined schedule.

In various embodiments different production systems with production servers and databases other than those explicitly described are operated on by the bulk to brick transformation described herein. For example any structured semi structured and unstructured data may be operated on as described herein.

Although messaging servers and databases that enable email are used to illustrate embodiments herein the embodiments described are equally application to instant messaging IM messages voiceover internet protocol VOIP messages and other forms of data. In some cases voice mail messages are already stored as email attachments in Exchange allowing the voice mail messages to be operated on as item data as described herein. Embodiments of the invention are further applicable to many other forms of data and system architectures not explicitly described herein.

In various embodiments the availability of the indexed object repository to data management applications may be to the actual data that is being modified by the process or it may be to a copy of that data or it may be some combination thereof. This may be available in the form of an API or web service or equivalent.

The components of the multi dimensional surrogation described above may include any collection of computing components and devices operating together. The components of the multi dimensional surrogation can also be components or subsystems within a larger computer system or network. Components of the multi dimensional surrogation can also be coupled among any number of components not shown for example other buses controllers memory devices and data input output I O devices in any number of combinations. Further functions of the multi dimensional surrogation can be distributed among any number combination of other processor based components.

The information management of an embodiment includes a method comprising receiving a copy of original data and delta data that includes information of changes to the original data. The method of an embodiment includes generating and maintaining an updated version of the copy by applying the delta data to the copy as the delta data is received. The method of an embodiment includes dynamically transforming the updated version from a first structure by organizing items of the data into a second structure that preserves the hierarchy of the first structure.

The applying of an embodiment is according to an interval. The interval of an embodiment is based on one or more of time and events.

The delta data of an embodiment includes data of an incremental difference between the original data at a plurality of instances.

The delta data of an embodiment includes data of a differential difference between the original data at a plurality of instances.

The method of an embodiment includes controlling the applying using modified information of a component.

The original data of an embodiment is stored at a first server and the copy is stored at a second server. The component of an embodiment is a component of the second server.

The second server of an embodiment includes one or more of local servers remote servers database servers messaging servers electronic mail servers instant messaging servers voice over Internet Protocol servers collaboration servers messaging and collaboration servers Exchange Servers portals customer relationship management CRM servers enterprise resource planning ERP servers business to business servers and content management servers.

Modifying of an embodiment comprises detecting a first state of the copy. The first state of an embodiment indicates the delta data has been applied to the copy.

Modifying of an embodiment comprises changing the first state to a second state. The second state of an embodiment is a state from which another updated version can be generated by applying additional delta data to the updated version.

Changing the first state to the second state of an embodiment includes modifying the structural metadata of the copy.

The delta data of an embodiment is a plurality of log files. The component of an embodiment is a log file of the plurality of log files.

The applying of an embodiment includes invoking an engine of a first server hosting the original data and the controlling includes stalling the engine.

The method of an embodiment includes determining the first structure. The first structure of an embodiment is a proprietary structure. The method of an embodiment includes parsing items of the data using information of the first structure. The parsing of an embodiment includes systematically reading information of the data. The information of an embodiment includes one or more of tables and items. The tables of an embodiment include one or more of a mailbox table a folder table a message table and an attachment table. The items of an embodiment include one or more of a folder a mailbox an electronic mail body an electronic mail header and an electronic mail attachment.

The organizing of an embodiment includes traversing the first structure and locating information of the items.

The indexing of an embodiment comprises generating indices corresponding to the items. The indexing of an embodiment comprises associating the items with the indices using the information of the items.

The method of an embodiment includes comparing modification information of each item with modification information of a previous instance of the item. The method of an embodiment includes ceasing the traversing at a current node of the first structure when the modification information shows the item is unmodified since the previous instance.

The method of an embodiment includes determining whether an item is a previously encountered item that has previously been stored in the second structure. The method of an embodiment includes generating a pointer to the previously encountered item.

The method of an embodiment includes comparing modification information of each item with modification information of a previous instance of the item. The method of an embodiment includes indexing difference information of the item when the modification information shows the item is modified since the previous instance.

The difference information of an embodiment includes information of changes between a current version and at least one previously modified version of the item.

The organizing of an embodiment comprises hashing information of each instance of the item by generating a checksum.

The method of an embodiment includes determining if the item has been altered since the organizing by comparing checksums of different instances of the item in the second structure. The determining of an embodiment is performed in response to a request to retrieve the item from the second structure.

The indexing of an embodiment includes transferring metadata of the items into a metadata repository.

The method of an embodiment includes providing access to at least a portion of the items in the second structure via a plurality of applications.

Providing access of an embodiment includes providing access using a second protocol that is different from a first protocol used to access the data of the first structure.

The method of an embodiment includes transferring the updated version to an indexed object repository.

The generating of an embodiment is in near real time and maintains complete integrity and consistency of the original data.

The original data of an embodiment includes one or more of application data databases storage groups mailbox data and server data.

The method of an embodiment includes maintaining the updated version. The maintaining of an embodiment includes generating another updated version by applying at least one set of log files to the updated version. The at least one set of log files of an embodiment is received later in time than the plurality of log files.

The information management of an embodiment includes a method comprising receiving a plurality of delta data at a first server. The delta data of an embodiment includes information of changes to original data of a second server. The method of an embodiment includes dynamically generating and maintaining an updated version of a copy of the original data at the first server by applying at least one of the plurality of delta data to the copy and controlling the applying using modified information of a component of the first server. The method of an embodiment includes dynamically transforming the data from a first structure by organizing items of the data into a second structure that preserves the hierarchy of the first structure.

The information management of an embodiment includes computer readable medium including executable instructions which when executed in a processing system support information management by receiving a copy of original data and delta data that includes information of changes to the original data. The instructions when executed generate and maintain an updated version of the copy by applying the delta data to the copy as the delta data is received. The instructions when executed dynamically transform the updated version from a first structure by organizing items of the data into a second structure that preserves the hierarchy of the first structure.

The information management of an embodiment includes a system comprising a near line server coupled to one or more servers that include original data. The system of an embodiment includes an information system coupled to the near line server and configured to receive a copy of original data and delta data that includes information of changes to the original data. The information system of an embodiment is configured to generate and maintain an updated version of the copy by applying the delta data to the copy as the delta data is received. The information system of an embodiment is configured to dynamically transform the updated version from a first structure by organizing items of the data into a second structure that preserves the hierarchy of the first structure.

The system of an embodiment is configured to generate and maintain is asynchronous with the receiving.

The system of an embodiment is configured to apply includes applying according to an interval. The interval of an embodiment is based on one or more of time and events.

The delta data of an embodiment includes data of an incremental difference between the original data at a plurality of instances.

The delta data of an embodiment includes data of a differential difference between the original data at a plurality of instances.

The system of an embodiment is configured to control the applying using modified information of a component.

The original data of an embodiment is stored at a first server and the copy is stored at a second server. The component of an embodiment is a component of the second server.

The second server of an embodiment includes one or more of local servers remote servers database servers messaging servers electronic mail servers instant messaging servers voice over Internet Protocol servers collaboration servers messaging and collaboration servers Exchange Servers portals customer relationship management CRM servers enterprise resource planning ERP servers business to business servers and content management servers.

The system of an embodiment is configured to modify by detecting a first state of the copy. The first state indicates the delta data has been applied to the copy.

The system of an embodiment is configured to modify by changing the first state to a second state. The second state is a state from which another updated version can be generated by applying additional delta data to the updated version.

The system of an embodiment is configured to change the first state to the second state by modifying the structural metadata of the copy.

The delta data of an embodiment is a plurality of log files. The component of an embodiment is a log file of the plurality of log files.

The system of an embodiment is configured to apply by invoking an engine of a first server hosting the original data and the controlling includes stalling the engine.

The system of an embodiment is configured to determine the first structure. The first structure of an embodiment is a proprietary structure.

The system of an embodiment is configured to parse items of the data using information of the first structure.

The system of an embodiment is configured to parse by systematically reading information of the data. The information of an embodiment includes one or more of tables and items.

The tables of an embodiment include one or more of a mailbox table a folder table a message table and an attachment table.

The items of an embodiment include one or more of a folder a mailbox an electronic mail body an electronic mail header and an electronic mail attachment.

The system of an embodiment is configured to organize by indexing items of the data into the second structure.

The system of an embodiment is configured to organize by traversing the first structure and locating information of the items.

The system of an embodiment is configured to index by generating indices corresponding to the items. The system of an embodiment is configured to index by associating the items with the indices using the information of the items.

The system of an embodiment is configured to compare modification information of each item with modification information of a previous instance of the item. The system of an embodiment is configured to terminate the traversing at a current node of the first structure when the modification information shows the item is unmodified since the previous instance.

The system of an embodiment is configured to determine whether an item is a previously encountered item that has previously been stored in the second structure. The system of an embodiment is configured to generate a pointer to the previously encountered item.

The system of an embodiment is configured to compare modification information of each item with modification information of a previous instance of the item. The system of an embodiment is configured to index difference information of the item when the modification information shows the item is modified since the previous instance.

The difference information of an embodiment includes information of changes between a current version and at least one previously modified version of the item.

The system of an embodiment is configured to organize by hashing information of each instance of the item by generating a checksum.

The system of an embodiment is configured to determine if the item has been altered since the organizing by comparing checksums of different instances of the item in the second structure. The determining of an embodiment is performed in response to a request to retrieve the item from the second structure.

The system of an embodiment is configured to index by transferring the items into an indexed object repository.

The system of an embodiment is configured to index by transferring metadata of the items into a metadata repository.

The system of an embodiment is configured to provide access to at least a portion of the items in the second structure via a plurality of applications.

The system of an embodiment is configured to provide access by providing access using a second protocol that is different from a first protocol used to access the data of the first structure.

The system of an embodiment is configured to transfer the updated version to an indexed object repository.

The system of an embodiment is configured to generate in near real time and maintain complete integrity and consistency of the original data.

The original data of an embodiment includes one or more of application data databases storage groups mailbox data and server data.

The system of an embodiment is configured to maintain the updated version by generating another updated version by applying at least one set of log files to the updated version. The at least one set of log files of an embodiment is received later in time than the plurality of log files.

The information management of an embodiment includes a system comprising a near line server coupled to one or more servers that include original data. The system of an embodiment includes an information system coupled to the near line server and configured to receive a plurality of delta data. The delta data of an embodiment includes information of changes to the original data. The information system of an embodiment is configured to dynamically generate and maintain an updated version of a copy of the original data at the near line server by applying at least one of the plurality of delta data to the copy and controlling the applying using modified information of a component of the near line server. The information system of an embodiment is configured to dynamically transform the data from a first structure by organizing items of the data into a second structure that preserves the hierarchy of the first structure.

The information management of an embodiment includes a device comprising an information system configured to receive a copy of original data and delta data that includes information of changes to the original data. The information system of an embodiment is configured to generate and maintain an updated version of the copy by applying the delta data to the copy as the delta data is received. The information system of an embodiment is configured to dynamically transform the updated version from a first structure by organizing items of the data into a second structure that preserves the hierarchy of the first structure.

Aspects of the multi dimensional surrogation described herein may be implemented as functionality programmed into any of a variety of circuitry including programmable logic devices PLDs such as field programmable gate arrays FPGAs programmable array logic PAL devices electrically programmable logic and memory devices and standard cell based devices as well as application specific integrated circuits ASICs . Some other possibilities for implementing aspects of the multi dimensional surrogation include microcontrollers with memory such as electronically erasable programmable read only memory EEPROM embedded microprocessors firmware software etc. Furthermore aspects of the multi dimensional surrogation may be embodied in microprocessors having software based circuit emulation discrete logic sequential and combinatorial custom devices fuzzy neural logic quantum devices and hybrids of any of the above device types. Any underlying device technologies may be provided in a variety of component types e.g. metal oxide semiconductor field effect transistor MOSFET technologies like complementary metal oxide semiconductor CMOS bipolar technologies like emitter coupled logic ECL polymer technologies e.g. silicon conjugated polymer and metal conjugated polymer metal structures mixed analog and digital etc.

It should be noted that the various components of multi dimensional surrogation disclosed herein may be described using data and or instructions embodied in various computer readable media. Computer readable media in which such formatted data and or instructions may be embodied include but are not limited to non volatile storage media in various forms e.g. optical magnetic or semiconductor storage media and carrier waves that may be used to transfer such formatted data and or instructions through wireless optical or wired signaling media or any combination thereof. Examples of transfers of such formatted data and or instructions by carrier waves include but are not limited to transfers uploads downloads e mail etc. over the Internet and or other computer networks via one or more data transfer protocols e.g. HTTP FTP SMTP etc. . When received within a computer system via one or more computer readable media such data and or instruction based expressions of the multi dimensional surrogation may be processed by a processing entity e.g. one or more processors within the computer system in conjunction with execution of one or more other computer programs.

Unless the context clearly requires otherwise throughout the description and the claims the words comprise comprising and the like are to be construed in an inclusive sense as opposed to an exclusive or exhaustive sense that is to say in a sense of including but not limited to. Words using the singular or plural number also include the plural or singular number respectively. Additionally the words herein hereunder above below and words of similar import refer to this application as a whole and not to any particular portions of this application. When the word or is used in reference to a list of two or more items that word covers all of the following interpretations of the word any of the items in the list all of the items in the list and any combination of the items in the list.

The above description of illustrated embodiments of the multi dimensional surrogation is not intended to be exhaustive or to limit the multi dimensional surrogation to the precise form disclosed. While specific embodiments of and examples for the multi dimensional surrogation are described herein for illustrative purposes various equivalent modifications are possible within the scope of the multi dimensional surrogation as those skilled in the relevant art will recognize. The teachings of the multi dimensional surrogation provided herein can be applied to other processing systems and methods not only for the systems and methods described above.

The elements and acts of the various embodiments described above can be combined to provide further embodiments. These and other changes can be made to the multi dimensional surrogation and methods in light of the above detailed description.

In general in the following claims the terms used should not be construed to limit the multi dimensional surrogation and methods to the specific embodiments disclosed in the specification and the claims but should be construed to include all processing systems that operate under the claims. Accordingly the multi dimensional surrogation is not limited by the disclosure but instead the scope of the multi dimensional surrogation is to be determined entirely by the claims.

While certain aspects of the multi dimensional surrogation are presented below in certain claim forms the inventors contemplate the various aspects of the multi dimensional surrogation in any number of claim forms. For example while only one aspect of the multi dimensional surrogation is recited as embodied in machine readable media other aspects may likewise be embodied in machine readable media. Accordingly the inventors reserve the right to add additional claims after filing the application to pursue such additional claim forms for other aspects of the multi dimensional surrogation.


---

title: Parallel computing system using coordinator and master nodes for load balancing and distributing work
abstract: Embodiments of the invention provide a method, system and article of manufacture for parallel application load balancing and distributed work management. In one embodiment, a hierarchy of master nodes may be used to coordinate the actions of pools of worker nodes. Further, the activity of the master nodes may be controlled by a “coordinator” node. A coordinator node may be configured to distribute work unit descriptions to the collection of master nodes. If needed, embodiments of the invention may be scaled to deeper hierarchies.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07647590&OS=07647590&RS=07647590
owner: International Business Machines Corporation
number: 07647590
owner_city: Armonk
owner_country: US
publication_date: 20060831
---
The present invention generally relates to data processing. More specifically the present invention relates to a process for parallel application load balancing and distributed work management in parallel computer systems.

One approach to developing very powerful computer systems is to design highly parallel systems where the processing activity of thousands of processors may be coordinated to perform computing tasks. These systems have proved to be highly useful for a broad variety of applications including financial modeling hydrodynamics quantum chemistry astronomy weather modeling and prediction geological modeling and prime number factoring to name but a few examples.

One family of parallel computing systems has been and continues to be developed by International Business Machines IBM under the name Blue Gene. The Blue Gene L system is a scalable system and currently Blue Gene L systems have been configured with as many as 65 536 2 compute nodes. Each compute node includes a single application specific integrated circuit ASIC with 2 CPU s and memory. The Blue Gene architecture has been extremely successful and on Oct. 27 2005 IBM announced that a Blue Gene L system had reached an operational speed of 280.6 teraflops 280.6 trillion floating point operations per second making it the fastest computer in the world at that time. Further as of June 2005 Blue Gene L installations at various sites world wide were among 5 out of the 10 top most powerful computers in the world.

IBM is currently developing a successor to the Blue Gene L system named Blue Gene P. Blue Gene P is expected to be the first computer system to operate at a sustained 1 petaflops 1 quadrillion floating point operations per second . Like the Blue Gene L system the Blue Gene P system is a scalable system with a planned system having 73 728 compute nodes. Each Blue Gene P node includes a single application specific integrated circuit ASIC with 4 CPU s and memory. A complete Blue Gene P system is projected to include 72 racks with 32 node boards per rack.

In addition to the Blue Gene architecture developed by IBM other computer systems may have similar architectures or otherwise provide a parallel architecture using hundreds thousands or even hundreds of thousands of processors. Other examples of a parallel computing system include clustered systems and grid based systems. For example the Beowulf cluster is one well known clustering architecture. A Beowulf cluster is a group of computer systems each running a Unix like operating system typically a version of the Linux or BSD operating systems . Nodes of the cluster are connected over high speed networks and have libraries and programs installed which allow processing to be shared among one another. Essentially the processing power of multiple commodity computer systems is chained together to function cooperatively. Libraries such as the Message Passing Interface MPI library may be used for node to node communications. MPI provides a standard library for communication among the nodes running a parallel program on a distributed memory system. MPI implementations consist of a library of routines that can be called from Fortran C C and Ada programs. Further computer systems are available that provide support for symmetric multi processing SMP using multiple CPUs in a single system and single CPUs are available with multiple processing cores.

Each of these architectures allows for parallel computing. Generally parallel computing refers to a process of executing a single task on multiple processors to obtain results more quickly. Parallel computing techniques typically solve a problem by dividing a large problem into smaller tasks which may be executed simultaneously in a coordinated manner. For example a common design pattern encountered in parallel computing problems is performing essentially the same calculations or operations for different data sets or work units. For these types of applications a master node may divide a problem into individual work units and distribute the work units to a collection of worker nodes. Each worker node then performs the appropriate operations on the work units assigned to that node. Because tens hundreds or thousands of nodes are performing the same calculations on different data sets extremely large datasets may be processed in a relatively short period of time. Many software programs have been developed that use this master worker paradigm whether used in supercomputing applications developed for a Blue Gene or similar system or for applications developed for clusters multi processor SMP systems or multi core processors.

The idea behind the master worker design pattern is that one node is designated as the master node and other nodes are designated as workers. The master generates work units and distributes them to the worker pool. In turn an available or selected worker node consumes the work unit. Depending on the workload there are several strategies for workload distribution. Among the most common are round robin or next available strategies.

The master worker approach is an excellent technique for developing programs to run in a parallel environment. However this approach does not scale well when the master node must coordinate and manage the activity of large numbers of worker nodes. Depending on the work load the generation of work units by the master can easily become a bottleneck in completing a computing task as many workers may sit idle waiting for work units to be assigned or made available. For example depending on the problem the master node may take a long time to generate a work unit relative to the time it takes a worker unit to process one. In this case a high master to worker ratio is required. At the worker end of the master worker paradigm when the time required to consume a work unit is very small the overhead of producing an adequate supply of work units can become a bottleneck on overall system performance. In this case a high master to worker ratio is also required. However the nodes used as master nodes may be unavailable for work unit consumption leading to system inefficiency.

Further if the time required for a node to process a work unit takes a variable amount of time to complete there can be a skew in the finishing time for all the workers. Near the end of a computing task some nodes may be idle and others still consuming work units. Given the number of nodes in highly parallelized super systems or large clusters and grids operations that require even small amounts of idle time for any individual node often translate into large amounts of time for the system as a whole. One approach in such a situation is to divide the work units into smaller chunks so they are more evenly distributed. However this division puts more stress on the master node which as described leads to bottlenecks in system performance. Due to these factors the master worker paradigm can lead to poor use of resources in some cases.

Accordingly there is a need in the art for techniques for parallel application load balancing and distributed work management in a parallelized computer system.

Embodiments of the invention provide a master worker paradigm for parallel computing applications that eliminates the bottleneck of a single master without reducing the work imbalance created by the use of large work units.

One embodiment of the invention provides a method of performing a parallel computing task. The method may generally include generating by a coordinator node a plurality of work unit descriptions wherein each description includes metadata describing a work unit to be performed as part of the parallel computing task and distributing by the coordinator node the work unit descriptions to at least one master node wherein each master node is configured to generate work units from the work unit descriptions and further configured to manage the distribution of work units to a pool of one or more worker nodes. The method may generally further include distributing by the master node the work units to the pool of worker nodes wherein a worker node in the pool is configured to process the work units distributed to the worker node.

Another embodiment of the invention provides a computer readable medium containing a program which when executed performs an operation of performing a parallel computing task. The operations may generally include generating by a master node work units from work unit descriptions each description having been generated by a coordinator node and including metadata describing a work unit to be performed as part of the parallel computing task and distributing by the master node the work units to a pool of worker nodes wherein a worker node in the pool is configured to process the work units distributed to the worker node.

Still another embodiment of the invention provides a system configured to perform a parallel computing task. The system may generally include a coordinator node configured to generate a plurality of work unit descriptions wherein each description includes metadata describing a work unit to be performed as part of the parallel computing task and to distribute the work unit descriptions to at least one master node. The system may further include a master node configured to generate a work unit from the work unit descriptions generated by the coordinator node and further configured to distribute the work units to a worker node and a pool of worker nodes wherein a worker node is configured to process work units distributed to the worker node.

Embodiments of the invention provide a method for parallel application load balancing and distributed work management. In one embodiment a hierarchy of master nodes may be used to coordinate the actions of pools of worker nodes. Further the activity of the master nodes may be controlled by a coordinator node. A coordinator node may be configured to distribute work unit descriptions to the collection of master nodes. If needed embodiments of the invention may be scaled to deeper hierarchies. For example a master coordinator may be used to coordinate the activity of a group of coordinator nodes which in turn manage a group of masters which themselves manage a pool of worker nodes . At higher levels of the hierarchy the data package managed by a node is simplified thus the volume may be larger without creating a bottleneck. For example the coordinator node may be configured to process work unit descriptions i.e. metadata describing a work unit where the master nodes generate the actual work units from the work unit descriptions. Thus the coordinator node may process many more work unit descriptions relative to the number of work units a master node could process in the same time.

Embodiments of the invention are described herein with respect to the Blue Gene computer architecture developed by IBM. Embodiments described herein are advantageous for massively parallel computer systems that include thousands of processing nodes such as a Blue Gene system. However embodiments of the invention may be adapted for use in a variety of parallel computer systems that employ multiple CPUs arranged to communicate over a network. For example embodiments of the invention may be readily adapted for use in distributed architectures such as clusters or grids. In such architectures each processing node may be a computer system communicating with others over local regional or global networks. Further embodiments of the invention may be adapted for use with multi threaded SMP systems or for systems with multiple processing cores.

In the following reference is made to embodiments of the invention. However it should be understood that the invention is not limited to specific described embodiments. Instead any combination of the following features and elements whether related to different embodiments or not is contemplated to implement and practice the invention. Furthermore in various embodiments the invention provides numerous advantages over the prior art. However although embodiments of the invention may achieve advantages over other possible solutions and or over the prior art whether or not a particular advantage is achieved by a given embodiment is not limiting of the invention. Thus the following aspects features embodiments and advantages are merely illustrative and are not considered elements or limitations of the appended claims except where explicitly recited in a claim s . Likewise reference to the invention shall not be construed as a generalization of any inventive subject matter disclosed herein and shall not be considered to be an element or limitation of the appended claims except where explicitly recited in a claim s .

One embodiment of the invention is implemented as a program product for use with a computer system. The program s of the program product defines functions of the embodiments including the methods described herein and can be contained on a variety of computer readable media. Illustrative computer readable media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM or DVD ROM disks readable by a CD or DVD ROM drive on which information is permanently stored ii writable storage media e.g. floppy disks within a diskette drive or hard disk drive on which alterable information is stored. Other media include communications media through which information is conveyed to a computer such as through a computer or telephone network including wireless communications networks. The latter embodiment specifically includes transmitting information to from the Internet and other networks. Such computer readable media when carrying computer readable instructions that direct the functions of the present invention represent embodiments of the present invention.

In general the routines executed to implement the embodiments of the invention may be part of an operating system or a specific application component program module object or sequence of instructions. The computer program of the present invention typically is comprised of a multitude of instructions that will be translated by the native computer into a machine readable format and hence executable instructions. Also programs are comprised of variables and data structures that either reside locally to the program or are found in memory or on storage devices. In addition various programs described herein may be identified based upon the application for which they are implemented in a specific embodiment of the invention. However it should be appreciated that any particular program nomenclature that follows is used merely for convenience and thus the invention should not be limited to use solely in any specific application identified and or implied by such nomenclature.

When configured as a master node a compute node may control the work units assigned to and processed by a collection of worker nodes. In turn when configured as a worker node a compute node may be configured to receive work units from its master node and to return any results from processing a work unit back to its master node. Note however other than the operational use as a worker node or master node the compute nodes may be generally indistinguishable from one another within the overall collection of compute nodes in system .

Additionally as described in greater detail herein one or more compute nodes may be configured as a coordinator node. When configured as a coordinator node a compute node may control a group of one or more master nodes. By introducing a hierarchical structure to the master worker paradigm embodiments of the invention may eliminate the performance bottlenecks inherent to a single master multiple worker system without having to concomitantly increase work unit size leading to poor system utilization . Further a hierarchical structure may reduce the number of compute nodes required to be configured as master nodes leading to improved overall system utilization as more compute nodes may be configured as workers used to process work units.

I O nodes provide a physical interface between the compute nodes and file servers over functional network . In one embodiment the compute nodes and I O nodes communicate with file servers front end nodes and service node over both a control network and or a functional network . In a Blue Gene system the I O nodes and compute nodes may differ from one another only by which network interfaces are enabled and how the node is used by the system . The I O nodes may also be configured to execute processes that facilitate the booting control job launch and debug of the computing system . By relegating these functions to the I O nodes an operating system kernel running on each compute node may be greatly simplified as each compute node is only required to communicate with a few I O nodes . The front end nodes store compilers linkers loaders and other applications used to interact with the system . Typically users access front end nodes submit programs for compiling and submit jobs to the service node .

The service node may include a system database and a collection of administrative tools provided by the system . Typically the service node includes a computing system configured to handle scheduling and loading of software programs and data on the compute nodes . In one embodiment the service node may be configured to assemble a group of compute nodes referred to as a block and dispatch a job to a block for execution. The service node is typically a computer system that includes an operating system memory storage and control console not shown . For example compute nodes on Blue Gene systems use an operating system similar to the Linux operating system. The service node communicates with the compute nodes over control network . The control network provides a communication channel for the service node to control aspects of the operation of system .

In one embodiment the service node may assemble a block to execute a parallel application. In turn a collection of compute nodes may be configured to form a hierarchy of coordinator nodes master nodes and worker nodes to process a computing job.

The Blue Gene P computer system includes multiple data communication networks. An I O processor located on some node boards process data communication between service node and a group of compute nodes . In one embodiment each I O node manages data communications over functional network for as many as 1024 compute nodes . In a Blue Gene P system the 73 728 compute nodes and 1024 I O nodes are configured to communicate over both a logical tree network and a torus network. The torus network connects the compute nodes in a lattice like structure that allows each compute node to communicate directly with its six nearest neighbors. Two neighbors in each of an x y and z plane. The last node in any plane maps wraps the connection back to the first node hence the description of a torus network. Nodes may communicate over the torus network using the well known Message Passing Interface MPI an application programming interface used in developing applications for a highly parallel or cluster computer system e.g. system . Any one compute node may route messages to another compute node over the torus network. In one embodiment a message passing network e.g. the torus network of a Blue Gene system may be used by a coordinator node to distribute work unit descriptions to a pool of master nodes which in turn generate work units distributed to pools of worker nodes.

User node may provide an interface to cluster . As such user node allows users to create submit and review the results of computing tasks submitted for execution to cluster . As shown user node is connected to head gateway node . Head gateway node connects the user node to the compute nodes . Compute nodes provide the processing power of cluster . As is known clusters are often built from racks of commonly available PC components. Thus each node may include one or more CPUs memory hard disk storage a connection to high speed network switch and other common PC components.

Like the compute nodes of parallel system a compute node of cluster may be configured to perform the functions of a master node or a worker node. When configured as a master node a compute node may control the work units assigned to and processed by a collection of worker nodes. In turn when configured as a worker node a compute node may be configured to receive work units from its master node and to return any results from processing a work unit back to its master node. Note however other than the operational use as a worker node or master node the compute nodes may be generally indistinguishable from one another within the overall collection of compute nodes in cluster . Additionally one or more compute nodes may be configured as a coordinator node. When configured as a coordinator node a compute node may generate work unit descriptions for a group of one or more master nodes.

Work unit pool stores the work to be done in as part of a computing job. For example in CGI rendering and animation work unit pool might contain descriptions of three dimensional graphic scenes for thousands of animation frames. In such a case coordinator node may include metadata about the work unit pool for example the number of frames to render from a given scene contained in the work unit pool . In one embodiment coordinator node may be configured to distribute work unit descriptions to the master nodes . The work unit description may be very lightweight and describe details such as where to get data how much data to get among other things. This is represented by the light dashed arrows . Accordingly coordinator node can process a larger volume of work Descriptions relative to the number of work units processed by master nodes in a similar time period.

Continuing through the hierarchy the second tier includes master nodes . Each master node controls the processing performed by a set of worker nodes in one of the worker pools . The master nodes are logically distributed throughout the worker pool. For example in one particular configuration of a Blue Gene L system every set of 64 compute nodes is part of a pool with 63 worker nodes controlled by one of the master node . In this configuration sets of 64 compute nodes are organized into blocks of 8 4 2 nodes however on other systems the master nodes can be distributed in other ways.

As stated master nodes may receive work unit descriptions from coordinator node . Using these descriptions master nodes construct work units . Creating work units is typically a heavier process than the creating work unit descriptions . For example generating a work unit may include fetching data from the work unit pool processing the data to create a distributable work unit and distributing the work units to worker pool . Hence the coordinator distributes this heavier workload across multiple master nodes . By creating lightweight work unit descriptions a single coordinator does not create a bottleneck even though it processes the metadata for the entire set of work units . On very large systems the workflow hierarchy illustrated in may be extended. For example multiple coordinators could be managed by a master coordinator which itself may distribute metadata about the work unit descriptions leading to a hierarchy of coordinator nodes.

Coordinator node has a global view of the work distribution. If one master node distributes work units that take less time to process than other master nodes there can be a load imbalance because the workload of the one master node finishes before the other. In one embodiment coordinator node may be configured to recognize this situation and to adaptively change the distribution of the work unit descriptions to obtain a more fully utilized system. The ability to distribute the master work load among multiple master nodes improves both system throughput and utilization as the master nodes do not become a bottleneck even when work units requires significant time or resources to generate. Further even where one master node is saturated with work requests coordinator node may maintain a balance of system utilization across the collective group of master nodes. In other words embodiments of the invention allows for the distribution of work requests among multiple master nodes with the coordinator node controlling the flow of work to ensure proper load balancing.

Work units can remain more granular creating better load balance without the consequence of swamping a single master node. In other words because the production of work units can be distributed finer grained work units may be used without over taxing the master node. Further work units are distributed more evenly throughout the system leading to better load balance.

At step the coordinator may generate work unit descriptions that describe characteristics of each work unit. As stated above work unit descriptions may be very lightweight and describe details such as where to get data and how much data to get for a given work unit. At step the coordinator node may distribute the work unit descriptions to a group of one or more master nodes. Once work unit descriptions are distributed to the master nodes the coordinator node may monitor the processing of the work unit descriptions by the master nodes. If for example one of the nodes is processing work unit descriptions more quickly than others then the coordinator node may distribute more descriptions to that node. Conversely if one of the master nodes is lagging behind others work unit descriptions may be rerouted to other master nodes or no new descriptions given to that master node until that node catches up.

While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof and the scope thereof is determined by the claims that follow.


---

title: System and method for transferring data between unrelated API contexts on one or more GPUs
abstract: One embodiment of the present invention sets forth a system configured for transferring data between independent application programming interface (API) contexts on one or more graphics processing units (GPUs). Each API context may derive from an arbitrary API. Data is pushed from one API context to another API context using a peer-to-peer buffer “blit” operation executed between buffers allocated in the source and target API context memory spaces. The source and target API context memory spaces may be located within the frame buffers of the source and target GPUs, respectively, or located within the frame buffer of a single GPU. The data transfers between the API contexts are synchronized using semaphore operator pairs inserted in push buffer commands that are executed by the one or more GPUs.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08223159&OS=08223159&RS=08223159
owner: NVIDIA Corporation
number: 08223159
owner_city: Santa Clara
owner_country: US
publication_date: 20060620
---
Embodiments of the present invention relate generally to computer graphics and more specifically a system and method for transferring data between unrelated application programming interface API contexts on one or more graphics processing units CPUs .

Computer graphics image data typically undergoes several processing steps before each graphics frame is completely rendered for display or storage. Each processing step typically operates on graphics image data utilizing programming steps defined through an application programming interface enabling the graphics application to utilize high performance hardware such as a graphics processing unit GPU to execute a set of processing steps with minimal real time supervision from a host CPU. For example a graphics application executing on a host central processing unit CPU may use an application programming interface to program processing steps in a GPU including physics geometric transform polygon setup rasterization and pixel shading resulting in the display of graphics image frames.

As graphics applications become more sophisticated the related processing steps are becoming more sophisticated and diverse in nature. The growing diversity of functional requirements by graphics applications results in situations where certain application programming interfaces are more suitable for certain types of processing steps relative to other application programming interfaces. However the current art only provides for single application programming interfaces to operate on data in a single application programming interface context. The requirement that all processing steps must be defined and executed within a single application programming interface context severely limits the ability of new graphics applications to exploit the potential of newly emerging application programming interfaces.

In addition as graphics applications begin to implement more steps with greater complexity in each step the computational load on the GPU executing the processing steps increases resulting in diminished overall rendering performance. One approach to improving overall processing time has been to configure multiple GPUs to concurrently process a single graphics frame or assign multiple GPUs to process alternating graphics frames. Such approaches generally involve synchronizing the GPUs to simultaneously render portions of the same frame or sequential frames to increase overall rendering performance. Again however in current multi GPU systems all of the GPUs have to specify their complete rendering pipelines within the same application programming interface and therefore cannot optimally select and utilize more suitable application programming interfaces as needed for each processing step.

As the foregoing illustrates what is needed in the art is a mechanism for enabling applications to exploit different application programming interfaces when processing operations are performed on one or more GPUs.

One embodiment of the invention sets forth a system for processing data within multiple application programming interface API contexts. The system includes a first API and a first software driver configured to interact with the first API to generate a first API context. The first API and the first software driver are further configured to receive a first set of commands from an application and to generate a first push buffer of commands based on the first set of commands received from the application for processing a set of data within the first API context on a graphics processing unit. The system also includes a second API and a second software driver configured to interact with the second API to generate a second API context. The second API and second software driver are configured to receive a second set of commands from the application and generate a second push buffer of commands based on the second set of commands received from the application for processing the set of data within the second API context on a graphics processing unit. Commands from the second push buffer are executed on the set of data within the second API context after commands from the first push buffer are executed on the set of data within the first API context.

One advantage of the disclosed system is that it enables data to be processed within a first API context and then transferred to a second API context for further processing. The processing operations within the first API context and the second API context may occur on one or more GPUs. Thus among other things the present invention enables a graphics processing system to be configured with different APIs so that an application can optimally select and utilize the API s most suitable for a particular processing step.

The present invention enables concurrent execution and communication between multiple unrelated application programming interface API contexts under the control of a common software application. By providing a context to context communications mechanism software performance and development time are both improved by allowing applications to exploit the most appropriate mix of capabilities for the specific needs of the application. Furthermore an application s data flow and computational load may be distributed over multiple GPUs where each GPU interacts with the application through a different API.

API context includes a buffer identified by the application as a destination buffer used as a destination for data transferred from API context to API context . For example the results of physics calculations performed in the API context the OpenGL context and deposited in a local buffer may be transferred into the buffer within the API context the D3D context for graphics rendering. describe systems and methods to enable this functionality.

Part of the initial API initialization process includes allocating all buffers to be used for API context to API context data transfers. Thus the application instructs the GPGPU API to allocate a buffer and the D3D API to allocate buffer for such transfers.

Once the API initialization process has completed the application conveys processing commands to the GPGPU API . These processing commands when further processed by the GPGPU API and the OpenGL driver include additional commands inserted into the push buffer command stream that facilitate synchronized data transfer from the first API context generated under the GPGPU API to the second API context generated under the D3D API . Similarly the application communicates processing commands to the D3D API which when processed by the D3D driver include transfer and synchronization commands inserted into the push buffer command stream that facilitate synchronized data transfers from the first API context to the second API context .

The first GPU has an attached frame buffer FB memory . The FB memory includes the buffer allocated to the first API context . Data is written to Buffer as a result of processing conducted on the first GPU using the GPGPU API . The resulting data stored in the buffer is transferred to the second GPU through a transfer operation . The transferred data is placed in the buffer which is allocated in an FB memory that is attached to the second GPU . The transfer operation may be conducted using any technically feasible mechanism. In one embodiment the transfer operation comprises a process known in the art as a direct memory access copy operation. Such an operation is commonly referred to as a blit and in this fashion the first GPU effectively pushes the data from the first buffer to the second buffer . In another embodiment the transfer operation is conducted using a mechanism known in the art as PCI Express peer to peer transfer. Such a transfer is conducted using a direct memory access DMA to facilitate efficient data transfers. As described in greater detail below in the transfer operation is triggered on the source side by a transfer data command inserted in the command stream of the push buffer . The transfer data command causes the first GPU i.e. the source to transfer the result data stored in the buffer to the buffer within the FB memory . As also described in greater detail below in the destination transfer buffer is examined by the second GPU when an acquire semaphore command inserted in the command stream of the push buffer indicates there is incoming data.

The second GPU processes data stored in buffer in conjunction with the commands received from push buffer to produce rendered data which is then stored in buffer . In one embodiment the rendered data stored in buffer consists of graphics frames that are scanned out to a video digital to analog converter DAC for display.

Importantly the architecture of enables application to process data within the first API context using the first API GPGPU API and then transfer the processed data to the second API context generated under the second API the D3D API for further processing.

As shown a transfer command from push buffer causes the first GPU i.e. the source to transfer the data stored in buffer in FB memory to a buffer in system memory . When the second GPU i.e. the destination receives a pull command from push buffer the second GPU transfers the buffer in system memory to a buffer in FB memory . Newly received data in buffer is acted upon by the second GPU once an acquire semaphore command is received from push buffer . Again the pull and acquire semaphore commands are described in greater detail below in .

System memory is typically slower than FB memories and involving system memory in the data transfer process also adds an additional buffer transfer operation reducing overall performance. Thus better performance is attained when the peer to peer transfers of are directly supported by the system. However the option of conducting transfers through system memory adds flexibility and robustness to the invention described herein especially in cases where the overall system does not support peer to peer transfers.

A semaphore is a pointer to a specific address in memory. A semaphore may be released or acquired. When a GPU executes a release semaphore command the GPU writes a specific value to the memory location associated with the semaphore. When a GPU executes an acquire semaphore command the GPU reads the memory location associated with the semaphore and compares the value of that memory location with the value reflected in the acquire semaphore command. If the two values do not match then the semaphore associated with the acquire semaphore command has not yet been released. In the case of a mismatch the GPU executing the acquire semaphore command continues reading the memory location associated with the semaphore until a match is found. Consequently the GPU executing the acquire semaphore command does not execute any additional push buffer commands until a match is found. For example assume that a first GPU is directed to release a semaphore having a value of 99 and subsequently a second GPU is directed to acquire the semaphore having a value of 99. The second GPU will continue reading the memory location associated with the semaphore until that memory location has a value of 99. Importantly the second GPU will not execute the next push buffer command until the memory location has a value of 99 and the memory will have a value of 99 only when the first GPU releases the semaphore having a value of 99.

Push buffer is associated with a first API context that is a source of certain data to be transferred to a second API context. Commands for processing data A are commands associated with a source process and may include without limitation commands necessary to process a frame of data such as physics or geometric data or to render a frame of graphics data. A transfer command transfer data A instructs the source GPU to transfer the processed data from a buffer within the first API context to a destination buffer within the second API context. After transferring the data the source GPU executes a semaphore release command release semaphore A in order to synchronize the data transfer with a destination GPU. As described above executing the semaphore release command causes the source GPU to release a semaphore.

Push buffer is associated with the second API context of the data and includes commands for processing data A that are executed on the destination GPU. However before the destination GPU can process any of the commands for processing data A the destination GPU first executes the acquire semaphore A command. If successful the acquire semaphore command A results in the acquisition by the destination GPU of the semaphore released by the source GPU. Thus successful acquisition of the semaphore indicates that the data transferred by the source GPU is available to the destination GPU for further processing. In this fashion the data transfer between the first API context and the second API context is synchronized. Extending this example the acquire semaphore B command should succeed before the destination GPU may process the commands for processing data B and so forth.

Referring back to the embodiment of when system memory is involved in context to context data transfers the source GPU transfers the processed data within the source buffer to a designated buffer in the system memory in response to the transfer data A command. After the transfer data command is completed the release semaphore A command is executed as described above. The destination GPU then executes the acquire semaphore A command. After acquiring the semaphore released by the source GPU the destination GPU executes a pull data command not shown which causes the destination GPU to transfer the data in the designated system memory buffer to a destination buffer within the FB memory of the destination GPU. Once the data has been transferred to the destination frame buffer the destination GPU is able to further process the data by executing one or more of the commands for processing data A . Thus when transferring data through system memory the push buffer command stream should include a pull data command to be executed subsequent to each acquire semaphore command. In this fashion data processed by the source GPU is known to be fully transferred from the source GPU and stable within the designated buffer within the system memory before being pulled into the FB memory by the destination GPU for further processing. Again the data transfer between the first API context and the second API context is synchronized using release and acquire semaphore commands in the relevant push buffers.

The method of transferring data from one API context to another API context begins in step where an application makes API calls to allocate buffers in each API context e.g. buffers and of . Any initialization unrelated to transferring data between API contexts is assumed to have occurred by this point in the execution of the application. The following pseudo code illustrates the API calls that may be invoked by an application to allocate a buffer in an OpenGL context and a D3D context where OpenGL is used for physics or related computations and D3D is used for rendering 

After the buffers are allocated in step the method proceeds to step where the application makes API calls to link the source and destination buffers e.g. buffers and respectively as illustrated by the following pseudo code 

After step the method proceeds to step where a GPU performs a first set of physics computations in the first API context populating the previously allocated source buffer e.g. buffer with the resulting data from the computations. Once the first set of computations are complete in the first API context for example when all physics computations are complete for a frame of graphics data the method proceeds to step .

In step the results from the first API context computation are transferred to the destination buffer e.g. buffer that is accessible to the second API context. In alternative embodiments such as the embodiment of a buffer in system memory may act as an intermediate buffer before reaching the destination buffer. The use of semaphore commands in the relevant push buffers as detailed in ensures that the data processed within the first API context is completely transferred to the second API context before any processing on that data begins within the second API context.

In step a second GPU performs computations on the data transferred to the second API context. For example the second GPU may perform shading and rendering operations on the physics data computed previously by the first GPU.

In one embodiment of the invention the destination buffer e.g. buffer of is a texture buffer. In alternative embodiments however the destination buffer may be any other technically feasible type of buffer such as a vertex buffer.

While the forgoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof. For example in an alternative embodiment one GPU may operate under two different APIs first processing data within a first API context under the first API transferring the data to a second API context under the second API and then processing the data within the second API context. In such an embodiment both the source and destination buffers would reside within the local memory of the GPU but the source buffer would be within the first API context and the destination buffer would be within the second API context. Thus the scope of the present invention is determined by the claims that follow.


---

title: Scalable architecture for subspace signal tracking
abstract: A real-time implementation of a subspace tracker is disclosed. Efficient architecture addresses the unique computational elements of the Fast Approximate Subspace Tracking (FAST) algorithm. Each of these computational elements can scale with the rank and size of the subspace. One embodiment of architecture described is implemented in digital hardware that performs variable rank subspace tracking using the FAST algorithm. In particular, the FAST algorithm is effectively implemented by a few processing elements, coupled with an efficient Singular Vector Decomposition (SVD), and the realization/availability of high density programmable logic devices. The architecture enables the ability to track the possibly changing dimension of the signal subspace.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07945061&OS=07945061&RS=07945061
owner: BAE Systems Information and Electronic Systems Integration Inc.
number: 07945061
owner_city: Nashua
owner_country: US
publication_date: 20060607
---
Portions of the present invention may have been made in conjunction with Government funding under contract number DMEA 90 99 D 0007 0045 and there may be certain rights to the United States Government.

The invention relates to signal processing and more particularly to a real time implementation of a subspace tracker.

Subspace estimation is coming to play an important role in a variety of modern signal processing applications. Various algorithms are proposed for efficiently tracking the principal singular values and the associated left or right singular vectors of successive data matrices formed from observations of a nonstationary signal in nonstationary noise. The ability to perform this tracking in real time and with sufficient accuracy is required for many signal processing applications in technology areas such as communications radar sonar and speech processing.

In some of these applications the component of the data referred to as the signal may actually be nonstationary interference. The subspace of that interference signal may be tracked for the purpose of suppressing that particular interference rather than enhancing the signal or estimating its parameters. An important attribute of these tracking algorithms is the ability to track the possibly changing dimension of the signal subspace.

Many subspace tracking algorithms are proposed in the literature but each are associated with one or more problems. For example the projection approximation subspace tracker PAST algorithm and its variant PASTd can be used for subspace tracking and employ only basic arithmetic computations. However PAST algorithms may not provide the desired level of accuracy. A rank adaptive fast subspace tracking FST algorithm is also available. However the FST algorithm does not estimate eigenvalues or singular values.

The fast approximate subspace tracking FAST algorithm and its variant FAST2 can be used for tracking singular values singular vectors and the dimension of a signal subspace through an overlapping sequence of data matrices. The speed and accuracy of the FAST algorithm appear to be superior or at least comparable to other algorithms such as the PAST and PASTd algorithms FST algorithm and the Prony Lanczos PL algorithm. However it is unclear how to implement an architecture that can address the unique computational elements of the FAST algorithm. Exacerbating this problem is that each of the computational elements should scale with the rank and size of the subspace.

Simply stated there are no currently available solutions for real time implementations of a subspace tracker. What is needed therefore is a real time implementation of a subspace tracker.

One embodiment of the present invention provides a device for subspace tracking using the FAST algorithm. The device includes a Project Vector computational block for multiplying Signal Observations by Subspace Estimates and accumulating to produce Low Rank Approximations e.g. represented as a vector of length r for each of c Signal Observations . The device further includes a Residual Vector computational block for determining a degree of match between the Subspace Estimates and a current Signal Observation based on a corresponding Low Rank Approximation. The device further includes an Outer Product computational block for computing a product of the Low Rank Approximations matrix across the Signal Observations. The device further includes a Singular Vector Decomposition SVD computational block for reducing the product into a Current Subspace Estimate and an Estimate Subspace computational block for updating each of the Subspace Estimates based upon the Current Subspace Estimate. In one such case the Low Rank Approximation for the current Signal Observation along with the current Signal Observation itself is reduced by the Residual Vector computation block into an Error vector and an Energy value. Here the error vector represents the degree of match between the Subspace Estimates and the current Signal Observation on a per element basis. The Energy value can be for example the sum of squares of the current Error vector. The device may include one or more ping pong storages for storing at least one of the Signal Observations Subspace Estimates Low Rank Approximations and Current Subspace Estimate. In another particular case each of the computational blocks scales with rank and size of the subspace being tracked. In another particular case the device maintains a set of Subspace Estimates for each of c by n Signal Observations that span a rank setting r where r is an integer value that is user selectable. In another particular case the SVD computational block is sized to accommodate a largest rank of processing. The system functionality can be implemented for example in software e.g. executable instructions encoded on one or more computer readable mediums hardware e.g. programmable logic or one or more FPGAs or ASICs firmware e.g. one or more microcontrollers with I O capability and embedded routines for carrying out the functionality described herein or some combination thereof. Many suitable means for implementing embodiments of the present invention will be apparent in light of this disclosure.

Another embodiment of the present invention provides a method for subspace tracking using the FAST algorithm. The method includes multiplying Signal Observations by Subspace Estimates and accumulating to produce Low Rank Approximations e.g. represented as a vector of length r for each of c Signal Observations . The method continues with determining a degree of match between the Subspace Estimates and a current Signal Observation based on a corresponding Low Rank Approximation. The method continues with computing a product of the Low Rank Approximations matrix across the Signal Observations. The method continues with reducing the product into a Current Subspace Estimate and updating each of the Subspace Estimates based upon the Current Subspace Estimate. In one such case the Low Rank Approximation for the current Signal Observation along with the current Signal Observation itself is reduced by the Residual Vector computation block into an Error vector and an Energy value. Here the error vector represents the degree of match between the Subspace Estimates and the current Signal Observation on a per element basis. The Energy value can be for example the sum of squares of the current Error vector. The method may include maintaining a set of Subspace Estimates for each of c by n Signal Observations that span a rank setting r where r is an integer value that is user selectable. The method may include storing at least one of the Signal Observations Subspace Estimates Low Rank Approximations and Current Subspace Estimate in one or more ping pong storages.

Another embodiment of the present invention provides a machine readable medium e.g. one or more compact disks diskettes servers memory sticks or hard drives encoded with instructions that when executed by one or more processors cause the processor to carry out a process for subspace tracking using the FAST algorithm. This process can be for example similar to or a variation of the previously described method.

The features and advantages described herein are not all inclusive and in particular many additional features and advantages will be apparent to one of ordinary skill in the art in view of the figures and description. Moreover it should be noted that the language used in the specification has been principally selected for readability and instructional purposes and not to limit the scope of the inventive subject matter.

A real time implementation of a subspace tracker disclosed. Efficient architecture addresses the unique computational elements of the Fast Approximate Subspace Tracking FAST algorithm. Each of these computational elements can scale with the rank and size of the subspace.

One embodiment of architecture described herein is implemented in digital hardware that performs variable rank subspace tracking using the FAST algorithm. In particular the FAST algorithm is effectively implemented by a few processing elements coupled with an efficient Singular Vector Decomposition SVD and the realization availability of high density programmable logic devices. This architecture is capable of tracking the principal singular values and the associated left or right singular vectors of successive data matrices formed from observations of a nonstationary signal in nonstationary noise. As previously explained the ability to perform this tracking in real time and with sufficient accuracy is required for many signal processing applications in technology areas such as communications radar sonar and speech processing. The architecture enables the ability to track the possibly changing dimension of the signal subspace.

Square blocks represent computational operations and include Project Vector Residual Vector Outer Product Estimate Subspace and Singular Vector Decomposition SVD . Rounded or oval blocks represent data that can span simple scalars to multidimensional matrices and include Signal Observations Subspace Estimates Low Rank Approximations Error vector Energy value and Current Subspace Estimate .

Each of the computational blocks will be discussed separately in turn with reference to . For now the overall process of the architecture will be described.

A set of Signal Observations is collected and maintained e.g. in a fast access memory such as flash memory configured in a ping pong storage scheme two memory banks where one bank can be read from while the other bank is being written to . A given observation will span time or space represented as a column vector of n elements. In this particular example there are a total of c observations in Signal Observations representing a time history of observations whereby a new observation will replace the oldest observation.

The architecture also maintains a set of Subspace Estimates for each of the c by n Signal Observations which will additionally span the rank setting r. The Subspace Estimates can be stored in memory like that which stores the Signal Observations . In one particular embodiment rank is an integer value that is set by the user e.g. via a pull down menu or other suitable user interface mechanism . Generally speaking the set of Subspace Estimates has the highest amount of hits i.e. sources and sinks and should receive the most attention for bandwidth in any parallel scaling of the architecture.

As a new Signal Observation is received the current observation and all previous observations are reduced by the Project Vector block using the current Subspace Estimates to produce a Low Rank Approximation which is a vector of length r for each of the c observations. The Low Rank Approximation for the current Signal Observation along with the observation itself is reduced by the Residual Vector block into an Error vector a vector of length n and an Energy value . The Error vector represents the measure of how well the set of Subspace Estimates models a given Signal Observation on a per element basis. In one particular embodiment Energy value is the sum of squares of the current Error vector .

The Low Rank Approximation of all Signal Observations along with the Energy value of the current observation is transformed by the Outer Product block which can be implemented using a simple outer product operation . The SVD in turn reduces this into a Current Subspace Estimate which is a vector of length c by r that is drawn from the r most significant left singular vectors . The Current Subspace Estimate is then applied by the Estimate Subspace block along with the Error vector Energy value and Subspace Estimates to produce an updated Subspace Estimate .

The Low Rank Approximations and Current Subspace Estimate can be stored in memory like that which stores the Signal Observations and Subspace Estimates . Likewise Energy value and Error vector can be stored if so desired.

As can be seen the Project Vector of this example configuration includes a multiplier and an accumulator which effectively operates as an adder . Embodiments of the Project Vector are discussed in more detailed with reference to . The Residual Vector includes multipliers accumulators sometimes referred to as adders or substractors depending on the designated operation and a square root module . Embodiments of the Residual Vector are discussed in more detailed with reference to . The Estimate Subspace includes multipliers accumulators and an inverter . Embodiments of the Estimate Subspace are discussed in more detailed with reference to .

As previously explained the Outer Product block can be implemented using a simple outer product operation or other suitable process for computing the product of the Low Rank Approximations matrix across all Signal Observations . The SVD is configured for carrying out singular vector decomposition and can be implemented for example as discussed in U.S. patent application Ser. No. 11 046 377 filed Jan. 28 2005 and titled Scalable 2 2 Rotation Processor For Singular Value Decomposition. This application is herein incorporated in its entirety by reference. Each of the Outer Product and SVD and there respective interaction with other computation modules and data are discussed in further detail with reference to .

In one particular embodiment each of the computational blocks which may include for example storage cells adders substractors multipliers square root modules and inverters is realized with programmable logic such as a field programmable gate array FPGA or other such configurable processing environment. Alternatively the computational blocks can be implemented in software e.g. C C executing on one or more processors. Alternatively the computation blocks can be implemented with a combination of hardware and software e.g. some programmable logic and some executable routines. Numerous such configurations will be apparent in light of this disclosure.

As can be seen the process of Project Vector is a matrix multiply. In more detail all Signal Observations are multiplied by multiplier against the Subspace Estimates and then accumulated by accumulator to produce the Low Rank Approximations . If storage bandwidth allows the matrix multiplies can be broken down for example into vector multiplies as illustrated in or half sized vector multiplies. The Project Vector block can be implemented for example with finite precision operations.

As another mechanism for achieving real time performance in an application once a new set of Subspace Estimates are calculated the Project Vector block can be re exercised against all current Signal Observations prior to the arrival of a new observation. A new Signal Observation would also require the exercise of Project Vector but there may be a window of time prior to this event in which to re evaluate the Low Rank Approximations on the older Signal Observations as will be appreciated in light of this disclosure.

As can be seen each of the c Signal Observations e.g. Signal Observation t Signal Observation t 1 . . . Signal Observation t c 1 is provided in parallel to r multiplier and accumulator sets making up the parallel realization of the Project Vector . Note that this embodiment does not represent a systolic array in that r Subspace Estimates e.g. Subspace Estimate Subspace Estimate . . . Subspace Estimate feed all of the multipliers directly and are not products. The data output by the Project Vector makes up the Low Rank Approximations which as previously explained is a vector of length r for each of the c Signal Observations .

The Residual Vector computation block e.g. x 0 . . . c 1 is used to determine the degree of match between the Subspace Estimates and a single Signal Observation . For a given Signal Observation at time t x its Low Rank Approximation is multiplied by multiplier against the Subspace Estimates and summed by adder across rank. This stage of the Residual Vector can be alternatively implemented with parallelism as will be discussed with reference to the example embodiment shown in . The result of this accumulation by adder is an estimate of the observation.

A difference by subtractor against the Signal Observation produces an Error score for each sample in the observation. The Residual Vector also determines the overall match with a scalar Energy score by in this particular embodiment taking the square root of the sum of the Error terms squared . Further parallelism can be achieved with the scoring of the Signal Observations but in practice this process is only exercised for two observations. As will be appreciated in light of this disclosure the Residual Vector can be implemented using finite precision processing elements.

Here for a given Signal Observation at time t x each of its r Low Rank Approximations is multiplied by a corresponding multiplier against the corresponding Subspace Estimates e.g. Subspace Estimate Subspace Estimate . . . . Subspace Estimate and summed by adder across rank. In this parallel embodiment note that adder is implemented as an adder tree to receive all r inputs. The result of this accumulation by adder is an estimate of the observation. The remainder of the full parallel realization of the Residual Vector operates as discussed with reference to .

The Outer Product operation is a relatively straight forward process. Its usage here is to take the outer product of the Low Rank Approximations e.g. populated with a row of zeros ending with the Energy value from the Residual Vector computational block matrix across all observations. Generally the size of the resulting matrix will be small. The result of the outer product computation is used as the matrix to be decomposed by the SVD . In one particular embodiment each of the Outer Product and the SVD are implemented with a minimum single precision floating point accuracy.

As previously explained the SVD reduces the matrix produced by the Outer Product into a Current Subspace Estimate . In more detail the SVD produces a set of eigenvalues by r as well as a set of left and right eigenvectors r by r . The eigenvalues need to be ordered from high to low with the left and right eigenvectors sorted accordingly. Since the SVD is performed on the output of the Outer Product the left and right eigenvectors are identical. Therefore the complexity of the SVD operation can be reduced. The Current Subspace Estimate is the resultant eigenvectors. The Current Subspace Estimate is then applied by the Estimate Subspace block along with the Error vector Energy value and Subspace Estimates to produce an updated Subspace Estimate .

In one particular embodiment the SVD is sized to accommodate the largest rank of processing r. Should r be chosen less than the maximum allotted for the SVD is sufficient but the Low Rank Approximation would need to contain zeros in the unused indices. Further detailed embodiments of the SVD are discussed in he previously incorporated U.S. patent application Ser. No. 11 046 377. In one such particular embodiment a two plane rotation TPR approach to Gaussian elimination Jacobi is used for computational efficiency in determining rotation parameters. A rotation processor is constructed using the TPR approach to perform SVD on two by two matrices yielding both eigenvalues and left and right eigenvectors. The rotation processor can then be replicated and interconnected to achieve higher dimensioned matrices. For higher dimensional matrices the rotation processors on the diagonal solve the 2 2 rotation angles broadcast the results to off diagonal processors whereby all processors perform matrix rotations in parallel. FIGS. 3 5 of U.S. patent application Ser. No. 11 046 377 illustrate an array of 2 2 rotation processors configured for handling decomposition of the matrix generated by the Outer Product in accordance with one embodiment of the present invention.

In general performing the Estimate Subspace block is the most time consuming process. Any degree of parallelism will therefore provide advantage. Also a ping pong memory or other suitable fast access memory for the storage of the Subspace Estimate will also provide desirable benefits particularly for large values of n. In accordance with one embodiment of the present invention the purpose of the Estimate Subspace computational block is to correct or otherwise update the Subspace Estimate n by r based upon the Current Subspace Estimate r by r as well as the recently measured Error by n . A relatively simple means of performing the update is illustrated in example architecture of .

As can be seen the Current Subspace Estimate by r which correlates with the most significant eigenvalue from the SVD computational block is normalized and by the Energy 1 by 1 and then multiplied against the Error by n . This value is accumulated with the accumulated product of the previous Subspace Estimates n by r 1 and the Current Subspace Estimate r 1 by r . For precision and in one particular embodiment the inversion of the Energy and multiplication and are implemented with floating point precision and the remainder of the Estimate Subspace processing block is implemented using fixed point precision.

A higher performance Estimate Subspace architecture is illustrated in . In particular illustrates a parallel realization of the Estimate Subspace computational block shown in configured in accordance with an embodiment of the present invention Note here that the fullest degree of parallelism would instantiate the architecture of r times. Also note that the product of the Current Subspace Estimate and the inverted Energy term is only computed once allowing for scheduling the Error multiplier for re use to reduce size of the architecture.

Here each of the previous r 1 Subspace Estimates n by is multiplied by a corresponding multiplier against the corresponding Current Subspace Estimates e.g. Current Subspace Estimate Current Subspace Estimate . . . Current Subspace Estimate and summed by accumulated by accumulator . In this parallel embodiment note that adder is implemented as an adder tree to receive all r 1 inputs. The accumulated product of the previous Subspace Estimates and the Current Subspace Estimate is accumulated with the value resulting from the process previously described with reference to where the Error is normalized and by the Energy and multiplied against the Current Subspace Estimate that correlates with the most significant eigenvalue from the SVD computational block . The remainder of this example Estimate Subspace operates as discussed with reference to .

The foregoing description of the embodiments of the invention has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise form disclosed. Many modifications and variations are possible in light of this disclosure. It is intended that the scope of the invention be limited not by this detailed description but rather by the claims appended hereto.


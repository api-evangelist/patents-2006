---

title: Dynamic database memory management policies
abstract: A database engine is provided with memory management policies to dynamically configure an area of memory called a buffer pool into which data pages are held during processing. The data pages are also buffered as an I/O (input/output) stream when read and written to a persistent storage medium, such as a hard disk, through use of a system file cache that is managed by the computer's operating system. The memory management policies implement capping the amount of memory used within the buffer pool to minimize the number of data pages that are double-buffered (i.e., held in both the buffer pool and system file cache). In addition, trimming data pages from the buffer pool, after the database engine completes all pending operations and requests, frees additional memory and further minimizes the number of processes associated with the database.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07840752&OS=07840752&RS=07840752
owner: Microsoft Corporation
number: 07840752
owner_city: Redmond
owner_country: US
publication_date: 20061030
---
A relational database is a collection of related data that is organized in related two dimensional tables of columns and rows. The data stored in a relational database is typically accessed by way of a user defined query that is constructed in a query language such as Structured Query Language SQL . SQL databases are generally organized using a fundamental storage unit of a data page where storage space such as that provided by a hard disk is logically divided into contiguously numbered data pages.

An SQL database engine is typically implemented using a database application i.e. a software program that when run on a processor or computer allows for management of data and information structured as fields records and files in the database . The SQL database engine provides among other functionalities the core service for running queries processing data and reading and writing pages to disk. Such database applications are operable on high performance computers including specialized database server hardware. However not all databases are implemented in such a manner. For example some users may wish to use lower cost and more general purpose hardware such as personal computers PCs that would typically host other applications in addition to the database. These other applications can be unpredictable in operation and widely vary in how they implement processes that utilize system and hardware resources such as memory. In some cases such processes may reduce the ability of the computer s operating system to optimize the overall performance of the computer. Disk access on the lower cost general purpose hardware is also generally much slower than access provided by the more specialized server hardware. Accordingly database input output I O and the PC s overall responsiveness may be reduced which can often negatively influence a user s perception of the interaction with the PC and the applications running on it.

This Background is provided to introduce a brief context for the Summary and Detailed Description that follows. This Background is not intended to be an aid in determining the scope of the claimed subject matter nor be viewed as limiting the claimed subject matter to only those implementations that may solve any or all of the disadvantages or problems presented above.

A database engine running on a computer system is provided with memory management policies to dynamically configure an area of memory called a buffer pool into which data pages are held during processing. The data pages are also buffered as an I O input output stream when read and written to a persistent storage medium such as a hard disk through use of a system file cache that is managed by the computer s operating system. The memory management policies implement capping the amount of memory used for the data pages within the buffer pool to minimize the number of data pages that are double buffered i.e. held in both the buffer pool and file system cache . In addition trimming data pages from the buffer pool after the database engine completes all pending operations and requests frees additional memory and further minimizes the number of processes associated with the database.

Buffering the I O in the system file cache capping the data pages within the buffer pool and trimming data pages when the database engine is idle reduces the memory footprint of the database application. Such reduction advantageously frees resources to enable the operating system to optimize the computer s overall performance but still maintains the benefit to database disk I O that results from caching. As a result the computer is responsive to database operations and disk access while enhancing the user s perceptions of the computer s performance by keeping the memory consumption of database processes small through the trimmed data pages. While generally applicable to most computer systems optimization of the database application s memory footprint can be particularly beneficial to users wishing to use relatively low cost general purpose computer systems as a database server.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. The benefits and advantages noted in this Summary are not intended to limit the claimed subject matter to only those implementations that contain those benefits or advantages. In addition this Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Caching is an important memory utilization and performance optimization that is commonly performed by applications and operating systems. The premise behind caching which is also termed buffering is that most applications frequently touch i.e. access a subset of data within one or more files. Bringing such data pages into memory and keeping them there for the duration of the application s accesses minimizes the number of disk reads and writes the system must perform. Without caching applications require relatively time consuming disk operations every time they access a file s data. Disk access is also expensive in terms of system resources that are expended.

Most modern operating systems employ a file cache for example a system file cache under the Microsoft Windows operating system into which the I O system maps recently used data from disk. When processes need to read from or write to the files mapped in the cache an I O manager copies the data from or to the cache without persisting to disk as if it were an array in memory. Because memory access is quicker than a disk operation the cache provides an important performance boost to the processes.

Database applications are often arranged to manage all aspects of a computer system s operations including processes I O and memory operations. Some database applications particularly those that are large and are intended for deployment in client server type architectures bypass use of the system file cache in favor of a specialized cache termed a buffer pool or buffer cache . A buffer pool is an area of memory into which data pages in the database are read modified and held during processing. The use of a buffer pool typically improves database performance. As with a file cache if a needed page of data is already in the buffer pool that data page is accessed faster than if the data page had to be read directly from disk.

Database application control over system resources and the bypass of the system file cache are techniques that perform very satisfactorily in tightly controlled environments where specific software applications are installed and their resource consumption carefully managed. However in the case where a general purpose computer system is used to support the database application such tight control of processes and resource consumption may not be possible. Less optimized database performance may result in such cases.

Turning now to the drawings in which like reference numerals indicate like elements is a simplified block diagram of an illustrative general purpose computer system such as a personal computer PC with which the present dynamic memory management policies may be implemented. Computer system includes a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. The system memory includes read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within the computer system such as during start up is stored in ROM . The computer system may further include a hard disk drive for reading from and writing to an internally disposed hard disk not shown a magnetic disk drive for reading from or writing to a removable magnetic disk e.g. a floppy disk and an optical disk drive for reading from or writing to a removable optical disk such as a CD compact disc DVD digital versatile disc or other optical media. The hard disk drive magnetic disk drive and optical disk drive are connected to the system bus by a hard disk drive interface a magnetic disk drive interface and an optical drive interface respectively. The drives and their associated computer readable media provide non volatile storage of computer readable instructions data structures program modules and other data for the computer system . Although this illustrative example shows a hard disk a removable magnetic disk and a removable optical disk other types of computer readable media which can store data that is accessible by a computer such as magnetic cassettes flash memory cards digital video disks data cartridges random access memories RAMs read only memories ROMs and the like may also be used in some applications of the present dynamic database memory management. In addition as used herein the term computer readable medium includes one or more instances of a media type e.g. one or more magnetic disks one or more CDs etc. .

A number of program modules may be stored on the hard disk magnetic disk optical disc ROM or RAM including an operating system one or more application programs other program modules and program data . A user may enter commands and information into the computer system through input devices such as a keyboard and pointing device such as a mouse. Other input devices not shown may include a microphone joystick game pad satellite disk scanner or the like. These and other input devices are often connected to the processing unit through a serial port interface that is coupled to the system bus but may be connected by other interfaces such as a parallel port game port or universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video adapter . In addition to the monitor personal computers typically include other peripheral output devices not shown such as speakers and printers. The illustrative example shown in also includes a host adapter a Small Computer System Interface SCSI bus and an external storage device connected to the SCSI bus .

The computer system is operable in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be selected as another personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer system although only a single representative remote memory storage device is shown in . The logical connections depicted in include a local area network LAN and a wide area network WAN . Such networking environments are often deployed for example in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the local area network through a network interface or adapter . When used in a WAN networking environment the computer system typically includes a broadband modem network gateway or other means for establishing communications over the wide area network such as the Internet. The broadband modem which may be internal or external is connected to the system bus via the serial port interface . In a networked environment program modules related to the computer system or portions thereof may be stored in the remote memory storage device . It is noted that the network connections shown in are illustrative and other means of establishing a communications link between the computers may be used depending on the specific requirements of an application of database memory management.

A variety of database types are contemplated as being suitable for use with the present dynamic database memory management including databases implemented using the Microsoft SQL Server database application. In the illustrative network environment shown in an instance of SQL Server is operative on server as indicated by reference numeral .

When the users at the clients want to access the database on the server they run an application on the client computer which in this case is a SQL Server client application . Each instance of the SQL Server client application typically provides its respective client computer with the logic and code required to formulate queries and display output to the user among other functionalities.

In some settings it may be desirable to run the database application locally i.e. not in a network environment . The SQL Server application and an instance of SQL Server client application are then arranged to operate on a single computer i.e. server . In this case the SQL server client application communicates to SQL server using various interprocess communications components indicated by line instead of using network .

SQL Server database engine is configured with a buffer pool and a database engine idle detection manager . Buffer pool and database idle detection manager operate together to provide buffer management for the SQL Server database engine . In the present arrangement the buffer pool is capped to some fixed amount of the available memory installed in server as described below in the text accompanying . Processes associated with the buffer pool address memory in the user mode i.e. application partition as indicated by reference numeral .

Buffer pool is arranged to buffer recently accessed data pages to reduce the need for disk I O. Buffer pool is further operatively coupled to a disk e.g. the hard disk in hard disk drive in so that the SQL Server database engine is capable of storing and retrieving data pages on the disk . Buffer pool is also arranged with a data page target which the SQL Server database engine reserves in the addressable memory space. The data page target is arranged to set an upper limit for data pages buffered in the buffer pool .

The idle state detection manager is arranged to determine the workload i.e. a measure of the level of server operations processes and activities required to process queries updates and requests for example that is borne by the SQL Server database engine . In some applications of dynamic memory management the idle state detection manager may be configured to monitor activities as necessary to detect the occurrence of the SQL Server database engine entering an idle state. Such idle state is considered entered for example when there are no current user requests e.g. queries current external requests or pending operations. However some internal background tasks may normally still run while the SQL server database engine is considered idle. The idle state detection manager includes trimming logic which acts on the data page target as described below in the text accompanying .

The disk I O stream is provided with two write paths. The first write path is a direct flush to disk path . This path is enabled for example in the Microsoft Windows operating system by setting the FILE FLAG WRITE THROUGH flag as indicated by reference numeral in so that the CreateFile function under the Win32 API application programming interface writes the data page directly to disk. Such direct flush to disk path provides for an additional measure of data security by directly persisting database changes to the permanent storage afforded by the disk.

The second write path in the disk I O stream is a system file cache path . Data pages from the buffer pool are buffered in a system file cache . The system file cache is an area of memory which is addressed in the kernel mode i.e. operating system partition . It typically stores recently used data for all of the applications that are running on the server for quick access. The size of the system file cache depends on the amount of physical memory installed and the memory required for applications . The operating system dynamically adjusts the size of the system file cache as needed sharing memory optimally between the applications processes and the cached data.

As noted above database applications traditionally bypass the system file cache in favor of the buffer pool addressed in user mode. By contrast the present database memory management policies employ both the system file cache and buffer pool to buffer data pages. Accordingly if a data page is required and it is not stored in the buffer pool the SQL Server database engine will try to access the data page held in the system file cache before needing to go to the disk for the required data. The data pages in the system file cache are read from and written to disk as the cached I O stream as shown in .

Policy includes maximizing the amount of memory that is available to the operating system to enable it to choose which data pages are important and should thus be buffered in the system file cache . This may be beneficial to avoid displacing physical memory pages on the system and reducing the impact on other applications running on the computer system that would occur when their memory pages are displaced in physical memory.

Policy includes minimizing the amount of double buffering i.e. duplicative buffering of the same data pages between the system file cache and the buffer pool . Although the present arrangement anticipates some double buffering will necessarily occur at times and is thus accepted implementation of policy configures the buffer pool to be relatively small as described in the text accompanying to control the number of duplicate data pages on the computer system .

Policy includes flushing dirty data pages from the buffer pool to disk e.g. disk in irrespective of memory pressure that is either internally imposed by operation of the SQL Server database engine or that is imposed by the operating system . A dirty page is a data page that has been modified during processing by the SQL Server database engine . One disadvantage in waiting for such memory pressure is that launch of the other applications and other memory hungry applications will take longer because they would otherwise need to wait for the SQL Server database engine to give up memory. This often requires the use of processor cycles and waiting for disk I O.

In addition if a database engine is holding onto memory in the buffer pool the remaining available memory may be exceeded by the sum of other applications working sets the portion of the address space that is currently resident in the physical memory and addressed by processes in the application . In such cases as the applications fight to maintain their working set size the operating system switches between the applications and often has to copy each application s data to and from disk as well as copy programming code from disk to memory. Such disk access can be time consuming and slow down application execution considerably. Thus the overall impact of waiting for memory to be released from the database can be a negative user perception of computer responsiveness particularly with the lower cost general purpose computer systems as employed in this illustrative example.

At block the data page target within a buffer pool e.g. data page target in buffer pool as shown in is capped to reduce the occurrence of double buffered data pages and free up memory for operating system use. Referring to two dimensions are considered for determining the cap size for the data page target in the buffer pool. As indicated by block the greater of the two is selected. The dimensions include 1 16of the size of the available physical memory and one half of the max server memory option . The max server memory is an option implemented in SQL Server that enables a user to select the amount of memory allowed for the buffer pool used by an instance of SQL Server. However other database applications use similar user configurable buffer pool sizes. Note that the values of one half and 1 16for the physical memory and max server memory option respectively can be expected to provide an optimized buffer pool size in many typical applications where the present database memory management polices are applied. However other values may be selected according to the specific requirements of a particular setting.

Referring again to at block data pages are buffered into the capped buffer pool . At block use of the system file cache e.g. system file cache in is enabled so that the disk I O may be buffered by the operating system e.g. operating system in . Note that the system file cache is typically configured to be used by applications by default unless a FILE FLAG NO BUFFERING flag is set which guarantees that all reads will come from a file on disk and not from any buffer or cache in the operating system. Accordingly as shown in the system file cache enablement step includes not setting the no buffering flag as indicated by reference numeral so that the default system file cache is utilized as indicated by reference numeral .

Referring again to block indicates that the FILE WRITE THROUGH flag is set so that dirty data pages are flushed directly to disk in addition to being buffered as I O in the system file cache .

At block data pages are trimmed from the buffer pool . Here the trimming logic in the idle state detection manager acts on the data page target to thereby trim data pages from the data pool after a time interval of approximately 10 seconds has elapsed once the idle state detection manager determines that the SQL Server database engine is idle and is not bearing a workload. The trimming is performed iteratively and dynamically as various data pages are processed over time are read from and written to disk and buffered in response to user interaction with the database.

The 10 second value is selected so that user activity is not substantially impacted by the trimming operation. Note that the 10 second value can be expected to provide an optimized buffer memory footprint in many typical applications where the present database memory management policies are applied. However other values may be selected according to the specific requirements of a particular setting.

As shown in the trimming step includes flushing dirty pages from the buffer pool to disk as indicated by reference numeral . As noted above a dirty page is a data page that has been modified during processing by the SQL Server database engine . The trimming step further includes dropping i.e. evicting all clean data pages from the buffer pool as indicated by reference numeral . A clean data page is a data page that was not modified during processing by the SQL Server database engine . The illustrative method ends at block .

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.


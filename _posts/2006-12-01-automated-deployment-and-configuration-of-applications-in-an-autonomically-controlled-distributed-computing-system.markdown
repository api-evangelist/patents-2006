---

title: Automated deployment and configuration of applications in an autonomically controlled distributed computing system
abstract: A distributed computing system conforms to a multi-level, hierarchical organizational model. One or more control nodes provide for the efficient and automated allocation and management of computing functions and resources within the distributed computing system in accordance with the organization model. The control node includes an automation subsystem having one or more rules engines that provide autonomic control of the application nodes in accordance with a set of one or more rules. A pluggable, application-specific application governor is selected and installed within the control node to provide an application-independent interface through which the rules engines interact to control the deployment, execution and monitoring of the applications within the distributed computing system. The application governor uses a set of application image objects to configure an application on an application node. Each of the application image objects share a common interface and are therefore interchangeable.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08595331&OS=08595331&RS=08595331
owner: CA, Inc.
number: 08595331
owner_city: Islandia
owner_country: US
publication_date: 20061201
---
This application claims the benefit of U.S. provisional Application Ser. No. 60 741 117 filed Dec. 1 2005 the entire content of which is incorporated herein by reference. In addition this application claims the benefit of U.S. provisional Application Ser. No. 60 780 097 filed Mar. 8 2006 the entire content of which is corporated herein by reference.

The invention relates to computing environments and more specifically to distributed computing systems.

Distributed computing systems are increasingly being utilized to support business as well as technical applications. Typically distributed computing systems are constructed from a collection of computing nodes that combine to provide a set of processing services to implement the distributed computing applications. Each of the computing nodes in the distributed computing system is typically a separate independent computing device interconnected with each of the other computing nodes via a communications medium e.g. a network.

One challenge with distributed computing systems is the organization deployment and administration of such a system within an enterprise environment. For example it is often difficult to manage the allocation and deployment of enterprise computing functions within the distributed computing system. An enterprise for example often includes several business groups and each group may have competing and variable computing requirements.

In general the invention is directed to a distributed computing system that conforms to a multi level hierarchical organizational model. One or more control nodes provide for the efficient and automated allocation and management of computing functions and resources within the distributed computing system in accordance with the organization model.

As described herein the model includes four distinct levels fabric domains tiers and nodes that provide for the logical abstraction and containment of the physical components as well as system and service application software of the enterprise. A user such as a system administrator interacts with the control nodes to logically define the hierarchical organization of the distributed computing system. The control nodes are responsible for all levels of management in accordance with the model including fabric management domain creation tier creation and node allocation and deployment.

In one embodiment a distributed computing system comprises a plurality of application nodes that provide execution environments in which an image of a software application is executable. The distributed computing system also comprises a control node to provide autonomic control over the plurality of application nodes and a communications network to facilitate communication among the application nodes and the control node. In this embodiment the control node comprises an automation subsystem to control deployment of instances of the software application to the application nodes. The control node also comprises a set of application image objects associated with software applications. Each of the application image objects implements a set of methods to control deployment of an image of a software application. An application programming interface common to each of the application image objects defines the set of methods. In addition the control node comprises an application governor to deploy an image of a software application to the application node by invoking one or more of the methods of one of the application image objects associated with the software application. The application governor deploys the software application subject to the control of the automation subsystem.

In another embodiment a computer implemented method comprises receiving status data for a distributed computing system having a plurality of application nodes interconnected via a communications network. The status data represents an actual state of the application nodes. The application nodes provide execution environments in which an instance of a software application is executable. The method further comprises processing the status data with rules in a set of rule engines to determine operations for reducing any difference between an expected state and the actual state of the distributed computing system. In addition the method comprises maintaining a set of application image objects associated with images of software applications wherein each of the application image objects implements a set of methods to control deployment of images of a software application and wherein an application programming interface common to each of the application image objects defines the set of methods The method also comprises directing in accordance with the determined operations an application governor to deploy an image of a software application to one of the application nodes by invoking one or more of the methods of the application image objects associated with the software application.

In another embodiment the invention is directed to a computer readable medium comprising instructions. The instructions cause the processor to receive status data for a distributed computing system having a plurality of application nodes interconnected via a communications network. The status data represents an actual state of the application nodes. The application nodes provide execution environments in which an instance of a software application is executable. The instructions further cause the processor to process the status data with rules in a set of rule engines to determine operations for reducing any difference between an expected state and the actual state of the distributed computing system. The instructions cause the processor to maintain a set of application image objects associated with images of software applications. Each of the application image objects implements a set of methods to control deployment of images of a software application. An application programming interface common to each of the application image objects defines the set of methods. In addition the instructions cause the processor to direct in accordance with the determined operations an application governor to deploy an image of a software application to one of the application nodes by invoking one or more of the methods of the application image objects associated with the image of the software application.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

In the illustrated example the collection of computing nodes forming distributed computing system are logically grouped within a discovered pool a free pool an allocated tiers and a maintenance pool . In addition distributed computing system includes at least one control node .

Within distributed computing system a computing node refers to the physical computing device. The number of computing nodes needed within distributed computing system is dependent on the processing requirements. For example distributed computing system may include 8 to 512 computing nodes or more. Each computing node includes one or more programmable processors for executing software instructions stored on one or more computer readable media.

Discovered pool includes a set of discovered nodes that have been automatically discovered within distributed computing system by control node . For example control node may monitor dynamic host communication protocol DHCP leases to discover the connection of a node to network . Once detected control node automatically inventories the attributes for the discovered node and reassigns the discovered node to free pool . The node attributes identified during the inventory process may include a CPU count a CPU speed an amount of memory e.g. RAM local disk characteristics or other computing resources. Control node may also receive input identifying node attributes not detectable via the automatic inventory such as whether the node includes I O such as HBA. Further details with respect to the automated discovery and inventory processes are described in U.S. patent application Ser. No. 11 070 851 entitled AUTOMATED DISCOVERY AND INVENTORY OF NODES WITHIN AN AUTONOMIC DISTRIBUTED COMPUTING SYSTEM filed Mar. 2 2005 the entire content of which is hereby incorporated by reference.

Free pool includes a set of unallocated nodes that are available for use within distributed computing system . Control node may dynamically reallocate an unallocated node from free pool to allocated tiers as an application node . For example control node may use unallocated nodes from free pool to replace a failed application node or to add an application node to allocated tiers to increase processing capacity of distributed computing system .

In general allocated tiers include one or more tiers of application nodes that are currently providing a computing environment for execution of user software applications. In addition although not illustrated separately application nodes may include one or more input output I O nodes. Application nodes typically have more substantial I O capabilities than control node and are typically configured with more computing resources e.g. processors and memory . Maintenance pool includes a set of nodes that either could not be inventoried or that failed and have been taken out of service from allocated tiers .

Control node provides the system support functions for managing distributed computing system . More specifically control node manages the roles of each computing node within distributed computing system and the execution of software applications within the distributed computing system. In general distributed computing system includes at least one control node but may utilize additional control nodes to assist with the management functions.

Other control nodes not shown in are optional and may be associated with a different subset of the computing nodes within distributed computing system . Moreover control node may be replicated to provide primary and backup administration functions thereby allowing for graceful handling a failover in the event control node fails.

Network provides a communications interconnect for control node and application nodes as well as discovered nodes unallocated nodes and failed nodes. Communications network permits internode communications among the computing nodes as the nodes perform interrelated operations and functions. Communications network may comprise for example direct connections between one or more of the computing nodes one or more customer networks maintained by an enterprise local area networks LANs wide area networks WANs or a combination thereof. Communications network may include a number of switches routers firewalls load balancers and the like.

In one embodiment each of the computing nodes within distributed computing system executes a common general purpose operating system. One example of a general purpose operating system is the Windows operating system provided by Microsoft Corporation. In some embodiments the general purpose operating system such as the Linux kernel may be used.

In the example of control node is responsible for software image management. The term software image refers to a complete set of software loaded on an individual computing node to provide an execution environment for one or more applications. The software image including the operating system and all boot code middleware files and may include application files. As described below embodiments of the invention provide application level autonomic control over the deployment execution and monitoring of applications onto software images associated with application nodes .

System administrator may interact with control node and identify the particular types of software images to be associated with application nodes . Alternatively administration software executing on control node may automatically identify the appropriate software images to be deployed to application nodes based on the input received from system administrator . For example control node may determine the type of software image to load onto an application node based on the functions assigned to the node by system administrator . Application nodes may be divided into a number of groups based on their assigned functionality. As one example application nodes may be divided into a first group to provide web server functions a second group to provide business application functions and a third group to provide database functions. The application nodes of each group may be associated with different software images.

Control node provides for the efficient allocation and management of the various software images within distributed computing system . In some embodiments control node generates a golden image for each type of software image that may be deployed on one or more of application nodes . As described herein the term golden image refers to a reference copy of a complete software stack for providing an execution environment for applications.

System administrator may create a golden image by installing an operating system middleware and software applications on a computing node and then making a complete copy of the installed software. In this manner a golden image may be viewed as a master copy of the software image for a particular computing function. Control node maintains a software image repository that stores the golden images associated with distributed computing system .

Control node may create a copy of a golden image referred to as an image instance for each possible image instance that may be deployed within distributed computing system for a similar computing function. In other words control node pre generates a set of K image instances for a golden image where K represents the maximum number of image instances for which distributed computing system is configured for the particular type of computing function. For a given computing function control node may create the complete set of image instance even if not all of the image instances will be initially deployed. Control node creates different sets of image instances for different computing functions and each set may have a different number of image instances depending on the maximum number of image instances that may be deployed for each set. Control node stores the image instances within software image repository . Each image instance represents a collection of bits that may be deployed on an application node.

Further details of software image management are described in co pending U.S. patent application Ser. No. 11 046 133 entitled MANAGEMENT OF SOFTWARE IMAGES FOR COMPUTING NODES OF A DISTRIBUTED COMPUTING SYSTEM filed Jan. 28 2005 and co pending U.S. patent application Ser. No. 11 046 152 entitled UPDATING SOFTWARE IMAGES ASSOCIATED WITH A DISTRIBUTED COMPUTING SYSTEM filed Jan. 28 2005 each of which is incorporated herein by reference in its entirety.

In general distributed computing system conforms to a multi level hierarchical organizational model that includes four distinct levels fabric domains tiers and nodes. Control node is responsible for all levels of management including fabric management domain creation tier creation and node allocation and deployment.

As used herein the fabric level generally refers to the logical constructs that allow for definition deployment partitioning and management of distinct enterprise applications. In other words fabric refers to the integrated set of hardware system software and application software that can be knitted together to form a complete enterprise system. In general the fabric level consists of two elements fabric components or fabric payload. Control node provides fabric management and fabric services as described herein.

In contrast a domain is a logical abstraction for containment and management within the fabric. The domain provides a logical unit of fabric allocation that enables the fabric to be partitioned amongst multiple uses e.g. different business services.

Domains are comprised of tiers such as a 4 tier application model web server application server business logic persistence layer or a single tier monolithic application. Fabric domains contain the free pool of devices available for assignment to tiers.

A tier is a logically associated group of fabric components within a domain that share a set of attributes usage availability model or business service mission. Tiers are used to define structure within a domain e.g. N tier application and each tier represents a different computing function. A user such as administrator typically defines the tier structure within a domain. The hierarchical architecture may provide a high degree of flexibility in mapping customer applications to logical models which run within the fabric environment. The tier is one construct in this modeling process and is the logical container of application resources.

The lowest level the node level includes the physical components of the fabric. This includes computing nodes that as described above provide operating environments for system applications and enterprise software applications. In addition the node level may include network devices e.g. Ethernet switches load balancers and firewalls used in creating the infrastructure of network . The node level may further include network storage nodes that are network connected to the fabric.

System administrator accesses administration software executing on control node to logically define the hierarchical organization of distributed computing system . For example system administrator may provide organizational data to develop a model for the enterprise and logically define the enterprise fabric. System administrator may for instance develop a model for the enterprise that includes a number of domains tiers and node slots hierarchically arranged within a single enterprise fabric.

More specifically system administrator defines one or more domains that each correspond to a single enterprise application or service such as a customer relation management CRM service. System administrator further defines one or more tiers within each domain that represent the functional subcomponents of applications and services provided by the domain. As an example system administrator may define a storefront domain within the enterprise fabric that includes a web tier an application tier and a database tier. In this manner distributed computing system may be configured to automatically provide web server functions business application functions and database functions.

For each of the tiers control node creates a number of node slots equal to the maximum number of application nodes that may be deployed. In general each node slot represents a data set that describes specific information for a corresponding node such as software resources for a physical node that is assigned to the node slot. The node slots may for instance identify a particular software image instance associated with an application node as well as a network address associated with that particular image instance.

In this manner each of the tiers include one or more node slots that reference particular software image instances to boot on the application nodes to which each software image instance is assigned. The application nodes to which control node assigns the image instances temporarily inherit the network address assigned to the image instance for as long as the image instance is deployed on that particular application node. If for some reason the image instance is moved to a different application node control node A moves the network address to that new application node.

System administrator may further define specific node requirements for each tier of the fabric. For example the node requirements specified by system administrator may include a central processing unit CPU count a CPU speed an amount of memory e.g. RAM local disk characteristics and other hardware characteristics that may be detected on the individual computing nodes. System administrator may also specify user defined hardware attributes of the computing nodes such as whether I O like HBA is required. The user defined hardware attributes are typically not capable of detection during an automatic inventory. In this manner system administrator creates a list of attributes that the tier requires of its candidate computing nodes. In addition particular node requirements may be defined for software image instances.

In addition to the node requirements described above system administrator may further define policies that are used when re provisioning computing nodes within the fabric. System administrator may define policies regarding tier characteristics such as a minimum number of nodes a tier requires an indication of whether or not a failed node is dynamically replaced by a node from free pool a priority for each tier relative to other tiers an indication of whether or not a tier allows nodes to be re provisioned to other tiers to satisfy processing requirements by other tiers of a higher priority or other policies. Control node uses the policy information input by system administrator to re provision computing nodes to meet tier processing capacity demands.

After receiving input from system administrator defining the architecture and policy of the enterprise fabric control node identifies unallocated nodes within free pool that satisfy required node attributes. Control node automatically assigns unallocated nodes from free pool to respective tier node slots of a tier. As will be described in detail herein in one embodiment control node may assign computing nodes to the tiers in a best fit fashion. Particularly control node assigns computing nodes to the tier whose node attributes most closely match the node requirements of the tier as defined by administrator . The assignment of the computing nodes may occur on a tier by tier basis beginning with a tier with the highest priority and ending with a tier with the lowest priority. Alternatively or in addition assignment of computing nodes may be based on dependencies defined between tiers.

As will be described in detail below control node may automatically add unallocated nodes from free pool to a tier when more processing capacity is needed within the tier remove nodes from a tier to the free pool when the tier has excess capacity transfer nodes from tier to tier to meet processing demands or replace failed nodes with nodes from the free pool. Thus computing resources i.e. computing nodes may be automatically shared between tiers and domains within the fabric based on user defined policies to dynamically address high processing demands failures and other events.

Distributed computing system may provide one or more advantages. For example the method of delivering computing as a service described here is based on providing application services independent of the computational infrastructure which generates the services. This is accomplished through a software infrastructure management facility which guarantees application service delivery by implementing service level automation.

This concept is itself based upon two concepts The Service Delivery Model and Service Level Automation.

1. Service Delivery Model Computing as a service commonly referred to as utility computing is based upon the ability of the consumer to be guaranteed availability of the quantity and quality of Applications Services AS required at the time and location necessary independent of the type of service provided e.g. audio video application programs etc . This can only be achieved if the AS delivered are provided independent of the location and condition of the physical and logical infrastructure which generates these services limited only by total capability and capacity available. For the purpose of this disclosure this concept shall be referred to as the Service Delivery Model SDM .

The physical and logical infrastructure facilities described in the preceding paragraph include all of the computing storage and communication hardware and related software infrastructure the application software and application containers and all other enabling facilities and services which are part of the environment necessary to generate and deliver AS . For the purpose this disclosure this concept shall be referred to as the Computing Infrastructure CI .

2. Service Level Automation In order to implement the SDM an Infrastructure Management Facility IMF independent of the CI provides service level automation.

The purpose of the IMF is to implement the SDM for single and or multiple application service flows for which it automatically and dynamically adjusts AS levels based on its policy automation facility. This facility shall automate AS delivery based upon dynamically extensible policies which include but are not limited to customer location priority duration date and time of day service type business demands and conditions quantity and quality of service. The policy facility also enables independent control of the service by the service provider and by the consumer and allow the service consumer to in turn be a service provider. Finally the IMF measures both efficiency and cost as reflected in the behavior of the product namely the ability to make decisions taking into account the best uses of the resources on a dynamic basis in order to choose between functionally equivalent alternatives that make the best usage of the economics of the infrastructure at the time. For the purpose of this disclosure this concept shall be referred to as the Service Level Automation SLA .

A key value of this concept is that it enables the separation of all aspects of the implementation of the CI from the delivery of application services. This allows the consumer of application services to not have to tradeoff risk business agility and cost. Its implementation is based upon a new abstraction which operates independent of existing software categories e.g. applications middleware data base operating systems and system management which for the purposes of this disclosure shall be referred to as Metaware.

Embodiments of an autonomic distributed system are described that provide an Infrastructure Management Facility IMF . The IMF is the Metaware which regulates the delivery of AS through its SLA independent of the CI which generate the services thus enabling the SDM. As application services are the result of applications running within the CI it is the application service levels that are automated and made independent of all aspects of the CI within the limits of the capability and capacity available for delivery to the consumer. The SLA provides extensible dynamic policy driven control of service delivery for both service providers and service consumers who themselves could also be service providers. In one embodiment the IMF includes at least the following six functional design capabilities 

1. Application Service Independence The basic concept which enables the IMF to delivery AS s is its ability to dynamically manage SLA independent of the CI. A key element of this is that applications themselves are independent and unmodified. This is accomplished through the IMF s ability to abstract the physical and logical computing infrastructure from the quantity and quality of application services generated. It is believed that without this capability it will not be possible to implement a fully general utility computing model.

2. Scale Independence The incremental overhead of the IMF required of any server included in the CI described herein is independent of the number of servers being managed. Unless this requirement can be met it will limit the scale of utility computing such that it will not be commercially viable for large scale utility computing.

3. Container Capacity Metering Application service independence is achieved by dynamically metering the higher level software facilities of the CI which generate the application services service flow and flux. For the purpose of this paper these facilities shall be referred to as Containers. Examples of containers include but are not limited to virtual machine managers operating systems and web application servers. Furthermore containers often contain other containers which in turn are managed by the IMF. The described embodiments of the system generalize the AS independent of the type of infrastructure that generate these services.

4. Policy Automation The IMF delivers the required SLA through its extensible policy management facility which dynamically adjusts the application service levels to implement user defined policy limited only by the capacity and capability of the available CI. This policy management facility is capable of providing guaranteed service levels for single and or multiple application service flow which is automatically and dynamically adjusted based on user defined and extensible policies in response to changes in the environment. Examples of policies include but are not limited to customer location service type cost and or pricing quantity and quality of service priority duration date and time of day business demands and conditions. The policy facility also allows the service provider to manage the delivery and meter the capacity that they delivery to individual customer s independent of the customer s ability to set and manage their own policy of how their capacity is utilized. Furthermore customers of the services can themselves in turn be providers of services. This can be effectively done by leveraging Application Service Independence.

5. Dynamic Provisioning The IMF described herein supports the ability to dynamically provision at boot and run time all software in order to support installation capacity management service levels and other policies and software upgrade. It is believed that this is original in offering an application service independent CI architecture that employs separation for the purpose of creating flexibility in deployment of a single instance of a service.

6. Data Capture Capability The IMF described herein acquires the data necessary to enable the service provider to implement the metering and auditing facilities necessary to control audit bill and account for services provided in complete detail. The ability to capture this data can also be used to educate the system about the performance characteristics of combinations of resources to allow it to improve the quality of its scheduling decisions over time enhancing its ability to provide the requisite services with high efficiency.

The described system allows customers to conveniently acquire the guaranteed quantity and quality of AS s required at competitive market driven rates with the ability to directly control the IMF s SLA capability in order to maximize their operations efficiency and effectiveness without having to tradeoff operation risk cost and business agility. Potential advantages include but are not limited to 

1. Efficiency and Effectiveness The customer need not manage the technology and can focus on the automation of the application service to improve their efficiency and effectiveness without having to tradeoff operation risk cost and business agility which is enabled through the IMF s SLA capability.

2. Capabilities Convenience By leveraging the Internet as a generic global delivery facility and separating the CI from the AS delivery the IMF simplifies the consumer s life in several ways 

i. Complexity Eliminates the complexity of purchasing installing configuring managing and administering the CI.

ii. Access Provides ubiquitous access to the full range of computing services wherever and whenever desired from any client capable of accessing the Internet.

iii. Service Provider Independence Consumers are free to change providers by simply transferring their software images to a new provider.

iv. Service Management Enables the consumer to in turn provide services to their customers. For example a corporation would be able to regulate and bill their individual business units for the capacity consumed.

3. Cost By logically abstracting the AS s from the CI the IMF will enable a competitive service provider industry that includes 

i. Capacity Utilization Customers may only be charged for capacity actually utilized. Today customers pay for the maximum capacity they would need at peak load.

ii. Choice of Suppliers As the basic CI facilities will be generic the customer may have choice of suppliers without the level lock in current vendors enjoy. This is enabled through the IMF s image and policy management implementation which allows portability across service providers through simply transferring the files to the new provider and initiating it with the IMF Metaware. This will drive the pricing competition. The IMF s ability to provide AS s independent of the CI enables choice of service provider.

iii. Capital Expense Leverage The customer need not have to own the capital equipment or software and the provider will be motivated to use the most cost effective components and amortize it across the customer base the cost to the consumer will be much lower.

iv. Operations Expense Leverage As the IMF automates most of the IT Operations functions these costs will drop dramatically.

1. The IMF may be used to implement a service delivery model for AS s independent of the CI or of the service types provided. This capability enables the service provider to deliver SLA on a global basis utilizing generic CI for all types of digital services including but not limited traditional applications and service flow processing digital audio and digital video services independently to any consumer type e.g. individual organization or corporation as long as the consumer has access to sufficient capacity of broadband communications for the services levels require. This capability of the IMF also allows a service consumer to in turn be a service provider as well. This cannot be done without Application Service Independence.

2. The IMF described herein enables the service provider to meter the AS s delivered by the capacity of CI consumed by customer in order to control audit bill and account for the services provided to multiple individual consumers. This capability represents a uniform model of SLA independent of the capacity of CI for all types of digital service AS . The resulting pricing model reflects the goal of utility computing by enabling the service provider to bill based upon any combination of AS delivery parameters including but not limited to customer type of service duration date and or time of day location point of delivery quantity and quality of service and excess or reserved capacity.

i. Utilization By managing the CI with the IMF the service provider will be able to maintain much higher average utilization levels thus dramatically reducing the amount of hardware purchased software licensed and operations expenses.

ii. Global Service Demand Levels Service demand for any organization varies primarily by local time of day. By leveraging the global reach of the internet the service provider will be able to much more effectively manage the utilization of the CI necessary to meet demand on a global follow the sun basis.

iii. Service Digitization As all services will eventually be digital they can be delivered with a generic CI architecture.

iv. Value Added Services Service providers will be free to provide any types and bundles of value added service beyond basic application services. A few examples include Virtual Client The bundle of personal computer services including applications storage security administration and related services Software as A Service and Compliance Verification and Certification

v. Hardware Commoditization The ability to implement services utilizing generic low cost servers storage facilities and network switches. These hardware elements are undergoing dramatic commoditization which allows for much lower capital expenses then in the past. This commoditization will continue for many years to come making capital expenses a much smaller part of the fixed costs for the service provider.

vi. Automation of Operations One of the primary economic values of the IMF is that it automates most of the runtime operations costs of the CI. Operations expenses are the majority of the costs of today once they are automated by the IMF these costs will be dramatically reduced.

Thus by leveraging the IMF s ability to dramatically reduce the capital and operations expenses the service provider will be able to implement an SDM which will easily justify the consumers switching cost.

vii. Software Commoditization Software pricing is being commoditized based upon several factors which will continue to accelerate. These factors include 

01. Service Oriented Architecture SOA which accelerates innovation opens it to incremental independent innovation eliminates down stream lock in by software vendors and makes it possible to use commodity servers to run applications that previously required large expensive servers. SOA also reduces complex applications to their most fundamental independently divisible functional services which will be the basis for the complete commoditization of software further leveraging the open source movement.

02. Open Source Software Open source serves as an accelerator for the commoditization of all type of software by both compromising the pricing power of software and serving as a reusable model of software services which will further compromise the value of horizontal software services.

03. Good Enough Application services are becoming more and more generic thus accelerating their commoditization further in combination with SOA and Open Source.

In this example e commerce fabric includes a storefront domain A and a financial planning domain B. Storefront domain A corresponds to the enterprise storefront domain and allows customers to find and purchase products over a network such as the Internet. Financial planning domain B allows one or more employees to perform financial planning tasks for the enterprise.

Tier level C includes one or more tiers within each domain that represent the functional subcomponents of applications and services provided by the domain. For example storefront domain A includes a web server tier labeled web tier A a business application tier labeled app tier B and a database tier labeled DB tier C. Web server tier A business application tier B and database tier C interact with one another to present a customer with an online storefront application and services. For example the customer may interact with web server tier A via a web browser. When the customer searches for a product web server tier A may interacts with business application tier B which may in turn access a database tier C. Similarly financial planning domain B includes a financial planning tier D that provides subcomponents of applications and services of the financial planning domain B. Thus in this example a domain may include a single tier.

Tier level D includes one or more logical node slots A H node slots within each of the tiers. Each of node slots include node specific information such as software resources for an application node that is assigned to a respective one of the node slots . Node slots may for instance identify particular software image instances within image repository and map the identified software image instances to respective application nodes . As an example node slots A and B belonging to web server tier A may reference particular software image instances used to boot two application nodes to provide web server functions. Similarly the other node slots may reference software image instances to provide business application functions database functions or financial application functions depending upon the tier to which the node slots are logically associated.

Although in the example of there are two node slots corresponding to each tier the tiers may include any number of node slots depending on the processing capacity needed on the tier. Furthermore not all of node slots may be currently assigned to an application node . For example node slot B may be associated with an inactive software image instance and when needed may be assigned to an application node for deployment of the software image instance.

In this example organizational data associates free node pool with the highest level of the model i.e. e commerce fabric . As described above control node may automatically assign unallocated nodes from free node pool to at least a portion of tier node slots of tiers as needed using the best fit algorithm described above or another algorithm. Additionally control node may also add nodes from free pool to a tier when more processing capacity is needed within the tier remove nodes from a tier to free pool when a tier has excess capacity transfer nodes from tier to tier to meet processing demands and replace failed nodes with nodes from the free tier.

Although not illustrated the model for the enterprise fabric may include multiple free node pools. For example the model may associate free node pools with individual domains at the domain level or with individual tier levels. In this manner administrator may define policies for the model such that unallocated computing nodes of free node pools associated with domains or tiers may only be used within the domain or tier to which they are assigned. In this manner a portion of the computing nodes may be shared between domains of the entire fabric while other computing nodes may be restricted to particular domains or tiers.

During this process control node may receive input specifying node requirements of each of the tiers of the hierarchical model . As described above administrator may specify a list of attributes e.g. a central processing unit CPU count a CPU speed an amount of memory e.g. RAM or local disk characteristics that the tiers require of their candidate computing nodes. In addition control node may further receive user defined custom attributes such as requiring the node to have I O such as HBA connectivity. The node requirements or attributes defined by system administrator may each include a name used to identify the characteristic a data type e.g. integer long float or string and a weight to define the importance of the requirement.

Control node identifies the attributes for all candidate computing nodes within free pool or a lower priority tier . As described above control node may have already discovered the computing nodes and inventoried the candidate computing nodes to identify hardware characteristics of all candidate computing nodes. Additionally control node may receive input from system administrator identifying specialized capabilities of one or more computing nodes that are not detectable by the inventory process.

Control node dynamically assigns computing nodes to the node slots of each tier based on the node requirements specified for the tiers and the identified node attributes . Population of the node slots of the tier may be performed on a tier by tier basis beginning with the tier with the highest priority i.e. the tier with the highest weight assigned to it. As will be described in detail in one embodiment control node may populate the node slots of the tiers with the computing nodes that have attributes that most closely match the node requirements of the particular tiers. Thus the computing nodes may be assigned using a best fit algorithm.

Next control node retrieves the node requirements associated with the selected tier . Control node may for example maintain a database having entries for each node slot where the entries identify the node requirements for each of the tiers. Control node retrieves the node requirements for the selected tier from the database.

In addition control node accesses the database and retrieves the computing node attributes of one of the unallocated computing nodes of free pool . Control node compares the node requirements of the tier to the node attributes of the selected computing node .

Based on the comparison control node determines whether the node attributes of the computing node meets the minimum node requirements of the tier . If the node attributes of the selected computing node do not meet the minimum node requirements of the tier then the computing node is removed from the list of candidate nodes for this particular tier . Control node repeats the process by retrieving the node attributes of another of the computing nodes of the free pool and compares the node requirements of the tier to the node attributes of the computing node.

If the node attributes of the selected computing node meet the minimum node requirements of the tier YES of control node determines whether the node attributes are an exact match to the node requirements of the tier . If the node attributes of the selected computing node and the node requirements of the tier are a perfect match YES of the computing node is immediately assigned from the free pool to a node slot of the tier and the image instance for the slot is associated with the computing node for deployment .

Control node then determines whether the node count for the tier is met . Control node may for example determine whether the tier is assigned the minimum number of nodes necessary to provide adequate processing capabilities. In another example control node may determine whether the tier is assigned the ideal number of nodes defined by system administrator . When the node count for the tier is met control node selects the next tier to enable e.g. the tier with the next largest priority and repeats the process until all defined tiers are enabled i.e. populated with application nodes .

If the node attributes of the selected computing node and the node requirements of the tier are not a perfect match control node calculates and records a processing energy of the node . As used herein the term processing energy refers to a numerical representation of the difference between the node attributes of a selected node and the node requirements of the tier. A positive processing energy indicates the node attributes more than satisfy the node requirements of the tier. The magnitude of the processing energy represents the degree to which the node requirements exceed the tier requirements.

After computing and recording the processing energy of the nodes control node determines whether there are more candidate nodes in free pool . If there are additional candidate nodes control node repeats the process by retrieving the computing node attributes of another one of the computing nodes of the free pool of computing nodes and comparing the node requirements of the tier to the node attributes of the computing node .

When all of the candidate computing nodes in the free pool have been examined control node selects the candidate computing node having the minimum positive processing energy and assigns the selected computing node to a node slot of the tier . Control node determines whether the minimum node count for the tier is met . If the minimum node count for the tier has not been met control node assigns the computing node with the next lowest calculated processing energy to the tier . Control node repeats this process until the node count is met. At this point control node selects the next tier to enable e.g. the tier with the next largest priority .

In the event there are an insufficient number of computing nodes in free pool or an insufficient number of computing nodes that meet the tier requirements control node notifies system administrator . System administrator may add more nodes to free pool add more capable nodes to the free pool reduce the node requirements of the tier so more of the unallocated nodes meet the requirements or reduce the configured minimum node counts for the tiers.

Control node then determines whether there are any computing nodes in the free pool of nodes that meet the minimum node requirements of the tier . When there are one or more nodes that meet the minimum node requirements of the tier control node selects the node from the free pool based the node requirements of the tier as described above and assigns the node to the tier . As described in detail with respect to control node may determine whether there are any nodes that have node attributes that are an exact match to the node requirements of the tier. If an exact match is found the corresponding computing node is assigned to a node slot of the tier. If no exact match is found control node computes the processing energy for each node and assigns the computing node with the minimum processing energy to the tier. Control node remotely powers on the assigned node and remotely boots the node with the image instance associated with the node slot. Additionally the booted computing node inherits the network address associated with the node slot.

If there are no adequate computing nodes in the free pool i.e. no nodes at all or no nodes that match the minimal node requirements of the tier control node identifies the tiers with a lower priority than the tier needing more processing capacity .

Control node determines which of the nodes of the lower priority tiers meet the minimum requirements of the tier in need of processing capacity . Control node may for example compare the attributes of each of the nodes assigned to node slots of the lower priority tiers to the node requirements of the tier in need of processing capacity. Lower priority tiers that have the minimum number of computing nodes may be removed from possible tiers from which to harvest an application node. If however all the lower priority tiers have the minimum number of computing nodes defined for the respective tier the lowest priority tier is selected from which to harvest the one or more nodes.

Control node calculates the processing energy of each of the nodes of the lower priority tiers that meet the minimum requirements . The energies of the nodes are calculated using the differences between the node attributes and the node requirements of the tier needing additional capacity. Control node selects the computing node with the lowest processing energy that meets the minimum requirements and assigns the selected computing node to the tier in need of processing capacity .

When harvesting a node control node calculates the processing energy of all the nodes in the tier as described above with reference to . Control node identifies the node within the tier with the highest processing energy and returns the identified node to the free pool of nodes . As described above the node with the highest processing energy corresponds to the node whose node attributes are the most in excess of the node requirements of the tier.

Returning the node to the free pool may involve remotely powering off the computing node and updating the database to associate the harvested node with free pool . In addition control node updates the database to disassociate the returned node with the node slot to which it was assigned. At this point the node no longer uses the network address associated with the image instance mapped to the node slot. Control node may therefore assign a temporary network address to the node while the node is assigned to free pool .

When configuring a tier system administrator may elect to activate email alerts. For example system administrator may activate the email alerts feature in order to receive email alerts providing system administrator with critical and or non critical tier information such as a notification that a tier has been upgraded a node of the tier has failed or the like. Furthermore system administrator may input various policies such node failure rules. For example system administrator may identify whether control node should reboot a node in case of failure or whether the failed node should automatically be moved to maintenance pool . Similarly system administrator may identify whether nodes assigned to the tier may be harvested by other tiers.

User interface may also allow system administrator to input node requirements of a tier. In order to input node requirements of a tier system administrator may click on the Requirements tab causing user interface to present an input area to particular node requirements of the tier.

User interface also displays properties that control node has identified during the computing node inventory process. In this example user interface presents system administrator with the a CPU node count a CPU speed the amount of RAM the disk size and other characteristics that are identifiable during the automated node inventory. User interface additionally presents interface information to system administrator . Specifically user interface provides system administrator with a list of components and their associated IP and MAC addresses.

User interface also allows system administrator to define other custom requirements. For example system administrator may define one or more attributes and add those attributes to the list of node attributes presented to system administrator .

System administrator or another user may select an image by clicking on the box in front of the image identifier name and perform one or more actions on the image. Actions that system administrator may perform on an image include deleting the image updating the image and the like. System administrator may select one of the image actions via dropdown menu . In some embodiments user interface may further display other details about the images such as the node to which the images are assigned if the node status is active the network address associated with the images and the like.

For each node user interface presents a node ID a status of the node the tier to which the node belongs a hostname associated with the node a NIC IP address a rack location a slot location the number of CPU s of the node the amount of RAM on the node the number of disks on the node whether the node has I O HBA and the number of NICs of the node.

System administrator or other user may select a node by clicking on the box in front of the node identifier name and perform one or more actions on the node. Actions that system administrator may perform on the node include deleting the node updating the node attributes or other properties of the node and the like. System administrator may select one of the node actions via dropdown menu .

System administrator or other user may select an alert by clicking on the box in front of the logged alert and perform one or more actions on the logged alert. Actions that system administrator may perform include deleting the alert changing the status of the alert or the like. System administrator may specify the log actions via dropdown menu .

Monitoring subsystem provides real time monitoring of the distributed computing system . In particular monitoring subsystem dynamically collects status data from the hardware and software operating within distributed computing system and feeds the status data in the form of monitor inputs to SLAI . Monitoring inputs may be viewed as representing the actual state of the fabric defined for the organizational model implemented by distributed computing system . Monitoring subsystem may utilize well defined interfaces e.g. the Simple Network Management Protocol SNMP and the Java Management Extensions JMX to collect and export real time monitoring information to SLAI .

SLAI may be viewed as an automation subsystem that provides support for autonomic computing and acts as a central nervous system for the controlled fabric. In general SLAI receives monitoring inputs from monitoring subsystem analyzes the inputs and outputs appropriate action requests to BLT . In one embodiment SLAI is a cybernetic system that controls the defined fabric via feedback loops. More specifically administrator may interact with BLT to define an expected state for the fabric. BLT communicates expected state to SLAI . SLAI receives the monitoring inputs from monitoring subsystem and applies rules to determine the most effective way of reducing the differences between the expected and actual states for the fabric.

For example SLAI may apply a rule to determine that a node within a high priority tier has failed and that the node should be replaced by harvesting a node from a lower priority tier. In this example SLAI outputs an action request to invoke BLT to move a node from one tier to the other.

In general BLT implements high level business operations on fabrics domains and tiers. SLAI invokes BLT to bring the actual state of the fabric into accordance with the expected state. In particular BLT outputs fabric actions to perform the physical fabric changes. In addition BLT outputs an initial expected state to SLAI and initial monitoring information to SLAI and monitoring subsystem respectively. In addition BLT outputs notifications to SLAI and monitoring subsystem to indicate the state and monitoring changes to distributed computing system . As one example BLT may provide control operations that can be used to replace failed nodes. For example BLT may output an action request indicating that a node having address 10.10.10.10 has been removed from tier ABC and a node having address 10.10.10.11 has been added to tier XYZ. In response monitoring subsystem stops attempting to collect status data from node 10.10.10.10 and starts monitoring for status data from node 10.10.10.11. In addition SLAI updates an internal model to automatically associate monitoring inputs from node 10.10.10.11 with tier XYZ.

As described further below monitoring subsystem employs a modular architecture that allows new detection and monitoring collectors to be plugged in for existing and new protocols and for existing and new hardware and software. As illustrated in monitoring subsystem provides a plug in architecture that allows different information collectors to be installed. In general collectors are responsible for protocol specific collection of monitoring information. The plug in architecture allows for new protocols to be added by simply adhering to a collector plug in signature. In this example monitoring subsystem includes collectors A and B for collecting information from operating systems and applications executing on nodes within tier A and tier B respectively.

In one embodiment collectors are loaded at startup of control node and are configured with information retrieved from BLT . Monitoring engine receives collection requests from SLAI sorts and prioritizes the requests and invokes the appropriate one of collectors based on the protocol specified in the collection requests. The invoked collector is responsible for collecting the required status data and returning the status data to monitoring engine . If the collector is unable to collect the requested status data the collector returns an error code.

In one embodiment collectors are Java code compiled into a jar file and loaded with a class loader at run time. Each of collectors has an associated configuration file written in a data description language such as the extensible markup language XML . In addition a user may interact with BLT to add run time configuration to dynamically configure collectors for specific computing environments. Each of collectors expose an application programming interface API to monitoring engine for communication and data exchange.

A user such as a system administrator specifies the protocol or protocols to be used for monitoring a software image when the image is created. In addition the users may specify the protocols to be used for monitoring the nodes and each service executing on the nodes. Example protocols supported by the collectors include Secure Shell SSH Simple Network Management Protocol SNMP Internet Control Message Protocol ICMP ping Java Management Extensions JMX and the Hypertext Transfer Protocol HTTP .

Some protocols require special privileges e.g. root privileges to perform the required data collection. In this case the corresponding collectors communicate with a separate process that executes as the root. Moreover some protocols may require deployment and or configuration of data providers within the fabric. Software agents may for example be installed and configured on nodes and configured on other hardware. If needed custom in fabric components may be deployed.

In this example the modular architecture of monitoring subsystem also supports one or more plug in interfaces for data collection from a wide range of third party monitoring systems . Third party monitoring systems monitor portions of the fabric and may be vendor specific.

In general sensor subsystem receives actual state data from monitoring subsystem in the form of monitoring inputs and supplies ongoing dynamic input data to analysis subsystem . For example sensor subsystem is notified of physical changes to distributed computing system by monitoring subsystem . Sensor subsystem uses the state data received from monitoring subsystem to maintain ongoing calculated values that can be sent to analysis subsystem in accordance with scheduler .

In one embodiment sensor subsystem performs time based hierarchical data aggregation of the actual state data in accordance with the defined organization model. Sensor subsystem maintains organizational data in a tree like structure that reflects the current configuration of the hierarchical organization model. Sensor subsystem uses the organizational data to perform the real time data aggregation and map tiers and domains to specific nodes. Sensor subsystem maintains the organizational data based on notifications received from BLT .

Sensor subsystem sends inputs to analysis subsystem to communicate the aggregated data on a periodic or event driven basis. Analysis subsystem may register an interest in a particular aggregated data value with sensor subsystem and request updates at a specified frequency. In response sensor subsystem interacts with monitoring subsystem and scheduler to generate the aggregated data required by analysis subsystem .

Sensor subsystem performs arbitrary data aggregations via instances of plug in classes referred to as triggers that define the aggregations. Each trigger is registered under a compound name based on the entity being monitored and the type of data being gathered. For example a trigger may be defined to aggregate and compute an average computing load for a tier every five minutes. Analysis subsystem requests the aggregated data based on the registered names. In some embodiments analysis subsystem may define calculations directly and pass them to sensor subsystem dynamically.

Analysis subsystem is composed of a plurality of forward chaining rule engines A N. In general rule engines match patterns in a combination of configuration data and monitoring data which is presented by extraction agent in the form of events. Events contain the aggregated data values that are sent to rule engines in accordance with scheduler .

Sensor subsystem may interact with analysis subsystem via trigger listeners that receives updates from a trigger within sensor subsystem when specified events occur. An event may be based on system state e.g. a node transitioning to an up or down state or may be time based.

Analysis subsystem allows rule sets to be loaded in source form and compiled at load time into discrimination networks. Each rule set specifies trigger delivered attributes. Upon loading the rule sets analysis subsystem establishes trigger listeners to receive sensor notifications and update respective working memories of rule engines . As illustrated in each of rule engines may serve a different tier defined within the fabric. Alternatively multiple rule engines may serve a single tier or a single rule engine may serve multiple tiers.

Rule engines process the events and invoke action requests via calls to effector subsystem . In addition rule engines provide a call back interface so that effector subsystem can inform a rule engine when an action has completed. Rule engines prevent a particular rule from re firing as long as any action invoked by the rule has not finished. In general rules contain notification calls and service invocations though either may be disabled by configuration of effector subsystem . BLT supplies initial system configuration descriptions to seed each of rule engines .

In general rule engines analyze the events and discover discrepancies between an expected state of the fabric and an actual state. Each of rule engines may be viewed as software that performs logical reasoning using knowledge encoded in high level condition action rules. Each of rule engines applies automated reasoning that works forward from preconditions to goals defined by system administrator . For example rule engines may apply modus ponens inferences rules.

Rule engines output requests to effector subsystem which produce actions requests for BLT to resolve the discrepancies. Effector subsystem performs all operations on behalf of analysis subsystem . For example event generator task invocation module and logger of effector subsystem perform event generation BLT action invocation and rule logging respectively. More specifically task invocation module invokes asynchronous operations within BLT . In response BLT creates a new thread of control for each task which is tracked by a unique task identifier task id . Rules engine uses the task id to determine when a task completes and if needed to re fire any rules that were pended until completion of the task. These tasks may take arbitrary amounts of time and rules engine tracks the progress of individual task via change notifications produced by BLT .

Event generator creates persistent event records of the state of processing of SLAI and stores the event records within a database. Clients uses these event records to track progress and determine the current state of the SLAI .

Logger generates detailed trace information about system activities for use in rule development and debugging. The logging level can be raised or lowered as needed without changing operation of SLAI .

In addition working memory includes a second data region that is modifiable i.e. read write and may be updated by monitoring subsystem or used internally by rule engines . In general data region stores aggregated data representing the actual state of the fabric and can be updated by sensor subsystem or by rule engines . The actual state may consist of a set of property annotations that can be attached to objects received from BLT or to objects locally defined within a rule engine such as local object .

As described BLT provides the facilities necessary to create and administer the organizational model e.g. fabric domains tiers and nodes implemented by distributed computing system . In general BLT abstracts access to the persisted configuration state of the fabric and controls the interactions with interfaces to fabric hardware services. As such BLT provides fabric management capabilities such as the ability to create a tier and replace a failed node. WSDL interfaces provide web service interfaces to the functionality of BLT that may be invoked by web service clients . Many of WSDL interfaces offered by BLT allow administrator to define goals such as specifying a goal of the expected state of the fabric. As further described below rule engines within SLAI in turn invoke task manger to initiate one or more BLT tasks to achieve the specified goal. In general web service clients may be presentation layer applications command line applications or other clients.

BLT abstracts all interaction with physical hardware for web service clients . BLT is an enabling component for autonomic management behavior but does not respond to real time events that either prevent a goal from being achieved or produce a set of deviations between the expected state and the actual state of the system. In contrast BLT originates goals for autonomic reactions to changing configuration and state. SLAI analyzes and acts upon these goals along with real time state changes. BLT sets the goals to which SLAI strives to achieve and provides functionality used by the SLAI in order to achieve the goals.

In general BLT does not dictate the steps taken in pursuit of a goal since these are likely to change based on the current state of distributed computing system and changes to configurable policy. SLAI makes these decisions based on the configured rule sets for the fabric and by evaluating monitoring data received from monitoring subsystem .

Fabric administration service implements a set of methods for managing all aspects of the fabric. Example methods include methods for adding viewing updating and removing domains tiers nodes notifications assets applications software images connectors and monitors. Other example methods include controlling power at a node and cloning capturing importing exporting or upgrading software images. Rule engines of SLAI may for example invoke these methods by issuing action requests .

Task manager receives action requests via task interface . In general task interface provides an interface for receiving action requests from SLAI or other internal subsystem. In response task manager manages asynchronous and long running actions that are invoked by SLAI to satisfy a goal or perform an action requested by a client.

Task manager generates task data that represents identification and status for each task. Task manager returns a task identifier to the calling web service clients or the internal subsystem e.g. SLAI that initiated the task. Rule engines and web service clients use the task identifiers to track progress and retrieve output results and errors associated with achieving the goal.

In one embodiment there are no WSDL interfaces for initiating specific tasks. Rather administrator interacts with BLT though goal interfaces presented by WSDL interfaces to define the goals for the fabric. In contrast the term task is used to refer to internal system constructs that require no user interaction. Tasks are distinct low level units of work that affect the state of the fabric. SLAI may combine tasks to achieve or maintain a goal state.

For example administrator can request configuration changes by either adding new goals to an object or by modifying the attributes on existing goals. Scheduled goals apply a configuration at a designated time. For example the goals for a particular tier may specify the minimum maximum and target node counts for that tier. As a result the tier can increase or decrease current node capacity by scheduling goals with different configuration values.

This may be useful for example in scheduling a software image upgrade. As another example entire domains may transition online and offline per a defined grid schedule. Administrator may mix and match goals on a component to achieve configurations specific to the application and environment. For example a tier that does not support autonomic node replacement would not be configured with a harvesting goal.

In some embodiments goals are either in force or out of force. SLAI only works to achieve and maintain those goals that are currently in force. SLAI may applies a concept of gravity as the goals transition from in force to out of force. For example SLAI may transition a tier offline when an online goal is marked out of force. Some goal types may have prerequisite goals. For example an image upgrade goal may require as a prerequisite that a tier be transitioned to offline before the image upgrade can be performed. In other embodiments goals are always in force until modified.

SLAI may automatically formulate dependencies between goals or may allow a user to specify the dependencies. For example a user may request that a newly created tier come online. As a result of this goal SLAI may automatically direct task manager to generate a task of harvesting a target number of nodes to enable the tier. Generally all goals remain in force by SLAI until modified by BLT . In one embodiment each goal remains in force in one of three states Satisfied Warning or Critical depending on how successful SLAI was in achieving the goal at the time the event record was generated and stored.

In this manner SLAI controls the life cycle of a goal i.e. the creation scheduling update deletion of the goal and provides a common implementation of these and other services such as timeout event writing goal conflicts management of intra goal dependencies and tracking tasks to achieving the goals.

Progress toward a goal is tracked though event subsystem . In particular event subsystem tracks the progress of each in force goal based on the goal identifiers. Tasks executed to achieve a particular goal produce events to communicate result or errors. The events provide a convenient time based view of all actions and behaviors.

Examples of goal types that may be defined by administrator include software image management goals node allocation goals harvest goals tier capacity goals asset requirement goals tier online offline goals and data gathering goals.

In one embodiment BLT presents a task interface to SLAI for the creation and management of specific tasks in order to achieve the currently in force goals. In particular rule engines invoke the task interface based on evaluation of the defined rule sets in view of the expected state and actual state for the fabric. Example task interfaces include interfaces to reserve node resources query resources for a node slot associate or disassociate an image with a node in a tier node slot allocate de allocate startup or shutdown a node move a node to a tier apply remove or cycle power of a node create a golden image create or delete an image instance and delete an activity node or tier.

Report generator provides an extensible mechanism for generating reports . Typical reports include image utilization reports that contain information with respect to the number of nodes running each software image inventory reports detailing both the logical and physical aspects of the fabric and system event reports showing all events that have occurred within the fabric. Report generator gathers localizes formats and displays data into report form for presentation to the user. Report generator may include one or more data gathering modules not shown that gather events in accordance with a schedule and update an events table to record the events. The data gathering modules may write the events in XML format.

Rule compiler compiles each of rules and translates match conditions of the rules into a discrimination network that avoids redundant tests during rule execution. Execution engine handles rule administration object insertion and retrieval rule invocation and execution of rule actions. In general execution engine first matches a current set of rules against a current state of working memory and local objects . Execution engine then collects all rules that match as well as the matched objects and selects a particular rule instantiation to fire. Next execution engine fires executes the instantiated rule and propagates any changes to working memory . Execution engine repeats the process until no more matching rule instantiations can be found.

Firing of a rule typically produces a very small number of changes to working memory . This allows sophisticated rule engines to scale by retaining match state between cycles. Only the rules and rule instantiations affected by changes are updated thereby avoiding the bulk of the matching process. One exemplary algorithm that may be used by execution engine to handle the matching process includes the RETE algorithm that creates a decision tree that combines the patterns in all the rules and is intended to improve the speed of forward chained rule system by limiting the effort required to re compute a conflict set after a rule is fired. One example of a RETE algorithm is described in Forgy C. L. 1982 RETE a fast algorithm for the many pattern many object pattern match problem Artificial Intelligence 19 1737 hereby incorporated by reference. Other alternatives include the TREAT algorithms and LEAPS algorithm as described by Miranker D. P. TREAT A New and Efficient Match Algorithm for AI Production Systems . ISBN 0934613710 Daniel P. Miranker David A. Brant Bernie Lofaso David Gadbois On the Performance of Lazy Matching in Production Systems. AAAI 1990 685692 each of which is hereby incorporated by reference.

Application matrix contains all the information needed by control unit to interact with one or more applications or application servers and provide autonomic control over a set of applications. Specifically application matrix provides a logical definition for deploying and controlling the set of applications to one or more tiers within distributed computing system . In one embodiment application matrix is an electronic document that conforms to a data description language e.g. the extensible markup language XML . Application SLAI includes an application rules engine dedicated to processing application level rules i.e. forward chaining rules to provide autonomic control over the applications defined within application matrix . Like rules engine application rules engine contains a rule compiler an execution engine and a working memory. In order to give effect to changes in application matrix application SLAI automatically updates application rules engine and monitoring subsystem . In particular application matrix sends an alert whenever application matrix changes. In response to this alert application SLAI captures application specific attributes from application matrix . Specifically application SLAI captures configuration attributes and rule attributes contained in application matrix . Application SLAI transfers any new rule attributes to the working memory of application rules engine to provide autonomic control over the deployment monitoring and the execution of the applications defined within application matrix . In addition application SLAI updates monitoring subsystem to collect information required to control the applications In this manner administrator may continue to add new application definitions and configurations to application matrix after distributed control node has started.

As described in further detail below configuration processor is a software module that generates an application matrix entry based on an application definition and application configuration properties of a staged application. A staged application is an application that has been deployed in a staging environment and customized for subsequent deployment within distributed computing system . After creating the application matrix entry administrator may insert the application matrix entry into application matrix .

Configuration processor is pluggable. That is administrator can plug in different implementations of configuration processor as needed. For example administrator may need to plug in a different implementation of configuration processor to handle applications that do not use an application server.

Application governor is a software engine that performs application level actions based on requests received from application rules engine . In this manner BLT effects fabric level actions e.g. deployment and monitoring of nodes and images based on request from fabric level rules engines while application governor performs application level actions e.g. deployment and monitoring of applications based on requests from application rules engine .

Application governor uses application matrix as a source of parameters when carrying out the application level operations requested by application rules engine . For example application rules engine may detect that a node to which the application is not deployed is ready for use by the application. As a result application rules engine directs application governor to handle the details of deploying the application to the node. In turn application governor accesses application matrix to retrieve application specific parameters necessary to deploy the application. Storing application specific parameters in application matrix allows the application specific parameters to change without having to recompile the rules within working memory of application rules engine .

Application governor performs a similar procedure to undeploy an application. That is application rules engine may detect that a second application needs to use a node more than a first application that is currently deployed to the node. In this situation application rules engine sends an instruction to application governor to undeploy the first application and deploy the second application. To carry out this instruction application governor accesses application matrix to discover configuration parameters of both applications. Application governor then uses the discovered configuration parameters to communicate with the applications.

Like configuration processor application governor is also pluggable. That is administrator can easily install or remove implementations of application governor depending on the circumstances. Because application governor and configuration processor represent interchangeable plug in modules the other parts of control unit and system including application rules engine can remain generic and application neutral while providing autonomic control over distributed computing system .

In this example application matrix contains seven columns and two rows. Each row represents a different application entry for deployment within distributed computing system . In only the first entry is shown in detail.

Each of the columns represents a different category of elements that make up an application s logical definition. In this example the columns include 

 1 an application column that includes elements generally related to the deployment of the application 

 2 an application nodes column that contains elements related to the tier node slots to which the application may be assigned 

 3 a services column that contains elements related to the executable services launched when the application is deployed 

 4 a node monitor values column that contains elements related to attributes of the nodes that are to be monitored after the application is deployed to that node 

 5 a service monitored attributes column that contains elements related to attributes of the services that are to be monitored after the application is deployed 

 6 a service levels column that contains elements related to attributes for use when constructing rules to monitor execution of the services and

 7 a deployment constraints column that contains elements related to attributes for use when constructing rules to control deployment of the application. Different types of applications may have different elements in each column and different numbers of columns.

In the example of application matrix contains two applications DataDomain and PortalDomain . DataDomain application has eleven attributes that define how control node accesses and launches the application. For instance the adminIP and adminPort attributes instruct governor as to which the server and port hosts the administrative part of the logically defined application. Other attributes like maxNodes and minNodes instruct application rules engine to run the application no less than minNodes and no more than maxNodes. Applications other than application may have different numbers or types of attributes. In XML format the attributes of application may appear as follows 

In addition to these attributes application contains a series of elements columns . In general application nodes contain a list of all of the available tier node slots to which control node may deploy application . In this example two tier node slots are specified. In XML managed servers appears as 

Each element in services column identifies a service that control node launches when deploying application on a given application node. A service element comprises a service name and a path to a file containing the executable service. Application governor uses the path to locate and launch the service. For example the following XML code indicates that governor must access the file at lib worklistApp worklistApp.ear .

Node Monitored Values elements represent characteristics for use in constructing rules for monitoring nodes to which the applications are deployed. Similarly Service Monitored Values elements represent characteristics for use in constructing rules for monitoring services that are launched once the application is deployed. In this example a nodeMonitoredValues element defines characteristics of a particular node that are to be monitored. For instance the amount of free memory in a node is one example of a characteristic listed as a nodeMonitoredValues element. On the other hand a serviceMonitoredValues element is specific attribute of a service that is to be monitored. For example the number of pending operating system level requests for the service the number of idle threads etc. could be service monitored values. In a XML rendition of application matrix node monitored values and service monitored values could appear as follows 

Deployment constraint elements specify characteristics of a node under which application rules engine should or should not deploy a service to the node. In this example a deployment constraint element has five attributes attribute expression frequency maxThreshold minThreshold and period. The attribute attribute names the deployment constraint. The expression attribute specifies an arithmetic expression manipulating a monitored value. For example the expression could be PercentMemoryFree 100 meaning monitor the value of the PercentMemoryFree node monitored value multiplied by 100. The expression attribute may specify one or more node monitored values. The frequency attribute informs application rules engine how frequently to check the monitored value. The maxThreshold attribute tells application rules engine to invoke a rule when the value of the expression exceeds the value specified by the maxThreshold attribute. Similarly the minThreshold attribute tells application rules engine to invoke the rule when the value of the expression drops below the value specified by the minThreshold attribute. Finally the period attribute informs application rules engine of the period over which to collect monitored value. For example a deployment constraint element may specify that application rules engine should monitor the PercentMemoryFree attribute of a node every 15 seconds for 60 seconds. If the value of PercentMemoryFree 100 should drop below 1.0 i.e. 1 of memory free for 60 seconds then application rules engine should not deploy the application to that node. In XML this rule would be represented as 

Service level elements have the same five attributes as deployment constraints elements attribute expression frequency maxThreshold minThreshold and period . However the expression attribute deals with service monitored attributes rather than node monitored attributes. For example a service level element may specify that application rules engine check the pendingRequest service monitored attribute every fifteen seconds for thirty seconds. Then if there are more than twenty pending requests for more than thirty seconds application rules engine should take the action of starting a new application. On the other hand if there are fewer than 5 pending requests for thirty seconds application rules engine enables the action to remove the application to free up space on a node. Such a service level element could be represented as 

Put together an XML representation of an application matrix logically defining a single application for deployment within automatically controlled distributed computing system may appear as follows 

During this process administrator directs the application to generate an application definition file that represents an application specific configuration based on the environment specified by the administrator. The application definition file contains information specific to an installation instance of the application. For example the application definition file typically specifies the locations paths for all software components of the application as staged within the staging environment a list of files to execute connectivity information for the application and other information that may be recorded by the configured application.

After staging the application administrator defines a set of application configuration properties that specify desired behavior of the application within distributed computing system . Examples of such application configuration properties include a minimum number of nodes minimum resource requirements for a node deployment timing information network addresses of tier node slots that are able to execute the application and other properties.

Next administrator directs configuration processor to generate an application entry for the application using the application definition and the application configuration properties . The application entry contains configuration attributes that application governor may use to interact with the application. In addition the application entry may contain rule attributes that application rules engine may use to define how control node monitors the deployment and execution of the application within distributed computing environment .

After configuration processor creates the application entry administrator may modify the application entry . In particular administrator may update start scripts or shell scripts for the application due to path changes between the staging environment and distributed computing system .

Once administrator has finished modifying the application entry administrator inserts the application entry into application matrix . Because application matrix detects the insertion of the application entry application matrix sends an alert to application SLAI .

In response to the alert application SLAI may update application rules engine and monitoring subsystem . For example application SLAI may automatically scan application matrix . If application SLAI detects new rule attributes application SLAI may create local objects that reflect the new rule attributes in the working memory of application rules engine . In addition if application SLAI detects new monitored values application SLAI may update monitoring subsystem to add new monitoring collectors .

After application SLAI updates application rules engine and monitoring subsystem control node has autonomic control over the deployment of applications to tiers based on the configuration of application matrix . For example control node may deploy images deploy applications monitor the state of nodes and the execution of the applications and apply tier level rules as well as application specific rules. Application SLAI continues to listen for alerts from application matrix .

This embodiment of application governor may allow administrator to easily add new software applications for use in distributed computing system . These software applications may be simple applications that run on a single node applications that run on multiple nodes but do not interact directly with each other and complex distributed applications that run on a collection of nodes and so on. Moreover these software applications execute within container resources. As used herein the term container resource refers to an execution environment in which an application may execute. For example an operating system and one or more layers of middleware may provide an execution environment for an application. In other words the operating system and the layers of middleware constitute a container resource. 

Application image objects are software modules associated with application images of software applications. As used in this section an application image is a bitwise image of a software application at a time when the software applications are ready to be executed. For example one of application image objects may be associated with an image of a Java 2 Enterprise Edition J2EE application.

Each of application image objects implement a common application programming interface. The common application programming interface defines methods that may be used to create and destroy instances of an application image configure container resources to execute the application image and so on. Each of application image objects implement the methods defined in the application programming interface but may do so in different ways. Because each of the application image objects implements a common application programming interface application governor may interact with differing applications in a common manner.

Administrator may interact with an administrative interface of control node to generate a new application image object. For example administrator may request that control node generate an application image object for an application that is installed on a device. Upon receiving the request from administrator control node may create a new application image object. Control node may then assign a unique name to the application image object. This name may conform to a formal naming convention. For instance names of application image objects may conform to the Java Management Extension Management Bean JMX MBean naming convention. After assigning a unique name to the application image object control node may capture an image of the application installed on a device. The image constitutes software code needed to execute the application. For example control node may capture the image of the application by copying the portion of a hard disk drive of the device related to the application. After control node captures the image of the application control node may store the captured image in software image repository . In addition control node may set a parameter in the new application image object to indicate that the application image object is associated with newly captured image. After control node associates the image with the application image object control node may prompt administrator to specify configuration parameters for the application. Upon receiving the configuration parameters from administrator control node may store the application specific configuration parameters in the new application image object. Once administrator specifies the configuration parameters for the application control node may serialize the new application image object. For example control node may create a file that contains the new application image object.

After control node serializes the new application image object administrator may provide the new application image object to application governor . For example administrator may copy a file that contains the new application image object into a deployment directory. When administrator provides the new application image object to application governor the application image object automatically registers itself with registration service . For instance an executable registration script in the new application image object may automatically execute. When the registration script executes the execution script provides the name of the new application image object to registration service . Registration service may then incorporate the name of the new application image object into a list of registered application image objects. In one example the names of application image objects may conform to the JMX MBean naming convention and registration service may be a JMX MBean server. In addition to registering the name of the new application image object registration service may add the configuration parameters included in the application image object into application matrix . Administrator may provide the new application image object to application governor while distributed computing system is operating.

After registration service registers an application image object application rules engine may determine that distributed computing system requires a certain number of instances of an application image of a software application. For example application rules engine may determine that there should be an application image instance for each slot in a tier. When application rules engine determines that distributed computing system requires a certain number of instances of an application image of a software application application rules engine may send instructions to configuration manager to create the certain number of instances of the application image. In response to the instructions configuration manager may send a request to registration service for a name of the application image object associated with the application image. Configuration manager may then use the name of the application image object to invoke a createImageInstance method of the application image object to create an instance of the application image from the application image. The instance of the application image is a separate copy of the application image associated with the application image object. Because the instance of the application image is a separate copy of the application image the instance of the application image may be modified independently of other instances of the same application image. The createImageInstance method may be one of the methods defined by the common interface implemented by the application image objects. After creating an instance of the application image using the createImageInstance method configuration manager may store the instance of the application image in software image repository . Configuration manager may invoke the createImageInstance method for each of the requested number of instances of the application image. In this way application rules engine may create the requested number of instances of the application image.

After configuration manager creates at least one instance of the application image application rules engine may determine that an application node in the tier associated with the application should execute the instance of the application image. In this situation application rules engine may issue instructions to configuration manager to deploy an instance of the application image to the application node. In response to the instructions from application rules engine configuration manager may request from registration service a name of one of software image objects that is associated with the instance of the application image. Configuration manager may then use the name of the software image object to invoke a configureContainerResource method of the application image object associated with the instance of the application image to configure a container resource on the application node to host the instance of the application image. When configuration manager invokes the configureContainerResource method of the application image object the application image object may apply a set of configuration parameters defined in application matrix to the container resource of the application node. To complete the deployment configuration manager may invoke a startInterpretation method of the application image object. The startInterpretation method of the application image object instructs the container resource of the application node to start running the instance of the application image.

Subsequently application rules engine may determine that it is no longer necessary to execute the instance of the application image on the application node. When this occurs application rules engine may send instructions to configuration manager to un deploy the instance of the application image from the application node. In response to the instructions from application rules engine configuration manager may request from registration service a name of one of software image objects that is associated with the instance of the application image. Configuration manager may then use the name of the software image object to invoke a stopInterpretation method of the application image object to stop the execution of the instance of the application image on the application node. After invoking the stopInterpretation method configuration manager may invoke an unconfigureContainerResource method of the application image object. The unconfigureContainerResource method resets the container resource to the configuration of the container resource prior to the invocation of the configureContainerResource method on the container resource. After configuration manager invokes the unconfigureContainerResource method the instance of the application image may be redeployed to another application node and the container resource may host another instance of an application image.

If application rules engine determines that conditions decides that it is no longer necessary to maintain an instance of an application image application rules engine may send instructions to configuration manager to release the instance of the application image. In response to the instruction configuration manager may invoke a releaseImageInstance method of the application image object. The releaseImageInstance method destroys the instance of the application image. For example the releaseImageInstance may delete the instance from software repository . Configuration manager may generate an error if the releaseImageInstance method is invoked on a software image object associated with an instance of an application image that is currently deployed.

The application governor illustrated in the example of may present several advantages. For instance the embodiment may provide a common pluggable framework for supporting diverse software applications. Because each of application image objects present a common application programming interface many parties may write application image objects to integrate new software applications with distributed computing system . Such parties may include vendors of distributed computing systems consultants and partners. Furthermore the use of application image objects may allow control node to deploy new instances of a software application during execution of the software application. The ability to allow control node to add new instances of a software application during execution may obviate the need for manual configuration of tiers during tier creation. As a result tiers may grow dynamically without requiring manual configuration.

The example embodiment of application governor may also provide platform differentiation for services that are fungible i.e. exchangeable . For a fungible application such as an application server this example embodiment may allow distributed computing system to treat each application instance fungibly. That is the image object may mask the configuration differences required by the different base platforms such as Linux and Solaris.

Various embodiments of the invention have been described. These and other embodiments are within the scope of the following claims.


---

title: Method and system for reliable access of expander state information in highly available storage devices
abstract: An apparatus and method for detecting an interface failure of a first interface of a first expander of a storage system, and retrieving state information from a second expander to the first expander using a second interface of the first expander when the interface failure of the first interface is detected. The state information is normally available to the first expander through the first interface, but is unavailable through the first interface due to the interface failure of the first interface.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07574630&OS=07574630&RS=07574630
owner: Network Appliance, Inc.
number: 07574630
owner_city: Sunnyvale
owner_country: US
publication_date: 20060814
---
This invention relates to the field of storage systems and in particular to disk drive storage enclosures.

A file server is a network connected storage server that stores and manages shared files in a set of storage devices e.g. disk drives on behalf of one or more clients. The disks within a file system are typically organized as one or more groups of Redundant Array of Independent Inexpensive Disks RAID . One configuration in which file servers can be used is a network attached storage NAS configuration. In a NAS configuration a file server can be implemented in the form of an appliance that attaches to a network such as a local area network LAN or a corporate intranet. An example of such an appliance is any of the Filer products made by Network Appliance Inc. in Sunnyvale Calif.

Another specialized type of network is a storage area network SAN . A SAN is a highly efficient network of interconnected shared storage devices. Such devices are also made by Network Appliance Inc. One difference between NAS and SAN is that in a SAN the storage appliance provides a remote host with block level access to stored data whereas in a NAS configuration the file server normally provides clients with file level access to stored data.

Current file server systems used in NAS environments are generally packaged in either of two main forms 1 an all in one custom designed system that is a standard computer with built in disk drives all in a single chassis enclosure or 2 a modular system in which one or more sets of disk drives each in a separate chassis are connected to an external file server head in another chassis.

In this context the term head means all of the electronics firmware and or software the intelligence that is used to control access to storage devices in a storage system it does not include the disk drives themselves. In a file server the head normally is where all of the intelligence of the file server resides. Note that a head in this context is not the same as and is not to be confused with the magnetic or optical head used to physically read or write data to a disk.

In a modular file server system the system can be built up by adding multiple chassis in some form of rack and then cabling the chassis together. The disk drive enclosures are often called shelves and more specifically just a bunch of disks JBOD shelves. The term JBOD indicates that the enclosure essentially contains only physical storage devices and little or no electronic intelligence. Some disk drive enclosures include one or more RAID controllers but such enclosures are not normally referred to as JBOD due to their greater functional capabilities. A modular file server system is illustrated in and is sometimes called a rack and stack system. In a file server storage server head is connected by external cables to multiple disk shelves mounted in a rack . The file server storage server head enables access to stored data by one or more remote client computers not shown that are connected to the storage server head by external cables.

Enterprise class storage appliances e.g. filers have unique and class defining characteristics of their own such as the stringent requirement to be up and running almost hundred percent of the time. In order to increase the amount of uptime these appliances need to be resistant to certain types of errors. One conventional method employed to ensure that appliances keep serving data despite catastrophic types of errors is to provide multiple data paths. Provisioning of multiple data paths can be accomplished at different system level components. For example in conventional storage appliances in highly available HA configurations dual heads are provided to insulate against a broad range of motherboard level errors that would cripple a single head but the remaining head would assume the identity of the fallen head still serving data to the user of the fallen head.

Small computer system interface SCSI which has been inexistence for well over twenty years has traditionally been used in a wide variety of enterprise class storage solutions. This multi layered architecture is extensively used and acknowledged by industry leaders as a stable feature rich extensible IO interconnect. The SCSI Architectural Model SAM has proven its reliability over four generations of parallel physical layers and asynchronous and synchronous transfer speeds as well as being adopted by other popular physical interconnects such as Fibre Channel and ATAPI.

The latest generation of the implementation of SAM is serial attached SCSI SAS succeeding over Ultra 320 parallel SCSI. SAS eliminated parallel bus issues such as multi drop skew limited distance small form factor and introduced some of the well defined advantages of using a high speed serial point to point link such as elimination of skew and cross talk fewer wires low power etc. SAS also became the unifying point of the two dominant disk command protocols of our time SCSI and Advanced Technology attachment ATA where SAS protocol ensured that SATA drives could be operated in a SAS domain.

SAS also addressed certain other deficiencies of SCSI implementation an important one being the maximum number of devices that can be connected to form a SCSI domain by introducing a different device addressing scheme and a routing scheme. The maximum number of devices in a SCSI domain which used to be 16 in parallel SCSI and 127 in Fibre Channel has grown out of proportion in SAS to well over a thousand devices as a theoretical maximum. This property coupled with the point to point nature in SAS links gave rise to the concept of expanders in SAS.

As the name implies expanders typically increase the number of devices that can be physically connected to a SAS domain. Most SAS host bus adapters found in the market today contain 4 or 8 physical links phys limiting the number of devices that can be connected to them directly to 4 or 8 respectively. But with the help of expander which can contain for example 12 or 24 or 36 phys and their ability to be deeply cascaded an extremely large number of drives can now be connected to a storage server. SAS also brings the added degree of resilience by allowing more than one phy in a wide port configuration to connect expanders to expander or expanders to host bus adapters. In such a connection e.g. 4 phys wide even if 3 phys fail the remaining phy can still provide access to all of the devices downstream of that connection.

The presence of multiple SAS expanders containing system enclosure services SES processors presents the problem of having to synchronize the states of the firmware in both expanders to present a uniform view of the enclosure to the external world. The state of firmware or state information in each expander can include for example data needed to form SAS addresses to SATA drives data read from sensors and data needed to serve certain diagnostic pages as defined by the SES specification. Current solutions of SAS based appliances use an Inter Integrated Circuit I2C connection between the expanders to allow them to communicate among themselves. I2C connections however are known for failures and hang ups preventing any kind of communication from taking place between the devices. Although certain precautions can be taken to minimize the possibility of an appliance failing in the case of a failure of the I2C connection there is no conventional method known that can be used to enable expanders to communicate with each other to synchronize their state upon such failure.

An apparatus and method for detecting an interface failure of a first interface of a first expander of a storage system and retrieving state information from a second expander to the first expander using a second interface of the first expander when the interface failure of the first interface is detected is described herein. The state information is normally available to the first expander through the first interface but is unavailable through the first interface due to the interface failure of the first interface.

An apparatus and method for retrieving state information for a first expander from a second expander upon detecting an interface failure between the first expander and a backplane is described herein. The following description sets forth numerous specific details such as examples of specific systems components methods and so forth in order to provide a good understanding of several embodiments of the present invention. It will be apparent to one skilled in the art however that at least some embodiments of the present invention may be practiced without these specific details. In other instances well known components or methods are not described in detail or are presented in simple block diagram format in order to avoid unnecessarily obscuring the present invention. Thus the specific details set forth are merely exemplary. Particular implementations may vary from these exemplary details and still be contemplated to be within the spirit and scope of the present invention.

As described in detail below the apparatus may include first and second expanders of a storage system. The first expander has two interfaces a first interface that is configured to retrieve state information of the storage system for the first expander and a second interface that is configured to retrieve the state information from the second expander to the first expander when an interface failure of the first interface of the first expander has been detected. The state information is available to the first expander through the first interface but is unavailable through the first interface due to the interface failure of the first interface. As described in detail with exemplary embodiments below the two expanders may be implemented each in either two storage server heads of an all in one storage system or two IO modules of a modular storage system.

SAS technology can be implemented in either JBODs or in storage servers such as filers. A JBOD can have e.g. 24 drives in a shelf and two IO modules each of which includes a SAS expander and a set of associated electronics. A storage server version may be a PC platform with a SAS controller and a SAS expander in each head among other circuits e.g. processors or the like.

In order to increase the competitiveness in the market place SAS expander vendors have incorporated SES processors into expanders. SES is a well defined entity in SAM. These SES processors can be used to monitor and control the environment in the enclosure. This is achieved by monitoring and controlling various enclosure components such as power supplies voltage and current sensors fans disk dongles light emitting diodes LEDs etc.

Given that SATA drives are to be operated in a SAS domain the SATA drives are presented as SCSI or SAS drives to upper layer software. Typically this is done in SAS host bus adapter firmware but it can also be done at an operating system driver level as well. One aspect of such emulation would be to generate a unique SAS address for SATA drives. Some SATA drives provided a worldwide name WWN but some did not. In order to maintain consistency the SAS protocol made it compulsory for expanders as well as host bus adapters to generate a WWN for each SATA device if such a device is directly connected to them. For storage appliances such as filers and JBODs where there may be multiple expanders to a given set of drives in a single enclosure it became necessary to generate WWNs in a uniform consistent fashion so that the operating system would see the same WWN no matter which expander is being used to connect to a particular SATA drive. One way of achieving this was to place a serial EEPROM or the like in the backplane of the enclosure containing some enclosure specific data which would be used by both expanders to generate consistent WWNs. Accordingly both expanders should be able to see a consistent view of the contents of the backplane EEPROM.

As previously mentioned SAS is the next generation IO interconnect that succeeded the parallel SCSI architecture. It is also a unifying effort where SATA drives can now be operated in a SAS domain. SAS based appliances may include SAS based JBODs or embedded filers. In both appliances there may be multiple SAS expanders in a single enclosure. Each expander will reside in an IO module or a storage server head. These expanders are connected through a backplane of the enclosure using I2C busses to synchronize their internal states. As previously mentioned I2C busses are known to be susceptible to different kinds of failures. Described herein therefore are embodiments of an apparatus and method for synchronizing the expander states when an I2C bus or other interconnect between SAS expanders fails thereby eliminating another potential point of failure.

SAS preserves useful features that were available in parallel SCSI. SES and SAS however define new enhancements to these useful features making it possible to define new mechanisms to solve the problem described above. In one embodiment of the invention an expander that detects an interface failure e.g. I2C failure uses the BROADCAST SES functionality described in the SAS specification in conjunction with event reporting mechanism defined by SAS HBA vendors used in a HA filer environment to reduce the dependency of I2C and similar busses in SAS expanders to communicate with devices such as EEPROMS temperature and voltage sensors drive dongles. A drive dongle may be a physical device attached to e.g. plugs into or hangs from an I O port of the backplane that adds hardware capabilities such as signal conversion for different types of connectors adapter capabilities e.g. USB adapter for such devices as memory cards storage capabilities e.g. flash memory drives or the like etc. Alternatively the drive dongles for example may act as copy protection for a particular software application or as a hardware key. The BROADCAST SES is a BROADCAST primitive that is a notification of an asynchronous event from a logical unit with a peripheral device type i.e. enclosure services device in the SAS domain.

In one embodiment the method may be performed by defining a new communication process that starts with the expander firmware module issuing a BROADCAST SES primitive that gets transmitted to a SAS HBA which in turn converts the primitive into an event sent to the SAS driver which then sends the event to the SES driver using an operating system signal. There are a number of possibilities from that point onwards to either communicate with the partner expander using the cluster interconnect or query the expander SES module and take required actions such as when certain inconsistencies arise in the expander firmware state.

Exemplary embodiments of the present invention as described herein provide a robust method for synchronizing the state of two or more expanders using infrastructure and protocols and thereby improving the reliability of the total solution.

The use of RAID protocols between the storage server head and the shelves enhances the reliability integrity of data storage through the redundant writing of data stripes across physical disks in a RAID group and the appropriate writing of parity information with respect to the striped data. In addition to acting as a communications interface between the storage server head and the disk drives the I O module also serves to enhance reliability by providing loop resiliency. Specifically if a particular disk drive within a shelf is removed the I O module in that shelf simply bypasses the missing disk drive and connects to the next disk drive within the shelf . This functionality maintains connectivity of the loop in the presence of disk drive removals and is provided by multiple Loop Resiliency Circuits LRCs not shown included within the I O module . In at least one embodiment the LRCs are implemented in the form of port bypass circuits PBCs within the I O module typically a separate PBC for each disk drive in the shelf . Note that a PBC is only one implementation of an LRC. Other ways to implement an LRC include a hub or a switch. The implementation details of I O modules and PBCs such as described here are well known in the relevant art and are not needed to understand the present invention.

As mentioned above access to data in a file server system is controlled by a file server head such as storage server head in the above described figures. Also as described above in a modular file server system the storage server head is contained within its own chassis and is connected to one or more external JBOD disk shelves in their own respective chassis.

The processor is the central processing unit CPU of the storage server head and may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices. The memory may be or include any combination of random access memory RAM read only memory ROM which may be programmable and or Flash memory or the like. The chipset may include for example one or more bus controllers bridges and or adapters. The peripheral bus may be for example a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire . Each network adapter provides the storage server head with the ability to communicate with remote devices such as clients in and may be for example an Ethernet adapter. Each storage adapter allows the storage server head to access the external disk drives in the various shelves and may be for example a Fibre Channel adapter.

Storage system may be an all in one custom designed system that is a computer with built in disk drives all in a single chassis enclosure or a modular system in which one or more sets of disk drives each in a separate chassis are connected to an external file server head in another chassis. In one embodiment the two expanders may each be implemented within a storage server head of a single enclosure as illustrated in A and B. Alternatively the two expanders may each be implemented within IO modules of a single enclosure that are coupled to two storage server heads that are external to the single enclosure in one or more separate enclosures as illustrated in and .

It should be noted that two or more storage server heads whether implemented in the same or separate enclosures may provide cluster failover CFO capability i.e. redundancy . CFO refers to a capability in which two or more interconnected heads are both active at the same time such that if one head fails or is taken out of service that condition is immediately detected by the other head which automatically assumes the functionality of the inoperative head as well as continuing to service its own client requests. In certain embodiments the two or more storage server heads communicate with each other via the backplane and interfaces and using Gigabit Ethernet protocol as illustrated below in . Alternatively the two or more storage server heads may communicate with each other via a third interface also as illustrated in . Backplane is used to connect multiple disks e.g. disks of the enclosure to the two expanders and . The backplane may be for example a circuit board usually a printed circuit board that connects several connectors of separate devices to each other. For example the pins of each connector may be linked to the same relative pin of all the other connectors forming a computer bus. The backplane may be either passive or active. Passive backplanes may offer no active bus driving circuitry while active backplanes may include circuitry that buffers the various signals to the different devices. Note that in other embodiments protocols other than Ethernet may be used for communication between the heads such as I2C protocols. In another embodiment the two or more expanders and of the two IO modules and communicate with each other via the backplane and interfaces and using Gigabit Ethernet protocol as illustrated below in . Alternatively the two or more expanders and may communicate with each other via a third interface also as illustrated in . Note that in other embodiments protocols other than Ethernet may be used for communication between the expanders and such as I2C protocols.

Connecting the heads and to the backplane may be advantageous because among other reasons it eliminates the need for cables or wires to connect the heads and . Note that although two heads are shown in the storage system can operate as a standalone system with only one head or alternatively with more heads than two. Each storage server head may contain the electronics firmware and software along with built in I O connections to allow the enclosure to be used as a NAS file server and or a SAN storage system. The circuit board of each storage server head has various conventional electronic components processor memory communication interfaces etc. mounted and interconnected on it. In one embodiment the head can be distributed between two or more circuit boards although a single board head is believed to be advantageous from the perspective of conserving space inside the chassis.

This standalone system can be easily grown in capacity and or performance by combining it with additional modular storage shelves as illustrated in and optionally with a separate more powerful file server head. This approach provides scalability and upgradeability with minimum effort required by the user. In addition this approach allows the user to add more performance or capacity to his system without physically moving disk drives from the original enclosure or having to copy the data from the original machine to the newer machine.

JBOD disk drive shelf is of the type which may be connected to separate external heads in a modular file server system such as storage server heads and . All of the illustrated components except the storage server heads and are contained within a single chassis enclosure . The storage server heads and may be contained within one or more separate chassis enclosures . As shown all of the major components of the shelf are connected to and communicate via a backplane . The backplane can be passive having no active electronic circuitry mounted on or in it. A passive backplane is a passive communications medium that does not include any bus driving circuitry for buffering various signals e.g. bus communications between the different devices. Alternatively backplane may include active electronic circuitry mounted on it such as memory memory controllers sensors bus driving circuitry for buffering various signals between the different devices or the like. The backplane can be comprised of one or more substantially planar substrate layers which may be conductive or which may be dielectric with conductive traces disposed on in it with various pin and socket type connectors mounted on it to allow connection to other components in the shelf .

The SAS expander is coupled to the SAS HBA through another interface . In this embodiment this interface includes four SAS links. Alternatively other interfaces may be used. SAS HBA includes a firmware module SAS HBA firmware module .

Operating system includes one or more drivers. For example OS includes SAS driver SES driver and cluster connect driver . The SAS HBA may communicate with the OS via software communication paths. These software communication paths are illustrated in as communication paths .

Storage server head includes the same components as storage server head namely a SAS expander SAS HBA and OS . These components are identical or similar to those of storage server head except that they reference numbers have been designated with for ease of discussion.

Coupled to both the storage server heads and is backplane . Backplane may include or be coupled to other components such as power supply backplane serial EEPROM temperature current voltage sensors disks and dongles . The storage server heads and are coupled to the backplane through two interfaces and respectively which each include communication channels. The communication channels may be used by the heads to retrieve state information from the components of the backplane . As previously mentioned in one embodiment the communication channels may be I2C interconnects. Alternatively other interconnects may be used.

Storage server heads and are also coupled together through cluster interconnect . Cluster interconnect may be an Infiniband interconnect for example. Alternatively cluster interconnect may be an SAS interconnect Ethernet interconnect or the like.

The two IO modules and include two expanders and a set of associated electronics. These expanders may be identical or similar to expanders and described with respect to . The IO module includes SAS processor which has SAS expander firmware module and SES processor which has SES processor firmware module . The SAS expander firmware module is configured to detect an interface failure on the interface between the SAS expander and backplane and to retrieve state information from another expander e.g. residing in another storage server head .

Similar to the interface between the expanders and and backplane IO modules and are coupled to the backplane through two interfaces interfaces and respectively which each include communication channels. Interface may include I2C interconnects or alternatively other types of interface interconnects.

The IO module is coupled to the SAS HBA through another interface. In this embodiment this interface includes four SAS links . Alternatively other interfaces may be used. SAS HBA may include a firmware module much like SAS HBA firmware module .

Storage server heads and both include an operating system. An exemplary operating system that can operate on a storage server heads is Data ONTAP commercially available from Network Appliance of Sunnyvale Calif. The OS and OS may be or similar to the two operating systems of the storage server heads and as described with respect to . Accordingly OS includes SAS driver SES driver and cluster connect driver . The SAS HBA may communicate with the OS via software communication paths. These software communication paths are illustrated in as communication paths .

Storage server head includes the same components as storage server head namely a SAS expander SAS HBA and OS . These components are the same as those of storage server head except that they reference numbers have been designated with for ease of discussion. In addition to the SAS HBA the storage server heads may include FC HBA and network interface card NIC .

Coupled to both the IO modules and is backplane . Backplane may include or be coupled to other components such as power supply backplane serial EEPROM temperature current voltage sensors disks and dongles . The IO modules and are coupled to the backplane through two interfaces interfaces and respectively which each include communication channels. The communication channels may be used by the IO modules to retrieve information e.g. state information from the components of the backplane . As previously mentioned in one embodiment the communication channels may be I2C interconnects. Alternatively other interconnects may be used.

Storage server heads and are also coupled together through cluster interconnect . Cluster interconnect may be an Infiniband interconnect. Alternatively cluster interconnect may be an SAS interconnect Ethernet interconnect or the like.

In one embodiment the IO modules are coupled to the disks using a single wire connection and . Alternatively the IO modules are coupled using multi wire interfaces.

It should be noted that the embodiments described in B and C have been described as having only two storage server heads in an HA configuration but are not limited to having two storage server heads. The embodiments may be used in highly available systems with N number of heads.

In one embodiment the SES processor is operable to alert the operating system driver that controls it e.g. SES driver via an initial interrupt and that it needs state information such as servicing or synchronization information. The state information of the expanders may be for example the state of the firmware in each expander. State information may include data needed to generate SAS addresses for SATA drives data seen and read by the sensors data regarding service of certain diagnostic pages as defined by the SES specification data available from power supply disks or dongles or the like. The state information may be stored in memory of the backplane e.g. EEPROM or may be read directly from the devices attached to or residing on the backplane . The initial interrupt can be caused by the use of a primitive command signal e.g. BROADCAST SES primitive by the expander that requires service expander . This will make the SES driver query the expander for the reason it raised the interrupt. Once the reason is obtained by the SES driver the SES driver can use the cluster interconnect e.g. using cluster connect driver to communicate with the partner expander in the same enclosure. The expander can communicate with the expander through cluster interconnect with the help of the cluster interconnect driver SES driver and the SAS driver of the partner operating system to retrieve the state information or to request an action on behalf of the expander which requested the state information because of the malfunctioning interface between the expander and backplane . The expander can communicate with expander using software communication paths OS messages hardware signals application programming interfaces APIs or the like. Feeding and querying of information to and from a malfunctioning expander interface may be accomplished by vendor specific SES diagnostic pages.

In another embodiment expander can detect an interface failure and communicate with expander to retrieve the state information from the backplane in a similar manner as described above.

A couple of examples are given below of interface failures on the interface between one expander and the backplane and the operations of the storage system to retrieve the state information from the other expander that has a functioning interface with the backplane. Both cases assume that the one of the I2C connection e.g. interface or between the expander and the backplane has failed.

In the first failure scenario assume that one expander is unable to read the backplane EEPROM to retrieve stored state information. Without the state information stored in backplane EEPROM that expander could potentially be unable to generate SAS addresses to SATA drives as it does when it is able to read the backplane EEPROM . This potentially could strain operations of the operating system and cause one head on an appliance to fail constituting downtime or running on a degraded mode both of which are not preferable. To prevent this from happening using the apparatus and method described herein the failing expander will transmit a BROADCAST SES primitive alerting the controlling SES driver that it needs service. The SES driver queries the failing expander and finds out the reason for the interrupt. Then the SES driver will communicate with the partner expander using the cluster interconnect for the missing pieces of synchronization information from the EEPROM and will feed it to the failing expander. Then the SES driver will reinitialize the failing expander so that it can generate a persistent set of SAS addresses to SATA drives.

In the second failure scenario assume that expander has a bad single wire connection to a dongle controller of a particular drive . In such a case all dongle controller provided services such as power on power off power cycle control of LEDs associated with that drive will not be available to the failing expander. In this case the malfunctioning expander will ask the controlling SES driver to ask the partner expander that has a similar single wire connection to the same dongle controller to carry out the operation on the drive in question preventing a drive failure and or other problems associated with a drive failure.

Upon power on reset during expander firmware initialization the expander notices that it cannot read the backplane EEPROM operation . This prevents expander from generating persistent SAS addresses for SATA drives which could prevent the operating system from booting on storage server head . The expander may notice that the interface is down. Alternatively the expander may notice that both interface and the interface between the partner expander are down. This would normally constitute a complete failure. However in this embodiment the SES expander firmware module in expander formulates a RECEIVE DIAGNOSTIC page as defined by the SES specification intended to inform the SES processor firmware module of the reason to require service operation . This page can be vendor specific but should contain as much information as possible to tell the host SES driver about the failure. The SES firmware module in expander transmits a BROADCAST SES primitive operation . BROADCAST SES primitive traverses the SAS topology to HBA through SAS links operation . HBA firmware of HBA notices the receipt of BROADCAST SES primitive and initiates an event to the host SAS driver operation . The method the HBA firmware uses to notify the operating system of the arrival of a BROADCAST SES primitive may differ based on the manufacturer of the HBA. For example an HBA manufactured by LSI Logic Corporation Milpitas Calif. may notify the operating system in a manner that is different than an HBA manufactured by Adaptec Inc. Milpitas Calif. SAS driver notices the received event sends an OS signal to the SES driver operation . The SES driver receives SAS driver signal BROADCAST SES operation . SES driver sends a RECEIVE DIAGNOSTIC SCSI command using the SAS driver to the SES processor in expander operation . SES processor in expander sends the already formulated SES page to SES driver operation . SES driver parses the received SES page. The reason for the interruption is then determined based on the SES page. For example the interruption may be caused by thermal runaway inability to read sensors inability to read the backplane EEPROM inability to perform drive dongle operations or other expander state synchronizing operations.

The SES driver may find the reason for the interruption to be the inability of the SES processor in expander to read the backplane EEPROM operation . It is possible that expander of storage server head was able to read the backplane EEPROM . So the SES driver in storage server head sends a request through the cluster interconnect to the SES driver in storage server head that it needs the contents of the backplane EEPROM . SES driver in storage server head sends a RECEIVE DIAGNOSTIC command to SES processor in expander to get the contents of backplane EEPROM . Upon successful receipt of the requested information SES driver on storage server head sends a reply to SES driver in storage server head with the contents of the backplane EEPROM . In addition SES driver in storage server head sends a SEND DIAGNOSTIC SCSI command with another vendor specific SES page that contains the information retrieved form expander in storage server head . SES processor in storage server head receives the SEND DIAGNOSTIC command and the data. SES processor parses the data and finds the contents of the backplane EEPROM . Now SES processor continues to initialize itself generating the SAS addresses for the SATA drives and thus enabling the operating system to boot in the presence of failures of the connections between expander and backplane EEROM on interface and or connection between expander and expander on interface . The storage server head may operate in a degraded mode but the storage server head can boot and up time is maintained.

In another scenario the interface failure may be thermal runaway. If a thermal runaway is the reason why SES processor interrupted the SES driver the SES driver can gracefully shut the storage server down preventing any possible loss of data operation . For example if the SES processor in SAS expander of head detects a thermal runaway condition but its interface with the power supply is down e.g. due to an interface failure then the SES processor can ask the SES processor in SAS expander of head to shut down the storage system. SES processor can ask the SES processor to shut down the storage system using interface . Alternatively the SES processor can use cluster interconnect as described above.

In another scenario the interface failure may be failure to perform a dongle operation. If the inability of expander to perform a drive dongle operation was the reason why the SES processor interrupted the SES driver a similar method can be used to get expander to execute the required operation on behalf of SES processor of expander operation .

Alternatively the interface failure may be caused other expander state synchronizing operations operation . For any other loss of synchronization of the expander states similar methods to those described above can be used to rectify the problem.

After performing any of the above operations operations it is determined whether anymore conditions require service operation . If so the method returns to operation in which the SES driver sends RECEIVE DIAGNOSTIC command for the vendor specific SES page. If no additional conditions require service then the operation completes operation .

Expander firmware may be operable to issue a BROADCAST SES primitive when it requires SES driver servicing. Expander firmware may be operable to generate a specific SES page that can be read by the host SES driver that conveys information about why the SES processor needs servicing. Reasons for asking for host services can include inability to the read sensors or backplane EEPROM to do drive operations etc. A field in this new SES page could be included to describe the exact reason why the BROADCAST SES was transmitted.

The SAS HBA can be configured to be capable of accepting the BROADCAST SES primitive and of converting the received BROADCAST SES primitive to an event that can be delivered to the SAS driver. In one embodiment the storage server head may use a HBA such as 1068E HBA manufactured by LSI Logic Milpitas Calif. The HBA may have a message passing interface MPI architecture. MPI defines a method to let the host driver know when the HBA receives a BROADCAST SES primitive.

The SAS driver is operable to accept events from the HBA indicating the receipt of a BROADCAST SES primitive. In such an event a signal is sent to the SES driver to alert the SES driver that the SES processor in the expander needs servicing.

The SES driver is operable to query the SES processor in the expander on why servicing is needed. The regular SCSI command RECEIVE DIAGNOSTIC may be used to accomplish this step. Note that the contents of the SES page that are returned may be vendor specific. The SES driver may also be operable to act on conditions indicated by the SES processor. For an example as described above if the SES processor indicates that it cannot read the EEPROM the SES driver can communicate with the SES drive of the partner using the cluster interconnect. To do so the SES driver can determine whether the expander on the partner head is able to read the backplane EEPROM and if so if it would return the data to the SES driver which will then be transferred over the cluster interconnect to the initiating party. The initiating SES driver then can send a SEND DIAGNOSTIC SCSI command to the SES processor in the expander to pass the state information retrieved form the backplane EEPROM. In one embodiment the storage server head may issue Automatic Supports ASUPs or Event Messaging System EMS logs when critical events occur. ASUPs can be emails sent by filers when the filer detects an error condition to whomever the filer administrator has configured to be the recipients.

The embodiments described herein may be used to reduce the downtime of storage system components by reducing the number of places of possible failures. For example if there is no I2C connection between the expanders and there is a failure on the interface between one of the expanders and the backplane the interface failure can be successfully handled using the embodiments described herein. In another example that assumes that the I2C connection between the expanders became inoperative which may prevent one expander from communicating with the other expander directly using the embodiments described herein the interface failures can also be successfully handled.

One example of possible failures is where the I2C connection between the expander and the backplane SEEPROM becomes inoperative. In this scenario the expander can request the partner expander to send the relevant information using the cluster interconnect. Another example is when the expander cannot read the chassis power supply status. In this scenario the expander can request the partner expander to send the relevant information using the cluster interconnect. Similarly if the expander cannot perform any disk power management actions items such as power on power off power cycle for any of the disks it controls the expander can ask the partner expander to perform the required action.

In one embodiment the I2C connection between the expanders can be eliminated as a single point of failure and expander states can be kept in synchronization using the BROADCAST SES mechanism and the cluster interconnect. Typically the SES driver of the OS polls for the status of the enclosure and associated sensors. The use of BROADCAST SES primitive makes it possible to notify the host and therefore the driver stack of any asynchronous event. This reduces the catastrophic events such as thermal runaways because as soon as the expander becomes aware of such a threat it can send a BROADCAST SES primitive to start a sequence of events which obtains the system action for correcting the situation.

It should be noted that there may be no side effects of using BROADCAST SES primitive because SAS initiators already pass the primitive to the driver stack. However conventionally SAS targets simply ignore such primitives according to the SAS specification.

Embodiments of the present invention include various operations. These operations may be performed by hardware components software firmware or a combination thereof. As used herein the term coupled to may mean coupled directly or indirectly through one or more intervening components. Any of the signals provided over various buses described herein may be time multiplexed with other signals and provided over one or more common buses. Additionally the interconnection between circuit components or blocks may be shown as buses or as single signal lines. Each of the buses may alternatively be one or more single signal lines and each of the single signal lines may alternatively be buses.

Certain embodiments may be implemented as a computer program product that may include instructions stored on a machine readable medium. These instructions may be used to program a general purpose or special purpose processor to perform the described operations. A machine readable medium includes any mechanism for storing or transmitting information in a form e.g. software processing application readable by a machine e.g. a computer . The machine readable medium may include but is not limited to magnetic storage medium e.g. floppy diskette optical storage medium e.g. CD ROM magneto optical storage medium read only memory ROM random access memory RAM erasable programmable memory e.g. EPROM and EEPROM flash memory electrical optical acoustical or other form of propagated signal e.g. carrier waves infrared signals digital signals etc. or another type of medium suitable for storing electronic instructions.

Additionally some embodiments may be practiced in distributed computing environments where the machine readable medium is stored on and or executed by more than one computer system. In addition the information transferred between computer systems may either be pulled or pushed across the communication medium connecting the computer systems.

The digital processing device s described herein may include one or more general purpose processing devices such as a microprocessor or central processing unit a controller or the like. Alternatively the digital processing device may include one or more special purpose processing devices such as a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or the like. In an alternative embodiment for example the digital processing device may be a network processor having multiple processors including a core unit and multiple microengines. Additionally the digital processing device may include any combination of general purpose processing device s and special purpose processing device s .

Although the operations of the method s herein are shown and described in a particular order the order of the operations of each method may be altered so that certain operations may be performed in an inverse order or so that certain operation may be performed at least in part concurrently with other operations. In another embodiment instructions or sub operations of distinct operations may be in an intermittent and or alternating manner.

In the foregoing specification the invention has been described with reference to specific exemplary embodiments thereof. It will however be evident that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. The specification and drawings are accordingly to be regarded in an illustrative sense rather than a restrictive sense.


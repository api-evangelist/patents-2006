---

title: Robotic gesture recognition system
abstract: A gesture recognition system enabling control of a robotic device through gesture command by a user is provided, comprising a robotic unit, a video or infrared camera affixed to the robotic unit, computing means, and high and low level of control gesture recognition application code capable of enabling the system to locate points of left hand, right hand, upper torso and lower torso of the user in the video imagery and convert it to waveform data, correlate the waveform data to user command data, and form corresponding control voltage command(s) for production of electric current voltage(s) to drive one or more of the electric motors or actuators of the robotic device to thereby control same. In addition, a computer software program is provided for use in the gesture recognition system described above.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07606411&OS=07606411&RS=07606411
owner: The United States of America as represented by the Secretary of the Navy
number: 07606411
owner_city: Washington
owner_country: US
publication_date: 20061005
---
The invention described herein may be manufactured and used by or for the Government of the United States of America for governmental purposes without the payment or any royalties thereon or therefor.

A gesture recognition system is provided for enabling video recognition of a user s gestures and transformation thereof to commands capable of being received and responded to by a robotic device. In addition a computer software is provided for use in the system herein.

Several approaches have been made to perform recognition of human motion with emphasis on real time computation. In addition several survey papers have reviewed vision based motion recognition human motion capture and human motion analysis. The most frequently used methodology for recognition of body motion and dynamic gestures was based on the analysis of temporal trajectories of the motion parameters hidden Markov models and state space models and static activity templates.

Other conventional techniques have attempted to represent motion to describe an action sequence by collecting of optical flow over the image or region of interest throughout the sequence but this is computationally expensive and often was not robust. Another conventional technique combined several successive layers of image frames of a moving person in a single template. This single template represented temporal history of a motion allowing a match of actual imagery to a memorized template to produce recognition of a motion gesture.

Further all of the above conventional techniques have been conducted in controlled laboratory environments with fixed lighting and constant distance to the subject. Obviously actual field conditions will present external stimuli and resulting difficulties in recognition. Thus it is apparent that different approaches are required for real life applications outside of a lab such as the flight deck of an aircraft carrier.

Flight deck operations are a dance of chaos with steam constant motion and crowding. These operations are conducted during day or night and rain or snow when visibility is extremely poor in the unforgiving maritime environment. Moreover fleet operations are continually subject to reduced manning and resistance to change. It is desired that in these kinds of conditions Unmanned Combat Air Vehicles UCAV shall be launched from the flight decks of aircraft carriers.

In order to launch a UCAV from a flight deck the UCAV must be controlled during taxiing somehow before takeoff and after landing. Simply hooking a tow tractor to the aircraft has been considered but was deemed too slow especially since the aircraft are required to recover every 45 seconds and need to taxi out of the landing area for the next aircraft. Alternatively providing the aircraft director controller with a joystick to control the aircraft would tax his her workload with an additional process and would negatively impact training and operations.

Further if the UCAV is to be controlled on deck using a joystick the UCAV would necessarily be controlled vi radio RF link. However a RF link from a control device to the UAV is undesirable because of the EMI electromagnetic interference intensive environment on the flight deck and the EMCON constraints. Another alternative is a tethered connection using a control device physically tethered to the UCAV. However such a tethered connection may be potentially unsafe for the personnel on the deck during high tempo operations.

Just like manned aircraft the UCAVs taxi before launch after recoveries or during re spotting. In the case of manned aircraft flight deck controllers signal directions to pilots for taxiing the aircraft around the deck or airfield. It would be most desirable if these signals were used to develop an automatic taxiing system for unmanned aircraft as well. If such a system were developed it would enable a seamless transition of UCAVs into naval aviation.

It is an object of the present invention to overcome the difficulties discussed above and provide a system and software program for use in such a system to allow a user to remotely control a robotic device using merely gestures or motion signals.

Further it is an object of the present invention to overcome the difficulties discussed above using a machine vision based approach which would least impact operations and training and therefore held the most promise from the operational point of view.

It is another object of the present invention to provide a system as described above using sensor s mounted on the robotic device in conjunction with image recognition software residing on an onboard computer to provide inputs to the robotic devices control system.

In order to achieve the objects of the present invention as discussed above the present inventors have earnestly endeavored to create a computer controlled system to enable control of robotic devices by a human user using gestures and movements that can be recognized by the system and cause the robotic device to react thereto. A gesture recognition system is provided comprising 

The present invention also provides the gesture recognition system with a robotic device further comprising panic buttons for manually overriding the gesture recognition system and

The present invention further provides a gesture recognition system with the feature of computer control that allows a user to override the computer application code by inputting direct user command data into the high level of control computing means and or the low level of control computing means.

The present invention further provides a gesture recognition system with the feature wherein the video camera is capable of recording in the visible or infrared region.

The present invention also provides computer software for recognition of gesture or movement of a user and transformation of the recognized gesture or movement to user commands and control voltage commands to remotely operate a robotic device is provided comprising 

The application software for implementing the present invention also includes the capacity of controlling zoom pan and tilt camera states comparing the relative left hand point and relative right hand point of a previous video frame to the relative left hand point and relative right hand point of a current video frame so as to transform the change on relative positions of the left hand points and right hand points to waveform data and the capability of extracting dynamic and static features of movement of the user from the waveform data and

The low level of computer control of the present invention is capable of decoding the dynamic and static features by correlating the features to user command rules and generating a user command therefrom receiving the user command from the high level of control application code determining current states of electric motors or actuators calculating desired states of the electric motors or actuators based on the received user command computing the errors between the current states of the electric motors or actuators and the desired states of the electric motors or actuators transforming said computed errors between the current states of the electric motors or actuator and the desired states of the electric motors or actuators into control voltage command s for production of electric current voltage s to drive one or more of the electric motors or actuators.

The low level of computer control of the present invention is capable of generating feedback messages to for display to the user to inform the user of lack of or acquisition of control of the robotic device. The low level of computer control is also operable to allow a user to override the computer application by inputting direct user command data.

Computer human interface systems may require understanding of gestures as a means of input or control. Directing of Unmanned Air Vehicles UAV s during taxiing operations on carrier decks or airfields is such a case. There are numerous other applications for automatic gesture recognition as well. For example if gesture recognition were available it would be possible to automate many tasks in construction such as crane operation while rigging construction elements in place or control of underwater assembly robots by divers.

Further understanding and recognizing human motion gestures is very desirable in tracking and surveillance as well as training systems. In addition entertainment applications may be used to analyze the actions of the person to better aid in the immersion or reactivity of the experience.

The present inventors endeavored to determine the feasibility of a gesture recognition system to control a robot device using only a video camera as an input means for controlling signals. Limited gesture lexicon was used to control a laboratory robot to perform controlled motions. All gesture recognition algorithms were based on pixel to waveform transformation and these waveforms were used for further interpretation of the gesture signals.

Several gesture recognition methods were implemented and tested. In particular a pure rule based system as well as a hybrid rule system based on neural network systems were built to recognize the dynamic and static gesturing. The experiments included a system that used subsumption architecture to suppress currently active behavior by another one. The subsumption was triggered by an urgent condition to force a new action i.e. immediate stop when necessary. As a result of this research we were able to control laboratory robot in real time using dynamic gesturing as well as static posturing depending on required robot motion.

The system of the present invention uses hand gestures to control a robotic device as shown in . In particular as described in the first embodiment of the present invention the system consists of high level of control computer video camera a low level of control computer and a robotic device . The high level of control computer running the high level of control gesture recognition application code of the present invention receives imagery data from the video camera filters the data emphasizing the target points to search and track finds the three important points X and Y coordinates from the regions of interest transforms pixel space data into waveform data and discards bad data. To address day night all weather operations for the real life systems it is necessary to implement Infrared Video Data Acquisition IVDA . The IVDA would require both camera and a set of markers to work in the infrared spectrum of light. Infrared markers should be placed on human controller in the important points such as cranial lower torso and both hands. The power supply with the control circuitry could be placed on the controller belt around the waist. A robotic device such as J UCAS in this case would use infrared camera that outputs video frames in gray scale instead of RGB or HSL formats. Optical filters would enhance the data by suppressing possible data noise present in the imagery even before the light is converted to electrical signals in the camera. Any data noise that was not filtered by the optical filtering could be filtered by digital signal processing means. To make the overall system work with taxiing deck operations the cranial and torso markers should be frequency modulated. The frequency modulation would serve two purposes. First it would help to detect a controller within the possible infrared noisy data. Since noise does not beacon it is clear that beaconing signal belongs to a human controller. Second it is possible to indicate an active controller with a specific frequency. It is important because deck operations are done with multiple aircraft and multiple controllers. Each controller can have different frequency infrared markers. If a robotic device is tuned to 5 HZ beacons it will ignore another controller with 10 HZ beacons that may work with another J UCAS at the same time.

The high level of control computer running the high level of control gesture recognition application code of the present invention then translates the coordinates in pixel coordinates of the important points to relative coordinates with reference to the center point extracts features from the waveforms determines a motion decision sends the user command to the low level of control computer .

The low level of control computer as shown in receives the user command from the high level of control computer . The low level of control computer then uses this command to assign target states for all four wheels of the robotic device . In the case of a UCAV the low level of control computer may control the flaps engine wheels etc. so as to properly control the movement of the UCAV.

The low level of control computer then computes control errors by monitoring and determining the current wheel states flap states etc. and desired target states. After the errors are known the low level of control computer drives the errors to zero by proportional control applied to all power amplifiers which control the motors.

As shown in and respectively the high level of control system has a vocabulary of seven commands forward left right reverse have control enable disable and stop. When using the system of the present invention generally the controlling director wears a light colored shirt yellow for example a cranial cap with dark goggles and dark gloves. Further the controlling director stands so that his chest is approximately in the center of the cameras field of view without any significant rotation with respect to the camera. However rotation shift and scaling problems for reliable gesture recognition have been minimized by the present computer program.

Proper classification of dynamic gestures uses static as well as dynamic information contained in gestures to provide reliable classification. For example a single snapshot of the move forward command is nearly identical to stop hands at face level about 1 to 2 fist widths from the head . Taken in time however it can be seen that the move forward is dynamic while stop is static. Thus the software and system herein can recognize that with the stop command the hands at face level have not changed over two or more frames.

Second the classification of dynamic gestures utilized by the computer software of the present invention is general enough to combine different styles of signals in one class if they mean the same command. Third the code of the computer software of the present invention is capable of distinguishing between two different gestures looking almost the same but meaning different signals.

The computer software of the present invention achieves the above effects by searching for the three relevant points and then storing their locations as pixel locations in the video frame. If data is missing or wrong the system of the present invention will compensate for this lost or corrupted data by predicting their proper values. The hand locations are then normalized to the head location to compensate for camera or director movement.

The normalized hand points are then split into 2 one dimensional locations x and y coordinates and stored in a hand specific and dimension specific history array. Since human gestures that are not static tend to be sinusoidal in nature each of the 4 arrays is converted to a waveform and critical information such as frequency phase and amplitude is extracted. This transformation from pixel domain to the waveform characteristics domain leads to a huge payoff data reduction and convenient data representation for elegant classification in real time.

The position of hands amplitude phase and frequency are major features that are analyzed by the computer software of the present invention for class indicators of gesture signals. A selection of class indicators that identifies a command across all styles of the same command is called generalization. It allows for grouping of various styles of signals in to the same class. For example two hands positioned below above or on the level of head and waved in 180 degree phase manner are used to signal move forward .

Therefore hands position in the Y axis cannot not be used as a class indicator. However if we considered the phase of these waveforms combined with a rather large amplitude in the X axis and a rather small amplitude in the Y axis and also approximately equal distance between the hands in the X axis during the gesturing the computer software determines that these characteristics are unique characteristics for the move forward class of gestures.

The command interpretation mechanism considers only unique class indicators pertinent to a specific class of gesturing. For instance with the gesture commands move forward and slow down both gestures are valid when hands are positioned below the head. Both gestures are motioned in a 180 degree phase manner and could have the same frequency. In this case too much generalization could work against reliable classification. Hence the class indicator space must use necessary and sufficient characteristics higher dimensionality to reduce generalization. In this case it is necessary to add to a decision mechanism for example the amplitudes in Y axis for both hands because this indicator distinguishes both commands on the indicator space.

Before any processing of the acquired video imagery can be performed by the high level of control computer running the computer software of the present invention a picture must be acquired and critical points identified. This process as shown in the flow diagram illustrated in is termed an acquisition phase. Grayscale mono vision imagery can be used as well as color imagery with stereovision information. Naturally better performance is expected because of more capable data acquisition.

For example with grayscale the video camera acquisition board takes a grayscale image of the gesturing subject. Since grayscale assigns a value of 0 255 for each pixel with 0 being black multiplying every pixel by 4 will white out most of the image. By changing the contrast of the image to 90 small non black areas can be filtered out. The next step is to establish search regions for the left and right hands. Since none of the gestures in our limited vocabulary involves hand crossovers left hand on the right side of the body or vice versa it is assumed that each hand will be on its respective side of the head.

Since the head has not yet been identified the head point from the previously identified image is used. In order to keep the primitive search algorithms from confusing the edges of the head with the hands a small offset is subtracted from the search region. shows a sample image with the left and right search regions. Note that the search regions do not meet in the center but are offset to keep the head from being included. Once the search regions have been established the left and right hand are found.

Once the hands are found the head must be located on X Y space as well. A search region for the head is defined as the area between the hands. shows a sample image with the head search region. As with the hands once the search region is determined the head is then easily found. The cycle of using the head to find the hands followed by using the hands to find the head can never start without an external initialization. Therefore until a head point is found a small region in the center of the image is defined as the head search area. Once the head is found left and right hand search regions can be established and the acquisition begins.

Since no acquisition is perfect it is possible that the search algorithms may find the wrong point or not find any matching point. The problem of finding the wrong point is solved by setting the search regions where the points are expected to be as described above and by setting the match score high enough to exclude non desired points. However this adds to the problem of not finding any points. The problem of imperfect data acquisition becomes even more pronounced when the target search and tracking algorithm for finding the importance gesturing points in space head and hands is not fast enough. This problem is solved by data analysis and error filtering methods.

Analysis of standard gesture control showed that frequency of command change could go as high as 2 signals per second. Thus the minimum data history requirement is at least 0.5 seconds worth of data. From the experiments and observation of waveforms the present inventors determined that for reliable class indicator extraction at least twice as much historical data was needed as previously thought. Therefore the system of the present invention maintains approximately 1 to 2 seconds worth of data with sampling time varying anywhere from 0.10 to 0.17 seconds per sample approx 5 to 10 samples per second . To simplify data handling the buffer is fixed to constant 13 data points. This provides the necessary desired length of data history.

Raising the sampling rate to at least 50 samples per second the higher the better improves overall system performance perhaps even eliminating the necessity for error filtering. Three error filtering methods are generally used by the high level control computer in image processing See and are described in detail below. The first method is based on data delay to forecast the missing point s . The second method uses a statistical approach to replace errors with statistically expected data. The third method uses a Least Mean Squares estimator to forecast the missing or bad data because of the video acquisition errors.

In order to account for missing points a delayed decision point predication method is used as shown in Equation 1 below. Instead of the current sample of points S being used to make a decision all decisions are based off the previously sampled set of points S . This allows Sto be used to predict where Swill be has been should the search algorithm not find a point at S. In the event a point is missed Sbecomes the average of the previous point and the next point or 2 Equation 1

In the event that both Sand Sare missing Sis assigned to Sto keep the acquisition going. Once the location of the points has been determined or predicted the points are then normalized to the head or center point S as can be seen in Equation 2 below.

Where Sis a sample of center point head Sis a sample of right hand and finally Sis a sample of left hand and primed S samples are normalized to the center. In effect the axis origin is moved from the computer defined upper left corner to the center point. This point is stored in a waveform that is axis and point specific there are 4 waveforms Left Y Left X Right Y and Right X . Since repetitive human motions tend to be sinusoidal features such as amplitude frequency and phase can be extracted for use in classifying the motion by using a point prediction routine. The point prediction routine carried out by the algorithm of the present invention is illustrated in and implemented by the high level computer control.

The previously discussed method of error filtering is based on delaying the data for further data normalization feature extraction and motion decision. The second method of error filtering a statistical method is implemented to eliminate the necessity for the time delay. In this method previously collected data is used to judge the quality of the presently received data sample. The availability of data history permits the computation of several statistical characteristics to judge the quality of new data points. Namely mean and standard deviation values for every data buffer are computed in real time for currently maintained set of samples.

As new data sample arrives at time N it is checked for belonging to the set using standard deviation of the set. If the new data is within the limits of standard deviation it is used to update the sample buffer. If it is outside of the standard deviation it is rejected and the expected mean value is used instead. illustrates the algorithm and flow chart of this filtering process.

Given a vector of sample data Y containing N entries it is desirable to determine if the next sample yis in fact a correct sample or erroneous one. The method is based on statistical least squares estimation to place a judgment on the next piece of data.

Using previously compiled data from time t 0 to t N inclusively the next point at time t N 1 is estimated then by comparing the estimated value with the actual measurement at time t N 1 when it finally has arrived the error between the two values is computed. If the error is acceptable the actual measurement is added to the list of samples Y. Otherwise the estimated sample is used. This one step look ahead method allows keeping outliers from affecting the data that is used for further decision making on gesture interpretation.

If small error thresholds are used to filter out the high value jumps or deep falls this process effectively filters out the high frequency components in the data stream. It must be noted that selecting a very small threshold can be damaging to the overall system because the system is not able to adjust to transition from one gesture to another and will get trapped in the previous steady state. Hence the value of the error threshold should be tuned for specific application. Generally gesturing is performed at approximately 0 5 HZ interval. Therefore it is relatively easy to select a good error threshold for the Least Squares Estimate as described below.

Let two vectors T t t t . . . t and Y y y y . . . y be sampling time values and measurement data respectively as shown in where N is a number of samples defined as needed. Then a system of linear. Equations B1 as illustrated below is used to find coefficients a a athat provide second order estimate for a function that guarantied a minimum least square fit shown as illustrated in Equation B2 below.

Equation B5 is an estimate of the next sampling time that can be used in Equation B2 to obtain an estimate of the next measurement y Equation B5

Consider a one dimensional case say dimension x and one tracked point in the imagery stream. Let N 6 and

All motion decision methods of the gesture recognition system of the present invention are based on pixel to waveform transformation. The X Y coordinates shown in for the three points of reference are arranged in data arrays representing their location history with reference to sampling time during the gesturing. These arrays are normalized to the center point coordinate system translation and then analyzed for the most important features amplitude frequency and phase and their relative locations.

Several decision making algorithms were developed by the present inventors to interpret the gestures. For initial experiments only three points were tracked head center left hand and right hand points. The pixel space was preprocessed to highlight the important three points as illustrated in and and converted to waveform representation. The tracked data was then arranged in data arrays representing X and Y waveforms as illustrated in .

The theory for the presently claimed system of gesture interpretation is that if someone can visually distinguish a signal from other signal s in spatial pixel domain then this difference should be encapsulated in the constituent waveforms that represent these visual differences. As shown in higher amplitudes on the X axis and lower amplitudes on the Y axis for both left and right hands characterizes forward motion. Another important characteristic of forward signal is 180 degrees phase of left and right waveforms.

The first motion decision technique used in the present invention is based on a threshold approach and executed as shown in in the Make Command Decision block wherein all of the acquired waveform features are passed through a set of thresholds as antecedents to the decision making rules. In essence this is an expert system that considers the features in combinations and forces one of the decision rules to fire i.e. to be implemented . The most important features are amplitude frequency and phase of the waveforms. Also an average X Y location of the important points i.e. hands of the gesturing director is used to determine other aspects of the signals such as hands height positions hands horizontal extensions hands overlap.

When all of the features are extracted from the waveforms a decision can be made by the system and computer program herein. The extracted features in addition to the current locations and an average of the most recent 3 points are compared to a set of rules describing the possible commands. The data is then compared to each possible command which in turn returns a true data matches this command or false data doesn t match this command . If only one command is true then that becomes the decision. If more then one is true then it is reported as an error and the stop command is enforced.

To prevent poor command decisions each command decision is placed into a buffer. The command output by the computer is the fourth command in the buffer. If a decision in the first and fourth slots in the buffer match each other but don t match the second or third decisions both the second decision and the third decision are forced to correspond to the first decision. This way transitions e.g. fwd to left movement of the vehicle are captured but system jitters are rejected.

Specifically five commands forward back left right and stop can be implemented using the rule based approach with thresholds. All threshold units are in pixels or Hz and their values can be found in the rules below. Top level logic to run this High Level Control is if two or more commands are true or if no commands are true an error is returned. Otherwise the resultant command is sent. Details of each of the five commands are as follows 

All gestures except for stop are turned off and stop is turned on if one of the following conditions applies 

1. Amplitude of any hand axis drops below 1 pixel or all hand axis amplitudes are below 10 pixels. This is to force a stop condition when there is no relevant movement. Usually when amplitudes drop to these levels it is because the search algorithms are returning a motionless object.

2. Frequency of all hand axes are above 3 Hz. When the primary frequency is above 3 Hz it means that the object that is found is not moving and that the search algorithms are returning varying points on the same object.

In addition to the previous two conditions stop is also found to be true if left y and right y positions are greater then 70 above the neck and absolute difference between the x positions of the two hands of the director is less then 50 i.e. when the hands are both about the same distance from the head .

A forward command is true when both the left and right hand Y positions are between 70 and 150 across the chest the difference between left hand X and right hand X is less then 25 i.e. when they are both about the same distance from the centerline of the body and left X and right X amplitude is greater then 10.

A back command is given whenever left and right hand Y positions are greater then 150 i.e. the hands of the director are positioned about the level of the waist line and below and as stated before there is enough movement to prevent a fall into stop condition.

A left command occurs when the difference between the left hand and right hand X positions is greater then 50 i.e. the right hand is positioned close to the centerline and the left hand is positioned far from the centerline or just the right hand X amplitude is greater then 20 while the left hand X amplitude is less then 10 i.e. the right hand is waving while the left hand is pointing . Finally the right command is the exact opposite of the left command.

The second method used in the present invention is based on subsumption architecture using rules as well as Fuzzy Adaptive Resonance Theory Fuzzy ART Neural Network to interpret the waveforms. In this approach all pertinent motions are divided into two types of behaviors reflexive and cognitive. Reflexive behavior is one that has a higher degree of urgency or higher priority of actions. This behavior is similar to a reaction of someone touching a hot surface the reflex is to stop the sensation immediately. These behaviors subsume all others and do not require a transient time for stabilization of data when a gesture signal is changed.

For this purpose the system and computer program of the present invention does not use data averaged over time. It simply checks the most recent hands location by constantly propagating the data through the neural network that imposed the reflexive action on the rest of the decision making system when data indicated such a command. The subsumption trigger occurs when hands enter certain regions defined by the Fuzzy ART weights and the current waveforms do not indicate dynamic motion i.e. the command is static posture . The subsumption decision making process is illustrated in .

The stop enable and disable signals are defined as reflexive motions while move forward left right and back are interpreted as the cognitive group. The major difference between the reflexive and cognitive groups is that for the reflexive motions the system does not wait until the transient period has ended. These commands are applied immediately even if the system is in the middle of other types of motion.

An example of subsumption behavior is a situation in which a vehicle is in motion forward while the director is changing the signal from forward to left. The system collects data even during transition of signaling from forward to left command. Since this data is inconclusive the vehicle will still perform forward motion but will slow down and not come to an abrupt stop. As data becomes clear the vehicle transitions to left motion.

However if the hand s enter a specific region around the head even by accident the system checks the current waveform for dynamic gesturing or static pose and might trigger stop or disable commands interrupting current vehicle motion. Hence stop or disable behavior subsumes all others. The major shortcoming of such an implementation is occasional choppy behavior manifesting itself in rear unexpected stops. However the benefit of this implementation is far greater safety of operation in a confined environment. It should be noted that an unwanted disable command never occurs due to the complex nature of pose one hand is hidden behind the directors back while the other hand is raised just above the head. This pose is difficult to reproduce accidentally since both hands are used during all other operations.

The rules of motion to recognize the commands depend on characteristics of the waveforms for the signals. It is desirable to reduce characteristics of the waveforms to some kind of class indicators preferably to some unit less numbers to make the system more robust or tolerable to scaling effect due to the different distances of the controlling director from the camera. The same applies to the rotation problem due to change of the director s orientation with respect to the camera. Specific decision rules for certain movement correspond to certain variables as shown in and as described in Table 1 Motion Variables for Class Indicators below.

Further in particular defines basic variables for the three important points left or right hands and center point. These points are found on each frame their X and Y coordinates X X yy as the video stream is coming from the camera.

The above variables are used to compute class indicators for forward left right stop and back motions. In addition to the above stop enable and disable commands are implemented in Fuzzy ART weights. Note that the stop command is duplicated in both systems of decision to provide redundancy for this important command.

Amplitudes in the X direction greater than in the Y direction with both hands positioned approximately symmetrically with respect to the head while both hands are waved in out of phase fashion characterize a forward command.

Both hands dropped below the waist without any motion characterize the go back command. It should be noted that the go back command was not implemented exactly as the NATOPS standard signal vocabulary suggested. This command can be implemented by stipulating large amplitudes in Y direction and small amplitudes in X direction while both hands are below the center point with palms facing forward.

The left side hand from the robots point of view extended outwards and kept still while the right side hand is raised to the head level and repeatedly pivoting with respect to the elbow characterizes the go left command.

The right side hand from the robots point of view extended outwards and kept still while the left side hand is raised to head level and repeatedly pivoting with respect to the elbow characterizes the go right command.

Both hands raised above the head without any motion and with fists closed defines a stop command. For implementation of the NATOPS like style stop signal it is necessary to recognize clinched fist and open palm gesturing. In an alternative embodiment of the present invention implementation of color video acquisition is provided. This color video acquisition in conjunction with appropriate marking on gloves allows the system of the present invention to distinguish a user s closed fist and open palm gestures and transform them to information in data stream.

The basic principle behind fuzzy logic is the argument that a set A can intersect with its compliment A A A 0 . The idea is that instead of a unique existence for A and A there exists a fuzziness at the boundaries between the two sets. The degree of fuzziness is a measure of this uncertainty. So it can be extended that if more than one set exist within this universe the measure of uncertainty is not a crisp result but a measure of degree fuzziness that one set belongs to another within this region of uncertainty. Using this relationship each set can be viewed as a fuzzy set. In these fuzzy sets the elements contain a measure of membership between all sets within their universe.

In the present invention three commands i.e. stop enable and disable are provided in a fuzzy ART recognition subsystem of the present invention. All commands involve a user s static posturing containing no dynamic information and therefore involve using only location variables. As soon as the user s hands enter a specific area determined by the weights shown below in Table 2 the waveforms are checked for dynamic characteristics such as amplitude and directional vectors. If the dynamic characteristics are not present then a subsumption trigger will occur. The fuzzy ART neural network will fire an appropriate node after propagating the current hand locations through the weights as described below.

In particular the system of the present invention can determine the relationship of memorized patterns to the current pattern movements of the user presented for recognition. The weighted vectors stored in memory represent the patterns to be recognized. The intersection of the weighted patterns forms a region of uncertainty that presents difficulty for the standard classifiers to deal with. The region of uncertainty for Fuzzy ART can actually work to its advantage.

First the input to the input layer F of Fuzzy ART normalizes the real number values within the range of 0 1 by the equation a IN IN where IN is the input vector supplied to the input layer and IN INi . Normalization is performed to establish membership functions. The compliment of ais a 1 a thus the input to the vigilance layer F becomes I a a . Notice the input set includes both the actual input and its compliment. Therefore for each entry there exists a unique compliment with the exception when a 0.5 a 1 a 0.5 .

Network weights initially are set to 1 where w 0 1 . . . w 0 1 AND where m is the number of inputs note that there are 2 m weight for each row including the compliment . The output nodes of layer Fare uncommitted. Weights are determined by the test for vigilance

w I w 1 w I 1 w and if is set to 1 fast learning occurs giving the update values by w I . As is increased the degree of fuzziness is decreased and as p is decreased the opposite is true. Thus defining the width of the membership between each training set.

The code for the Gesture Recognition System was written in the National Instruments Inc. object oriented graphical computer language named LabVIEW. The LabVIEW is a development environment geared towards control automation processes. The source code listing is presented in Appendix B.

Main function Combined HLC and LLC ver 5.vi contains the main control loop. This loop runs three major sequences for the High Level Control. In addition to the High Level Control this main loop runs function called LLC for PID with Dig Inhibit.vi . That function contains Low Level Control for the Gesture Recognition System.

First sequence contains Image Acquisition and Controller Marker Tracking algorithms. This is done by the track three points ver 4.vi function. The second sequence contains feature extraction algorithms ran by function named hand features.vi The third sequence contains motion decision algorithm and the LLC goal generation algorithm. Motion decision is done by function called command decoder.vi and the goal generation is done by convertCommand4LLC from differencial ver 51.vi function.

In addition to the above key function there are several utility functions Trigger command.vi . It checks if command has stabilized. If current command is the same as the previous commands in command buffer then trigger action. Normally it is sufficient to have 2 or 3 previous commands in the buffer. Latch on enable.vi function allows to enable the vehicle and retain this state of control until disable command is given. Get Img Acq Params.vi function demultiplexes a bundle of initialized image parameters. The group of functions located outside the control loop is initialization functions and closing functions necessary to graceful start up and shutdown of the system.

Although this invention has been described in relation to an exemplary embodiment thereof it will be understood by those skilled in the art that still other variations and modifications can be affected in the preferred embodiment without detracting from the scope and spirit of the invention as described in the claims.


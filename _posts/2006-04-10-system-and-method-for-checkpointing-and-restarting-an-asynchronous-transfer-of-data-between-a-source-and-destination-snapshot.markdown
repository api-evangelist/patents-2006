---

title: System and method for checkpointing and restarting an asynchronous transfer of data between a source and destination snapshot
abstract: A method for transferring data from a source computer to a destination computer, and restarting the source computer transmission after a halt of the transmission, has the following steps. The source computer inserts a checkpoint number into a data stream, the data flowing in the data stream. The checkpoint number is stored at the source computer. The data stream is halted in response to an error in a path of the data stream. The destination computer locates the last checkpoint number successfully received before halting the data stream. The last checkpoint number successfully received before halting the data stream is transferred to the source computer; and the source computer compares it with checkpoint numbers stored in the source computer to determine where in the data stream to resume transfer of the data. Transfer of the data is resumed in response to the last checkpoint number.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07769717&OS=07769717&RS=07769717
owner: NetApp, Inc.
number: 07769717
owner_city: Sunnyvale
owner_country: US
publication_date: 20060410
---
This application is a Continuation of U.S. Ser. No. 10 126 822 filed on Apr. 19 2002 entitled SYSTEM AND METHOD FOR CHECKPOINTING AND RESTARTING AN ASYCHRONOUS TRANSFER OF DATA BETWEEN A SOURCE AND DESTINATION SNAPSHOT now issued as U.S. Pat. No. 7 039 663 on May 2 2006.

Ser. No. 10 100 950 filed Mar. 19 2002 entitled SYSTEM AND METHOD FOR ASYNCHRONOUS MIRRORING OF SNAPSHOTS AT A DESTINATION USING A PURGATORY DIRECTORY AND INODE MAPPING now issued as U.S. Pat. No. 7 225 204 on May 29 2007 by Stephen L. Manley et al. the teachings of which are expressly incorporated herein by reference 

Ser. No. 10 100 945 filed Mar. 19 2002 entitled SYSTEM AND METHOD FOR STORAGE OF SNAPSHOT METADATA IN A REMOTE FILE now issued as U.S. Pat. No. 7 043 485 on May 9 2006 by Stephen L. Manley et al. the teachings of which are expressly incorporated herein by reference 

Ser. No. 10 100 434 filed Mar. 19 2002 entitled SYSTEM AND METHOD FOR REDIRECTING ACCESS TO A REMOTE MIRRORED SNAPSHOT now issued as U.S. Pat. No. 7 010 553 on Mar. 7 2006 by Raymond C. Chen et al. the teachings of which are expressly incorporated herein by reference 

Ser. No. 10 100 879 filed Mar. 19 2002 entitled FORMAT FOR TRANSMISSION OF FILE SYSTEM INFORMATION BETWEEN A SOURCE AND A DESTINATION now issued as U.S. Pat. No. 7 007 046 on Feb. 28 2006 by Stephen L. Manley et al. the teachings of which are expressly incorporated herein by reference and

Ser. No. 10 100 967 filed Mar. 19 2002 entitled SYSTEM AND METHOD FOR DETERMINING CHANGES IN TWO SNAPSHOTS AND FOR TRANSMITTING CHANGES TO A DESTINATION SNAPSHOT now issued as U.S. Pat. No. 6 993 539 on Jan. 31 2006 by Michael L. Federwisch et al. the teachings of which are expressly incorporated herein by reference.

This invention relates to storage of data using file servers and more particularly to mirroring or replication of stored data in remote storage locations over a network.

A file server is a computer that provides file service relating to the organization of information on storage devices such as disks. The file server or filer includes a storage operating system that implements a file system to logically organize the information as a hierarchical structure of directories and files on the disks. Each on disk file may be implemented as a set of data structures e.g. disk blocks configured to store information. A directory on the other hand may be implemented as a specially formatted file in which information about other files and directories are stored.

A filer may be further configured to operate according to a client server model of information delivery to thereby allow many clients to access files stored on a server e.g. the filer. In this model the client may comprise an application such as a database application executing on a computer that connects to the filer over a direct connection or computer network such as a point to point link shared local area network LAN wide area network WAN or virtual private network VPN implemented over a public network such as the Internet. Each client may request the services of the file system on the filer by issuing file system protocol messages in the form of packets to the filer over the network.

A common type of file system is a write in place file system an example of which is the conventional Berkeley fast file system. By file system it is meant generally a structuring of data and metadata on a storage device such as disks which permits reading writing of data on those disks. In a write in place file system the locations of the data structures such as inodes and data blocks on disk are typically fixed. An inode is a data structure used to store information such as metadata about a file whereas the data blocks are structures used to store the actual data for the file. The information contained in an inode may include e.g. ownership of the file access permission for the file size of the file file type and references to locations on disk of the data blocks for the file. The references to the locations of the file data are provided by pointers in the inode which may further reference indirect blocks that in turn reference the data blocks depending upon the quantity of data in the file. Changes to the modes and data blocks are made in place in accordance with the write in place file system. If an update to a file extends the quantity of data for the file an additional data block is allocated and the appropriate inode is updated to reference that data block.

Another type of file system is a write anywhere file system that does not over write data on disks. If a data block on disk is retrieved read from disk into memory and dirtied with new data the data block is stored written to a new location on disk to thereby optimize write performance. A write anywhere file system may initially assume an optimal layout such that the data is substantially contiguously arranged on disks. The optimal disk layout results in efficient access operations particularly for sequential read operations directed to the disks. A particular example of a write anywhere file system that is configured to operate on a filer is the Write Anywhere File Layout WAFL file system available from Network Appliance Inc. of Sunnyvale Calif. The WAFL file system is implemented within a microkernel as part of the overall protocol stack of the filer and associated disk storage. This microkernel is supplied as part of Network Appliance s Data ONTAP software residing on the filer that processes file service requests from network attached clients.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer that manages data access and may in the case of a filer implement file system semantics such as the Data ONTAP storage operating system implemented as a microkernel and available from Network Appliance Inc. of Sunnyvale Calif. which implements a Write Anywhere File Layout WAFL file system. The storage operating system can also be implemented as an application program operating over a general purpose operating system such as UNIX or Windows NT or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

Disk storage is typically implemented as one or more storage volumes that comprise physical storage disks defining an overall logical arrangement of storage space. Currently available filer implementations can serve a large number of discrete volumes 150 or more for example . Each volume is associated with its own file system and for purposes hereof volume and file system shall generally be used synonymously. The disks within a volume are typically organized as one or more groups of Redundant Array of Independent or Inexpensive Disks RAID . RAID implementations enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate caching of parity information with respect to the striped data. In the example of a WAFL file system a RAID 4 implementation is advantageously employed. This implementation specifically entails the striping of data across a group of disks and separate parity caching within a selected disk of the RAID group. As described herein a volume typically comprises at least one data disk and one associated parity disk or possibly data parity partitions in a single disk arranged according to a RAID 4 or equivalent high reliability implementation.

In order to improve reliability and facilitate disaster recovery in the event of a failure of a filer its associated disks or some portion of the storage infrastructure it is common to mirror or replicate some or all of the underlying data and or the file system that organizes the data. In one example a mirror is established and stored at a remote site making it more likely that recovery is possible in the event of a true disaster that may physically damage the main storage location or it s infrastructure e.g. a flood power outage act of war etc. . The mirror is updated at regular intervals typically set by an administrator in an effort to catch the most recent changes to the file system. One common form of update involves the use of a snapshot process in which the active file system at the storage site consisting of inodes and blocks is captured and the snapshot is transmitted as a whole over a network such as the well known Internet to the remote storage site. Generally a snapshot is an image typically read only of a file system at a point in time which is stored on the same primary storage device as is the active file system and is accessible by users of the active file system. By active file system it is meant the file system to which current input output operations are being directed. The primary storage device e.g. a set of disks stores the active file system while a secondary storage e.g. a tape drive may be utilized to store backups of the active file system. Once snapshotted the active file system is reestablished leaving the snapshotted version in place for possible disaster recovery. Each time a snapshot occurs the old active file system becomes the new snapshot and the new active file system carries on recording any new changes. A set number of snapshots may be retained depending upon various time based and other criteria. The snapshotting process is described in further detail in U.S. patent application Ser. No. 09 932 578 entitled INSTANT SNAPSHOT by Blake Lewis et al. which is hereby incorporated by reference as though fully set forth herein. In addition the native Snapshot capabilities of the WAFL file system are further described in 3002 by David Hitz et al. published by Network Appliance Inc. and in commonly owned U.S. Pat. No. 5 819 292 entitled METHOD FOR MAINTAINING CONSISTENT STATES OF A FILE SYSTEM AND FOR CREATING USER ACCESSIBLE READ ONLY COPIES OF A FILE SYSTEM by David Hitz et al. which are hereby incorporated by reference.

The complete recopying of the entire file system to a remote destination site over a network may be quite inconvenient where the size of the file system is measured in tens or hundreds of gigabytes even terabytes . This full backup approach to remote data replication may severely tax the bandwidth of the network and also the processing capabilities of both the destination and source filer. One solution has been to limit the snapshot to only portions of a file system volume that have experienced changes. Hence shows a prior art volume based mirroring where a source file system is connected to a destination storage site consisting of a server and attached storage not shown via a network link . The destination receives periodic snapshot updates at some regular interval set by an administrator. These intervals are chosen based upon a variety of criteria including available bandwidth importance of the data frequency of changes and overall volume size.

In brief summary the source creates a pair of time separated snapshots of the volume. These can be created as part of the commit process in which data is committed to non volatile memory in the filer or by another mechanism. The new snapshot is a recent snapshot of the volume s active file system. The old snapshot is an older snapshot of the volume which should match the image of the file system replicated on the destination mirror. Note that the file server is free to continue work on new file service requests once the new snapshot is made. The new snapshot acts as a checkpoint of activity up to that time rather than an absolute representation of the then current volume state. A differencer scans the blocks in the old and new snapshots. In particular the differencer works in a block by block fashion examining the list of blocks in each snapshot to compare which blocks have been allocated. In the case of a write anywhere system the block is not reused as long as a snapshot references it thus a change in data is written to a new block. Where a change is identified denoted by a presence or absence of an X designating data a decision process shown in in the differencer decides whether to transmit the data to the destination . The process compares the old and new blocks as follows a Where data is in neither an old nor new block case as in old new block pair no data is available to transfer. b Where data is in the old block but not the new case as in old new block pair such data has already been transferred and any new destination snapshot pointers will ignore it so the new block state is not transmitted. c Where data is present in the both the old block and the new block case as in the old new block pair no change has occurred and the block data has already been transferred in a previous snapshot. d Finally where the data is not in the old block but is in the new block case as in old new block pair then a changed data block is transferred over the network to become part of the changed volume snapshot set at the destination as a changed block . In the exemplary write anywhere arrangement the changed blocks are written to new unused locations in the storage array. Once all changed blocks are written a base file system information block that is the root pointer of the new snapshot is then committed to the destination. The transmitted file system information block is committed and updates the overall destination file system by pointing to the changed block structure in the destination and replacing the previous file system information block. The changes are at this point committed as the latest incremental update of the destination volume snapshot. This file system accurately represents the new snapshot on the source. In time a new new snapshot is created from further incremental changes.

Approaches to volume based remote mirroring of snapshots are described in detail in commonly owned U.S. patent application Ser. No. 09 127 497 entitled FILE SYSTEM IMAGE TRANSFER by Steven Kleiman et al. and U.S. patent application Ser. No. 09 426 409 entitled FILE SYSTEM IMAGE TRANSFER BETWEEN DISSIMILAR FILE SYSTEMS by Steven Kleiman et al. both of which patents are expressly incorporated herein by reference.

This volume based approach to incremental mirroring from a source to a remote storage destination is effective but may still be inefficient and time consuming as it forces an entire volume to be scanned for changes and those changes to be transmitted on a block by block basis. In other words the scan focuses on blocks without regard to any underlying information about the files inodes and data structures which the blocks comprise. The destination is organized as a set of volumes so a direct volume by volume mapping is established between source and destination. Again where a volume may contain a terabyte or more of information the block by block approach to scanning and comparing changes may still involve significant processor overhead and associated processing time. Often there may have been only minor changes in a sub block beneath the root inode block being scanned. Since a list of all blocks in the volume is being examined however the fact that many groupings of blocks files inode structures etc. are unchanged is not considered. In addition the increasingly large size and scope of a full volume make it highly desirable to sub divide the data being mirrored into sub groups because some groups are more likely to undergo frequent changes it may be desirable to update their replicas more often than other less frequently changed groups. In addition it may be desirable to mingle original and replicated snapshotted sub groups in a single volume and migrate certain key data to remote locations without migrating an entire volume. Accordingly a more sophisticated approach to scanning and identifying changed blocks may be desirable as well as a sub organization for the volume that allows for the mirroring of less than an entire volume.

One such sub organization of a volume is the well known qtree. Qtrees as implemented on an exemplary storage system such as described herein are subtrees in a volume s file system. One key feature of qtrees is that given a particular qtree any file or directory in the system can be quickly tested for membership in that qtree so they serve as a good way to organize the file system into discrete data sets. The use of qtrees as a source and destination for snapshotted data is desirable.

A transfer of update data from a source to a destination may take a significant amount of time. As an asynchronous lazy write procedure is used to perform the transfer several minutes or hours may elapse before the transfer completes. Within the intervening time it is possible that a crash communication failure or other exigent circumstance may interrupt the transfer.

Accordingly it is desirable to provide a technique for tracking the data committed to persistent storage by a destination receiving mirrored or other data streams.

This invention overcomes the disadvantages of the prior art by providing a system and method for generating checkpoints into an asynchronous transfer of data between a source file system and a replicated destination file system so as to allow the source to maintain synchronization in the transfer with the current state of the destination. More specifically in an illustrative embodiment the data transferred from the source to the destination typically represents changes between a base and incremental snapshot on the source file system used for updating the replicated destination file system so that it reflects the state of the incremental snapshot.

In an illustrative embodiment a discrete checkpoint value or number is inserted into a segment of the overall data stream being sent from a source to a destination at regular intervals. These intervals can be based upon a predetermined an amount of data transferred an elapsed time or both. Concurrently with the insertion of this checkpoint number the state of the source is stored in a source based registry that is associated with the checkpoint number. In the event of a loss of communication between the source and the destination file systems after some predetermined time lapsed or event indicating a communication loss the destination can alert the source to the last checkpoint reached by the destination in committing the data stream to a destination related persistent storage medium such as a disk NVRAM or the like. The source can follow up the alert by utilizing the checkpoint information to reinitialize the so as to retransmit the data stream starting from the segment following the last fully committed segment i.e. the beginning of the next incremental checkpoint number after the number provided by the destination . Alternatively an earlier checkpoint than the last checkpoint can be used. In other words the resumed procedure will send any new data starting from the last checkpointed segment successfully written or stored by the destination.

In an illustrative embodiment the checkpoint numbers are provided to the data stream as entries in data format headers. The data format can be an extensible file system independent format adapted to be encapsulated in a networking protocol such as TCP IP. Additionally the changes sent from the source file system to the destination file system relate to a sub organization of a volume on the source. In one embodiment this sub organization is a qtree identified by a qtree identifier ID in the associated modes and data stream.

By way of further background is a schematic block diagram of a storage system environment that includes a pair of interconnected file servers including a source file server and a destination file server that may each be advantageously used with the present invention. For the purposes of this description the source file server is a networked computer that manages storage one or more source volumes each having an array of storage disks described further below . Likewise the destination filer manages one or more destination volumes also comprising arrays of disks . The source and destination file servers or filers are linked via a network that can comprise a local or wide area network such as the well known Internet. An appropriate network adapter residing in each filer facilitates communication over the network . Also for the purposes of this description like components in each of the source and destination filer and respectively are described with like reference numerals. As used herein the term source can be broadly defined as a location from which the subject data of this invention travels and the term destination can be defined as the location to which the data travels. While a source filer and a destination filer connected by a network is a particular example of a source and destination used herein a source and destination could be computers filers linked via a direct link or via loopback a networking arrangement internal to a single computer for transmitting a data stream between local source and local destination in which case the source and the destination are the same filer. As will be described further below the source and destination are broadly considered to be a source sub organization of a volume and a destination sub organization of a volume. Indeed in at least one special case the source and destination sub organizations can be the same at different points in time.

In the particular example of a pair of networked source and destination filers each filer and can be any type of special purpose computer e.g. server or general purpose computer including a standalone computer. The source and destination filers each comprise a processor a memory a network adapter and a storage adapter interconnected by a system bus . Each filer also includes a storage operating system that implements a file system to logically organize the information as a hierarchical structure of directories and files on the disks.

It will be understood to those skilled in the art that the inventive technique described herein may apply to any type of special purpose computer e.g. file serving appliance or general purpose computer including a standalone computer embodied as a storage system. To that end the filers and can each be broadly and alternatively referred to as storage systems. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client host computer. The term storage system should therefore be taken broadly to include such arrangements.

In the illustrative embodiment the memory comprises storage locations that are addressable by the processor and adapters for storing software program code. The memory comprises a form of random access memory RAM that is generally cleared by a power cycle or other reboot operation i.e. it is volatile memory . The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The operating system portions of which are typically resident in memory and executed by the processing elements functionally organizes the filer by inter alia invoking storage operations in support of a file service implemented by the filer. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the inventive technique described herein.

The network adapter comprises the mechanical electrical and signaling circuitry needed to connect each filer to the network which may comprise a point to point connection or a shared medium such as a local area network. Moreover the source filer may interact with the destination filer in accordance with a client server model of information delivery. That is the client may request the services of the filer and the filer may return the results of the services requested by the client by exchanging packets encapsulating e.g. the TCP IP protocol or another network protocol format over the network .

The storage adapter cooperates with the operating system executing on the filer to access information requested by the client. The information may be stored on the disks that are attached via the storage adapter to each filer or other node of a storage system as defined herein. The storage adapter includes input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance Fibre Channel serial link topology. The information is retrieved by the storage adapter and processed by the processor as part of the snapshot procedure to be described below prior to being forwarded over the system bus to the network adapter where the information is formatted into a packet and transmitted to the destination server as also described in detail below.

Each filer may also be interconnected with one or more clients via the network adapter . The clients transmit requests for file service to the source and destination filers respectively and receive responses to the requests over a LAN or other network . Data is transferred between the client and the respective filer using data packets defined as an encapsulation of the Common Internet File System CIFS protocol or another appropriate protocol such as NFS.

In one exemplary filer implementation each filer can include a nonvolatile random access memory NVRAM that provides fault tolerant backup of data enabling the integrity of filer transactions to survive a service interruption based upon a power failure or other fault. The size of the NVRAM depends in part upon its implementation and function in the file server. It is typically sized sufficiently to log a certain time based chunk of transactions for example several seconds worth . The NVRAM is filled in parallel with the buffer cache after each client request is completed but before the result of the request is returned to the requesting client.

In an illustrative embodiment the disks are arranged into a plurality of volumes for example source volumes and destination volumes in which each volume has a file system associated therewith. The volumes each include one or more disks . In one embodiment the physical disks are configured into RAID groups so that some disks store striped data and some disks store separate parity for the data in accordance with a preferred RAID 4 configuration. However other configurations e.g. RAID 5 having distributed parity across stripes are also contemplated. In this embodiment a minimum of one parity disk and one data disk is employed. However a typical implementation may include three data and one parity disk per RAID group and a multiplicity of RAID groups per volume.

To facilitate generalized access to the disks the storage operating system implements a write anywhere file system that logically organizes the information as a hierarchical structure of directories and files on the disks. Each on disk file may be implemented as a set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which references to other files and directories are stored. As noted and defined above in the illustrative embodiment described herein the storage operating system is the NetApp Data ONTAP operating system available from Network Appliance Inc. of Sunnyvale Calif. that implements the Write Anywhere File Layout WAFL file system. It is expressly contemplated that any appropriate file system can be used and as such where the term WAFL is employed it should be taken broadly to refer to any file system that is otherwise adaptable to the teachings of this invention.

The organization of the preferred storage operating system for each of the exemplary filers is now described briefly. However it is expressly contemplated that the principles of this invention can be implemented using a variety of alternate storage operating system architectures. In addition the particular functions implemented on each of the source and destination filers may vary. As shown in the exemplary storage operating system comprises a series of software layers including a media access layer of network drivers e.g. an Ethernet driver . The operating system further includes network protocol layers such as the Internet Protocol IP layer and its supporting transport mechanisms the Transport Control Protocol TCP layer and the User Datagram Protocol UDP layer . A file system protocol layer provides multi protocol data access and to that end includes support for the CIFS protocol the NFS protocol and the Hypertext Transfer Protocol HTTP protocol . In addition the storage operating system includes a disk storage layer that implements a disk storage protocol such as a RAID protocol and a disk driver layer that implements a disk control protocol such as the small computer system interface SCSI .

Bridging the disk software layers with the network and file system protocol layers is a file system layer of the storage operating system . Generally the layer implements a file system having an on disk format representation that is block based using e.g. 4 kilobyte KB data blocks and using inodes to describe the files. In response to transaction requests the file system generates operations to load retrieve the requested data from volumes if it is not resident in core i.e. in the filer s memory . If the information is not in memory the file system layer indexes into the inode file using the inode number to access an appropriate entry and retrieve a volume block number. The file system layer then passes the volume block number to the disk storage RAID layer which maps that volume block number to a disk block number and sends the latter to an appropriate driver for example an encapsulation of SCSI implemented on a fibre channel disk interconnection of the disk driver layer . The disk driver accesses the disk block number from volumes and loads the requested data in memory for processing by the filer . Upon completion of the request the filer and storage operating system returns a reply e.g. a conventional acknowledgement packet defined by the CIFS specification to the client over the respective network connection .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the filer may alternatively be implemented in hardware or a combination of hardware and software. That is in an alternate embodiment of the invention the storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the file service provided by filer in response to a file system request packet issued by the client .

Overlying the file system layer is the snapshot mirroring or replication application in accordance with an illustrative embodiment of this invention. This application as described in detail below is responsible on the source side for the scanning and transmission of changes in the snapshot from the source filer to the destination filer over the network. This application is responsible on the destination side for the generation of the updated mirror snapshot from received information. Hence the particular function of the source and destination applications are different and are described as such below. The snapshot mirroring application operates outside of the normal request path as shown by the direct links and to the TCP IP layers and the file system snapshot mechanism . Notably the application interacts with the file system layer to gain knowledge of files so it is able to use a file based data structure inode files in particular to replicate source snapshots at the destination.

The inherent Snapshot capabilities of the exemplary WAFL file system are further described in 3002 by David Hitz et al. published by Network Appliance Inc. which is hereby incorporated by reference. Note Snapshot is a trademark of Network Appliance Inc. It is used for purposes of this patent to designate a persistent consistency point CP image. A persistent consistency point image PCPI is a point in time representation of the storage system and more particularly of the active file system stored on a storage device e.g. on disk or in other persistent memory and having a name or other unique identifiers that distinguishes it from other PCPIs taken at other points in time. A PCPI can also include other information metadata about the active file system at the particular point in time for which the image is taken. The terms PCPI and snapshot shall be used interchangeably through out this patent without derogation of Network Appliance s trademark rights.

Snapshots are generally created on some regular schedule. This schedule is subject to great variation. In addition the number of snapshots retained by the filer is highly variable. Under one storage scheme a number of recent snapshots are stored in succession for example a few days worth of snapshots each taken at four hour intervals and a number of older snapshots are retained at increasing time spacings for example a number of daily snapshots for the previous week s and weekly snapshot for the previous few months . The snapshot is stored on disk along with the active file system and is called into the buffer cache of the filer memory as requested by the storage operating system or snapshot mirror application as described further below. However it is contemplated that a variety of snapshot creation techniques and timing schemes can be implemented within the teachings of this invention.

An exemplary file system inode structure according to an illustrative embodiment is shown in . The inode for the inode file or more generally the root inode contains information describing the inode file associated with a given file system. In this exemplary file system inode structure root inode contains a pointer to the inode file indirect block . The inode file indirect block points to one or more inode file direct blocks each containing a set of pointers to modes that make up the inode file . The depicted subject inode file is organized into volume blocks not separately shown made up of inodes which in turn contain pointers to file data or disk blocks A B and C. In the diagram this is simplified to show just the inode itself containing pointers to the file data blocks. Each of the file data blocks A C is adapted to store in the illustrative embodiment 4 kilobytes KB of data. Note however where more than a predetermined number of file data blocks are referenced by an inode one or more indirect blocks shown in phantom are used. These indirect blocks point to associated file data blocks not shown . If an inode points to an indirect block it cannot also point to a file data block and vice versa.

When the file system generates a snapshot of a given file system a snapshot inode is generated as shown in . The snapshot inode is in essence a duplicate copy of the root inode of the file system . Thus the exemplary file system structure includes the same inode file indirect block inode file direct block inodes. and file data blocks A C as depicted in . When a user modifies a file data block the file system layer writes the new data block to disk and changes the active file system to point to the newly created block. The file layer does not write new data to blocks that are contained in snapshots.

In accordance with an illustrative embodiment of this invention the source utilizes two snapshots a base snapshot which represents the image of the replica file system on the destination and an incremental snapshot which is the image that the source system intends to replicate to the destination to perform needed updates of the remote snapshot mirror to the destination. In one example from the standpoint of the source the incremental snapshot can comprise a most recent snapshot and the base can comprise a less recent snapshot enabling an up to date set of changes to be presented to the destination. This procedure shall now be described in greater detail.

Having described the general procedure for deriving a snapshot the mirroring of snapshot information from the source filer to a remote destination filer is described in further detail. As discussed generally above the transmission of incremental changes in snapshot data based upon a comparison of changed blocks in the whole volume is advantageous in that it transfers only incremental changes in data rather than a complete file system snapshot thereby allowing updates to be smaller and faster. However a more efficient and or versatile procedure for incremental remote update of a destination mirror snapshot is contemplated according to an illustrative embodiment of this invention. Note as used herein the term replica snapshot replicated snapshot or mirror snapshot shall be taken to also refer generally to the file system on the destination volume that contains the snapshot where appropriate for example where a snapshot of a snapshot is implied.

As indicated above it is contemplated that this procedure can take advantage of a sub organization of a volume known as a qtree. A qtree acts similarly to limits enforced on collections of data by the size of a partition in a traditional Unix or Windows file system but with the flexibility to subsequently change the limit since qtrees have no connection to a specific range of blocks on a disk. Unlike volumes which are mapped to particular collections of disks e.g. RAID groups of n disks and act more like traditional partitions a qtree is implemented at a higher level than volumes and can thus offer more flexibility. Qtrees are basically an abstraction in the software of the storage operating system. Each volume may in fact contain multiple qtrees. The granularity of a qtree can be a sized to just as a few kilobytes of storage. Qtree structures can be defined by an appropriate file system administrator or user with proper permission to set such limits.

Note that the above described qtree organization is exemplary and the principles herein can be applied to a variety of file system organizations including a whole volume approach. A qtree is a convenient organization according to the illustrative embodiment at least in part because of its available identifier in the inode file.

Before describing further the process of deriving changes in two source snapshots from which data is transferred to a destination for replication of the source at the destination general reference is made again to the file block structures shown in . Every data block in a file is mapped to disk block or volume block . Every disk volume block is enumerated uniquely with a discrete volume block number VBN . Each file is represented by a single inode which contains pointers to these data blocks. These pointers are VBNs each pointer field in an inode having a VBN in it whereby a file s data is accessed by loading up the appropriate disk volume block with a request to the file system or disk control layer. When a file s data is altered a new disk block is allocated to store the changed data. The VBN of this disk block is placed in the pointer field of the inode. A snapshot captures the inode at a point in time and all the VBN fields in it.

In order to scale beyond the maximum number of VBN pointers in an inode indirect blocks are used. In essence a disk block is allocated and filled with the VBNs of the data blocks the inode pointers then point to the indirect block. There can exist several levels of indirect blocks which can create a large tree structure. Indirect blocks are modified in the same manner as regular data blocks are every time a VBN in an indirect block changes a new disk volume block is allocated for the altered data of the indirect block.

In the example of a write anywhere file layout storage blocks are not immediately overwritten or reused. Thus changes in a file comprised of a series of volume blocks will always result in the presence of a new volume block number newly written to that can be detected at the appropriate logical file block offset relative to an old block. The existence of a changed volume block number at a given offset in the index between the base snapshot inode file and incremental snapshot inode file generally indicates that one or more of the underlying inodes and files to which the inodes point have been changed. Note however that the system may rely on other indicators of changes in the modes or pointers this may be desirable where a write in place file system is implemented.

A scanner searches the index for changed base incremental inode file snapshot blocks comparing volume block numbers or another identifier. In the example of block in the base snapshot inode file now corresponds in the file scan order to block in the incremental snapshot inode file . This indicates a change of one or more underlying inodes. In addition block in the base snapshot inode file appears as block in the incremental snapshot inode file. Blocks and are unchanged in both files and thus are quickly scanned over without further processing of any inodes or other information. Hence scanned blocks at the same index in both snapshots can be efficiently bypassed reducing the scan time.

Block pairs e.g. blocks and that have been identified as changed are forwarded as they are detected by the scan to the rest of the source process which includes an inode picker process . The inode picker identifies specific modes based upon qtree ID from the forwarded blocks that are part of the selected qtree being mirrored. In this example the qtree ID Q is selected and inodes containing this value in their file metadata are picked for further processing. Other inodes not part of the selected qtree s e.g. inodes with qtree IDs Q and Q are discarded or otherwise ignored by the picker process . Note that a multiplicity of qtree IDs can be selected causing the picker to draw out a group of inodes each having one of the selected qtree associations.

The appropriately picked inodes from changed blocks are then formed into a running list or queue of changed inodes . These inodes are denoted by a discrete inode number as shown. Each inode in the queue is handed off to an inode handler or worker and as a worker becomes available. is a table detailing the basic set of rules the inode picker process uses to determine whether to send a given inode to the queue for the workers to process.

The inode picker process queries whether either 1 the base snapshot s version of the subject inode a given inode number is allocated and in a selected qtree box or 2 the incremental snapshot s version of the inode is allocated and in a selected qtree box . If neither the base nor incremental version are allocated and in the selected qtree then both inodes are ignored box and the next pair of inode versions are queried.

If the base inode is not in allocated or not in the selected qtree but the incremental inode is allocated and in the selected qtree then this implies an incremental file has been added and the appropriate inode change is sent to the workers box . Similarly if the base inode is allocated and in the selected qtree but the incremental inode is not allocated or not in the selected qtree then the this indicates a base file has been deleted and this is sent on to the destination via the data stream format as described below box .

Finally if a base inode and incremental inode are both allocated and in the selected qtree then the process queries whether the base and incremental inodes represent the same file box . If they represent the same file then the file or its metadata permissions owner permissions etc may have changed. This is denoted by different generation numbers on different versions of the inode number being examined by the picker process. In this case a modified file is sent and the inode works compare versions to determine exact changes as described further below box . If the base and incremental are not the exact same file then this implies a deletion of the base file and addition of an incremental file box . The addition of the incremental file is noted as such by the picker in the worker queue.

Inode is allocated in both inode files. It is in the proper qtree Q and the two versions of this inode share the same generation number. This means that the inode represents the same file in the base and the incremental snapshots. It is unknown at this point whether the file data itself has changed so the inode picker sends the pair to the changed inode queue and a worker determines what data has changed. Inode is allocated in the base inode file but not allocated in the incremental inode file. The base version of the inode was in the proper qtree Q. This means this inode has been deleted. The inode picker sends this information down to the workers as well. Finally inode is allocated in the base inode file and reallocated in the incremental inode file. The inode picker can determine this because the generation number has changed between the two versions from 1 2 . The new file which this inode represents has been added to the qtree so like inode this is sent to the changed inode queue for processing with a note that the whole file is new.

A predetermined number of workers operate on the queue at a given time. In the illustrative embodiment the workers function in parallel on a group of inodes in the queue. That is the workers process inodes to completion in no particular order once taken from the queue and are free process further inodes from the queue as soon as they are available. Other processes such as the scan and picker are also interleaved within the overall order.

The function of the worker is to determine changes between each snapshot s versions of the files and directories. As described above the source snapshot mirror application is adapted to analyze two versions of modes in the two snapshots and compares the pointers in the modes. If the two versions of the pointers point to the same block we know that that block hasn t changed. By extension if the pointer to an indirect block has not changed then that indirect block has no changed data so none of its pointers can have changed and thus none of the data blocks underneath it in the tree have changed. This means that in a very large file which is mostly unchanged between two snapshots the process can skip over overlook VBN pointers to each data block in the tree to query whether the VBNs of the data blocks have changed.

The operation of a worker is shown by way of example in . Once a changed inode pair are received by the worker each inode base and incremental respectively and is scanned to determine whether the file offset between respective blocks is a match. In this example blocks and do not match. The scan then continues down the tree of blocks and respectively arriving at underlying indirect blocks and . Again the file offset comparison indicates that blocks both arrive at a common block and thus have not changed . Conversely blocks and do not match due to offset differences and point to changed blocks and . The changed block and the metadata above can be singled out for transmission to the replicated snapshot on the destination described below see also . The tree in an illustrative embodiment extends four levels in depth but this procedure may be applied to any number of levels. In addition the tree may in fact contain several changed branches requiring the worker to traverse each of the branches in a recursive manner until all changes are identified. Each inode worker thus provides the changes to the network for transmission in a manner also described below. In particular new blocks and information about old deleted blocks are sent to the destination. Likewise information about modified blocks is sent.

Notably because nearly every data structure in this example is a file the above described process can be applied not only to file data but also to directories access control lists ACLs and the inode file itself.

It should be again noted that the source procedure can be applied to any level of granularity of file system organization including an entire volume inode file. By using the inherent qtree organization a quick and effective way to replicate a known subset of the volume is provided.

With further reference to the transmission of changes from the source snapshot to the replicated destination snapshot is described in an overview . As already described the old and new snapshots present the inode picker with changed inodes corresponding to the qtree or other selected sub organization of the subject volume. The changed inodes are placed in the queue and then their respective trees are walked for changes by a set of inode workers and . The inode workers each send messages and containing the change information to a source pipeline . Note that this pipeline is only an example of a way to implement a mechanism for packaging file system data into a data stream and sending that stream to a network layer. The messages are routed first to a receiver that collects the messages and sends them on to an assembler as a group comprising the snapshot change information to be transmitted over the network . Again the network as described herein should be taken broadly to include anything that facilitates transmission of volume sub organization e.g. qtree change data from a source sub organization to a destination sub organization even where source and destination are on the same file server volume or indeed in the case of rollback as described in the above incorporated U.S. Patent Application entitled SYSTEM AND METHOD FOR REMOTE ASYNCHRONOUS MIRRORING USING SNAPSHOTS are the same sub organization at different points in time. An example of a network used as a path back to the same volume is a loopback. The assembler generates a specialized format for transmitting the data stream of information over the network that is predictable and understood by the destination. The networker takes the assembled data stream and forwards it to a networking layer. This format is typically encapsulated within a reliable networking protocol such as TCP IP. Encapsulation can be performed by the networking layer which constructs for example TCP IP packets of the formatted replication data stream

The format is described further below. In general its use is predicated upon having a structure that supports multiple protocol attributes e.g. Unix permissions NT access control lists ACLs multiple file names NT streams file type file create modify time etc. . The format should also identity the data in the stream i.e. the offset location in a file of specific data or whether files have holes in the file offset that should remain free . The names of files should also be relayed by the format. More generally the format should also be independent of the underlying network protocol or device in the case of a tape or local disk non volatile storage protocol and file system that is the information is system agnostic and not bound to a particular operating system software thereby allowing source and destination systems of different vendors to share the information. The format should thus be completely self describing requiring no information outside the data stream. In this manner a source file directory of a first type can be readily translated into destination file directory of a different type. It should also allow extensibility in that newer improvements to the source or destination operating system should not affect the compatibility of older versions. In particular a data set e.g. a new header that is not recognized by the operating system should be ignored or dealt with in a predictable manner without triggering a system crash or other unwanted system failure i.e. the stream is backwards compatible . This format should also enable transmission of a description of the whole file system or a description of only changed blocks information within any file or directory. In addition the format should generally minimize network and processor overhead.

As changed information is forwarded over the network it is received at the destination pipeline piece . This pipeline also includes a networker to read out TCP IP packets from the network into the snapshot replication data stream format encapsulated in TCP IP. A data reader and header stripper recognizes and responds to the incoming format by acting upon information contained in various format headers described below . A file writer is responsible for placing file data derived from the format into appropriate locations on the destination file system.

The destination pipeline forwards data and directory information to the main destination snapshot mirror process which is described in detail below. The destination snapshot mirror process consists of a directory stage which builds the new replicated file system directory hierarchy on the destination side based upon the received snapshot changes. To briefly summarize the directory stage creates removes and moves files based upon the received formatted information. A map of modes from the destination to the source is generated and updated. In this manner inode numbers on the source file system are associated with corresponding but typically different inode numbers on the destination file system. Notably a temporary or purgatory directory described in further detail below is established to retain any modified or deleted directory entries until these entries are reused by or removed from the replicated snapshot at the appropriate directory rebuilding stage within the directory stage. In addition a file stage of the destination mirror process populates the established files in the directory stage with data based upon information stripped from associated format headers.

The format into which source snapshot changes are organized is shown schematically in . In the illustrative embodiment the format is organized around 4 KB blocks. The header size and arrangement can be widely varied in alternate embodiments however. There are 4 KB headers in that are identified by certain header types. Basic data stream headers data are provided for at most every 2 megabytes 2 MB of changed data. With reference to the 4 KB standalone header includes three parts a 1 KB generic part a 2 KB non generic part and an 1 KB expansion part. The expansion part is not used but is available for later versions.

The generic part contains an identifier of header type . Standalone header types i.e. headers not followed by associated data can indicate a start of the data stream an end of part one of the data stream an end of the data stream a list of deleted files encapsulated in the header or the relationship of any NT streamdirs. Later versions of Windows NT allow for multiple NT streams related to particular filenames. A discussion of streams is found in U.S. patent application Ser. No. 09 891 195 entitled SYSTEM AND METHOD FOR REPRESENTING NAMED DATA STREAMS WITHIN AN ON DISK STRUCTURE OF A FILE SYSTEM by Kayuri Patel et al the teachings of which are expressly incorporated herein by reference. Also in the generic part is a checksum that ensures the header is not corrupted. In addition other data such as a checkpoint used by the source and destination to track the progress of replication is provided. By providing a list of header types the destination can more easily operate in a backwards compatible mode that is a header type that is not recognized by the destination provided from a newer version of the source can be more easily ignored while recognized headers within the limits of the destination version are processed as usual.

The kind of data in the non generic part of the header depends on the header type. It could include information relating to file offsets in the case of the basic header used for follow on data transmission deleted files in a standalone header listing of such files that are no longer in use on the source or whose generation number has changed or other header specific information to be described below . Again the various standalone headers are interposed within the data stream format at an appropriate location. Each header is arranged to either reference an included data set such as deleted files or follow on information such as file data .

Next a series of headers and follow on data in the format define various part information . Significantly each directory data set being transmitted is preceded by a basic header with no non generic data. Only directories that have been modified are transmitted and they need not arrive in a particular order. Note also that the data from any particular directory need not be contiguous. Each directory entry is loaded into a 4 KB block. Any overflow is loaded into a new 4 KB block. Each directory entry is a header followed by one or more names. The entry describes an inode and the directory names to follow. NT stream directories are also transmitted.

The part format information also provides ACL information for every file that has an associated ACL. By transmitting the ACLs before their associated file data the destination can set ACLs before file data is written. ACLs are transmitted in a regular file format. Deleted file information described above is sent with such information included in the non generic part of one or more standalone headers if any . By sending this information in advance the directory tree builder can differentiate between moves and deletes.

The part format information also carries NT stream directory streamdir relationship information. One or more standalone headers if any notifies the destination file server of every changed file or directory that implicates NT streams regardless of whether the streams have changed. This information is included in the non generic part of the header .

Finally the part format information includes special files for every change in a symlink named pipe socket block device or character device in the replicated data stream. These files are sent first because they are needed to assist the destination in building the infrastructure for creation of the replicated file system before it is populated with file data. Special files are like ACLs transmitted in the format of regular files.

Once various part information is transmitted the format calls for an end of part of the data stream header . This is a basic header having no data in the non generic part . This header tells the destination that part is complete and to now expect file data.

After the part information the format presents the file and stream data . A basic header for every 2 MB or less of changed data in a file is provided followed by the file data itself The files comprising the data need not be written in a particular order nor must the data be contiguous. In addition referring to the header in the basic header includes a block numbers data structure associated with the non generic part works in conjunction with the holes array within in this example the generic part . The holes array denotes empty space. This structure in essence provides the mapping from the holes array to corresponding blocks in the file. This structure instructs the destination where to write data blocks or holes.

In general files are written in 4 KB chunks with basic headers at every 512 chunks 2 MB at most. Likewise streams also are transmitted like regular files in 4 KB chunks with at most 2 MB between headers.

Finally the end of the replicated data stream format is marked by a footer consisting of standalone header of the type end of data stream. This header has no specific data in its non generic part .

When the remote destination e.g. a remote file server remote volume remote qtree or the same qtree receives the formatted data stream from the source file server via the network it creates a new qtree or modifies an existing mirrored qtree or another appropriate organizational structure and fills it with data. shows the destination snapshot mirror process in greater detail. As discussed briefly above the process consists of two main parts a directory stage and a data or file stage .

The directory stage is invoked first during a transmission the data stream from the source. It consists of several distinct parts. These parts are designed to handle all part format non file data. In an illustrative embodiment the data of part is read into the destination stored as files locally and then processed from local storage. However the data may alternatively be processed as it arrives in realtime.

More particularly the first part of the directory stage involves the processing of deleted file headers . Entries in the inode map described further below are erased with respect to deleted files thereby severing a relation between mapped inodes on the replicated destination snapshot and the source snapshot.

Next the directory stage undertakes a tree cleaning process . This step removes all directory entries form the replicated snapshot directory that have been changed on the source snapshot. The data stream format indicates whether a directory entry has been added or removed. In fact directory entries from the base version of the directory and directory entries from the incremental version of the directory are both present in the format. The destination snapshot mirror application converts the formatted data stream into a destination directory format in which each entry that includes an inode number a list of relative names e.g. various multi protocol names and a create or delete value. In general each file also has associated therewith a generation number. The inode number and the generation number together form a tuple used to directly access a file within the file system on both the source and the destination . The source sends this tuple information to the destination within the format and the appropriate tuple is stored on the destination system. Generation numbers that are out of date with respect to existing destination files indicate that the file has been deleted on the source. The use of generation numbers is described further below.

The destination processes base directory entries as removals and incremental directory entries as additions. A file which has been moved or renamed is processed as a delete from the old directory or from the old name then as an add to the new directory or with a new name . Any directory entries that are deleted or otherwise modified are moved temporarily to the temporary or purgatory directory and are not accessible in this location by users. The purgatory directory allows modified entries to be in essence moved to the side rather than completely removed as the active file system s directory tree is worked on. The purgatory directory entries themselves point to data and thus prevent the data from becoming deleted or losing a link to a directory altogether.

On a base transfer of a qtree to the destination the directory stage tree building process is implemented as a breadth first traversal of all the files and directories in the data stream starting with the root of the qtree. The directory stage then undertakes the tree building process which builds up all the directories with stub entries for the files. However the depicted incremental directory stage as typically described herein differs from a base transfer in that the tree building process begins with a directory queue that includes all modified directories currently existing on both the source and the destination i.e. the modified directories that existed prior to the transfer . The incremental directory stage tree building process then processes the remainder of the directories according to the above referenced breadth first approach.

For efficiency the source side depends upon inode numbers and directory blocks rather than pathnames. In general a file in the replicated directory tree a qtree in this example on the destination cannot expect to receive the same inode number as the corresponding file has used on the source although it is possible . As such an inode map is established in the destination. This map shown generally in enables the source to relate each file on the source to the destination. The mapping is based generally upon file offsets. For example a received source block having offset 20 KB in inode maps to the block at offset 20 KB in replicated destination inode . The block can then be written to the appropriate offset in the destination file.

More specifically each entry in the inode map contains an entry for each inode on the source snapshot. Each inode entry in the map is indexed and accessed via the source inode number . These source modes are listed in the map in a sequential and monotonically ascending order notwithstanding the order of the mapped destination inodes. Under each source inode number the map includes the source generation number to verify that the mapped inode matches the current file on the source the destination inode number and destination generation number . As noted above the inode number and generation number together comprise a tuple needed to directly access an associated file in the corresponding file system.

By maintaining the source generation number the destination can determine if a file has been modified or deleted on the source and its source associated inode reallocated as the source generation number is incremented upwardly with respect to the stored destination. When the source notifies the destination that an inode has been modified it sends the tuple to the destination. This tuple uniquely identifies the inode on the source system. Each time the source indicates that an entirely new file or directory has to be created e.g. create the destination file system creates that file. When the file is created the destination registers data as a new entry in its inode map . Each time the source indicates that an existing file or directory needs to be deleted the destination obliterates that file and then clears the entry in the inode map. Notably when a file is modified the source only sends the tuple and the data to be applied. The destination loads the source inode s entry from the inode map. If the source generation number matches then it knows that the file already exists on the destination and needs to be modified. The destination uses the tuple recorded in the inode map to load the destination inode. Finally it can apply the file modifications by using the inode.

As part of the tree building process reused entries are moved back from the purgatory directory to the replicated snapshot directory . Traditionally a move of a file requires knowledge of the name of the moved file and the name of the file it is being moved to. The original name of the moved file may not be easily available in the purgatory directory. In addition a full move would require two directories purgatory and replicated snapshot to be modified implicating additional overhead.

However in the illustrative embodiment if the source inodes received at the destination refer to inodes in the inode map then the directory stage creates on the current built up snapshot directory a file entry having the desired file name. This name can be exactly the name derived from the source. A hard link i.e. a Unix based link enables multiple names to be assigned to a discrete file is created between that file on the snapshot directory and the entry in the purgatory directory. By so linking the entry it is now pointed to by both the purgatory directory and the file on the snapshot directory itself. When the purgatory directory root is eventually deleted thereby killing off purgatory at the end of the data stream transfer the hard link will remain to the entry ensuring that the specific entry in the purgatory directory will not be deleted or recycled given that the entry s link count is still greater than zero and a path to the data from the file on the new directory is maintained. Every purgatory entry that eventually becomes associated with a file in the newly built tree will be similarly hard linked and thereby survive deletion of the purgatory directory. Conversely purgatory entries that are not relinked will not survive and are effectively deleted permanently when purgatory is deleted.

It should now be clear that the use of mapping and generation number tuples avoids the expensive from a processing standpoint use of conventional full file pathnames or relative pathnames in the data stream from the source. Files that are modified on the source can be updated on the destination without loading a directory on either the source or destination. This limits the information needed from the source and the amount of processing required. In addition the source need not maintain a log of directory operations. Likewise since the destination need not maintain a central repository of the current file system state multiple subdirectories can be operated upon concurrently. Finally neither the source nor the destination must explicitly track deleted files as such deleted files are automatically removed. Rather the source only sends its list of deleted files and the destination uses this list to conform the inode map. As such there is no need to selectively traverse a tree more than once to delete files and at the conclusion of the transfer simply eliminating the purgatory directory is the only specific file cleaning step.

The directory stage sets up any ACLs on directories as the directories are processed during tree building substep . As described above the ACL and NT stream relationships to files are contained in appropriate standalone headers. ACLs are then set on files during the below described file stage. NT streams are created on files as the files are themselves created. Since an NT steam is in fact a directory the entries for it are processed as part of the directory phase.

The new directory tree may contain files with no data or old data. When the end of part format header is read the destination mirror process enters the file stage in which snapshot data files referenced by the directory tree are populated with data e.g. change data . shows a simplified procedure for writing file data received from the source. In general each up to 2 MB of data in 4 KB blocks arrives with corresponding source inode numbers. The inode map is consulted for corresponding entries . Appropriate offsets are derived for the data and it is written into predetermined empty destination snapshot data files .

At the end of both the directory stage and data stage when all directory and file data have been processed and the data stream transfer from the source is complete the new replicated snapshot is exposed atomically to the user. At this time the contents of the purgatory directory which includes any entries that have not be moved back into the rebuilt tree is deleted.

It should be noted that the initial creation the level zero transfer of the replicated snapshot on the destination follows the general procedures discussed above. The difference between a level zero transfer and a regular update is that there is no base snapshot so the comparisons always process information in the incremental snapshot as additions and creates rather than modifications. The destination mirror application starts tree building by processing any directories already known to it. The initial directory established in the destination is simply the root directory of the replicated snapshot the qtree root . A destination root exists on the inode map. The source eventually transmits a root other files received may be buffered until the root arrives and the root is mapped to the existing destination root. Files referenced in the root are then mapped in turn in a create process as they are received and read by the destination. Eventually the entire directory is created and then the data files are populated. After this a replica file system is complete.

As described above in reference to a source utilizes an inode picker process and a set of inode workers and detecting changes in inodes and blocks of a snapshot. These changes are then collected by the workers and sent to a pipeline for transmission to the destination file system where a source file system snapshot pair is replicated by the above described destination mirror application process. The transmission of data occurs via a generalized network which can include loopback mechanisms and other techniques that can provide the change data stream to the same volume or even the same qtree sub organization in a roll back of the replicated snapshot for example as taught in above incorporated U.S. patent application Ser. No. 10 100 950 entitled SYSTEM AND METHOD FOR ASYNCHRONOUS MIRRORING OF SNAPSHOTS AT A DESTINATION USING A PURGATORY DIRECTORY AND INODE MAPPING .

In the event of a loss of communication between the source and destination caused by for example a failure of the network it is desirable to restart the data transfer process from a fixed point within the overall progress of the transfer. While the process could simply be restarted from the beginning of the procedure this would result in substantial duplication of data transfer and computational overhead. To provide intermediate points to which the process can be restarted the source inserts a number of discrete checkpoint numbers into the data stream sent from the source to the destination.

This checkpointing procedure is shown in detail in with reference also to . Initially in step the processing by the inode workers is temporarily halted or otherwise benchmarked at a fixed point in time thereby pausing the scan and collection of snapshot changes at the source. The procedure then writes the inode that is on top of the inode queue to a top of queue registry step . This registry described further below is used to store state information relating to the state of the inode workers and inode picker process at the time the given checkpoint is created. Next the status of each inode worker is written to the top of queue registry in step . This status information includes in the illustrative embodiment the number of the inode that the inode worker is currently working on and the volume block number presently being checked. The registry is then associated with a new checkpoint number in step . In this embodiment checkpoint numbers are provided as a sequentially increasing monotonic number set e.g. checkpoint nos. . . . used to identify successive synchronization points in the overall data stream transfer session e.g. at least one full update of the replicated destination file system.

The newly created and associated checkpoint number from step is then inserted into the data stream for transmission to the destination step . Note particularly that the checkpoint number can be inserted into a checkpoint data section of the data stream header in the above described data stream format see also . Once the checkpoint number is transmitted to the destination then the processing performed by the inode workers is restarted step . Alternatively processing may not actually halt until a certain time has elapsed. Rather the checkpoints just continue to track the worker progress on the source side while change data is buffered on the source for later transfer to the destination file system when communication is restored and the appropriate checkpoint number is provided to the source by the destination. Accordingly the term halt should be taken broadly to include a continuation of progress by the source after the state is fixed in the registry with no guarantee of actual data stream transfer to the destination.

In the procedure after the inode workers have been restarted the source manages the set of registries that it has accumulated step . As a new registry entry is created each time a checkpoint number is placed into the data stream the source accumulates a number of registry entries. These registered entries can be managed in a preset and predetermined manner. For example the source may keep a set of ten registry entries with the oldest being deleted round robin each time a new registry entry is created. After managing the registry entries the procedure in step then waits a preset time period or alternatively for a certain level of data processing throughput before initiating another halt and looping to step .

A schematic block diagram of an exemplary top of queue registry is shown in . The registry includes entries that identify the associated checkpoint number the inode on top of the queue and the modes and and corresponding volume block numbers and that are being worked on by each of the plurality of inode workers. The exemplary registry is shown with entries for two inode workers. However it is expressly contemplated that the registry may have additional entries for additional inode workers and or additional data fields. By accessing these data entries the source can reset the inode workers and the queue to the timestamp associated with the creation of the particular checkpoint.

The destination receives and acts upon the checkpoint numbers packaged in the formatted data stream while receiving and performing the destination process on the stream. In the illustrative embodiment the destination stores the checkpoint number for the last segment of the transfer that has been fully committed to persistent storage. For example if all data up to checkpoint number has been written to disk the destination stores checkpoint number for use during the novel restart procedure which is now further described.

The restart procedure performed mutually by the destination and source in response to a loss of a communication path or other error is shown in . Initially in step once the destination discovers a loss or error the destination transmits when source destination communication is available or restored to the source the checkpoint number identifying the last group of data that the destination has committed to persistent storage e.g. the disk array NVRAM and or the like . The loss or error can be determined by the source in a variety of ways such as a conventional network error a time out in which no data is received or a specific error failure signal sent from the source or within the destination itself . The destination negotiates the last committed checkpoint number to the source using an appropriate communication networking protocol.

By way of example if the destination has written all of the data up to checkpoint number plus some of the data between checkpoint number and checkpoint number then if the loss of communication or other error occurs the destination would transfer back to the source an alert stating that it has completed checkpoint number . After the destination alerts the source that is completed checkpoint number the destination has an expectation that it will receive retransmission of all data starting no later than the end of the checkpoint number segment. The source can restart from any previous checkpoint e.g. checkpoint number however the closer that the source can begin to the checkpoint number sent by the destination the less work needs to be redone. Thus for example if the destination had sent checkpoint number but the source begins at checkpoint number then the data between checkpoint number and checkpoint number will need to be rewritten.

In the restart procedure after receiving the last checkpoint number alert from the destination the source then selects in step the appropriate stored registry that is associated with the checkpoint number or the entry with the largest checkpoint number less than the requested value. This selected registry contains all of the information that the source requires to restart the inode workers in the exact state that they were in at the time of the particular checkpoint number or otherwise transfer an appropriate buffer containing stored worked on changes to the pipeline network . Where the inode workers are restarted the source initializes the workers at the appropriate inode and volume block number that is indicated in the selected checkpoint s registry step . The inode picker process is also initialized at the inode that was at the top of the queue in step . At this point the source has recreated the state that it was in at the time of the selected checkpoint. Thus the data stream generated by this restored configuration will include the same set of changes as before the loss of communication between the source and destination. A checkpoint number is appended to the new data stream upon restart. This checkpoint number is of course the next increment from the last fully committed number provided by the destination.

After the restart procedure is performed the transmission of changes from the source to the destination can continue with both the source and destination having agreed on a mutual starting point namely the selected checkpoint transmitted by the destination. Note that the destination may receive change data after the restart procedure that the destination had already committed to persistent storage i.e. a change that was written after the selected checkpoint but before the error condition requiring a restart. The destination simply rewrites the change data to its persistent storage using the methodology described above. This rewriting of change data is possible because the data stream sent from the source to the destination specifies the location in the file where the data needs to be written. Typical replication software simply appends data to a destination side file. By sending the location in the file where the change is to be written the destination can accommodate the need to redo work by overwriting already written data.

The above described purgatory directory is employed by the destination during as part of the restart of directory processing. As the directory entries for modified and deleted files are moved into the purgatory directory they are not deleted until the entire update of the destination file system is fully completed. In the interim any changes that occur after an agreed upon checkpoint for restarting can be undone by moving the deleted and modified entries back from the purgatory directory to the destination file system. This ensures that the destination side directory processing restarts from the exact point of restart by the source side. The use of the purgatory directory is described further in the above incorporated U.S. Patent Application entitled SYSTEM AND METHOD FOR ASYNCHRONOUS MIRRORING OF SNAPSHOTS AT A DESTINATION USING A PURGATORY DIRECTORY AND INODE MAPPING.

The foregoing has been a detail description of illustrative embodiments of the invention. Various modifications and additions can be made without departing form the spirit and scope of the invention. For example the number of interconnected source and or destination servers depicted can be varied. In fact the source and destination servers can be the same machine. It is expressly contemplated that a plurality of sources can transfer data to a destination and vice versa. Likewise the internal architecture of the servers or their respective storage arrays as well as their network connectivity and protocols are all highly variable. The operating systems used on various source and destination servers can differ. In addition it is expressly contemplated that any of the operations and procedures described herein can be implemented using hardware software comprising a computer readable medium having program instructions executing on a computer or a combination of hardware and software.


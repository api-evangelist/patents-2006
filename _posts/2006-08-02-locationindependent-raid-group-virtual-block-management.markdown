---

title: Location-independent RAID group virtual block management
abstract: A storage operating system is configured to assign volume block numbers (VBNs) to a volume. The system has a plurality of disks, and each disk of the plurality of disks is assigned disk block numbers (DBNs). A raidmap is configured to map the VBNs to the DBNs of the plurality of physical disks, the mapping for a particular disk stored in a disk label for the particular disk. The disk label for the particular disk is then written to the particular disk.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07660966&OS=07660966&RS=07660966
owner: NetApp, Inc.
number: 07660966
owner_city: Sunnyvale
owner_country: US
publication_date: 20060802
---
This application is a continuation of U.S. Ser. No. 10 394 890 filed on Mar. 21 2003 entitled LOCATION INDEPENDENT RAID GROUP VIRTUAL BLOCK MANAGEMENT by Strange et al. now issued as U.S. Pat. No. 7 111 147 on Sep. 19 2006.

The present invention relates to storage systems and more specifically to a technique for mapping the capacity of storage devices into any RAID group of a storage system.

A storage system typically comprises one or more storage devices into which data may be entered and from which data may be obtained as desired. The storage system includes a storage operating system that functionally organizes the system by inter alia invoking storage operations in support of a storage service implemented by the system. The storage system may be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The storage devices are typically disk drives organized as a disk array wherein the term disk commonly describes a self contained rotating magnetic media storage device. The term disk in the context is synonymous with hard disk drive HDD or direct access storage device DASD .

Storage of information on the disk array is preferably implemented as one or more storage volumes defining an overall logical arrangement of disk space. The disks within a volume are typically organized as one or more groups wherein each group is operated as a Redundant Array of Independent or Inexpensive Disks RAID . Most RAID implementations enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of redundant information with respect to the striped data. The redundant information may thereafter be retrieved to enable recovery of data lost when a storage device fails.

In the operation of a disk array it is anticipated that a disk can fail. A goal of a high performance system is to make the mean time to data loss as long as possible preferably much longer than the expected service life of the system. Data can be lost when one or more disks fail making it impossible to recover data from the device. Typical schemes to avoid loss of data include mirroring backup and parity protection. Mirroring stores the same data on two or more disks so that if one disk fails the mirror disk s can be used to serve e.g. read data. Backup periodically copies data on one disk to another disk. Parity schemes are common because they provide a redundant encoding of the data that allows for loss of one or more disks without the loss of data while requiring a minimal number of disk drives in the storage system.

Parity protection is used in a computer system to protect against loss of data on a storage device such as a disk. A parity value may be computed by summing usually modulo data of a particular word size usually 1 bit across a number of similar disks holding different data and then storing the results on the disk s . That is parity may be computed on 1 bit wide vectors composed of bits in predetermined positions on each of the disks. Addition and subtraction on 1 bit vectors are an equivalent to exclusive OR XOR logical operations these addition and subtraction operations can thus be replaced by XOR operations. The data is then protected against the loss of any one of the disks or of any portion of the data on any one of the disks. If the disk storing the parity is lost the parity can be regenerated from the data. If one of the data disks is lost the data can be regenerated by adding the contents of the surviving data disks together and then subtracting the results from the stored parity.

Typically the disks are divided into parity groups each of which comprises one or more data disks and a parity disk. The disk space is divided into stripes with each stripe containing one block from each disk. The blocks of a stripe are usually at equivalent locations on each disk in the parity group. Within a stripe all but one block contain data data blocks with the one block containing parity parity block computed by the XOR of all the data. If the parity blocks are all stored on one disk thereby providing a single disk that contains all and only parity information a RAID 4 implementation is provided. If the parity blocks are contained within different disks in each stripe usually in a rotating pattern then the implementation is RAID 5. The term RAID and its various implementations are well known and disclosed in by D. A. Patterson G. A. Gibson and R. H. Katz Proceedings of the International Conference on Management of Data SIGMOD June 1988.

The storage operating system of the storage system may implement a file system to logically organize the information stored on the disks of a volume as a hierarchical structure of directories files and blocks. The storage operating system may also include a RAID subsystem that manages the storage and retrieval of the information to and from the disks in accordance with input output I O operations. There is typically a one to one mapping between the information stored on the disks in e.g. a disk block number DBN space and the information organized by the file system in e.g. volume block number VBN space. The file system consists of a contiguous range of VBNs from zero to N for a file system of size N 1 blocks. The storage operating system may further include administrative interfaces such as a user interface that enable operators system administrators to access the system in order to implement e.g. configuration management decisions.

Configuration management in the RAID subsystem generally involves a defined set of modifications to the topology or attributes associated with a storage array such as a disk a RAID group parity group a volume or set of volumes. Examples of these modifications includes but are not limited to disk addition disk failure handling volume splitting volume online offline and changes to a RAID group size. The RAID group size is the maximum number of disks that may be contained within a RAID group. For example if a RAID group size is 3 then the number of disks in the group can be less than or equal to 3 but not more than 3. The RAID group size is typically a property of a volume such that all RAID groups of the volume typically have the same RAID group size. When the RAID group reaches the maximum number a new RAID group is created upon the addition of new disks.

Volume capacity is typically linked to the linear growth of the RAID groups that in turn are organized linearly within the VBN space of a volume. Linear growth and organization of volume capacity is generally due to prior RAID subsystem support for only one contiguous VBN to DBN mapping range across all disks of a RAID group. Thus if the maximum RAID group size is increased it is generally not possible to insert disks into the middle of the linear list of RAID groups. Moreover a prior approach supports only one contiguous DBN to VBN mapping range on each disk of the RAID group typically because the entire DBN space on the disk is mapped into the VBN space of the volume. If a disk in an existing RAID group other than the last RAID group is exchanged for a larger disk it is generally not possible to make use of the additional space on the larger disk. For example if a disk of a RAID group failed and was replaced with a larger disk the additional space on the larger disk could not be used. The larger replacement disk could only use the VBN range that was previously allocated to the failed disk.

These restrictions also make it difficult to dynamically expand the storage space of an existing file system when upgrading from smaller disks of a volume to larger disks and then utilizing the additional capacity for the volume. An example of a prior approach used to upgrade smaller disks of a storage system to larger disks involves creating a new volume with the larger disks and copying the data from the smaller disks to the larger disks in accordance with e.g. a volume copy operation. Thereafter the data stored on the smaller disks are deleted and those disks are removed from the storage system. This approach represents a time consuming procedure that involves a period of time during which the data is not accessible by a client.

Often it is desirable to migrate smaller disks of a volume to larger disks in accordance with a synchronous capacity upgrade for a storage system that supports synchronous RAID mirroring. However a prior approach used to perform such a synchronous capacity upgrade results in system downtime or client reconfiguration. For example assume that it is desired to mirror an existing volume with disks of capacity size X to a new volume with disks of capacity 2X. A prior approach involves creation of an entirely new volume and copying of the data from the old volume to the new volume in accordance with the volume copy operation.

Since a new volume is created that volume has a name that is different from the original volume. Moreover the original volume is brought offline for a discrete period of time in order to ensure consistency of information that is written to the new volume during the copy operation. As a result the file system is aware of the configuration change as not only is the file service temporarily disrupted but a renaming operation occurs that renames the newly created volume to that of the original volume. In addition the file system identifier must be changed and other house keeping duties must be performed to ensure that the operators clients are not aware that their data has moved.

The present invention overcomes the disadvantages of the prior art by providing a technique for mapping the capacity of storage devices such as disks into any RAID group of a volume of a storage system regardless of the location of the RAID group within a volume block number VBN space of the volume. To that end the inventive technique separates disks and mapped VBN ranges allowing for flexibility in the description and extension of RAID group capacities while providing disk addition policies that support location independent disk insertion into RAID groups. The inventive technique also provides a disk label structure that supports the provision of multiple VBN ranges within a RAID group and within individual disks. Moreover the technique provides file system support for allocation and topology management of the multiple mapped VBN ranges within disks and RAID groups as well as noncontiguous VBN ranges across the RAID groups in the volume.

According to an aspect of the inventive technique raidmap data structures are used to map VBN ranges into a disk block number DBN space of each disk. The raidmap is maintained by a RAID subsystem of the storage system and stored in the disk label structure i.e. a RAID label on the disk. Another aspect of the technique relates to the mapping of multiple VBN ranges to the DBN space of a disk through the ability to store more than one raidmap in a RAID label on each disk. Each raidmap comprises mapping parameters such as a base offset into a VBN range of a volume a base offset into a DBN range of the disk and a size of the DBN to VBN mapping used to describe the topology of the volume. These mapping parameters enable separation of DBN ranges on a disk from mapped VBN ranges of the volume. A write allocator of the file system uses the raidmap and its description of that topology to issue I O operations when accessing data stored in the VBN range of a volume. Notably the write allocator uses the mapping parameters of the raidmap to determine the noncontiguous VBN to DBN mapping of a disk within a RAID group. A topology mechanism exports the raidmap information from the RAID subsystem to the write allocator.

Advantageously the present invention isolates disks from VBN ranges providing flexibility in RAID group topologies. Isolation of VBN ranges from disks further allows for dynamic extension of the VBN range at the RAID level without requiring reorganization of the existing blocks in the file system. The inventive technique supports dynamic extension of capacity using existing but formerly unused disk space. Moreover the on disk label structure contains placeholders for multiple per disk VBN range data thereby facilitating instantiation of the raidmap information when a volume is brought into the system during initialization.

In the illustrative embodiment the memory comprises storage locations that are addressable by the processor and adapters for storing software program code and data structures associated with the present invention. For example a portion of the memory may be organized as a raidmap cache having locations used to store mapping data structures in accordance with the present invention. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. Storage operating system portions of which are typically resident in memory and executed by the processing elements functionally organizes the system by inter alia invoking storage operations executed by the storage system. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the inventive technique described herein.

The network adapter comprises the mechanical electrical and signaling circuitry needed to connect the storage system to a client over a computer network which may comprise a point to point connection or a shared medium such as a local area network. Illustratively the computer network may be embodied as an Ethernet network or a Fibre Channel FC network. The client may communicate with the storage system over network by exchanging discrete frames or packets of data according to pre defined protocols such as the Transmission Control Protocol Internet Protocol TCP IP .

The client may be a general purpose computer configured to execute applications . Moreover the client may interact with the storage system in accordance with a client server model of information delivery. That is the client may request the services of the storage system and the system may return the results of the services requested by the client by exchanging packets over the network . The clients may issue packets including file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over TCP IP when accessing information in the form of files and directories. Alternatively the client may issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks.

The storage adapter cooperates with the storage operating system executing on the system to access information requested by a user or client . The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the disks such as HDD and or DASD of array . The storage adapter includes input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC serial link topology.

Storage of information on array is preferably implemented as one or more storage volumes that comprise a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number VBN space on the volume s . Each volume is generally although not necessarily associated with its own file system. The disks within a volume file system are typically organized as one or more groups wherein each group is operated as a Redundant Array of Independent or Inexpensive Disks RAID . Most RAID implementations such as a RAID 4 level implementation enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of parity information with respect to the striped data. Although a RAID 4 level implementation is illustratively described herein it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.

To facilitate access to the disks the storage operating system implements a write anywhere file system that cooperates with virtualization modules to virtualize the storage space provided by disks . The file system logically organizes the information as a hierarchical structure of named directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization modules allow the file system to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers luns .

In the illustrative embodiment the storage operating system is preferably the NetApp Data ONTAP operating system available from Network Appliance Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system including for example a write in place file system may be enhanced for use in accordance with the inventive principles described herein. As such where the term WAFL is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the storage system. The FC and iSCSI drivers provide FC specific and iSCSI specific access control to the blocks and thus manage exports of luns to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing the blocks on the storage system. In addition the storage operating system includes a storage device manager embodied as a RAID subsystem that manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations and a disk driver subsystem that implements a disk access protocol such as e.g. the SCSI protocol.

Bridging the disk software layers with the integrated network protocol stack layers is a virtualization system that is implemented by a file system interacting with virtualization modules illustratively embodied as e.g. vdisk module and SCSI target module . The vdisk module is layered on the file system to enable access by administrative interfaces such as a user interface UI in response to a user system administrator issuing commands to the storage system. The SCSI target module is disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as blocks. The UI is disposed over the storage operating system in a manner that enables administrative or user access to various layers and subsystems such as the RAID subsystem .

The file system is illustratively a message based system that provides volume management capabilities for use in access to the information stored on the storage devices such as disks. That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as mirroring and or parity RAID . The file system illustratively implements the WAFL file system having an on disk format representation that is block based using e.g. 4 kilobyte kB blocks and using index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store metadata describing the layout of its file system these metadata files include among others an inode file. A file handle i.e. an identifier that includes an inode number is used to retrieve an inode from disk.

Operationally a request from the client is forwarded as a packet over the computer network and onto the storage system where it is received at the network adapter . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the file system layer . Here the file system generates operations to load retrieve the requested data from disk if it is not resident in core i.e. in the memory . If the information is not in memory the file system indexes into the inode file using the inode number to access an appropriate entry and retrieve a logical VBN. The file system then passes a message structure including the logical VBN to the RAID subsystem which maps that logical number to a disk block number DBN and sends the latter to an appropriate driver e.g. SCSI of the disk driver subsystem . The disk driver accesses the DBN from disk and loads the requested data block s in memory for processing by the storage system. Upon completion of the request the storage system and operating system returns a reply to the client over the network .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage system may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by storage system in response to a request issued by client . Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the system. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

As used herein the term storage operating system generally refers to the computer executable code operable to perform a storage function in a storage system e.g. that manages data access and may in the case of a file server implement file system semantics. In this sense the ONTAP software is an example of such a storage operating system implemented as a microkernel and including the WAFL layer to implement the WAFL file system semantics and manage data access. The storage operating system can also be implemented as an application program operating over a general purpose operating system such as UNIX or Windows NT or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the inventive technique described herein may apply to any type of special purpose e.g. file server or filer or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system . Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems.

The present invention is implemented in the context of a configuration management framework used to implement the RAID subsystem in the storage operating system . In a preferred embodiment the configuration management framework provides an object oriented approach to RAID configuration management as described herein with respect to an implementation of the RAID subsystem. is a schematic block diagram illustrating one or more modules or processes and threads of the RAID subsystem wherein each process has among other things a virtual memory address space executable code and data. A process is started with a single thread but can create additional threads from any of its threads. The threads execute in the same memory address space and can therefore work concurrently on shared data. For example an instantiator module implements a configuration config thread adapted to maintain relationships among and invoke behaviors of decomposed software components RAID objects that collectively form the behaviors associated with a collection of RAID volumes on the storage system. In addition an I O manager module implements an I O thread configured to issue I O transaction requests from the RAID subsystem to the disk driver subsystem and upon completion process the results.

A finite state machine FSM module or engine is used to arbitrate a set of events and states that a process or thread of the RAID subsystem may encounter. Transactional semantics isolate the behavior of state changes in the RAID subsystem from concurrent I O operations. The framework provides a two phase commit procedure coordinated with updates to on disk configuration data labels . Errors during disk label updates are handled by aborting the transaction releasing partially committed data and unwinding any pending state transitions. A state notification mechanism integrated with the FSM engine propagates state changes through the threads in order to provide a coordinated behavior.

According to the configuration management framework a volume comprises the aggregate behavior of a number of RAID objects. Each RAID object object comprises operational code and static state such as configuration information relating to the topology of the underlying physical storage devices e.g. disks contained in disk array . The objects are organized into a configuration tree with configuration interfaces defining a set of services provided by one or more processes of the RAID subsystem. Although the objects may be implemented in accordance with an object oriented programming paradigm the present invention is not limited to such an implementation. More broadly the objects of the configuration tree refer to abstract entities representing a logical combination configuration of the disks. That is the objects are used to present a view of the underlying topology of the storage array managed by the RAID subsystem.

Objects have an associated type with each object type providing its own implementation of the configuration interfaces. A volume is organized into a hierarchical configuration tree of objects that includes a tree object responsible for coordinated behavior with the file system and a volume object responsible for managing the RAID aspects of volume management. Specifically the volume object represents the WAFL file system at the highest level i.e. root node of the configuration tree . To that end the volume object stores metadata that describes a volume file system wherein the metadata includes information such as the name of the volume and address range in physical blocks of the volume. The name of the volume resides in a volume namespace that is exported by the UI of the storage operating system . The logical address space of the file system is mapped to the physical block address space in the RAID subsystem .

The configuration tree and in particular the volume object represent a logical disk that is presented to the file system by the RAID subsystem as a container for the file system to store its data. That is the objects of the configuration tree are organized to create an address space that resembles a single logical disk but in reality comprises a plurality of physical disks. In this context the volume object is equivalent to the tree object wherein the tree object stores additional metadata about the logical volume that is presented to the file system. This additional metadata includes the type level of parity implementation configured for the particular volume e.g. RAID 4 RAID 0 mirror RAID 4 mirror RAID 0 row diagonal parity . Since the tree object is a one to one representation of the volume the additional metadata stored in the tree object includes redundant information about the volume such as its name and physical address block range.

A next object level comprises a mirror object that is responsible for coordinating one or more copies of the volume termed plexes in support of data mirroring. In synchronous data mirroring two mirror copies are provided that are at all times synchronized. That is changes to the data in one mirrored copy are immediately reflected in the other mirrored copy. The two identical mirrored copies have matching address spaces that are within the volume address space and that provide identical synchronized full copies of the data in the volume.

A plex object is responsible for managing an instance of a copy of volume data and thus represents each mirrored copy within another object level of the configuration tree. Whereas the mirror object stores metadata that is used to coordinate one or more copies or plexes of the volume in support of data mirroring each plex object stores metadata that is used to manage an instance of a copy of volume data. The plex object may include an ordinal placement e.g. 1 2 indicating that for example a first part 1 of the address space is associated with a particular RAID group and that a second part 2 of the address space is associated with another RAID group.

A next object level comprises one or more RAID group objects per plex object. Each RAID group object contains metadata that provides data protection and I O coordination over a set of disks. The metadata of the RAID group object includes information such as the number of disks within the RAID group and the address block range of each disk within the RAID group. In this context a RAID group is defined as a number of disks and the address block space associated with those disks. Finally there is another object level comprising one or more disk objects per RAID group object wherein each disk object contains metadata that provides data access to the physical disks .

The configuration tree is constructed in the memory of the storage system by a label assimilation process of the RAID subsystem . According to the assimilation process each disk associated with a volume includes a disk label structure that describes its placement and association with that volume. The on disk label is in essence self describing information for each disk that is actively attached to the storage system . The labels are used to dynamically assemble the disks into a volume and to construct an in core configuration tree for that volume starting from the disk object level up to the volume object level. Therefore a label on a disk identifies that disk s participation in a RAID group and furthermore that group s association with plex mirror and ultimately volume objects in the configuration tree. As described herein the label is located in a well known location of the disk so that it can be queried by the RAID subsystem in accordance with e.g. a discovery process during a boot operation.

Each object type instance of the configuration tree comprises the following components service interfaces committed transactional state pre committed transactional state and non transactional state. The service interfaces comprise an application programming interface API that the object exports to other software components. In the illustrative embodiment the service interfaces include initialization and destruction child object management add replace online offline transaction management join commit abort label I O state change notify virtual block management and I O handling context creation deletion resource management I O throttling . Each object type defines the commit pre committed and non transactional state that it holds. The transaction management interfaces are provided as a mechanism to create and modify transactional state in a manner that is coordinated across all objects in a volume.

The basic flow of control starts with a configuration management operation issued by e.g. the file system and received by the RAID subsystem . The file system passes the configuration management operation in the form of a message request that specifies a target volume and defines a named operation with specific parameters. The configuration thread of the RAID subsystem receives the request and determines the object e.g. volume RAID group of the configuration tree for the volume to which the request is directed. The configuration thread then locates the tree object for the volume and invokes relevant configuration operations using service interfaces of the appropriate object. When a service interface is invoked the object is transparently joined to the request prior to activation of the interface. Joining of an object to a request results in copying of the currently committed state into a pre committed state area called the trans area of the object . The trans area is a portion of memory that records changes to the object that are pending commitment. The service interface makes its changes to the trans area. If the changes to the object result in a change of the state of the object the FSM engine is invoked.

The FSM engine provides a critical component in managing the interrelationship between objects in a RAID volume. Specifically the FSM engine defines the state event pairs that are legal and for each state event provides a mechanism to implement the invocation and determine any subsequent state transitions. In addition the FSM engine provides tracing mechanisms to track the set of transitions that have occurred and provides object notification for pre committed and committed state transitions. More specifically the FSM engine is responsible for determining the new state of the object based upon a per object type state event table and invoking the state notification interface of its superior object in the volume hierarchy. The FSM engine includes an upward notification mechanism e.g. from child to parent object to notify the parent object about a state change in the child object.

When the configuration request completes all service interface invocations it requests a label commit operation to persistently store the modified state. The label commit pulls data from the trans area in order to construct its new version of the label data. Label commit is deemed to be successful only if all labels in a plex can be successfully written. Once all labels have been successfully written each object that has been joined to the request is responsible for copying its trans area data back to the committed state portion of the object. If labels are not successfully written the trans area is discarded any disk failures are identified and resulting configuration changes are initiated the successfully written labels are re written with the previous state and the original configuration request is retried.

The present invention relates to a technique for mapping the capacity of storage devices such as disks into any RAID group of a volume of a storage system regardless of the location of the RAID group within a VBN space of the volume. To that end the inventive technique separates disks and mapped VBN ranges allowing for flexibility in the description and extension of RAID group capacities while providing disk addition policies that support location independent disk insertion into RAID groups. The inventive technique also is directed to the disk label structure that supports the provision of multiple VBN ranges within a RAID group and within individual disks.

Moreover the technique provides file system support for allocation and topology management of the multiple mapped VBN ranges within disks and RAID groups as well as noncontiguous VBN ranges across the RAID groups in the volume. Note that there is a one to one mapping between the information stored on the disks in e.g. DBN space and the information organized by the file system in e.g. VBN space. That is each 4 kB file system block in the VBN space is mapped to a 4 kB disk block in the DBN space. An alternate embodiment of the invention may include multi tiered mapping schemes maps that are back ended by multiple disks in a mirrored configuration or maps that are back ended by non disk storage devices e.g. battery backed memory .

Whereas the file system map contains VBN to DBN mapping information for all disks in the volume a raidmap contains DBN to VBN mapping information for each disk in the volume. The raidmap is a data structure containing information maintained by the RAID subsystem to correlate the DBN space of a single disk to the VBN space of the volume. In particular the information contained in the raidmap comprises mapping parameters that enable separation of DBN ranges on a disk from mapped VBN ranges of the volume. Each raidmap is stored on the disk label structure i.e. a RAID label of a disk and generally comprises a base offset into a VBN range of the volume a base offset into a DBN range on the disk and a size length of the DBN to VBN mapping. Each RAID label is stored at a fixed location e.g. at the end of a disk and functions as an on disk database of configuration information.

The control information outside the DBN range includes but is not limited to a table of contents that describes the entire layout of the disk the locations of subsequent sections and sections associated with disk ownership RAID control information and other configuration data. In addition as illustratively shown a section of the disk may be used to store kernel and boot image code of the storage operating system and a section of the disk may be used to store the RAID labels and in particular the configuration information needed to describe how the particular disk fits into the volume.

The configuration information stored in the RAID label describes the RAID group to which the disk belongs along with all parent objects associated with the disk e.g. plex mirror volume . Therefore there is a region within the RAID label for each parent object of the configuration tree associated with the disk storing the RAID label. The parent objects of the configuration tree above the disk drives are in core memory objects that are created at the time the volume is assimilated. As noted assimilation of a volume comprises reading retrieving the configuration information from all labels on all disks of the volume and then using that information to construct the configuration tree for the volume. The configuration information retrieved from the labels during assimilation includes the raidmaps used to describe the topology of the volume.

Although the raidmaps and their mapping ranges are illustratively not exposed to operators as configurable entities the raidmap configuration may be important to system administrators when determining how to physically configure or alter a storage system. Disk addition techniques are knowledgeable about the current use of raidmap information when determining when to place new disks into a volume. I O task components i.e raidio structures utilize raidmaps rather than directly referencing the disks. The raidmaps also support transactional behaviors when configuration operations execute on the volume.

A write allocator of the file system is aware of the underlying RAID topology of the storage devices in order to achieve performance. To that end the write allocator uses the raidmap and its description of that topology to issue I O operations when accessing via write allocation data stored in the VBN range of a volume. A topology mechanism e.g. the raidmap cache exports the raidmap information from the RAID subsystem to the write allocator. Write allocation is directed to one RAID group at a time to enable writing of full stripes across the disks of the group. In the case of the illustrative RAID 4 level implementation parity calculation for a write allocated stripe is performed only once. Notably write allocation is based on RAID group layout rather than VBN space. That is the VBN space is used internally within the file system to identify blocks but not to allocate and write data to disk s . Instead the RAID group layout size is used for write allocation.

Typically a maximum number of disks may be specified for a RAID group of a volume and in the absence of a specified maximum a default maximum number of disks for a RAID group may be established. For example assume that the maximum RAID group size for a RAID 4 level implementation is 3 disks and RAID group comprises two data disks and one parity disk not shown . Assume further that an operator wants to add a third data disk to the volume. The third data disk is thus added to a new RAID group e.g. RAID group rg as data disk object d since the maximum number of disks in RAID group has been reached.

Assume now that the operator adds two data disks and one parity disk to RAID group . These data disk objects d and d are mapped to the VBN space of the volume in a manner that is contiguous with the data disk objects of RAID group object . That is d has DBNs that map to VBNs of the volume and d has DBNs that map to VBNs of the volume. Once the maximum number of disks in a RAID group has been satisfied and additional disks are inserted into a next RAID group the size of a previous RAID group typically cannot change e.g. cannot increase. This is due in part to the contiguous VBN to DBN mapping for e.g. write allocation in the file system. In other words VBN ranges typically cannot be mixed within RAID groups.

According to an aspect of the inventive technique a new disk may be added to a RAID group e.g. RAID group object that is not the most recently established i.e. the last RAID group e.g. RAID group object of a volume. The ability to add a new disk to a RAID group that is not the last RAID group is achieved because each disk stores a raidmap describing the DBN to VBN mapping of its blocks and because that mapping information along with other configuration information is cached stored in memory raidmap cache for use by the file system. Specifically the write allocator advantageously uses the mapping parameters of the raidmap to determine the noncontiguous VBN to DBN mapping of a disk e.g. the disk newly inserted of the RAID group. Configuration changes to the physical disks managed by the RAID subsystem are reflected as changed configuration information that is cached in raidmap cache for use by the write allocator of the file system . As long as the write allocator is aware of the underlying changed topology via the raidmaps there is no performance impact by having noncontiguous VBN to DBN mappings in a RAID group.

For example assume data disk object d is added to RAID group object rg even though data disk objects d d are members of RAID group object . depicts a first raidmap for d of rg e.g. rg d map . Raidmap specifies a starting disk block number dbn start equal to 0 a starting volume block number vbn start equal to 28 and a length of DBN to VBN mapping equal to 7. Thus the DBN to VBN mapping for newly inserted d is DBNs mapped to VBNs . Although this results in a noncontiguous VBN to DBN mapping within RAID group the raidmaps of the present invention enable the RAID subsystem to support such noncontiguous mappings and in particular the ability to have multiple VBN ranges per RAID group.

Another aspect of the present invention relates to the mapping of multiple VBN ranges to the DBN space of a disk through the ability to store more than one raidmap in a RAID label on each disk. This aspect of the invention is particularly useful in the event of a failure to a disk of a RAID group and the replacement of that failed disk with a larger disk. For example assume data disk d of RAID group fails and a new disk is added to RAID group to replace the failed disk. The data stored on the failed disk is reconstructed on the replacement disk in accordance with a conventional reconstruction process using the constituent data parity stored on the other disks of the RAID group. If the replacement disk has the same size as the failed disk then the DBN to VBN mapping of the replacement disk is similar to that of the failed disk. However if the replacement disk is larger than the failed disk the present invention enables use of the additional space on the larger disk through the use of second raidmap for the disk.

There is initially one raidmap for data disk object of RAID group object rg d map . If d fails and that disk is replaced with a larger disk the first portion of the larger disk is used to reconstruct the failed d such that DBNs are mapped to VBNs in the volume. In addition the second portion of the larger disk is used to map into the volume at the end of the VBN range that has been previously mapped. depicts a second raidmap for data disk object of RAID group object e.g. rg d map pertaining to the second portion of the larger disk inserted into RAID group . The raidmap specifies a starting disk block number dbn start equal to 7 a starting volume block number vbn start equal to 35 and a length of DBN to VBN mapping equal to 10.

Advantageously this aspect of the present invention provides a means for expanding the storage space of an existing file system when mapping the capacity of a new disk into a volume wherein the new disk is larger than the disks of the volume and making use of the additional space on the larger disk. An example of such a situation involves replacing a failed disk with a larger disk of a RAID group. The expansion aspect of the invention allows efficient use of inevitable increases in the sizes of disks that may be used as replacement disks for previously installed disks of a storage system. The expansion aspect of the present invention also provides a more efficient approach to utilizing the extra space on replacement disks as compared to prior approaches by allowing use of the additional space on replacement disks in an efficient manner that is online and involves no disruption to the file service provided by the storage system to an operator client.

Therefore when mapping the capacity of a disk into any RAID group of a volume such as when replacing a failed disk with a larger disk the present invention enables creation of i a first raidmap to reconstruct the failed disk using a first portion of the large disk and ii a second raidmap to map a second portion of the large disk into the VBN space of the file system volume. To that end the invention allows storage of two raidmaps in the RAID label of a particular disk. As noted there is space within each RAID label on each disk for many raidmaps.

Note that if a disk fails a disk error handling process modifies the state of the raidmap to indicate that maps associated with the disk are stale no longer valid . Disk error handling flips the stale map with a new map indicating that a hole exists in the VBN space corresponding to the size of the stale map. The stale map remains in memory until all uses by raidio structures and other components e.g. the raidmap cache of the RAID subsystem have terminated. Meanwhile a new map marked as unmapped space is integrated into the RAID group s raidmap configuration. The raidio structures and components recognize the stale map condition as part of their error handling and typically abort and restart with the RAID group s updated raidmap configuration.

Flipping of a map may occur when a disk is inserted into a RAID group to replace a failed disk or when a disk is removed from a RAID group. Map flipping is designed to operate transactionally allowing for concurrent I O operations to view the configuration in a coherent manner. When new disks are added to extend RAID group capacity transactional behaviors and file system semantics prevent access to the newly added VBN range until the operation has been committed to disk.

Another advantage of the expansion feature of the invention is the ability to move an existing volume from smaller disks to larger disks using synchronous mirroring and the additional mapping capabilities within those larger disks. For example assume it is desired to mirror an existing volume with disks of capacity size X e.g. 36 gigabytes GB to a new volume with disks of capacity 2X e.g. 72 GB. In the illustrative embodiment the RAID subsystem desires a mirroring behavior using exact sized disks implying that only half of the disk capacity is initially used in the new volume mirror. Once the mirror is synchronized the original volume mirror may be disabled leaving the volume with only 2X disks. The inventive technique enables the volume to extend the additional capacity resulting in multiple VBN ranges for the RAID group.

Assume that the existing plex plex has disks of capacity size X e.g. 36 GB and that the new mirrored plex plex has disks of capacity 2X e.g. 72 GB . Because of the mirroring behavior of desired by the RAID subsystem there is a one to one correspondence between each disk of each plex i.e. the disks contain the same data and map into the same VBN space. Thus if RAID group object rg has data disk objects d d and d then rg has d d and d . In addition the mirrored disks must have the same sized raidmaps therefore initially only an upper portion 36 GB of the 72 GB disks are used to provide symmetry with the disks in the other plex.

A resynchronization resync operation is then performed which essentially copies all the data with the original 36 GB disks to the mirrored 72 GB disks. While the resync operation is in progress an operator client has full access to the data stored on the original 36 GB disks. From the perspective of the file system no changes have occurred to the underlying topology of the volume. That is the file system has access to its entire VBN space with respect to reading or writing data to the mapped DBNs on the disks. Transparently though the RAID subsystem is copying the data from the original 36 GB disks to the mirrored 72 GB disks.

Once the data is fully copied to the mirrored plex a normal mirrored state is reached wherein there is full redundancy such that any access read or write operation to data served by the storage system occurs on disks of both plexes. However the additional storage space on the 72 GB disks cannot be used because there are no equivalent spaces on the disks of the other original plex. Since all the user data is now stored on the 72 GB disks of the second plex the original plex and its constituent disks can be destroyed and replaced with similarly sized 72 GB disks. Here a goal is to move or migrate an existing volume having 36 GB disks to larger 72 GB disks while also migrating the volume from an unmirrored to mirrored configuration. Once the original plex is destroyed the additional space on the 72 GB disks can be used and therefore mapped into the VBN space of the volume since there is no longer any restriction to its use. As a result the file system VBN space can be dynamically increased to take advantage of the additional physical storage space DBN space on the newly inserted 72 GB disks.

According to the invention each 72 GB disk has two raidmaps used to map the physical disk space DBN on the disk to the volume space VBN in a manner that is transparent to the file system as long as that configuration information is cached in memory for use by the file system. That is using the raidmaps and configuration information the file system can read or write to all the mapped ranges on the disks. Since the original plex has been destroyed and the 36 GB disks have been removed from the storage system those disks can be replaced with 72 GB disks and recreate the original plex using those disks. Thereafter the data stored on the other plex is copied over onto the newly created plex in accordance with a mirror resync operation wherein the data that is copied includes the raidmaps. Throughout the entire process of replacing original 36 GB disks with larger 72 GB disks the file system continues to provide file service for the data stored on the disks and in fact the file system is unaware of the configuration change.

By providing two full copies of the data for a volume the use of mirroring in connection with the present invention advantageously enables i off lining of a plex bringing the plex offline ii unplugging of the disks of a disk shelf from the storage system iii relocating of the disk shelf to another location iv reconnecting of the disk shelf and disks with the storage system and then v onlining the plex bringing the plex online without disturbing file service to the information stored on the volume. The present invention obviates the burdensome procedure of the prior art because only one volume is utilized between the two plexes.

Assume now that a disk in one plex e.g. d in plex is replaced by a yet larger disk d. In order to extend the DBN to VBN mappings on replacement disk d to utilize the extra space on that disk the mirrored disk in the mirrored plex e.g. d in plex must also be extended to utilize the equivalent amount of extra space in order to maintain symmetry between the mirrored copies of data. Therefore a restriction with respect to the present invention when utilized in a mirroring arrangement is that the additional space provided by a larger replacement disk may not be utilized until the mirrored disk is replaced with an equivalently sized disk.

Advantageously the present invention isolates disks from VBN ranges providing flexibility in RAID group topologies. Isolation of VBN ranges from disks further allows for dynamic extension of the VBN range at the RAID level without requiring reorganization of the existing blocks in the file system. The inventive technique supports dynamic extension of capacity using existing but formerly unused disk space. Moreover the on disk label structure contains placeholders for multiple per disk VBN range data thereby facilitating instantiation of the raidmap information when a volume is brought into the system during initialization.

While there has been shown and described an illustrative embodiment for mapping the capacity of disks into any RAID group location within a VBN space of a volume of a storage system it is to be understood that there are other advantages provided by the technique of the present invention. For example another application of the present invention enables a disk to utilize its physical disk space DBN ranges among different volumes of the storage system. Typically the physical DBN storage space of a disk is restricted to residing in the same volume. The use of multiple raidmaps per disk allows a second portion of the DBN space on a disk to reside in another volume that is different from the volume within which a first portion of the disk s DBN space resides. The configuration information stored in the RAID label must be extended to differentiate between the set of objects associated with a first portion of the physical disk space and the set of objects associated with the second portion of the physical disk space.

In summary restrictions applicable to previous RAID subsystems include i each disk had exactly one raidmap stored in each RAID label on the disk and ii the ordering of disks as they appeared in the file system map is exactly the same as the ordering of disks in the RAID groups. The present invention relaxes those restrictions by i allowing each disk to have one or more raidmaps in each RAID label and ii allowing ordering of disks as they appear in the file system map to be different from the ordering of disks in the RAID groups i.e. a disk can be added into a RAID group that is other than the last established RAID group .

A preferred embodiment of the invention has been described herein with reference to a file server having a storage operating system with a file system layer and a RAID subsystem among other components which manages file semantics in order to access data organized in files. It should be understood however that the invention can be practiced in any system or device that maps the capacity of disks into any RAID group of a volume of a storage system particularly in light of configuration management changes. One type of system or device in which the invention can be embodied is designed to perform a data storage function and if so may perform data related operations e.g. in response to data access requests. Such requests may use file based and or block based semantics depending on the implementation and correspondingly the system or device may organize data in files or in another manner. Moreover such systems and devices may or may not incorporate features and functions described herein such as for example a file system layer or a RAID subsystem or may combine or otherwise modify their operation without departing from the principles of the invention.

The foregoing description has been directed to specific embodiments of this invention. It will be apparent however that other variations and modifications may be made to the described embodiments with the attainment of some or all of their advantages. For instance it is expressly contemplated that the teachings of this invention can be implemented as software including a computer readable medium having program instructions executing on a computer hardware firmware or a combination thereof In addition it is understood that the data structures described herein can include additional information while remaining within the scope of the present invention. Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the invention. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention.


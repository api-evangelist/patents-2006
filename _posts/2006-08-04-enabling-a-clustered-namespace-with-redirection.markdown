---

title: Enabling a clustered namespace with redirection
abstract: Provided is a method and system for redirecting data access requests in a clustered computing environment. A clustered namespace applied to the clustered computing environment includes multiple modules for storing and manipulating data. The clustered namespace is enabled by using a replicated storage location repository listing storage locations distributed throughout the clustered computing environment. When searching for data, pathnames to storage locations are examined and compared with values listed in the storage location repository to identify a storage location to visit. A module associated with an identified storage location is visited to satisfy a data access request. If a redirection identifier is encountered when examining the metadata of the identified storage location, then the storage location repository is examined to find the next storage location to visit to satisfy the data access request.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07987167&OS=07987167&RS=07987167
owner: NetApp, Inc.
number: 07987167
owner_city: Sunnyvale
owner_country: US
publication_date: 20060804
---
Embodiments of the present invention relate to storage systems and in particular to redirecting data access requests in a clustered computing environment.

Networks of computers implement a namespace to identify particular computers of the network. For example in computer network implementing a Domain Name System DNS namespace a particular computer may be identified by myname.network.com where the computer is located in a subdomain myname of a secondary domain network of a top level domain com. By uniquely identifying computers within a namespace it is possible to find a particular computer of the network.

During write operations of data to computers in a network the data may be written to any computer in the network. During a read operation for previously written data the data can be found by first identifying the computer where the data was originally written and second obtaining the data from the identified computer. However data may have moved from the first identified computer or may have been deleted.

In some computer networks if the data is not found at the identified computer during a read operation then the result of the read operation is a message of data not found. In other computer networks the data may have moved so instead of ending the read operation the third step is to visit another computer for the written data. The identifier of this new computer is listed in the first identified computer that was visited to obtain the written data. Consequently when data is constantly moved a chain of identifiers is distributed throughout the computer network in order to find data. However following the chain of identifiers is time consuming and is difficult to administer because administration requires the explicit management of the distributed identifiers along the chain. Specifically any mistake in the chain of identifiers results in a broken chain.

For example suppose three computers of a network include a chain of identifiers. Computer A includes an identifier B for computer B and computer B includes an identifier C for computer C. If identifier C is corrupted and not updated by an administrator then there is a broken link in the chain of identifiers. Consequently a search for data stored on computer C may start with a visit to computer A which leads to computer B. However since identifier C is corrupted computer C is never identified correctly and the data stored on computer C is never provided.

Accordingly what is needed is a method and system to navigate a network of computers where data may be distributed in any fashion without requiring difficult time consuming administration of identifiers.

Embodiments of the present invention provide a method and a system for redirecting requests for data stored in a clustered computing environment by enabling a clustered namespace. Since data can be written to any storage location of the clustered computing environment the clustered computing environment is called a clustered storage system. When a data access request is made for data stored in the clustered storage system unique identifiers of the clustered namespace identify particular storage locations of the clustered storage system.

The unique identifiers are included in a storage location repository that is replicated throughout the clustered storage system instead of creating a chain of identifiers. Further instead of including an identifier in a storage location to act as a link in a chain of identifiers a redirection identifier is included which is an indication that the data is not stored in the storage location. When encountering the redirection identifier during a data access request the storage location repository is examined to find the next storage location that may contain the data. Thus instead of explicitly managing a chain of identifiers to multiple storage locations redirection identifiers can be used to indicate that the replication storage location repository should be examined. By replicating the storage location repository identifiers are updated in a central repository instead of the difficult and time consuming administration task of updating chains of identifiers.

In an exemplary embodiment of a method for redirecting a data access request the method includes receiving the data access request for data stored in a clustered storage system. Further the method includes identifying a first storage location by examining a storage location repository and accessing the first storage location identified by the storage location repository to retrieve the data. The method also includes encountering a redirection identifier associated with the first storage location indicating that the data is not stored in the first storage location thereby avoiding continued examination of the first storage location because the redirection identifier lacks an identifier to a second storage location.

In an exemplary embodiment of a storage system for redirecting data access requests the storage system includes multiple disk modules in communication with a storage location repository. The storage location repository identifies at least one disk module and a redirection identifier of an identified disk module indicates that the data is not stored in the identified disk module. Thus encountering the redirection identifier avoids continued examination of the identified disk module because the redirection identifier lacks an identifier to another disk module.

In an exemplary embodiment of a computing environment the computing environment includes a memory store and a communication channel. The memory store of a first module is configured to store a storage location repository of volume identifiers. At least one of the volume identifiers identifies a location of a second module. The communication channel of the first module is communicably coupled to the second module such that the second module includes a processor for processing instructions for the examination of a redirection identifier of the second module. The redirection identifier indicates that data is not stored in the second module thereby avoiding continued examination of the second module because the redirection identifier lacks an identifier to a third module.

It should be appreciated that other aspects of the invention will become apparent from the following detailed description taken in conjunction with the accompanying drawings which illustrates by way of example the principles of the invention.

Embodiments of the present invention provide a method and a system for redirecting requests for data in a clustered computing environment by enabling a clustered namespace. The clustered namespace may be implemented with multiple namespaces such that the clustered computing environment can be shared among multiple customers. For example customer A may have access only to namespace A and customer B may have access only to namespace B. Both namespaces exist in the clustered namespace of a single clustered computing environment such that company A and company B cannot access each other s data nor do they realize that they share the same environment.

Since data can be written to any storage location of the clustered computing environment the clustered computing environment is called a clustered storage system. When a data access request is made for data stored in the clustered storage system unique identifiers of the clustered namespace identify particular storage locations of the clustered storage system. The unique identifiers are included in a storage location repository that is replicated throughout the clustered storage system instead of creating a chain of identifiers. Further instead of including an identifier in a storage location to act as a link in a chain of identifiers a redirection identifier is included which is an indication that the data is not stored in the storage location. An exemplary redirection identifier includes a junction which is later described with respect to . When encountering the redirection identifier during a data access request the storage location repository is examined to find the next storage location that may contain the data. Thus instead of explicitly managing a chain of identifiers to multiple storage locations redirection identifiers can be used to indicate that the replication storage location repository should be examined. By replicating the storage location repository identifiers are updated in a central repository instead of the difficult and time consuming administration task of updating chains of identifiers.

The clients may be general purpose computers configured to interact with the node in accordance with a client server model of information delivery. For example interaction between the clients and nodes can enable the provision of storage services. That is each client may request the services of the node and the node may return the results of the services requested by the client by exchanging packets over the connection system which may be a wire based or wireless communication system. The client may issue packets including file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively the client may issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks.

Each node is illustratively embodied as a dual processor storage system executing a storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on the disks. However it will be apparent to those of ordinary skill in the art that the node may alternatively comprise a single or more than two processor system. Illustratively one processor can execute the functions of the N module on the node while the other processor can execute the functions of the D module. It should also be appreciated that processors may include multiple processing cores thus improving the processing speed of the processors

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures associated with the present invention. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node by inter alia invoking storage operations in support of the storage service implemented by the node .

The network adapter comprises a plurality of ports adapted to couple the node to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively the connection system may be embodied as an Ethernet network or a Fibre Channel FC network. Each client may communicate with the node over the connection system by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node to access information requested by the clients . The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the disks of the disk array . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

In accordance with an exemplary embodiment of the present invention storage of information on each disk array is preferably implemented as one or more storage locations or volumes that comprise a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number vbn space on the volume s . Each logical volume is generally although not necessarily associated with its own file system. The disks within a logical volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID . Most RAID implementations such as a RAID 4 level implementation enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of parity information with respect to the striped data. An illustrative example of a RAID implementation is a RAID 4 level implementation although it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.

To facilitate access to the disks the storage operating system of implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by the disks . A file system logically organizes the information as a hierarchical structure of named directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers luns .

In the illustrative embodiment the storage operating system is preferably the NetApp Data ONTAP operating system available from Network Appliance Inc. of Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such the storage operating system should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

In addition the storage operating system includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on the disks of the D blade . To that end the storage server includes a file system module for managing volumes a RAID system module and a disk driver system module . The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations while the disk driver system implements a disk access protocol such as e.g. the SCSI protocol.

The file system also implements processes such as a redirection process in an exemplary embodiment of the present invention. The redirection process includes one or more computer readable instructions that manage redirection identifiers of storage locations or volumes as previously described above. Within the clustered storage system with multiple D modules multiple volumes may be associated with a single D module or multiple volumes may be allocated among multiple D modules. For example volumes distributed among multiple D modules may be implemented with striped volumes of data e.g. round robin allocation of data among the striped volumes. However any method of distributing multiple volumes among D modules or multiple volumes sharing a single D module are possible as long as the volumes include redirection identifiers that redirect data access requests when the data is not found in a particular volume. The redirection process manages the redirection identifiers by interfacing with management commands from an administrator who can enter a command to create a redirection identifier for a particular volume by using a graphical user interface GUI command line interface CLI or the like. The creation of redirection identifiers is further described with respect to .

Further the file system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The vdisk module enables access by administrative interfaces such as a user interface of a management framework not shown in response to a user system administrator issuing commands to the node . The SCSI target module is generally disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as blocks.

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks . That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as mirroring and or parity RAID . The file system illustratively implements the WAFL file system hereinafter generally the write anywhere file system having an on disk format representation that is block based using e.g. 4 kilobyte kB blocks and using index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store meta data describing the layout of its file system these meta data files include among others an inode file. A filehandle i.e. an identifier that includes an inode number is used to retrieve an inode from disk.

Broadly stated all inodes of the write anywhere file system are organized into the inode file. A file system fs info block specifies the layout of information in the file system and includes an inode of a file that includes all other inodes of the file system. Each logical volume has an fsinfo block that is preferably stored at a fixed location within e.g. a RAID group. The inode of the inode file may directly reference point to data blocks of the inode file or may reference indirect blocks of the inode file that in turn reference data blocks of the inode file. Within each data block of the inode file are embedded inodes each of which may reference indirect blocks that in turn reference data blocks of a file.

Operationally a request from the client is forwarded as a packet over the connection system and onto the node where it is received at the network adapter . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write anywhere file system . Here the file system generates operations to load retrieve the requested data from disk if it is not resident in core i.e. in memory . If the information is not in memory the file system indexes into the inode file using the inode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the connection system .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by node in response to a request issued by the client . Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the node . It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

In an illustrative embodiment the storage server is embodied as D module of the storage operating system to service one or more volumes of the disk array . In addition the multi protocol engine is embodied as N module to i perform protocol termination with respect to a client issuing incoming data access request packets over the connection system as well as ii redirect those data access requests to any storage server of the cluster . Moreover the N module and D module cooperate to provide a highly scalable distributed storage system architecture of a clustered storage system. To that end each module includes a cluster fabric CF interface module adapted to implement intra cluster communication among the modules including D module to D module communication for data container e.g. a file access operations.

The protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of the N module function as protocol servers that translate file based and block based data access requests from clients into CF protocol messages used for communication with the D module. That is the N module convert the incoming data access requests into file system primitive operations commands that are embedded within CF messages by the CF interface module for transmission to the D modules of the cluster . Notably the CF interface modules cooperate to provide a single file system image across all D modules in the cluster . Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module of the cluster .

Further to the illustrative embodiment the N module and D module are implemented as separately scheduled processes of storage operating system however in an alternate embodiment the modules may be implemented as pieces of code within a single operating system process. Communication between an N module and D module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from Network Appliance Inc.

In an exemplary embodiment luns blocks directories qtrees and files may be contained within flexible volumes such as dual vbn flexible volumes that in turn are contained within the aggregate . The aggregate is illustratively layered on top of the RAID system which is represented by at least one RAID plex depending upon whether the storage configuration is mirrored wherein each plex comprises at least one RAID group . Each RAID group further comprises a plurality of disks e.g. one or more data D disks and at least one P parity disk. Whereas the aggregate is analogous to a physical volume of a conventional storage system a flexible volume is analogous to a file within that physical volume. That is the aggregate may include one or more files wherein each file contains a flexible volume and wherein the sum of the storage space consumed by the flexible volumes is physically smaller than or equal to the size of the overall physical volume.

A file system layout is provided that apportions an underlying physical volume into one or more virtual volumes or flexible volume of a storage system such as node . In an exemplary embodiment the underlying physical volume is an aggregate comprising one or more groups of disks such as RAID groups of the node . The aggregate has its own physical volume block number pvbn space and maintains meta data such as block allocation structures within that pvbn space. Each flexible volume has its own virtual volume block number vvbn space and maintains meta data such as block allocation structures within that vvbn space. Each flexible volume is a file system that is associated with a container file the container file is a file in the aggregate that contains all blocks used by the flexible volume. Moreover each flexible volume comprises data blocks and indirect blocks that contain block pointers that point at either other indirect blocks or data blocks.

In one embodiment pvbns are used as block pointers within buffer trees of files such as file stored in a flexible volume. This hybrid flexible volume embodiment involves the insertion of only the pvbn in the parent indirect block e.g. inode or indirect block . On a read path of a logical volume a logical volume vol info block has one or more pointers that reference one or more fsinfo blocks each of which in turn points to an inode file and its corresponding inode buffer tree. The read path on a flexible volume is generally the same following pvbns instead of vvbns to find appropriate locations of blocks in this context the read path and corresponding read performance of a flexible volume is substantially similar to that of a physical volume. Translation from pvbn to disk dbn occurs at the file system RAID system boundary of the storage operating system .

In an illustrative dual vbn hybrid flexible volume embodiment both a pvbn and its corresponding vvbn are inserted in the parent indirect blocks in the buffer tree of a file. That is the pvbn and vvbn are stored as a pair for each block pointer in most buffer tree structures that have pointers to other blocks e.g. level 1 L1 indirect blocks inode file level 0 L0 blocks. For example in a root top level inode such as an embedded inode references indirect e.g. level 1 blocks . Note that there may be additional levels of indirect blocks e.g. level 2 level 3 depending upon the size of the file. The indirect blocks and inode contain pvbn vvbn pointer pair structures that ultimately reference data blocks used to store the actual data of the file.

The pvbns reference locations on disks of the aggregate whereas the vvbns reference locations within files of the flexible volume. The use of pvbns as block pointers in the indirect blocks provides efficiencies in the read paths while the use of vvbn block pointers provides efficient access to required meta data. That is when freeing a block of a file the parent indirect block in the file contains readily available vvbn block pointers which avoids the latency associated with accessing an owner map to perform pvbn to vvbn translations yet on the read path the pvbn is available.

The junction type is a hint that there is a volume mounted at the junction. However the junction does not explicitly include the location for the volume mounted at the junction. Instead a storage location repository includes the location information of the volume. Thus the junction is a redirection identifier associated with a storage location indicating that the data is not stored at the storage location where the data is originally sought but is available at some other storage location. The information about the mounted volume is retained by a storage location repository as later described with respect to .

Junctions are mounted during volume creation by the implementation of a management command from a CLI GUI or the like. For example the command may be create a volume and mount it on the pathname a b c. The pathname a b c indicates that the directory names are parent directory a followed by sub directory b. The last component c is a junction . The new volume is created on an aggregate . The new volume identification and the junction inode information are recorded in a storage location repository such as the VLDB . Thus when looking for the a b c file the junction at the volume containing the component of the pathname c would be a hint that the file is located on another volume.

For example if an exemplary file system operation is find a b c file then the search begins with a. At the volume including a the pathname is parsed or separated to look for b. However at a a junction may indicate that another volume contains the file. This is a hint that the storage location repository should be examined. Specifically the storage location repository is examined for a volume location identifier and inode identifier. Alternatively if no junction type is encountered at a then there is no reason to examine the storage location repository.

When creating junction types some rules can be applied. For example one rule can be that junction types cannot be stored in other junction types which can be implemented by mounting volumes only once in the namespace. Another rule is that permission for read access to junctions is fully allowed for all file system operations. This rule is a presumption that authentication to the clustered storage system has occurred. Specifically if authentication has not yet occurred meaning that users may not access data stored on the clustered storage system then permissions can be associated per junction to grant or restrict access to data. It should be appreciated that other rules for junctions are possible as long as the rules allow for efficient and simple management of junctions thus minimizing administrative oversight.

In accordance with an embodiment of the invention is a diagram illustrating a collection of management processes to manage data stored on one or more storage locations of the clustered storage system. The management processes execute as user mode applications on the storage operation system of the clustered storage system to provide management of configuration information i.e. management data for all the nodes . To that end the management processes include a management framework process and a volume location database VLDB process each utilizing a data replication service RDB linked as a library. The management framework provides an administrator interface via a command line interface CLI a web based graphical user interface GUI or the like. The management framework is illustratively based on a conventional common interface model CIM object manager that provides the entity to which administrators interact with a node in order to manage the cluster. The VLDB process is a database process that tracks the storage locations of various storage components e.g. flexible volumes aggregates etc. within the cluster to thereby facilitate routing of requests throughout the clustered storage system.

The management processes have interfaces to are closely coupled to RDB . The RDB comprises a library that provides a persistent object store storing of objects for the management data processed by the management processes. Notably the RDB replicates and synchronizes the management data object store access across all nodes of the cluster to thereby ensure that the RDB database image is identical on all of the nodes . At system startup each node records the status state of its interfaces and IP addresses those IP addresses it owns into the RDB database. Specifically the VLDB process and the RDB operate together to function as a storage location repository. It should be appreciated that separation of the VLDB process and the RDB is purely exemplary. Accordingly in another exemplary embodiment a storage location repository may be implemented as a VLDB having an integrated database functioning as a persistent object store. Via the administrator interface which may be operated on a node or client management tools may be used to create modify and check inconsistencies of the entries of the storage location repository. Such exemplary tools are useful for maintaining the replicated database within the clustered storage system thus avoiding or preventing data inconsistencies within the storage system repository.

In an exemplary embodiment the storage location repository also includes a junction table not shown having table entries such as an inode identifier or filehandle a child of a master set identifier MSID that indicates the child volume pathname identifier and a data set identifier DSID that uniquely identifies a single instance of a volume. The MSID identifies a master or active volume that is identically replicated in the clustered storage system. As each junction type is created in an inode the junction table of the storage location repository is updated. Further as volume pathnames are updated the junction table entries associated with the volume pathname is updated. However as previously described the junction type of an inode is not updated. Thus upon encountering a junction type the storage location repository and specifically the junction table is examined to find the next storage location to search for data. In this fashion the junction table can function as a mount table that is replicated throughout the clustered storage system.

The VLDB is capable of tracking the locations of volumes and aggregates of nodes . Specifically the VLDB includes a plurality of entries which in turn provide the contents of entries in the configuration table . Among other things the VLDB entries keep track of the locations of the flexible volumes hereinafter generally volumes and aggregates within the clustered storage system. is a diagram illustrating an exemplary VLDB volume entry in accordance with an embodiment of the invention. Further is a diagram illustrating a VLDB aggregate entry in accordance with an embodiment of the invention. Thus examples of VLDB entries include the VLDB volume entry and the VLDB aggregate entry .

The VLDB entry of includes a volume ID field an aggregate ID field and in alternate embodiments additional fields . The volume ID field contains an ID that identifies a volume used in a volume location process. The aggregate ID field identifies the aggregate containing the volume identified by the volume ID field . Likewise illustrates an exemplary VLDB aggregate entry . The VLDB aggregate entry includes an aggregate ID field a D blade ID field and in alternate embodiments additional fields . The aggregate ID field contains an ID of a particular aggregate in the clustered storage system. The D blade ID field contains an ID of the D blade hosting the particular aggregate identified by the aggregate ID field .

The VLDB illustratively implements a RPC interface e.g. an ONC RPC interface which allows the N blade to query the VLDB. When encountering contents of a data container handle that are not stored in its configuration table the N blade sends an RPC to a VLDB process. In response the VLDB process returns to the N blade the appropriate mapping information including an ID of the D blade that owns the data container. The N blade caches the information in its configuration table and uses the D blade ID to forward the incoming request to the appropriate data container. All functions and communication between the N blade and D blade are coordinated on a cluster wide basis through the collection of management processes and the RDB library user mode applications as described with respect to .

Upon arrival of the data access request at N module the VLDB is examined to determine the location of a volume that contains the pathname z1 such as described with respect to . Illustratively the VLDB lists the locations of volume A and volume C on D module and volume D and volume B on D module . Initially upon creation of info.txt the VLDB lists the location of z1 at volume C . However during the operation of the clustered storage system the file info.txt moves to volume B . Thus when the search for info.txt proceeds to examining the inode of volume C a junction is encountered. Since the junction is an indication that data is elsewhere the VLDB is once more examined. Specifically the filehandle of volume C is returned after encountering the junction type in order to use the filehandle to look up the appropriate pathname in the junction table. It should be appreciated that in some exemplary embodiments the next volume that may contain the sought for data may be stored on another volume of the D module . In yet other exemplary embodiments the sought for data may reside on a volume of another D module e.g. D module

Upon examining the VLDB or cache for VLDB information e.g. cached junction table entries by using the returned filehandle of the examined volume the search is directed to volume B . Thus the search proceeds via the cluster switching fabric to the D module . Ultimately if the another junction type is not encountered then the file info.txt is found under the pathname z1 z2. Alternatively if the junction type or data is not found the data is no longer accessible. By using the junction type and the junction table the continued parsing of the pathname z1 z2 in volume C is unnecessary thus saving time during the search. Further by using the junction table the individual volumes need not store volume identification information that requires complex administration to prevent broken chains.

After operation verifies the existence of the first component of a pathname the component s inode structure is read from a storage location such as memory in operation . For example verifying whether or not a component exists can be to identify the existence of an inode structure. When examining an inode structure of a component of a particular volume a junction may be encountered in operation . If a junction is encountered in operation then the process step includes a returned error code value and the junction information in operation . It should be appreciated that the error code can be any value expressed in binary hexadecimal or the like that indicates an error of not finding the data sought. Further the junction information includes filehandle information to perform a junction table lookup in the storage location repository. Accordingly in operation the next process step is to examine the volume location database to identify a volume associated with the junction filehandle returned when encountering the junction. Specifically the volume location database is examined to find an entry for a volume identifier and inode identifier matching the junction filehandle information. If the entry is not found in the volume location database then the process step includes responding to the client computer with inode information associated with an unpopulated directory i.e. an unpopulated directory contains a reference to the unpopulated directory and a reference to the parent directory of the unpopulated directory .

Thereafter in operation if the junction table lists another volume to visit then the next process step is to visit the identified volume in operation . Alternatively if there is no identified volume then the next process step is operation further described below. Specifically an identified volume may or may not reside on the same D module where the junction was encountered. Thus when encountering an identified volume the process step then moves to operation to process any remainder of the pathname including any additional junctions. Thus if junctions are encountered continuously then the process repeats. However the process does not repeat indefinitely because during the creation of junctions rules prevent such cycles. For example one rule is to only allow the mounting of a volume in the clustered namespace only once thus only causing one listing in the junction table.

Alternatively if a junction is not encountered in operation then the next step proceeds to operation to determine whether the examined inode represented the last component of the pathname and therefore the process has found the data sought. If this inode was the last component and there is no other component to examine then in operation the inode information and any data is returned to the client computer that initiated the data access request. Thereafter the process ends. However if there are more components to process in operation then the next component of the pathname is checked for its existence in operation . If such a component exists then the next step in the process moves to operation . If another component does not exist then the next step is proceed to operation to inform the requestor such as the client computer that the data was not found for the pathname. Thereafter the process ends.

The operations herein described are purely exemplary and imply no particular order. Further the operations can be used in any sequence when appropriate and can be partially used. With the above embodiments in mind it should be understood that the invention can employ various computer implemented operations involving data stored in computer systems. These operations are those requiring physical manipulation of physical quantities. Usually though not necessarily these quantities take the form of electrical magnetic or optical signals capable of being stored transferred combined compared and otherwise manipulated.

Any of the operations described herein that form part of the invention are useful machine operations. The invention also relates to a device or an apparatus for performing these operations. The apparatus can be specially constructed for the required purpose or the apparatus can be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular various general purpose machines can be used with computer programs written in accordance with the teachings herein or it may be more convenient to construct a more specialized apparatus to perform the required operations.

The invention can also be embodied as computer readable code on a computer readable medium. The computer readable medium is any data storage device that can store data which can be thereafter be read by a computer system. Examples of the computer readable medium include hard drives accessible via network attached storage NAS Storage Area Networks SAN read only memory random access memory CD ROMs CD Rs CD RWs magnetic tapes and other optical and non optical data storage devices. The computer readable medium can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion. The computer readable medium can also be distributed using a switching fabric such as used in compute farms.

The foregoing description has been directed to particular embodiments of this invention. It will be apparent however that other variations and modifications may be made to the described embodiments with the attainment of some or all of their advantages. Specifically it should be noted that the principles of the present invention may be implemented in non distributed file systems. Furthermore while this description has been written in terms of N and D modules the teachings of the present invention are equally suitable to systems where the functionality of the N and D modules are implemented in a single system. Alternately the functions of the N and D modules may be distributed among any number of separate systems wherein each system performs one or more of the functions. Additionally the procedures processes and or modules described herein may be implemented in hardware software embodied as a computer readable medium having program instructions firmware or a combination thereof. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention


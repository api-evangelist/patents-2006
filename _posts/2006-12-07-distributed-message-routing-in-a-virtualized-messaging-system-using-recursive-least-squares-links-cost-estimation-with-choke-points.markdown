---

title: Distributed message routing in a virtualized messaging system using recursive least squares links cost estimation with choke points
abstract: Routing data packet traffic in packet networks, expediting the flow of the data packets and reducing the effects of congestion at nodes is accomplished by capturing latency data for nodes for initial measurement times; and computing a least squares estimate of the delay as a recursive least-squares solution.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07742422&OS=07742422&RS=07742422
owner: International Business Machines Corporation
number: 07742422
owner_city: Armonk
owner_country: US
publication_date: 20061207
---
The invention relates to methods systems and computer program products for transferring data among a plurality of spatially distributed computers or digital data processing systems via one or more communications media such as computer networks. Specifically the invention relates to routing data packet traffic in such networks and expediting the flow of the data packets and reducing the effects of congestion at nodes.

Data is communicated with data services data storage and data users as data packets between nodes through networks under the control of routers.

As used herein a packet is the unit of data that is routed between an origin and a destination on the Internet or any other packet switched network. When any file e mail message file HTML file GIF file URL request and so forth is routed from one place to another within a message oriented middleware system MOM the file is divided into chunks or packets of an efficient size for routing. Each of these packets is separately numbered and includes the Internet address of the destination. The individual packets for a given file may travel different routes through the Internet. When they have all arrived they are reassembled into the original file by the TCP layer at the receiving end . In this context packet switching is an efficient way to handle transmissions on a connectionless network such as the Internet.

As used herein a router means a device or in some cases software in a computer that determines the next network point to which a packet should be forwarded on its way to its destination. The router is connected to at least two networks and decides which way to send each information packet based on the router s current understanding of the state of the networks it is connected. A router is located at any gateway where one network meets another including each gateway on the Internet. A router is often included as part of a network.

A router may create or maintain a table of the available routes and their conditions and use this information along with distance and cost algorithms to determine the best route for a given packet. Typically a packet may travel through a number of network points with routers before arriving at its destination. Routing is a function associated with the standard model of network programming the Open Systems Interconnection model.

One application of networks between nodes under the control of routers is Message Oriented Middleware. As used herein message oriented middleware MOM refers to a kind of asynchronous messaging system where multiple applications communicate via a shared message bus that is an asynchronous messaging system interconnects the applications. Application Programming Interfaces APIs that extend across diverse platforms and networks are typically provided by the MOM.

In client server applications message oriented middleware provides support for asynchronous communication between client and server applications. Message queues provide temporary storage when the destination program is busy or not connected. Implementing message oriented middleware within a client server application decouples the client application from the server application.

An extension to MOM known as the Enterprise Service Bus ESB provides a messaging infrastructure for Service Oriented Architectures. Virtualizing the ESB enables an infrastructure that is reliable low cost and easily scalable. In this context an ESB can be virtualized through the deployment of a collection of full function nodes. Each individual node is capable of delivering the entire ESB but with limited bandwidth for processing messages. Each node that is added to the virtualized ESB increases the message processing bandwidth of the ESB. This work is applicable to virtualized ESB deployments with both homogeneous and heterogeneous nodes e.g. nodes may or may not be capable of delivering the same level of message throughput.

Producers of messages within a virtualized ESB must select a node to process outgoing messages. That is producers must select the best node for routing their messages. The best node for processing a message is defined as one that provides the minimum message latency. The general problem then becomes how to implement a message routing heuristic that minimizes message latency without compromising reliability or scalability.

A common technique for routing packets on a network is through shortest path first SPF . Shortest Path First is a common technique in network packet routing where the packet is sent along the shortest path from the current router to its destination. This technique alone is well known to have performance issues.

Network packet routing is a kind of distributed system where each router sees a delayed view of the current state of the network. When each router utilizes a SPF approach then a race condition can occur when the network is under high load. Essentially when a single link becomes under utilized all routers race to use that link since it is under utilized and offers the shortest path. The result is that the under utilized link quickly becomes overwhelmed causing network traffic to be routed away from that link and an oscillation occurs that degrades network performance.

In the SPF algorithm a router estimates the path length from the router to the packet s destination through each of its output ports and sends the packet through the port that offers the shortest path to the packet s destination. As described herein an analogous construct is used to estimate the delay associated with routing a message from the message producer to its destination through any of the nodes adjacent to the producer.

Delay and congestion can be modeled using choke packets. Choke packets as they are used in networking are a mechanism for congestion control that limits message transfers. In the context of networking when traffic through a switch passes a predefined threshold then the switch transmits a choke packet to the source of the current traffic informing the source to reduce traffic volumes by a given percentage. In a virtualized ESB choke packets are used to communicate transmission delays to the message producer to enable the sender to select a more effective channel for transmission of their message. Choke packets are used to redirect the transmission of messages away from highly loaded nodes to more effective nodes.

We avoid problems of the prior art by providing a distributed scheduling procedure for minimizing round trip message latency through a network of servers in a virtualized Message Oriented Middleware MOM system using a recursive least squares prediction model for estimating the message latency in a virtualized MOM environment. Further we utilize choke packets in the messaging layer to provide congestion control to the virtualized MOM environment that is directly compatible with the basic message latency estimation technique. This estimation procedure is then used by the distributed message routers to select nodes in the MOM on a per message basis that will minimize message latency without introducing a single point of failure to the virtualized MOM. This solution provides a more efficient and scalable means of effectively implementing a virtualized message bus.

Specifically the problems associated with node congestion are avoided by routing message packets from one node in a network to another node by using message latency predictions to guide future message transfers and augmenting the predictions with direct node feedback in the form of choke packets used to correct anomalous behavior within the network. According to our invention the latency through a node for a subsequent packet is predicted using a well known recursive least squares procedure. The recursive least squares procedure attempts to fit a linear model to past message latency measurements to predict future message latencies for a next packet through each of the attached nodes.

The recursive least squares procedure produces an initial latency estimate from previously measured latency data. The initial estimate is updated or corrected by applying a correction term to the previous estimate that is constructed from more recently measured message latencies.

The method of the invention utilizes a collection of past data points i.e. message latencies where latency is bound to the current load on a node in the system to predict future message latencies. This is inherently time dependent and subject to change therefore a recursive least squares approach is justified as it regularly corrects the latency estimate by applying new measurements to the estimation process. Further the invention incorporates node feedback transmitted in the form of choke packets into the recursive least squares estimation process. That is when a choke packet is received from a node the implied processing delay incorporated within the choke packet is used to re execute the recursive least squares procedure to produce a new latency measurement for future messages through that node.

The invention described herein controls routing of message packets from one node e.g. a requester node in a network to another node e.g. a provider node in the network through the network from the requester node i.e. the origin node to a provider node i.e. the destination node using a chosen path through the network. is a high level schematic diagram of a packet network with nodes including requester nodes A B and C associated with Routing Agent Software and provider nodes A B and C and ESB nodes as intermediate nodes A B and C. The requester nodes A B and C the ESB intermediate nodes A B and C and the provider nodes A B and C are all characterized by the presence of processors memory and communications interfaces for data and control.

Consider the system represented in above. A requester as requester A B or C requests an ESB node to send a packet to a provider node as provider node A B or C. The packets are associated in a queue or queues .

Routing from Requester Nodes as A B or C to ESB Nodes as nodes A B or C to Provider Nodes as nodes A B or C is accomplished by a Routing Agent associated with a requester as a software application . The Routing Agent software application scans the ESB Nodes A B or C and sends the packet to the ESB Node A B or C with the lowest latency.

Latency is determined by analysis of time stamp data. Specifically the ESB nodes A B and C duplex receipt of a packet by sending an acknowledgement back to the requester. The time delay for the round trip is a measure of the latency at the ESB node A B or C.

This round trip measure of latency is measured against a threshold. The threshold is neither static nor arbitrary but is dynamic and a function of round trip time delay history. The measure is dynamic because even if the latency is apparently a large number the particular node may be part of a best path through a network.

The acknowledgement time delays are operated on by a recursive least squares procedure processing a sequence of previous acknowledgement round trip delays to determine predicted future delays.

The best path through the network is obtained by predicting latency through a node e.g. ESB Node A B or C for a subsequent packet. However a Routing Agent s view of the network is inherently delayed i.e. when a Routing Agent must decide on a route for a message it is doing so without complete knowledge of network traffic. Thus it is possible for the predicted latency through a node to deviate from the actual latency through a node. Consequently a mechanism is required to correct the differences in actual system performance and expected near term future performance. The chosen mechanism within this invention is to communicate to Routing Agents an aggregate expected delay based on the node s current backlog of messages. This aggregate is referred to as a choke packet and provides an latency estimate of the node s current messaging backlog. The latency estimate provided by the choke packet is incorporated into future latency predictions for that node. According to our invention the recursive least squares algorithm identifies past latency measurements as a function of time extrapolates from past message latencies to generate a predicted future latency for a next packet.

In the equation the x axis corresponds to the time that the measurements were taken and the y axis corresponds to the actual message latency. This latency prediction is updated for each completed message transfer with the vector 1 where tis the time the latency measure was received and bis the actual message latency. If the acknowledgement contains a choke packet then the value of bwill be equal to the delay expressed in the choke packet.

These terms update the latency prediction where updates are expressed as a combination of a two dimensional vector representing time a t1 and a scalar representing delay b m. We let p aa therefore x pab. Thus the equation for updating pgiven a a vector and b a scalar is given by

To obtain a current real time prediction of delay through a node that is compared to a threshold to determine if subsequent packets should be rerouted to avoid congestion .

If in response to a request for processing the Routing Agent receives a choke packet the agent updates the link cost estimate for that node with the delay expressed in the choke packet and finds the new minimum link cost estimate node. If no other node can process the request sooner given the delay expressed in the choke packet then the message is routed through the original node after the delay period expressed in the choke packet.

The deployment of a MOM provider as a global enterprise service or even as a smaller more localized service precludes the use of a centralized routing scheme for messages sent through the service. Therefore a virtualization mechanism must be introduced that enables the MOM provider to be globally deployed as a collection of distributed nodes acting together as a single service. Each node in the MOM service is functionally capable of transferring all messages e.g. each node is a replica of the entire service. In this virtualized definition of a globally deployed MOM provider individual nodes are interconnected into a network that provides the capability for any message source to communicate with any message destination via any node on the network. Senders and receivers connect to MOM nodes through a Routing Agent. In this system all individual communications are assumed to be encapsulated in a single message. Also message responses may be transported on any route through the MOM infrastructure e.g. responses may be treated as independent messages from the requests that caused their transmission.

The performance of MOM for each request is measured according to message latency. Message latency is defined as the time difference between the time of the initial transmission of a message from the input of a Routing Agent to the time that an acknowledgement of the message is received at the Routing Agent. Each Routing Agent is capable of sending messages through any one of the N nodes in the network in this system it is assumed that all nodes are capable of reaching all destinations.

Nodes are assumed to be geographically dispersed around the network or the internet of connected networks to meet the needs of a global community. Consequently the communication time from any source to any destination will potentially differ depending on the node that is selected to process the message. Further individual nodes in the network are heterogeneous e.g. each node may have a different throughput capacity for routing messages.

In this system a hierarchy of functionality is established to differentiate between the components that comprise the system. Senders and receivers communicate with MOM nodes through Routing Agents that insulate them from explicit knowledge of the nodes on the network. Routing Agents provide the platform for deploying the distributed scheduling heuristic described herein.

Sources and destinations are assumed to be capable of connecting to any node in the system. Each node is capable of connecting to all sources and destinations either directly or through communication with other nodes. Sources and destinations on the network only have knowledge of their assigned Routing Agents. In this system receivers of messages register a need for the receipt of a specific type of message with the message Routing Agents possibly by describing the message contents. When a Routing Agent receives a message from a source the agent must transfer that message to a node for processing. The node checks all recipients that have registered a need for that type of message and ensures that the message is transferred to the recipient.

In one embodiment each message transfer requires an acknowledgement. If a node sends a message to multiple destinations then the sender will receive a successful response only after a successful response has been received from all destinations of the message. Erroneous message transfers are handled on a per client basis. A Routing Agent attempting to send a message on behalf of a source may send the message through any one of the N nodes in the network. To select a specific node to send a message requires that the Routing Agent have a means for differentiating between the levels of service that can be offered by the available nodes in the system.

Each Routing Agent is assumed to greedily transfer its messages at the highest rate achievable on the network e.g. the optimization goal of the network is to minimize message latency for each message. Message latency is the difference between the arrival time of the sender s message at some message router in the network and the arrival time of a response to that message at the original sender. This measurement has some inherent noise that is unrelated to the performance of a particular node. That is if some message destination is overloaded and delays the response then the message latency may be artificially high though no other message router could have improved the message latency.

In an actual network multiple Routing Agents transfer messages through multiple nodes simultaneously. Therefore it is not possible for a Routing Agent to know the exact load on any given node at any point in time. The Routing Agent must instead rely on the latest information received from a node to estimate the cost of transmitting its message through that node. A subset of the most recent message latency measurements is retained at the Routing Agent for each node that the agent transmits messages through. This set of measurements is then used to estimate near term future message latency provided by that node.

In the network described herein the estimated message latency for a message sent through a node serves as an estimate of a link cost associated with transferring a message through that node. As described herein we develop a link cost estimation approach based on the application of recursive least squares fitting where past link costs are used to generate near term future link cost estimates.

To accommodate the impact on noisy measurements of message latency an additional mechanism is introduced to enable nodes to inform senders that load has unexpectedly increased. To inform the senders of the increased load Routing Agents will send a choke packet to the Routing Agent informing the agent to hold all transmissions through that node for some amount of time specified in the choke packet. This allows the load on the node to fall. The use of choke packets results in an effective decrease in the link cost of all links that pass through the node that sent the choke packet.

Estimating future link costs from any single node to a given destination in the MOM is required for the distributed message routing technique of our invention. Link costs in this model are equated with message latency as defined earlier. In one embodiment each transmitted message is acknowledged by the recipient. Therefore message latency can be viewed as the time between a message transmission from a Routing Agent to the time at which an acknowledgment to the message is received. If a message has multiple destinations then it is assumed that the acknowledgement will not be sent until all destinations have received the message. In this model the Routing Agent has no knowledge of the destinations for the message and therefore must rely on the node that transferred the message to collect the individual acknowledgements and provide a single response to the Routing Agent.

Each Routing Agent maintains a recent history for measured message latencies through each node that it communicates. In a preferred embodiment of the invention the size of the history that is used for the initial latency estimate for each node is tailored to a level appropriate for the deployed system.

For a specific instance of the model I is defined to be the count of past message latency measurements to be used in the initial link cost estimation procedure. Define A 0 included in Rthat consists of 1 rows with two elements per row. Given initial measurement times t . . . tand initial message latency measurements m . . . m the matrix Acan be expressed as

Define a second matrix b included in Rconsisting of the first I message latency measurements for communication through a specific node e.g. b mm. . . m . The employment of a recursive least squares approach means that each new measurement of message latency for this node will update the model by using the new data to correct the model. Updates to the model are expressed as the combination of a two dimensional vector a t1 and a scalar b m. To simplify the expression of the general model let P AA therefore x PAb. The equation for updating Pgiven aa vector and b k 1 a scalar is

The two dimensional vector xcorresponds to the slope and y intercept of a line through the last l message latency measurements where the x axis corresponds to the time that the measurements were taken and the y axis corresponds to message latency.

In a distributed scheduling environment there is the potential to experience race conditions where Routing Agents race to take advantage of under utilized nodes. This occurs if communication from Routing Agents to nodes in the virtualized ESB is one directional e.g. nodes have no mechanism to throttle Routing Agent communications. According to the invention described herein nodes in the virtualized ESB are able to send choke packets to Routing Agents to control the rate of messages that are sent through the node. A choke packet is a special command message that originates in a node and is communicated to a Routing Agent. The choke message tells the Routing Agent that the node cannot currently accept traffic the message also includes the time in the future when the node will be accepting traffic again.

The choke packet in effect reduces the processing time of a message on a node to the time required to send an acknowledgement of the message. The choke packet serves as a kind of negative acknowledgement to the Routing Agent that originated the message. Initial selection of the delay time included in the choke packet communication to the Routing Agent could be a constant amount of time. An extension would include the use of a variable delay time where the included delay time is derived from an estimate of the time required to process the messages pending processing on the node.

The Routing Agent upon receiving the choke packet must then incorporate this information into its link cost estimate calculation. This requires updating the message router s internal model of the link cost estimate for this node with the vector 1 where tis the current time and bis equal to the delay expressed in the choke packet. This in effect increases the link cost estimate of this node tremendously due to the delay.

At a high level the distributed message routing protocol followed by the message routers in the system can be described at one level thusly. Upon receiving a message to route the message router selects the node with the smallest link cost estimate at the current time.

Let the vector xbe the link cost estimation parameters of the i th node in the virtualized ESB. Then given the current time the Routing Agent calculates the link cost estimate for each node in the virtualized ESB that it is authorized to use. The message router selects the node with the minimum link cost estimate to process the outgoing message. If in response to the request for processing the Routing Agent receives a choke packet it updates the link cost estimate for that node with the delay expressed in the choke packet and finds the new minimum link cost estimate node. In the unlikely event that the minimum link cost node remains unchanged after updating the link cost estimate with the delay information from the choke packet then the message router will wait for the delay time to pass before repeating the procedure.

The link cost estimates are updated independently of the message routing procedure. Every latency update causes an update to the link cost estimate for the node as soon as it is processed. Therefore link cost estimates for any given node may change during execution of the message routing procedure e.g. link cost estimates should not be assumed static during the message routing procedure.

In the event that the message router does not receive an acknowledgement from the node chosen to process the outgoing message in a reasonable time frame then the Routing Agent infers that the node is no longer available to process messages. The Routing Agent treats the lack of response from the node as a negative acknowledgement recalculating the link cost estimate of the chosen node with a large incurred delay. The current model chooses a reasonable time for response as three times the standard deviation from the mean as calculated from the I most recent observations of message latency from the chosen node.

While the method of the invention has been illustrated and described with respect to a single node and the passage of a message to and through a single node it is to be understood that the method of the invention may be used for a network or sequence of nodes in series.

The capabilities of the present invention can be implemented in hardware. Additionally the invention or various implementations of it may be implemented in software. When implemented in software at least one program storage device readable by a machine tangibly embodying at least one program of instructions executable by the machine to perform the capabilities of the present invention can be provided by the program code.

The invention may be implemented for example by having the system and method for routing data packet traffic in packet networks and expediting the flow of the data packets and reducing the effects of congestion at nodes by least squares prediction of congestion and delay may be carried out in a dedicated processor or set of processors or in a dedicated processor or dedicated processors with dedicated code. The code executes a sequence of machine readable instructions which can also be referred to as code. These instructions may reside in various types of signal bearing media. In this respect one aspect of the present invention concerns a program product comprising a signal bearing medium or signal bearing media tangibly embodying a program of machine readable instructions executable by a digital processing apparatus to perform a method for having the system and method for routing data packet traffic in packet networks and expediting the flow of the data packets and reducing the effects of congestion at nodes by least squares prediction of congestion and delay.

This signal bearing medium may comprise for example memory in a server. The memory in the server may be non volatile storage a data disc or even memory on a vendor server for downloading to a processor for installation. Alternatively the instructions may be embodied in a signal bearing medium such as the optical data storage disc. Alternatively the instructions may be stored on any of a variety of machine readable data storage mediums or media which may include for example a hard drive a RAID array a RAMAC a magnetic data storage diskette such as a floppy disk magnetic tape digital optical tape RAM ROM EPROM EEPROM flash memory magneto optical storage paper punch cards or any other suitable signal bearing media including transmission media such as digital and or analog communications links which may be electrical optical and or wireless. As an example the machine readable instructions may comprise software object code compiled from a language such as C Java Pascal ADA assembler and the like.

Additionally the program code may for example be compressed encrypted or both and may include executable code script code and wizards for installation as in Zip code and cab code. As used herein the term machine readable instructions or code residing in or on signal bearing media include all of the above means of delivery.

While the preferred embodiment to the invention has been described it will be understood that those skilled in the art both now and in the future may make various improvements and enhancements which fall within the scope of the claims which follow. These claims should be construed to maintain the proper protection for the invention first described.


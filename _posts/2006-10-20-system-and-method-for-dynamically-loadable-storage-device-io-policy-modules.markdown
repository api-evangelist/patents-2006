---

title: System and method for dynamically loadable storage device I/O policy modules
abstract: Systems, methods, apparatus and software can implement a multipathing driver using dynamically loadable device policy modules that provide device specific functionality for providing at least one of input/output (I/O) operation scheduling, path selection, and I/O operation error analysis. Because the device policy modules include device specific functionality, various different devices from different manufacturers can be more efficiently and robustly supported.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07694063&OS=07694063&RS=07694063
owner: Symantec Operating Corporation
number: 07694063
owner_city: Mountain View
owner_country: US
publication_date: 20061020
---
This application is a division of U.S. patent application Ser. No. 10 717 037 entitled System And Method For Dynamically Loadable Storage Device I O Policy Modules filed Nov. 19 2003 now U.S. Pat. No. 7 127 545 and naming Siddhartha Nandi Abhay Kumar Singh and Oleg Kiselev as the inventors. The above referenced application is hereby incorporated herein by reference in its entirety.

The present invention relates to storage devices in distributed computer systems and more particularly to coordinating the use of storage devices with multiple paths.

Distributed computing systems are an increasingly important part of research governmental and enterprise computing systems. Among the advantages of such computing systems are their ability to handle a variety of different computing scenarios including large computational problems high volume data processing situations and high availability situations. Such distributed computing systems typically utilize one or more storage devices in support of the computing systems operations. These storage devices can be quite numerous and or heterogeneous. In an effort to aggregate such storage devices and to make such storage devices more manageable and flexible storage virtizalization techniques are often used. Storage virtualization techniques establish relationships between physical storage devices e.g. disk drives tape drives optical drives etc. and virtual or logical storage devices such as volumes virtual disks and virtual logical units sometimes referred to as virtual LUNs . In so doing virtualization techniques provide system wide features e.g. naming sizing and management better suited to the entire computing system than those features dictated by the physical characteristics of storage devices. Additionally virtualization techniques enable and or enhance certain computing system operations such as clustering and data backup and restore.

Other elements of computing system include storage area network SAN and storage devices such as tape library typically including one or more tape drives a group of disk drives i.e. just a bunch of disks or JBOD and intelligent storage array . As shown in both hosts and are coupled to SAN . SAN is conventionally a high speed network that allows the establishment of direct connections between storage devices and and hosts and . SAN can also include one or more SAN specific devices such as SAN switches SAN routers SAN hubs or some type of storage appliance. Thus SAN is shared between the hosts and allows for the sharing of storage devices between the hosts to provide greater availability and reliability of storage. Although hosts and are shown connected to storage devices and through SAN this need not be the case. Shared resources can be directly connected to some or all of the hosts in the computing system and computing system need not include a SAN. Alternatively hosts and can be connected to multiple SANs.

The DMP functionality enables greater reliability and performance by using path failover and load balancing. In general the multipathing policy used by DMP drivers and depends on the characteristics of the disk array in use. Active active disk arrays A A arrays permit several paths to be used concurrently for I O operations. Such arrays enable DMP to provide greater I O throughput by balancing the I O load uniformly across the multiple paths to the disk devices. In the event of a loss of one connection to an array the DMP driver automatically routes I O operations over the other available connections to the array. Active passive arrays in so called auto trespass mode A P arrays allow I O operations on a primary active path while a secondary passive path is used if the primary path fails. Failover occurs when I O is received or sent on the secondary path. Active passive arrays in explicit failover mode A PF arrays typically require a special command to be issued to the array for failover to occur. Active passive arrays with LUN group failover A PG arrays treat a group of LUNs that are connected through a controller as a single failover entity. Failover occurs at the controller level and not at the LUN level as would typically be the case for an A P array in auto trespass mode . The primary and secondary controller are each connected to a separate group of LUNs. If a single LUN in the primary controller s LUN group fails all LUNs in that group fail over to the secondary controller s passive LUN group.

Implementation of the above described multipathing policies depends in large part on the specific hardware present in the storage devices in use. Where there is sufficient commonality among the storage devices to be supported or where only a single type of storage device is to be supported implementation of theses multipathing policies is relatively straight forward. However in heterogeneous environments where storage devices of different types different models and or from different manufactures are present and or anticipated multipathing support will typically need to rely on implementations specific to the various different devices.

Accordingly it is desirable to have efficient and convenient mechanisms for providing multipathing functionality that is specific to particular storage devices without having to provide completely separate multipathing drivers for each supported storage device.

It has been discovered that systems methods apparatus and software can implement a multipathing driver using dynamically loadable device policy modules that provide device specific functionality for providing at least one of input output I O operation scheduling path selection and I O operation error analysis. Because the device policy modules include device specific functionality various different devices from different manufacturers can be more efficiently and robustly supported.

Accordingly one aspect of the present invention provides a method. A request to load a device policy module into a memory is received. The device policy module is for use by a device driver and the device policy module includes at least one of a function a procedure and an object oriented method operable to perform at least one of input output I O operation scheduling path selection and I O operation error analysis. The device policy module is loaded into the memory. The device driver is informed of the availability of the device policy module.

In another aspect of the present invention a system includes a storage device discovery module and a multipath driver. The storage device discovery module is configured to determine information about at least one storage device belonging to a distributed computing system. The multipath driver is in communication with the storage device discovery module and configured to direct input output I O operations along at least one of a plurality of communication pathways to the at least one storage device. The multipath driver includes an interface configured to communicate with a device policy module including at least one of a function a procedure and an object oriented method operable to perform at least one of I O operation scheduling path selection and I O operation error analysis.

In another aspect of the present invention a computer readable medium comprising program includes instructions executable on a processor. The computer readable medium is at least one of an electronic storage medium a magnetic storage medium an optical storage medium and a communications medium conveying signals encoding the instructions. The program instructions are operable to implement each of receiving a request to load a device policy module into a memory wherein the device policy module is for use by a device driver and wherein the device policy module includes at least one of a function a procedure and an object oriented method operable to perform at least one of input output I O operation scheduling path selection and I O operation error analysis loading the device policy module into the memory and registering the device policy module with the device driver.

Yet another aspect of the present invention provides an apparatus including a means for directing input output I O operations along at least one of a plurality of communication pathways to at least one storage device a means for providing storage device specific I O operation scheduling and communication pathway selection in conjunction with the means for directing I O operations and a means for selectively making the means for providing storage device specific I O operation scheduling and communication pathway selection available to the means for directing I O operations.

The foregoing is a summary and thus contains by necessity simplifications generalizations and omissions of detail consequently those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting. As will also be apparent to one of skill in the art the operations disclosed herein may be implemented in a number of ways and such changes and modifications may be made without departing from this invention and its broader aspects. Other aspects inventive features and advantages of the present invention as defined solely by the claims will become apparent in the non limiting detailed description set forth below.

The following sets forth a detailed description of at least the best contemplated mode for carrying out the one or more devices and or processes described herein. The description is intended to be illustrative and should not be taken to be limiting.

Device discovery layer stores information about various storage devices in database . Moreover since hardware and or software characteristics of storage devices can differ significantly e.g. because of differences among device manufacturers the procedures used to discover device attributes can differ among devices. Consequently device discovery layer can use a set of device support libraries that provide device information specific to the storage devices of particular vendors. In one implementation each device support library in the set of device support libraries is a storage device specific dynamically loadable library. Thus support for a particular type of storage device can be added or removed dynamically from without stopping volume manager or rebooting host system . Moreover if a disk array contains active volumes the disk volumes can remain available during and after the addition or removal of the support.

In order to perform the device discovery function device discovery layer uses code to scan an operating system device tree in platform specific manner. In one embodiment device discovery layer discovers all storage devices available to host computer system . In yet another embodiment partial discovery of available storage devices is achieved using constraint based discovery. For example a set of predefined storage device attributes can be used to constrain the discovery process to a subset of the storage devices identifies by the host operating system.

Device discovery layer gathers attributes of the storage devices connected to a host and configures DMP driver for a particular storage device such as storage device so that volume manager can access and use the device. Configuration of DMP driver enables multipathing features as described above within corresponding storage device .

In one embodiment device discovery layer uses SCSI commands to discover attributes of different disk arrays. Use of these commands can be hard coded into device discovery layer or it can be dictated in whole or in part by information from one or more of the device support libraries . The device support libraries typically include one or more functions procedures and or object oriented methods for use in determining storage device attributes. Examples of the types of storage device attributes discovered by device discovery layer include vendor identification product identification enclosure serial number LUN serial number disk array type e.g. A A A P A PG A PF and LUN ownership. Still other storage device attributes will be well known to those having ordinary skill in the art. In still another example dynamic attributes e.g. storage device attributes that can change between two invocations of a device discovery operation are supported. In such a situation a device support library may declare to the device discovery layer that one or more attributes are dynamic. When one of these dynamic attribute is retrieved a function within the support library can be invoked to get the latest value of the attribute.

Some or all of the storage device attributes discovered by device discovery layer are typically stored in database . In one embodiment database is maintained as a database of name value pairs called property list. The name is the property name and the value is one of the values of the associated property. This scheme allows a new property to be created with an associated value and further allows expanding or shrinking the set of values of a particular property. Another component of database can be an association list. This list maintains the association between a tuple with another tuple. The association list is typically used to maintain the property values associated with various device discovery layer objects. Access to and manipulation of database is typically handled through an application program interface API not shown that includes a number of functions procedures and or object oriented methods designed for accessing modifying and manipulating data stored in database such as the aforementioned property and association lists.

Device discovery module is responsible for passing storage device information to DMP driver . For example device discovery layer can transmit a stream of opcodes and operands to DMP driver . DMP driver and in particular core functionality interprets these instructions and performs a variety of operations based on the instructions such as configuring or reconfiguring its device database . In addition to core functionality and device database DMP Driver includes one or more default I O policies and can include one or more device policy modules . Device discovery layer provides I O policy configuration information to DMP driver only for those storage device types or models which are applicable to the devices discovered by device discovery layer . For example a support library for a particular storage device may specify that the corresponding storage device can or should only use a particular array model e.g. A A A P A PG A PF . In response core functionality will ensure that the proper I O policy is supported by DMP driver . This operation might involve loading certain default I O policy modules or routines enabling certain default I O policy modules or routines and or de selecting certain incompatible default I O policy modules or routines. In one example default I O policies includes all default I O policies supported by the DMP driver and those policies that are to be used for a particular storage device are noted by core functionality . In another example various default I O policies are available to DMP driver but only those needed by storage devices in use are loaded into or activated by DMP driver .

DMP driver uses the storage device attributes received from device discovery layer along default I O policies to perform I O operation scheduling path failover and other I O operations e.g. SCSI reservation in the multipath environment of computing system . However because it is not practical for the core functionality of DMP driver to provide an unlimited number of different I O policies for every possible storage device for use in computing system . DMP supports the use of dynamically loadable device policy modules DPMs to modify augment or replace the fixed set of procedures in storage device specific manner. Thus DPMs such as DPM typically include one or more functions procedures or object oriented methods for performing various I O operations. This functionality is typically designed to be device specific i.e. the some or all of the functionality of the DPM takes advantage of specific features or attributes of a particular manufacturer s or provider s storage device. Examples of the I O policy functionality present in DPM include path select procedure failover procedure and load balance procedure .

Because each DPM is expected to implement device specific features it may be the case that DPMs will be developed by the corresponding storage device vendors. To that end an application programming interface or some other set of functions and or procedures can be provided to assist the development of such modules. Moreover DMP Driver typically includes a set of functions and or procedures to integrate DPMs with the DMP driver and provide for their management. As shown in this functionality is part of core functionality module management . In this example core functionality module management provides an interface to one or more DPMs loaded in DMP Driver . Just as the case with storage device attributes information about various DPMs can also be stored in database . DPMs can come from respective device support libraries or loaded from some other repository. DPMs can be loaded into DMP driver as part of the normal configuration process e.g. a device is discovered its attributes determined and information is provided to the DMP driver or DPMs can be loaded upon specific request by an application such as application .

When making a request to load a DPM the requesting entity e.g. an application or other software program will typically pass a variety of parameters to core functionality . Those parameters can include filename module name module version number number of device types supported by the module name value pairs describing storage device attributes flags and the like. In addition to loading and any DPMs and performing any needed configuration steps core functionality will typically keep track of loaded modules so that multiple load or unload requests will only cause a single or none load unload of the module. Moreover because of the dynamic nature of the implementation unload requests will typically not succeed unless or until a DPM is no longer in use. To further manage operation of the DPM modules can be designed to register and unregister with the DMP driver by for example calling particular functions or procedures passing parameters or setting flag values. In typical implementations some mechanism is used to make the DMP driver aware of the availability or unavailability of DPM modules and registration and unregistration processes are merely examples. Thus in some embodiments the DMP driver may discover the presence or absence of DPM modules or a third party may inform the DMP driver of DPM module availability unavailability.

In one embodiment the process of DPM registration involves a call made by the registering DPM to a registration function along with the passing one or more parameters associated with the module s operation. Examples of such parameters include a name of the storage device type exported by the module an I O policy name version information instructions to add remove reconfigure a device controlled by the module current I O path information failover initiation commands failover path information instructions to issue SCSI commands and the like. In a similar manner a DPM can unregister from DMP driver by calling an unregister function. In one example the unregister call passes the same information as the original register call so that DMP management functionality e.g. core functionality can accurately identify the correct module and adequately complete the unregistration process. In both examples the result of the registration unregistration call can be a message or code indicating either success or failure of the operation.

In a typical implementation some or all of the components of DMP driver operate in a kernel portion of the host computer system s memory. In general the software components shown in are divided into those components operating at the kernel level and those operating at the user level as is well known in the art. Kernel memory space is generally reserved for the computer operating system kernel and associated programs. Programs residing in kernel memory space typically have unrestricted privileges including the ability to write and overwrite in user memory space. By contrast programs residing in user space typically have limited privileges. Thus depending on the implementation of DMP driver . DPM can be a kernel module or a user space module. However because of the nature of driver software e.g. the need to interface with low level portions of the operating system the need to protect the driver from other programs the handling of I O operations etc. DPMs are typically implemented as kernel module.

Device policy modules such as DPM allow some or all of the built in device or device type specific I O procedures to be substituted by those in the module. As previously noted default support usually includes support for supports four array models A A A P A PG and A PF . An array model embodies the procedures and functions used to perform a variety of multipathing I O operations such as selecting an I O path when multiple paths to a disk within an array are available selecting the disk s which will be affected due to failure of one of the paths to the disk selecting an alternate path in case of a path failure effecting path changeover after a path failure responding to SCSI reservation release requests persistent or otherwise and implementing I O operation load balancing schemes. Thus while DMP driver has default procedures for all the four array models via policies or other hard coded support the DPM facility can be used to add a new array model to the DMP driver or to create a variant within a particular model wherein the base model supplies some of the procedures which are specific to the variant. In one embodiment a data structure maintained by DMP driver contains pointers to the array specific functions and procedures present in the one or more DPMs. Each DPM typically provides such information as part of its registration process.

As noted above each DPM can implement one or more load balancing schemes which may be device specific. In a simple example for A A arrays the load balancing scheme comprises breaking up the storage in the array into contiguous regions whose number depend on a tunable value set by or through the DMP driver. Each region is then assigned to one of the active paths available in a round robin fashion. Any I O operation targeting a particular disk region always goes through the assigned path. This has the advantage of round robin scheduling without loosing the sequential I O performance benefits due to track caches that are part of the array itself or the individual disks making up the array. This is technique can be referred to as a balanced path algorithm.

Since the size of a track cache can be disk array specific this balanced path algorithm may not be optimal in a heterogeneous environment. A track cache property can be optionally discovered by device discovery layer typically through a support library which will be downloaded to DMP driver to take the place of the DMP tunable parameter of the balanced path algorithm. Such an attribute may be specified at the disk array level or individual LUN level. In the absence of such an attribute DMP driver can continue to use the aforementioned tunable parameter.

Additionally low end disk arrays or JBODs may not have significant amounts of track cache. In such cases a minimum I O queue algorithm can be used. Here DMP driver typically through a DPM specific to the array or JBOD schedules I O operations through a path which has the least number of I Os pending from the DMP driver. Such arrays can also use round robin algorithms where the DMP driver or DPM module schedules I O operations in a round robin on average fashion. In a SAN environment the balanced path scheduling algorithm may not be optimal because the number of hops may be different for different paths. If DMP driver possesses information about which path will cause the least delay it can select a path based on this information. To determine the delay DMP driver can use the time taken to complete an I O operation to compute an expected throughput for a particular path. Based on that information path selection occurs such that the larger I O operations are scheduled through the expected greater throughput path in a proportionate manner.

DMP driver can also gather I O statistics and periodically calculate a delay per unit transfer e.g. mean time to transfer a unit load for all paths. Priorities can then be assigned to the paths in proportion to the measured delay. This approach is called an adaptive priority algorithm. The adaptive priority algorithm can also handle track cache values if specified.

As noted above DPM can also be used to perform path failover operations. In particular DPM can implement device specific failover procedures. For example some disk arrays use special SCSI commands to change the active path i.e. the path that is used for performing I O operations. When DMP driver determines that the current active path is no longer functioning e.g. I O operations through the current active path are failing DMP driver can attempt to use another path as the active path. To perform this failover operation an array specific procedure of the DPM can be invoked to issue the SCSI command s needed to affect the path change operation.

In any of the aforementioned path selection examples e.g. failover path selection load balancing path selection etc. information about the appropriate path to use can be provided by DPM to DMP driver in a number of ways. In the simplest example DPM passes one or more parameters to DMP driver e.g. to core functionality and the parameters indicate which path s should be used. In another example DPM supplies one or more functions or procedures that can be called by DMP . Such functions or procedures can result in path selection and or configuration. In general because DPMs possess information about or are typically designed for specific storage devices they can efficiently provide one or more mechanisms by which a DMP can select appropriate paths for particular tasks.

In still another example DPM can be used to perform disk array specific error analysis procedures. For example when an I O error occurs DPM of DMP driver may use a disk array specific procedure to determine if the path upon which errors are occurring is a read only path. With such information DMP driver may decide to use the path for read only operations instead of causing a path failover.

Other system components illustrated in function in a manner similar to corresponding components shown in . For example host bus adapters and provide a hardware interface between the host bus of host computer system and SAN . Although the dynamically loadable device policy modules and related DMP driver features have been described in the context of a standard host computer system it should be noted that these features and functionality can be implemented in a variety of other architectures such as clustered computing systems and specialized storage devices e.g. SAN switches SAN routers SAN hubs or some type of storage appliance . Moreover the present systems methods devices and software can be implemented in conjunction with a variety of different virtualization schemes e.g. host based appliance based storage based in band out of band etc. and indeed with no virtualization scheme at all. Similarly a variety of different storage devices and indeed addressable storage objects generally can be used in conjunction with the methods devices and software disclosed.

Operation of systems and methods for providing device specific multipath I O policies are illustrated in . In particular a process for loading a DPM is shown at . Once the system is initiated operation begins with where the device discovery layer identifies a specific target storage device that is available for use by the computer system or appliance implementing the discovery functionality. Based on information about the identified target device the device discover layer further determines whether a support library exists for the target device . If such a corresponding support library does exist as determined in operation transitions to step where device attributes corresponding to the target device are obtained either from the support library itself or using the support library e.g. calling functions or procedures provided by the library or executing other code made available through the library. Among the information that can be determined about the target device is whether there is a device policy module available for the target device. This determination is made at . Availability of a DPM can be indicated by one or more attribute values the presence of the DPM itself in the support library or some similar mechanism. If a corresponding DPM does exist operation transitions to where the DMP driver is instructed to load the DPM. The DPM is loaded typically into a kernel portion of memory in step and the DPM registers with the DMP driver at .

Upon completion of the DPM loading and registering or in the case that there is no DPM available as determined in other device attributes are downloaded to the DMP driver . As noted above various different types of device attributes can be used to configure and manage the DMP driver for use with different storage devices. Moreover although attribute download step is shown as occurring after the DPM loading steps this need not be the case. DPM related stapes can be part of the typical attribute downloading process or can be performed after all device attributes have been sent to the DMP driver. In the case of a reconfiguration of a device or an explicit request to load a DPM e.g. by an application outside of the normal discovery process or activated at a time other than initial configuration of the computing system attribute downloading might not occur. Upon completion of the attribute downloading the process terminates at .

In the event that there is no support library for the target device as determined at operation transitions to where it is determined whether the target device has any features for which DMP driver configuration is needed or desired. If so default attributes are determined and subsequently downloaded . If not the process terminates at . Note that in the event that multiple devices need be configured the entire process illustrated in can be repeated for each device.

Once it is safe to remove the module the DMP driver instructs the DPM to unregister . In step the DPM unregisters itself from the DMP driver. In step the DMP driver unloads the module. Assuming the unloading process succeeds operation terminates normally at . In the event that it does not succeed and error condition can also be reported.

If the same device types are supported the operation transitions to where the DMP driver instructs the DPM to unregister . In step the DPM unregisters itself from the DMP driver. In step the DMP driver replaces the old module with the new module or in the case where the update simply requires a change in module configuration the change is performed. In the updated module registers with the DMP driver. Assuming the registration process succeeds operation terminates normally at . In the event that it does not succeed and error condition can also be reported.

The flow charts of illustrate some of the many operational examples of the multipathing techniques disclosed in the present application. Those having ordinary skill in the art will readily recognize that certain steps or operations illustrated in can be eliminated or taken in an alternate order. Moreover the methods described in and many of the modules illustrated in are typically implemented as one or more software programs for a computer system and are encoded in a computer readable medium as instructions executable on one or more processors. The computer readable medium can be any one of an electronic storage medium a magnetic storage medium an optical storage medium and a communications medium conveying signals encoding the instructions. Separate instances of these programs can be executed on separate computer systems in keeping with the multi process methods described above. Thus although certain steps have been described as being performed by certain devices software programs processes or entities this need not be the case and a variety of alternative implementations will be understood by those having ordinary skill in the art.

Additionally those having ordinary skill in the art will readily recognize that the techniques described above can be utilized in a variety of different storage devices and computing systems with variations in for example the number of nodes the type of operation of the computing system e.g. cluster operation failover parallel etc. the number and type of shared data resources and the number of paths between nodes and shared data resources.

Those having ordinary skill in the art will readily recognize that the techniques and methods discussed below can be implemented in software using a variety of computer languages including for example traditional computer languages such as assembly language Pascal and C object oriented languages such as C C and Java and scripting languages such as Perl and Tcl Tk. Additionally software and can be provided to the computer system via a variety of computer readable media including electronic media e.g. flash memory magnetic storage media e.g. hard disk a floppy disk etc. optical storage media e.g. CD ROM and communications media conveying signals encoding the instructions e.g. via a network coupled to network interface .

Computer system also includes devices such as keyboard mouse SCSI interface network interface graphics display hard disk and CD ROM all of which are coupled to processor by communications bus . It will be apparent to those having ordinary skill in the art that computer system can also include numerous elements not shown in the figure such as additional storage devices communications devices input devices and output devices as illustrated by the ellipsis shown. An example of such an additional computer system device is a fibre channel interface.

Although the present invention has been in some cases described is in terms of providing support for multipath disk arrays the present invention can also be used to support disk arrays having only a single path. Multipath disk arrays are used to illustrate the usefulness of the invention although one of skill in the art will recognize that the invention is not limited to support for multipath disk arrays. In contrast the present invention can be used in conjunction with a variety of different types of storage devices including discrete disks solid state storage devices including flash memory storage appliances and other storage devices.

Loadable module techniques as described herein can also be applied to other remote device access technologies. For example standard protocols e.g. the SCSI protocol can be extended to provide some common functionality in different ways or manufacturers may disagree on some interpretation aspects of a standard and provide different behaviors in their devices. Consequently the loadable module techniques provide extensible polymorphic uniform mechanisms for accessing this non standard but common functionality or for providing specific handling for different behaviors that cannot be efficiently handled by common code. Using simple type specific loadable modules with a set of attributes and functionality handlers allows accommodation of unusual new devices without having to change established products. Moreover one can rely on either a storage device vendors of such a new device or on storage management software vendors to provide the module to handle this new device. This technique generally has broad application and can also be used for example to access extended protocol mechanisms using protocols other than the block access protocols like SCSI.

Although the present invention has been described with respect to a specific preferred embodiment thereof various changes and modifications may be suggested to one skilled in the art and it is intended that the present invention encompass such changes and modifications fall within the scope of the appended claims.


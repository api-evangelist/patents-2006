---

title: Method and system for using bundle decoders in a processing pipeline
abstract: A method and system for using bundle decoders in a processing pipeline is disclosed. In one embodiment, to perform a context switch between a first process and a second process operating in a processing pipeline, the first state information that is associated with the first process is placed on a connection separate from the processing pipeline. A number of decoders are coupled to this connection. The decoders obtain the first state information from a number of pipeline units on the processing pipeline by monitoring the data stream going into these pipeline units. Also, to restore the first state information after having switched out the second state information that is associated with the second process, the first state information is placed on the connection for the decoders to retrieve.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08537167&OS=08537167&RS=08537167
owner: Nvidia Corporation
number: 08537167
owner_city: Santa Clara
owner_country: US
publication_date: 20061017
---
Embodiments of the present invention relate generally to graphics applications and more specifically to a method and system for using bundle decoders in a processing pipeline.

Unless otherwise indicated herein the approaches described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.

A context switch is a feature of a multitasking operating system that allows for a switch in execution from one computing thread or process to another. This feature ensures that a processor cannot be monopolized by any one processor intensive thread or process. During a context switch the states of the processor of the currently running process are stored in memory and the processor is switched to states of another process that were previously stored in memory.

In graphics applications a number of threads may be processed concurrently through one or more graphics pipelines that are managed by a graphics processing unit GPU . is a simplified block diagram of processing pipeline that includes pipeline units and collectively referred to as pipeline units . also shows front end FE which manages the context switch operation for processing pipeline by sending information via bundles to the various pipeline units. A bundle is a data structure that contains a header which indicates the intended destination for the bundle and payload which contains information such as state information or trigger information for a pipeline unit. To illustrate suppose FE sends three versions of a bundle B0 at three different times time 1 time 2 and time 3. The version at time 1 also denoted as B0 time 1 contains state A B0 time 2 contains state B and B0 time 3 contains state C. Thus as the three versions of B0 flow down processing pipeline it is possible that at time 3 B0 time 1 has reached pipeline unit B0 time 2 has reached pipeline unit but has not reached pipeline unit and B0 time 3 has reached pipeline unit but has not reached pipeline unit . In this scenario pipeline units and have states C B A respectively. In other words as the different versions of bundle B0 flow down processing pipeline state information that is previously stored in pipeline units is rewritten with the state information stored in these different versions of B0.

According to the wait for idle WFI protocol when FE receives a context switch command FE suspends sending commands down processing pipeline and then waits for an idle status signal from each of pipeline units . A context switch occurs only after FE receives these idle status signals. During this idle period all the bundles in flight in processing pipeline are completely drained. Using the example discussed above all three versions of B are drained by reaching pipeline unit . As a result each of pipeline units has state C. To proceed with the context switch rather than retrieving and storing state C from each of pipeline units FE maintains a shadow copy of the last state that it encapsulates in a bundle and sends down processing pipeline in a memory region reserved for the context associated with the currently running process. In this example the last state is state C. Then FE switches processing pipeline to the context associated with another process after that context is retrieved from a memory region reserved for that context. Each of these reserved memory regions resides in memory and is accessed through memory interface .

As shown above the aforementioned WFI protocol does not provide FE with the flexibility to proceed with a context switch operation when there are bundles in flight in processing pipeline . Using the example above FE cannot switch the context of processing pipeline at time 3 in accordance with the WFI protocol because at time 3 pipeline units and do not yet have the same state information. In addition current implementations of processing pipeline fail to impose uniformity on the formats and processing of the bundles. Again using the example discussed above this lack of uniformity may result in FE not recognizing and therefore not utilizing B after the bundle flows down processing pipeline and is operated on by various pipeline units . Another drawback of the current approach to context switching is that using shadow copies to track the information needed for context switch operations is costly due to the additional storage space and computational overhead necessary to maintain and manage the shadow copies.

As the foregoing illustrates what is needed is a way to intelligently manage the bundles in a processing pipeline to improve the efficiency of switching the context of the processing pipeline and thereby enhancing the overall performance of the processing pipeline.

A method and system for using bundle decoders in a processing pipeline is disclosed. In one embodiment to perform a context switch between a first process and a second process operating in a processing pipeline the first state information that is associated with the first process is placed on a connection separate from the processing pipeline. A number of decoders are coupled to this connection. The decoders obtain the first state information from a number of pipeline units on the processing pipeline by monitoring the data stream going into these pipeline units. Also to restore the first state information after having switched out the second state information that is associated with the second process the first state information is placed on the connection for the decoders to retrieve.

One advantage of the disclosed method and system is that a context switch can be performed without waiting for the idling condition of a processing pipeline thereby enhancing the flexibility and performance of the processing pipeline. Further shadow copies of the pipeline state do not have to be maintained and managed by the front end unit with the disclosed approach.

A method and system for using bundle decoders in a processing pipeline is described. In the following description for the purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the present invention. It will be apparent however to one skilled in the art that the present invention may be practiced without these specific details.

Throughout this disclosure a bundle generally refers to a data structure which a processing pipeline such as processing pipeline shown in uses to transmit information to the various units of the pipeline. This information in one instance refers to the state information in a pipeline stage and such state information is at the Application Programming Interface API level. In graphics applications some examples of the API include OpenGL and DirectX. A bundle encapsulating such API level state information is referred to as a state bundle. API level state information follows the naming conventions and initialization and default values specified by the API. In another instance the information in a bundle could trigger a pipeline unit to perform certain actions. A bundle encapsulating such trigger information is referred to as a trigger bundle. The term bundle decoder is used interchangeably with decoder in this disclosure. In addition each processing pipeline stage may include one or more processing pipeline units. To avoid unnecessarily obscuring the present invention unless otherwise indicated each stage is assumed to include one pipeline unit.

In one implementation each of the decoders in sideband path is associated with at least one of pipeline units and monitors the bundle traffic on processing pipeline . Subsequent sections will further detail the connectivity and the interactions among a decoder pipeline units and pipeline interfaces. Each of these decoders has at least two predetermined lists. One is a list of bundles for that particular decoder to decode and the other is a list of bundles that the decoder should prevent from continuing to flow down processing pipeline . This operation of stopping a bundle from flowing downstream is also referred to as a kill operation. If a bundle is neither on the list to be decoded nor on the list to be killed then the decoder does not perform any operation on that bundle. The bundle simply continues to flow down processing pipeline . In one implementation the topology of all the decoders in sideband path and the associations to pipeline units Topology is maintained. If a new decoder is added or if an existing decoder is removed here the Topology is updated to reflect the addition or removal. The aforementioned predetermined lists of bundles to decode and to kill are established at individual decoder levels. These predetermined lists together with the Topology govern the flow of the bundles in processing pipeline .

As an illustration suppose according to the Topology decoder monitors traffic going into pipeline unit and decoders and both monitor traffic going into pipeline unit . Suppose further that decoder is configured to decode state bundles A B and C decoder is configured to decode state bundles B and D and decoder is configured to decode state bundle C and kill state bundle A. As state bundle A flows down processing pipeline before reaching pipeline unit decoder identifies state bundle A from the stream of data in the pipeline and recognizes that state bundle A is on its predetermined list of bundles i.e. A B and C to decode. Decoder decodes state bundle A stores a copy of the decoded state bundle A or some portions of state bundle A locally and does not interfere with state bundle A from proceeding downstream to pipeline unit via interface . In other words decoder does not assert a signal via signal path to prevent state bundle A from proceeding downstream. Because pipeline unit has two different downstream paths in one implementation decoders and are configured to monitor state bundles heading towards pipeline unit and pipeline unit respectively. Referring back to the example since decoder does not find state bundle A on either its predetermined list of bundles to decode i.e. B and D or its predetermined list of bundles to kill decoder ignores state bundle A and does not interfere with state bundle A from proceeding downstream to pipeline unit via interface . In other words decoder does not assert a signal via signal path to prevent state from proceeding downstream. On the other hand since decoder finds state bundle A on its predetermined list of bundles to kill decoder asserts a signal to interface via signal path to stop bundle A from advancing to pipeline unit . This mechanism of stopping the downstream flow of selected bundles prevents irrelevant bundles from clogging up processing pipeline especially at the bottom portion of the pipeline.

In addition to monitoring and regulating the flows of the bundles in processing pipeline sideband path enables FE to perform context switching without waiting for the idling condition of processing pipeline . As mentioned above since the decoders in sideband path are connected to one another via dedicated connections FE can utilize these dedicated connections to switch out the stored data from the decoders which are the states associated with a currently running process during a context switch operation and restore these switched out states back to the decoders after the operation is completed. Furthermore to switch in the states associated with a different process FE can send the state bundles encapsulating these states down processing pipeline after the states associating with the currently running process have been switched out as mentioned above. Based on the predetermined lists of each decoder in sideband path and the Topology these state bundles should flow down the intended paths and reach the appropriate decoders so that the decoders can extract the state information stored in the state bundles store the decoded state information locally in the decoders and make available the stored state information to the intended pipeline units. Subsequent sections will further describe the aforementioned dedicated connections in sideband path and the mechanism of facilitating a context switch operation using sideband path .

In one embodiment on the cycle following the assertion of the bundle valid signal bundle address decoder decodes the incoming bundle and either asserts certain signals based on the decoded data or stores the decoded states in decoder storage . In particular if the incoming bundle is a trigger bundle bundle address decoder asserts a trigger signal for each trigger bundle that it can decode on the same cycle that the trigger bundle is decoded. Thus using and as an illustration bundle address decoder of decoder decodes a trigger bundle and asserts a trigger signal for pipeline unit via path on the same cycle. In addition bundle address decoder also asserts a kill signal if the incoming bundle matches one of the bundles on the predetermined list of bundles to be killed. In one implementation this asserted kill signal is logically combined with the advance signal discussed above such that the bundle is stopped from advancing further at the time it is supposed to advance. Again using and as an illustration bundle address decoder asserts the kill signal via path which is logically combined with the advance signal from pipeline unit . This combined signal goes to interface so that the bundle to be killed does not advance to pipeline unit .

If the incoming bundle is a state bundle and is on the list of bundles to be decoded by decoder bundle address decoder decodes the bundle and stores the decoded state information in decoder storage . In one implementation the entire decoded state bundle is stored. Alternatively selective fields of the decoded state bundle are stored. The decoded state information is available for use on the cycle following the assertion of the bundle valid signal. Using and as an illustration the decoded state information in decoder storage is available for pipeline unit to access via path on the cycle following the assertion of the bundle valid signal by bundle filter .

In addition to storing decoded bundles from processing pipeline decoder storage may also store state information from ramchain controller . In particular a ramchain is a dedicated connection chaining up all the decoders in sideband path and this ramchain facilitates context switch operations for processing pipeline . To illustrate the operations of the ramchain in a downstream direction suppose FE of intends to restore states M N and O to pipeline unit states P and Q to pipeline unit and states R S and T to pipeline unit . Each of decoders and has a predetermined list of states that it decodes and stores and these stored states are accessible by pipeline units and respectively. Suppose the list of states for decoder includes states M N and O the list for decoder includes states P and Q and the list for decoder includes states R S and T. Suppose further that FE sends the states down the ramchain in the sequence of M N O P Q R S and T and also a token indicating the number of states on the ramchain which in this case the number is 8. As shown in ramchain controller in decoder takes the first states stores the states in the decoder storage in the order that decoder receives them and decrements the number in the token by 3. Similarly ramchain controllers takes and stores the next 2 states and decrements the number in the token by 2 and ramchain controller takes and stores the last 3 states and decrements the number in the token by 3. When the number in the token reaches 0 all the states that FE intends to restore have been restored.

On the other hand suppose FE of intends to switch out the states stored in decoders and in a context switch operation. FE sends a token down the ramchain requesting each of the decoders to place its stored state information on the ramchain. As shown in after the token reaches ramchain controller the stored states are retrieved in a reversed order from the order of storing the states in the decoder storage. For decoder this means state O is retrieved first and state M is retrieved last. Similarly after the token reaches ramchain controller state Q is retrieved first and state P is retrieved second. Lastly when the token reaches ramchain controller state T is retrieved first state R is retrieved last. Alternatively as shown in the states stored in decoders and are retrieved after the token reaches ramchain controller . By the time ramchain controller retrieves and places the stored states of decoder on the ramchain all of the M N O P Q R S and T states are present and in a reversed sequence from the order they are stored in the decoder storage. It should be apparent to one with ordinary skill in the art to use different buffering mechanisms than the one discussed above for storing and retrieving state information using the ramchain connection without exceeding the scope of the claimed invention.

In one implementation to reduce the decoders in sideband path described to digital circuitry the Topology and the flow of bundles are first described in a configuration file using a high level programming language such as C . Each instance of the decoder and its connections to the pipeline units need to be properly declared in this configuration file. Also the bundles to be decoded by a particular decoder are defined in a separate definition file. As discussed above for uniformity the definitions of the bundles for each of the decoders will follow certain predefined naming conventions initialization values and default values. Each bundle may be further associated with member functions. Then a global build process that takes all the files mentioned above into account and generates a Verilog module for each decoder in the Topology. This Verilog module typically goes through further verifications before it is finally reduced to a semiconductor device.

The above description illustrates various embodiments of the present invention along with examples of how aspects of the present invention may be implemented. The above examples embodiments and drawings should not be deemed to be the only embodiments and are presented to illustrate the flexibility and advantages of the present invention as defined by the following claims.


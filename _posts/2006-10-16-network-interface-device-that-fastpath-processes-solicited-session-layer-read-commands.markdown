---

title: Network interface device that fast-path processes solicited session layer read commands
abstract: A network interface device connected to a host provides hardware and processing mechanisms for accelerating data transfers between the host and a network. Some data transfers are processed using a dedicated fast-path whereby the protocol stack of the host performs no network layer or transport layer processing. Other data transfers are, however, handled in a slow-path by the host protocol stack. In one embodiment, the host protocol stack has an ISCSI layer, but a response to a solicited ISCSI read request command is nevertheless processed by the network interface device in fast-path. In another embodiment, an initial portion of a response to a solicited command is handled using the dedicated fast-path and then after an error condition occurs a subsequent portion of the response is handled using the slow-path. The interface device uses a command status message to communicate status to the host.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07664883&OS=07664883&RS=07664883
owner: Alacritech, Inc.
number: 07664883
owner_city: San Jose
owner_country: US
publication_date: 20061016
---
The present application claims the benefit under 35 USC 120 of is a continuation of U.S. patent application Ser. No. 09 970 124 filed Oct. 2 2001 now U.S. Pat. No. 7 124 205 which claims the benefit under 35 USC 120 of is a continuation in part of U.S. patent application Ser. No. 09 855 979 filed May 14 2001 now U.S. Pat. No. 7 133 940 U.S. patent application Ser. No. 09 804 553 filed Mar. 12 2001 now U.S. Pat. No. 6 393 487 U.S. patent application Ser. No. 09 802 550 filed Mar. 9 2001 now U.S. Pat. No. 6 658 480 U.S. patent application Ser. No. 09 802 426 filed Mar. 9 2001 now U.S. Pat. No. 7 042 898 U.S. patent application Ser. No. 09 802 551 filed Mar. 9 2001 now U.S. Pat. No. 7 076 568 U.S. patent application Ser. No. 09 801 488 filed Mar. 7 2001 now U.S. Pat. No. 6 687 758 U.S. patent application Ser. No. 09 789 366 filed Feb. 20 2001 now U.S. Pat. No. 6 757 746 U.S. patent application Ser. No. 09 748 936 filed Dec. 26 2000 now U.S. Pat. No. 6 334 153 U.S. patent application Ser. No. 09 692 561 filed Oct. 18 2000 U.S. patent application Ser. No. 09 675 700 filed Sep. 29 2000 U.S. patent application Ser. No. 09 675 484 filed Sep. 29 2000 now U.S. Pat. No. 6 807 581 U.S. patent application Ser. No. 09 514 425 filed Feb. 28 2000 now U.S. Pat. No. 6 427 171 U.S. patent application Ser. No. 09 464 283 filed Dec. 15 1999 now U.S. Pat. No. 6 427 173 U.S. patent application Ser. No. 09 416 925 filed Oct. 13 1999 now U.S. Pat. No. 6 470 415 U.S. patent application Ser. No. 09 384 792 filed Aug. 27 1999 now U.S. Pat. No. 6 434 620 U.S. patent application Ser. No. 09 141 713 filed Aug. 28 1998 now U.S. Pat. No. 6 389 479 all of which are incorporated by reference herein. Each of the above applications with the exception of Ser. Nos. 09 675 484 09 675 700 09 801 488 and 09 416 925 claims the benefit under 35 USC 120 of is a continuation in part of U.S. patent application Ser. No. 09 067 544 filed Apr. 27 1998 now U.S. Pat. No. 6 226 680 which is incorporated by reference herein. Each of the above applications with the exception of Ser. Nos. 09 675 484 09 675 700 09 801 488 09 692 561 09 514 425 09 416 925 09 384 792 and 09 141 713 claims the benefit under 35 USC 120 of is a continuation in part of U.S. patent application Ser. No. 09 439 603 filed Nov. 12 1999 now U.S. Pat. No. 6 247 060 which is incorporated by reference herein and which claims the benefit under 35 USC 120 of is a continuation in part of U.S. patent application Ser. No. 09 067 544 filed Apr. 27 1998 now U.S. Pat. No. 6 226 680 . Each of the above applications with the exception of Ser. Nos. 09 675 484 09 675 700 09 801 488 and 09 416 925 claims the benefit under 35 USC 119 of U.S. patent application Ser. No. 60 061 809 filed Oct. 14 1997 and U.S. patent application Ser. No. 60 098 296 filed Aug. 27 1998 which are incorporated by reference herein.

Over the past decade advantages of and advances in network computing have encouraged tremendous growth of computer networks which has in turn spurred more advances growth and advantages. With this growth however dislocations and bottlenecks have occurred in utilizing conventional network devices. For example a CPU of a computer connected to a network may spend an increasing proportion of its time processing network communications leaving less time available for other work. In particular demands for moving file data between the network and a storage unit of the computer such as a disk drive have accelerated. Conventionally such data is divided into packets for transportation over the network with each packet encapsulated in layers of control information that are processed one layer at a time by the CPU of the receiving computer. Although the speed of CPUs has constantly increased this protocol processing of network messages such as file transfers can consume most of the available processing power of the fastest commercially available CPU.

This situation may be even more challenging for a network file server whose primary function is to store and retrieve files on its attached disk or tape drives by transferring file data over the network. As networks and databases have grown the volume of information stored at such servers has exploded exposing limitations of such server attached storage. In addition to the above mentioned problems of protocol processing by the host CPU limitations of parallel data channels such as conventional small computer system interface SCSI interfaces have become apparent as storage needs have increased. For example parallel SCSI interfaces restrict the number of storage devices that can be attached to a server and the distance between the storage devices and the server.

As noted in the book by Tom Clark entitled Designing Storage Area Networks copyright 1999 incorporated by reference herein one solution to the limits of server attached parallel SCSI storage devices involves attaching other file servers to an existing local area network LAN in front of the network server. This network attached storage NAS allows access to the NAS file servers from other servers and clients on the network but may not increase the storage capacity dedicated to the original network server. Conversely NAS may increase the protocol processing required by the original network server since that server may need to communicate with the various NAS file servers. In addition each of the NAS file servers may in turn be subject to the strain of protocol processing and the limitations of storage interfaces.

Storage area networking SAN provides another solution to the growing need for file transfer and storage over networks by replacing daisy chained SCSI storage devices with a network of storage devices connected behind a server. Instead of conventional network standards such as Ethernet or Fast Ethernet SANs deploy an emerging networking standard called Fibre Channel FC . Due to its relatively recent introduction however many commercially available FC devices are incompatible with each other. Also a FC network may dedicate bandwidth for communication between two points on the network such as a server and a storage unit the bandwidth being wasted when the points are not communicating.

NAS and SAN as known today can be differentiated according to the form of the data that is transferred and stored. NAS devices generally transfer data files to and from other file servers or clients whereas device level blocks of data may be transferred over a SAN. For this reason NAS devices conventionally include a file system for converting between files and blocks for storage whereas a SAN may include storage devices that do not have such a file system.

Alternatively NAS file servers can be attached to an Ethernet based network dedicated to a server as part of an Ethernet SAN. Marc Farley further states in the book Building Storage Networks copyright 2000 incorporated by reference herein that it is possible to run storage protocols over Ethernet which may avoid Fibre Channel incompatibility issues. Increasing the number of storage devices connected to a server by employing a network topology such as SAN however increases the amount of protocol processing that must be performed by that server. As mentioned above such protocol processing already strains the most advanced servers.

An example of conventional processing of a network message such as a file transfer illustrates some of the processing steps that slow network data storage. A network interface card NIC typically provides a physical connection between a host and a network or networks as well as providing media access control MAC functions that allow the host to access the network or networks. When a network message packet sent to the host arrives at the NIC MAC layer headers for that packet are processed and the packet undergoes cyclical redundancy checking CRC in the NIC. The packet is then sent across an input output I O bus such as a peripheral component interconnect PCI bus to the host and stored in host memory. The CPU then processes each of the header layers of the packet sequentially by running instructions from the protocol stack. This requires a trip across the host memory bus initially for storing the packet and then subsequent trips across the host memory bus for sequentially processing each header layer. After all the header layers for that packet have been processed the payload data from the packet is grouped in a file cache with other similarly processed payload packets of the message. The data is reassembled by the CPU according to the file system as file blocks for storage on a disk or disks. After all the packets have been processed and the message has been reassembled as file blocks in the file cache the file is sent in blocks of data that may be each built from a few payload packets back over the host memory bus and the I O bus to host storage for long term storage on a disk typically via a SCSI bus that is bridged to the I O bus.

Alternatively for storing the file on a SAN the reassembled file in the file cache is sent in blocks back over the host memory bus and the I O bus to an I O controller configured for the SAN. For the situation in which the SAN is a FC network a specialized FC controller is provided which can send the file blocks to a storage device on the SAN according to Fibre Channel Protocol FCP . For the situation in which the file is to be stored on a NAS device the file may be directed or redirected to the NAS device which processes the packets much as described above but employs the CPU protocol stack and file system of the NAS device and stores blocks of the file on a storage unit of the NAS device.

Thus a file that has been sent to a host from a network for storage on a SAN or NAS connected to the host typically requires two trips across an I O bus for each message packet of the file. In addition control information in header layers of each packet may cross the host memory bus repeatedly as it is temporarily stored processed one layer at a time and then sent back to the I O bus. Retrieving such a file from storage on a SAN in response to a request from a client also conventionally requires significant processing by the host CPU and file system.

An interface device such as an intelligent network interface card INIC for a local host is disclosed that provides hardware and processing mechanisms for accelerating data transfers between a network and a storage unit while control of the data transfers remains with the host. The interface device includes hardware circuitry for processing network packet headers and can use a dedicated fast path for data transfer between the network and the storage unit the fast path set up by the host. The host CPU and protocol stack avoid protocol processing for data transfer over the fast path releasing host bus bandwidth from many demands of the network and storage subsystem. The storage unit which may include a redundant array of independent disks RAID or other configurations of multiple drives may be connected to the interface device by a parallel channel such as SCSI or by a serial channel such as Ethernet or Fibre Channel and the interface device may be connected to the local host by an I O bus such as a PCI bus. An additional storage unit may be attached to the local host by a parallel interface such as SCSI.

A file cache is provided on the interface device for storing data that may bypass the host with organization of data in the interface device file cache controlled by a file system on the host. With this arrangement data transfers between a remote host and the storage units can be processed over the interface device fast path without the data passing between the interface device and the local host over the I O bus. Also in contrast to conventional communication protocol processing control information for fast path data does not travel repeatedly over the host memory bus to be temporarily stored and then processed one layer at a time by the host CPU. The host may thus be liberated from involvement with a vast majority of data traffic for file reads or writes on host controlled storage units.

Additional interface devices may be connected to the host via the I O bus with each additional interface device having a file cache controlled by the host file system and providing additional network connections and or being connected to additional storage units. With plural interface devices attached to a single host the host can control plural storage networks with a vast majority of the data flow to and from the host controlled networks bypassing host protocol processing travel across the I O bus travel across the host bus and storage in the host memory. In one example storage units may be connected to such an interface device by a Gigabit Ethernet network offering the speed and bandwidth of Fibre Channel without the drawbacks and benefiting from the large installed base and compatibility of Ethernet based networks.

In some embodiments a host computer whose protocol stack includes an ISCSI layer is coupled to a network interface device. A solicited ISCSI read request command is sent from the network interface device to a network storage device and the network storage device sends an ISCSI response back. The network interface device fast path processes the ISCSI response such that a data portion of the ISCSI response is placed into a destination memory on the host computer without the protocol stack of the host computer doing any network layer or transport layer processing. In some embodiments there is no slow path processing of ISCSI responses on the host. In other embodiments there is a slow path for handling ISCSI responses such that the host protocol stack does carry out network layer and transport layer processing on ISCSI responses in certain circumstances.

Embodiments are described wherein an ISCSI read request command is sent from a network interface device to an ISCSI target. The network interface device does fast path processing on an initial part of the response from the ISCSI target but then switches to slow path processing such that a subsequent part of the response from the ISCSI target is processed by the host protocol stack. In some embodiments the network interface device sends a command status message to the host computer to inform the host computer of a status condition associated with the ISCSI read request command. The status condition may be an error condition. The status portion of the command status message may include a command sent bit and or a flushed bit. The command status message may also indicate a part of a storage destination in the host that will remain to be filled after the connection is flushed back to the host for slow path processing. In this way a single network cable extending from a single port on a network interface device can simultaneously carry both ordinary network traffic as well as IP storage traffic. The same fast path hardware circuitry on the network interface device is used to accelerate both the ordinary network traffic as well as the IP storage traffic. Error conditions and exception conditions for both the ordinary network traffic as well as the IP storage traffic are handled in slow path by the host protocol stack.

Other embodiments are described. This summary does not purport to define the invention. The claims and not this summary define the invention.

An overview of a network data communication system in accordance with the present invention is shown in . A host computer is connected to an interface device such as intelligent network interface card INIC that may have one or more ports for connection to networks such as a local or wide area network or the Internet . The host contains a processor such as central processing unit CPU connected to a host memory by a host bus with an operating system not shown residing in memory for overseeing various tasks and devices including a file system . Also stored in host memory is a protocol stack of instructions for processing of network communications and a INIC driver that communicates between the INIC and the protocol stack . A cache manager runs under the control of the file system and an optional memory manager such as the virtual memory manager of Windows NT or 2000 to store and retrieve file portions termed file streams on a host file cache .

The host is connected to the INIC by an I O bus such as a PCI bus which is coupled to the host bus by a host I O bridge . The INIC includes an interface processor and memory that are interconnected by an INIC bus . INIC bus is coupled to the I O bus with an INIC I O bridge . Also connected to INIC bus is a set of hardware sequencers that provide upper layer processing of network messages. Physical connection to the LAN WAN and the Internet is provided by conventional physical layer hardware PHY . Each of the PHY units is connected to a corresponding unit of media access control MAC the MAC units each providing a conventional data link layer connection between the INIC and one of the networks.

A host storage unit such as a disk drive or collection of disk drives and corresponding controller may be coupled to the I O bus by a conventional I O controller such as a SCSI adapter. A parallel data channel connects controller to host storage unit . Alternatively host storage unit may be a redundant array of independent disks RAID and I O controller may be a RAID controller. An I O driver e.g. a SCSI driver module operating under command of the file system interacts with controller to read or write data on host storage unit . Host storage unit preferably contains the operating system code for the host including the file system which may be cached in host memory .

An INIC storage unit such as a disk drive or collection of disk drives and corresponding controller is coupled to the INIC bus via a matching interface controller INIC I O controller which in turn is connected by a parallel data channel to the INIC storage unit. INIC I O controller may be a SCSI controller which is connected to INIC storage unit by a parallel data channel . Alternatively INIC storage unit may be a RAID system and I O controller may be a RAID controller with multiple or branching data channels . Similarly I O controller may be a SCSI controller that is connected to a RAID controller for the INIC storage unit . In another implementation INIC storage unit is attached to a Fibre Channel FC network and I O controller is a FC controller. Although INIC I O controller is shown connected to INIC bus I O controller may instead be connected to I O bus . INIC storage unit may optionally contain the boot disk for the host from which the operating system kernel is loaded. INIC memory includes frame buffers for temporary storage of packets received from or transmitted to a network such as LAN WAN . INIC memory also includes an interface file cache INIC file cache for temporary storage of data stored on or retrieved from INIC storage unit . Although INIC memory is depicted in as a single block for clarity memory may be formed of separate units disposed in various locations in the INIC and may be composed of dynamic random access memory DRAM static random access memory SRAM read only memory ROM and other forms of memory.

The file system is a high level software entity that contains general knowledge of the organization of information on storage units and and file caches and and provides algorithms that implement the properties and performance of the storage architecture. The file system logically organizes information stored on the storage units and and respective file caches and as a hierarchical structure of files although such a logical file may be physically located in disparate blocks on different disks of a storage unit or . The file system also manages the storage and retrieval of file data on storage units and and file caches and . I O driver software operating on the host under the file system interacts with controllers and for respective storage units and to manipulate blocks of data i.e. read the blocks from or write the blocks to those storage units. Host file cache and INIC file cache provide storage space for data that is being read from or written to the storage units and with the data mapped by the file system between the physical block format of the storage units and and the logical file format used for applications. Linear streams of bytes associated with a file and stored in host file cache and INIC file cache are termed file streams. Host file cache and INIC file cache each contain an index that lists the file streams held in that respective cache.

The file system includes metadata that may be used to determine addresses of file blocks on the storage units and with pointers to addresses of file blocks that have been recently accessed cached in a metadata cache. When access to a file block is requested for example by a remote host on LAN WAN the host file cache and INIC file cache indexes are initially referenced to see whether a file stream corresponding to the block is stored in their respective caches. If the file stream is not found in the file caches or then a request for that block is sent to the appropriate storage unit address denoted by the metadata. One or more conventional caching algorithms are employed by cache manager for the file caches and to choose which data is to be discarded when the caches are full and new data is to be cached. Caching file streams on the INIC file cache greatly reduces the traffic over both I O bus and data channel for file blocks stored on INIC storage unit .

When a network packet that is directed to the host arrives at the INIC the headers for that packet are processed by the sequencers to validate the packet and create a summary or descriptor of the packet with the summary prepended to the packet and stored in frame buffers and a pointer to the packet stored in a queue. The summary is a status word or words that describes the protocol types of the packet headers and the results of checksumming. Included in this word is an indication whether or not the frame is a candidate for fast path data flow. Unlike prior art approaches upper layer headers containing protocol information including transport and session layer information are processed by the hardware logic of the sequencers to create the summary. The dedicated logic circuits of the sequencers allow packet headers to be processed virtually as fast as the packets arrive from the network.

The INIC then chooses whether to send the packet to the host memory for slow path processing of the headers by the CPU running protocol stack or to send the packet data directly to either INIC file cache or host file cache according to a fast path. The fast path may be selected for the vast majority of data traffic having plural packets per message that are sequential and error free and avoids the time consuming protocol processing of each packet by the CPU such as repeated copying of the data and repeated trips across the host memory bus . For the fast path situation in which the packet is moved directly into the INIC file cache additional trips across the host bus and the I O bus are also avoided. Slow path processing allows any packets that are not conveniently transferred by the fast path of the INIC to be processed conventionally by the host .

In order to provide fast path capability at the host a connection is first set up with the remote host which may include handshake authentication and other connection initialization procedures. A communication control block CCB is created by the protocol stack during connection initialization procedures for connection based messages such as typified by TCP IP or SPX IPX protocols. The CCB includes connection information such as source and destination addresses and ports. For TCP connections a CCB comprises source and destination media access control MAC addresses source and destination IP addresses source and destination TCP ports and TCP variables such as timers and receive and transmit windows for sliding window protocols. After a connection has been set up the CCB is passed by INIC driver from the host to the INIC memory by writing to a command register in that memory where it may be stored along with other CCBs in CCB cache . The INIC also creates a hash table corresponding to the cached CCBs for accelerated matching of the CCBs with packet summaries.

When a message such as a file write that corresponds to the CCB is received by the INIC a header portion of an initial packet of the message is sent to the host to be processed by the CPU and protocol stack . This header portion sent to the host contains a session layer header for the message which is known to begin at a certain offset of the packet and optionally contains some data from the packet. The processing of the session layer header by a session layer of protocol stack identifies the data as belonging to the file and indicates the size of the message which are used by the file system to determine whether to cache the message data in the host file cache or INIC file cache and to reserve a destination for the data in the selected file cache. If any data was included in the header portion that was sent to the host it is then stored in the destination. A list of buffer addresses for the destination in the selected file cache is sent to the INIC and stored in or along with the CCB. The CCB also maintains state information regarding the message such as the length of the message and the number and order of packets that have been processed providing protocol and status information regarding each of the protocol layers including which user is involved and storage space for per transfer information.

Once the CCB indicates the destination fast path processing of packets corresponding to the CCB is available. After the above mentioned processing of a subsequently received packet by the sequencers to generate the packet summary a hash of the packet summary is compared with the hash table and if necessary with the CCBs stored in CCB cache to determine whether the packet belongs to a message for which a fast path connection has been set up. Upon matching the packet summary with the CCB assuming no exception conditions exist the data of the packet without network or transport layer headers is sent by direct memory access DMA unit to the destination in file cache or file cache denoted by the CCB.

At some point after all the data from the message has been cached as a file stream in INIC file cache or host file cache the file stream of data is then sent by DMA unit under control of the file system from that file cache to the INIC storage unit or host storage unit under control of the file system. Commonly file streams cached in host file cache are stored on INIC storage unit while file streams cached in INIC file cache are stored on INIC storage unit but this arrangement is not necessary. Subsequent requests for file transfers may be handled by the same CCB assuming the requests involve identical source and destination IP addresses and ports with an initial packet of a write request being processed by the host CPU to determine a location in the host file cache or INIC file cache for storing the message. It is also possible for the file system to be configured to earmark a location on INIC storage unit or host storage unit as the destination for storing data from a message received from a remote host bypassing the file caches.

An approximation for promoting a basic understanding of the present invention is depicted in which segregates the main paths for information flow for the network data storage system of by showing the primary type of information for each path. shows information flow paths consisting primarily of control information with thin arrows information flow paths consisting primarily of data with thick white arrows and information flow paths consisting of both control information and data with thick black arrows. Note that host is primarily involved with control information flows while the INIC storage unit is primarily involved with data transfer.

Information flow between a network such as LAN WAN and the INIC may include control information and data and so is shown with thick black arrow . Examples of information flow between network such as LAN WAN and the INIC include control information such as connection initialization dialogs and acknowledgements as well as file reads or writes which are sent as packets containing file data encapsulated in control information. The sequencers process control information from file writes and pass data and control information to and from INIC frame buffers and so those transfers are represented with thick black arrow . Control information regarding the data stored in frame buffers is operated on by the processor as shown by thin arrow and control information such as network connection initialization packets and session layer headers are sent to the protocol stack as shown by thin arrow . When a connection has been set up by the host control information regarding that connection such as a CCB may be passed between host protocol stack and INIC memory as shown by thin arrow . Temporary storage of data being read from or written to INIC storage unit is provided by INIC file cache and frame buffers as illustrated by thick white arrows and . Control and knowledge of all file streams that are stored on INIC file cache is provided by file system as shown by thin arrow . In an embodiment for which host storage unit does not store network accessible data file system information is passed between host file cache and host storage unit as shown by arrow . Other embodiments not shown in this figure may not include a host storage unit or alternatively may use a host storage unit and host file cache primarily for network file transfers.

It is apparent from that the data of network file reads or writes primarily pass through the INIC and avoid the host whereas control information is primarily passed between the host and INIC. This segregation of control information from data for file transfers between a network and storage allows the host to manage the file transfers that flow through the INIC between the network and storage while the INIC provides a fast path for those file transfers that accelerates data throughput. Increased throughput afforded by the INIC data fast path allows host and INIC to function for example as a database server for high bandwidth applications such as video in addition to functioning as a file server.

For the case in which the packet is a fast path candidate the packet summary is then compared with a set of fast path connections being handled by the card each connection represented as a CCB by matching the summary with CCB hashes and the CCB cache. If the summary does not match a CCB held in the INIC memory the packet is sent to host memory for processing the headers of the packet by the CPU running instructions from the protocol stack. For the case in which the packet is part of a connection initialization dialog the packet may be used to create a CCB for the message. If the packet summary instead matches a CCB held in the INIC memory the processor checks for exception conditions which may include e.g. fragmented or out of order packets and if such an exception condition is found flushes the CCB and the packet to the host protocol stack for protocol processing. For the case in which a packet summary matches a CCB but a destination for the packet is not indicated with the CCB the session layer header of the packet is sent to the host protocol stack to determine a destination in the host file cache or INIC file cache according to the file system with a list of cache addresses for that destination stored with the CCB in the INIC. The INIC also checks the packet summary for exception conditions that would cause the CCB to be flushed to the host and the packet to be sent to the host for processing by the stack.

For the case in which a packet summary matches a CCB and a destination for the packet is stored with the CCB and no exception conditions exist the data from the packet is sent by DMA to the destination in the host file cache or the INIC file cache designated by the CCB. The message packet in this case bypasses processing of the headers by the host protocol processing stack providing fast path data transfer. For the situation in which the data of the packets is sent via the fast path to the INIC file cache and INIC storage the packets not only avoid protocol processing by the host but do not cross the I O bus or the host memory bus providing tremendous savings of time CPU processing and bus traffic compared to traditional network storage.

For the case in which the file blocks are cached in the host file cache the host determines whether to send the file by fast path processing by noting whether a CCB corresponding to the request is being held by the INIC. If the host chooses not to use the fast path but to send the file from the host by the slow path the CPU runs the protocol stack to create headers for the data held in the host file cache and then adds the headers and checksums to the data creating network frames for transmission over the network by the INIC as is conventional. The INIC then uses DMA to acquire the frames from the host and the INIC then sends the frames onto the network. If instead the file is to be sent by the fast path the INIC processor uses the CCB to create headers and checksums and to DMA frame sized segments of data from the host file cache and then prepends the headers and checksums to the data segments to create network frames freeing the host from protocol processing.

Similarly if the file blocks are cached in the INIC file cache the host determines whether to send the file by fast path processing by noting whether a CCB is being held by the INIC. If the host chooses not to use the fast path the host CPU prepares headers and checksums for the file block data storing the headers in host memory. The host then instructs the INIC to assemble network frames by prepending headers from host memory to data in INIC memory creating message frames that are then sent over the network by the INIC. Even for this non fast path case the data is not moved over the I O bus to the host and back to the INIC reducing I O traffic compared to a conventional transmit of file blocks located on a storage unit connected to a host by an I O bus or network. If instead the fast path is selected the INIC processor creates headers and checksums corresponding to the CCB and prepends the headers and checksums to data segments from the INIC file cache to create network frames which are then sent by the INIC over the network. In this fast path case the host is relieved of protocol processing and host memory bus traffic as well as being relieved of I O bus traffic.

The file system may be arranged so that network accessible files are stored on one of the network storage units or and not on the host storage unit which instead includes the file system code and protocol stack copies of which are cached in the host. With this configuration the host controls network file transfers but a vast majority of the data in those files may be transferred by fast path through INIC or without ever entering the host. For a situation in which file blocks are transferred between a network and a storage unit that are connected to the same INIC the file data may never cross the I O bus or host memory bus. For a situation in which a file is transferred between a network and a storage unit that are connected to the different INICs the file blocks may be sent by DMA over the I O bus to or from a file cache on the INIC that is connected to the storage unit still avoiding the host memory bus. In this worst case situation that is usually avoided data may be transferred from one INIC to another which involves a single transfer over the I O bus and still avoids the host memory bus rather than two I O bus transfers and repeated host memory bus accesses that are conventional.

The host is connected to the INIC by the I O bus such as a PCI bus which is coupled to an INIC bus by an INIC I O bridge such as a PCI bus interface. The INIC includes a specialized processor connected by the I O bus to an INIC memory . INIC memory includes frame buffers and an INIC file cache . Also connected to INIC bus is a set of hardware sequencers that provide processing of network messages including network transport and session layer processing. Physical connection to the LANs and and SANs and is provided by conventional physical layer hardware PHY . Each of the PHY units is connected to a corresponding unit of media access control MAC the MAC units each providing a data link layer connection between the INIC and one of the networks.

One difference between adapter and INIC is that adapter is not necessarily connected to a host having a CPU and protocol stack for processing slow path messages. Connection setup may in this case be handled by adapter for example by INIC sending an initial packet to adapter during a connection initialization dialog with the packet processed by sequencers and then sent to processor to create a CCB. Certain conditions that require slow path processing by a CPU running a software protocol stack are likely to be even less frequent in this environment of communication between adapter and INIC . The messages that are sent between adapter and INIC may be structured in accordance with a single or restricted set of protocol layers such as SCSI TCP and simple network management protocol SNMP and are sent to or from a single source to a single or limited number of destinations. Reduction of many of the variables that cause complications in conventional communications networks affords increased use of fast path processing reducing the need at adapter for error processing. Adapter may have the capability to process several types of storage protocols over IP and TCP for the case in which the adapter may be connected to a host that uses one of those protocols for network storage instead of being connected to INIC . For the situation in which network is not a SAN dedicated to storage transfers but also handles communication traffic an INIC connected to a host having a CPU running a protocol stack for slow path packets may be employed instead of adapter .

As shown in additional INICs similar to INIC may be connected to the host via the I O bus with each additional INIC providing additional LAN connections and or being connected to additional SANs. The plural INICs are represented by Nth INIC which is connected to Nth SAN and Nth LAN . With plural INICs attached to host the host can control plural storage networks with a vast majority of the data flow to and from the host controlled networks bypassing host protocol processing travel across the I O bus travel across the host bus and storage in the host memory.

The processing of message packets received by INIC of from a network such as network is shown in more detail in . A received message packet first enters the media access controller which controls INIC access to the network and receipt of packets and can provide statistical information for network protocol management. From there data flows one byte at a time into an assembly register which in this example is 128 bits wide. The data is categorized by a fly by sequencer as will be explained in more detail with regard to which examines the bytes of a packet as they fly by and generates status from those bytes that will be used to summarize the packet. The status thus created is merged with the data by a multiplexer and the resulting data stored in SRAM . A packet control sequencer oversees the fly by sequencer examines information from the media access controller counts the bytes of data generates addresses moves status and manages the movement of data from the assembly register to SRAM and eventually DRAM . The packet control sequencer manages a buffer in SRAM via SRAM controller and also indicates to a DRAM controller when data needs to be moved from SRAM to a buffer in DRAM . Once data movement for the packet has been completed and all the data has been moved to the buffer in DRAM the packet control sequencer will move the status that has been generated in the fly by sequencer out to the SRAM and to the beginning of the DRAM buffer to be prepended to the packet data. The packet control sequencer then requests a queue manager to enter a receive buffer descriptor into a receive queue which in turn notifies the processor that the packet has been processed by hardware logic and its status summarized.

For most packets the network sequencer validates that the header length received has the correct length and checksums the network layer header. For fast path candidates the network layer header is known to be IP or IPX from analysis done by the media access control sequencer . Assuming for example that the type field is 802.3 and the network protocol is IP the network sequencer analyzes the first bytes of the network layer header which will begin at byte in order to determine IP type. The first bytes of the IP header will be processed by the network sequencer to determine what IP type the packet involves. Determining that the packet involves for example IP version directs further processing by the network sequencer which also looks at the protocol type located ten bytes into the IP header for an indication of the transport header protocol of the packet. For example for IP over Ethernet the IP header begins at offset and the protocol type byte is offset which will be processed by network logic to determine whether the transport layer protocol is TCP for example. From the length of the network layer header which is typically 20 40 bytes network sequencer determines the beginning of the packet s transport layer header for validating the transport layer header. Transport sequencer may generate checksums for the transport layer header and data which may include information from the IP header in the case of TCP at least.

Continuing with the example of a TCP packet transport sequencer also analyzes the first few bytes in the transport layer portion of the header to determine in part the TCP source and destination ports for the message such as whether the packet is NetBios or other protocols. Byte of the TCP header is processed by the transport sequencer to determine and validate the TCP header length. Byte of the TCP header contains flags that may aside from ack flags and push flags indicate unexpected options such as reset and fin that may cause the processor to categorize this packet as an exception. TCP offset bytes and are the checksum which is pulled out and stored by the hardware logic while the rest of the frame is validated against the checksum.

Session sequencer determines the length of the session layer header which in the case of NetBios is only four bytes two of which tell the length of the NetBios payload data but which can be much larger for other protocols. The session sequencer can also be used for example to categorize the type of message as a read or write for which the fast path may be particularly beneficial. Further upper layer logic processing depending upon the message type can be performed by the hardware logic of packet control sequencer and fly by sequencer . Thus the sequencers intelligently directs hardware processing of the headers by categorization of selected bytes from a single stream of bytes with the status of the packet being built from classifications determined on the fly. Once the packet control sequencer detects that all of the packet has been processed by the fly by sequencer the packet control sequencer adds the status information generated by the fly by sequencer and any status information generated by the packet control sequencer and prepends adds to the front that status information to the packet for convenience in handling the packet by the processor . The additional status information generated by the packet control sequencer includes media access controller status information and any errors discovered or data overflow in either the assembly register or DRAM buffer or other miscellaneous information regarding the packet. The packet control sequencer also stores entries into a receive buffer queue and a receive statistics queue via the queue manager .

An advantage of processing a packet by hardware logic is that the packet does not in contrast with conventional sequential software protocol processing have to be stored moved copied or pulled from storage for processing each protocol layer header offering dramatic increases in processing efficiency and savings in processing time for each packet. The packets can be processed at the rate bits are received from the network for example 100 megabits second for a 100 baseT connection. The time for categorizing a packet received at this rate and having a length of sixty bytes is thus about 5 microseconds. The total time for processing this packet with the hardware logic and sending packet data to its host destination via the fast path may be an order of magnitude less than that required by a conventional CPU employing conventional sequential software protocol processing without even considering the additional time savings afforded by the reduction in CPU interrupts and host bus bandwidth savings. For the case in which the destination resides in the INIC cache additional bandwidth savings for host bus and I O bus are achieved.

The processor chooses for each received message packet held in frame buffers whether that packet is a candidate for the fast path and if so checks to see whether a fast path has already been set up for the connection to which the packet belongs. To do this the processor first checks the header status summary to determine whether the packet headers are of a protocol defined for fast path candidates. If not the processor commands DMA controllers in the INIC to send the packet to the host for slow path processing. Even for a slow path processing of a message the INIC thus performs initial procedures such as validation and determination of message type and passes the validated message at least to the data link layer of the host.

For fast path candidates the processor checks to see whether the header status summary matches a CCB held by the INIC. If so the data from the packet is sent along the fast path to the destination in the host. If the fast path candidate s packet summary does not match a CCB held by the INIC the packet may be sent to the host for slow path processing to create a CCB for the message. The fast path may also not be employed for the case of fragmented messages or other complexities. For the vast majority of messages however the INIC fast path can greatly accelerate message processing. The INIC thus provides a single state machine processor that decides whether to send data directly to its destination based upon information gleaned on the fly as opposed to the conventional employment of a state machine in each of several protocol layers for determining the destiny of a given packet.

Caching the CCBs in a hash table in the INIC provides quick comparisons with words summarizing incoming packets to determine whether the packets can be processed via the fast path while the full CCBs are also held in the INIC for processing. Other ways to accelerate this comparison include software processes such as a B tree or hardware assists such as a content addressable memory CAM . When INIC microcode or comparator circuits detect a match with the CCB a DMA controller places the data from the packet in the destination in host memory or INIC File cache without any interrupt by the CPU protocol processing or copying. Depending upon the type of message received the destination of the data may be the session presentation or application layers in the host or host file cache or INIC file cache .

One of the most commonly used network protocols for large messages such as file transfers is server message block SMB over TCP IP. SMB can operate in conjunction with redirector software that determines whether a required resource for a particular operation such as a printer or a disk upon which a file is to be written resides in or is associated with the host from which the operation was generated or is located at another host connected to the network such as a file server. SMB and server redirector are conventionally serviced by the transport layer in the present invention SMB and redirector can instead be serviced by the INIC. In this case sending data by the DMA controllers from the INIC buffers when receiving a large SMB transaction may greatly reduce interrupts that the host must handle. Moreover this DMA generally moves the data to its destination in the host file cache or INIC file cache from which it is then flushed in blocks to the host storage unit or INIC storage unit respectively.

An SMB fast path transmission generally reverses the above described SMB fast path receive with blocks of data read from the host storage unit or INIC storage unit to the host file cache or INIC file cache respectively while the associated protocol headers are prepended to the data by the INIC for transmission via a network line to a remote host. Processing by the INIC of the multiple packets and multiple TCP IP NetBios and SMB protocol layers via custom hardware and without repeated interrupts of the host can greatly increase the speed of transmitting an SMB message to a network line. As noted above with regard to for the case in which the transmitted file blocks are stored on INIC storage unit additional savings in host bus bandwidth and I O bus bandwidth can be achieved.

A storage fast path is provided by the INIC under control of the server for data transferred between network storage units or and client that does not cross the I O bus. Data is communicated between INIC and network storage unit in accordance with a block format such as SCSI TCP or ISCSI whereas data is communicated between INIC and NAS storage unit in accordance with a file format such as TCP NetBios SMB. For either storage fast path the INIC may hold another CCB defining a connection with storage unit or . For convenience in the following discussion the CCB held by INIC defining its connection over network with server is termed the client CCB the CCB held by INIC defining its connection over network with client is termed the server CCB. A CCB held by INIC defining its connection over network with network storage unit is termed the SAN CCB and a CCB held by INIC defining its connection over network with NAS storage unit is termed the NAS CCB. Additional network lines and may be connected to other communication and or storage networks.

Assuming that the client wishes to read a 100 KB file on the server that is stored in blocks on network storage unit the client may begin by sending a SMB read request across network requesting the first 64 KB of that file on the server. The request may be only 76 bytes for example and the INIC on the server recognizes the message type SMB and relatively small message size and sends the 76 bytes directly to the ATCP filter layer which delivers the request to NetBios of the server. NetBios passes the session headers to SMB which processes the read request and determines whether the requested data is held on a host or INIC file cache. If the requested data is not held by the file caches SMB issues a read request to the file system to read the data from the network storage unit into the INIC file cache.

To perform this read the file system instructs INIC to fetch the 64 KB of data from network storage unit into the INIC file cache. The INIC then sends a request for the data over network to network storage unit . The request may take the form of one or more SCSI commands to the storage unit to read the blocks with the commands attached to TCP IP headers according to ISCSI or similar protocols. A controller on the storage unit responds to the commands by reading the requested blocks from its disk drive or drives adding ISCSI or similar protocol headers to the blocks or frame sized portions of the blocks and sending the resulting frames over network to INIC . The frames are received by the INIC processed by the INIC sequencers matched with the storage CCB and reassembled as a 64 KB file stream in the INIC file cache that forms part of the requested 100 KB file. Once the file stream is stored on INIC file cache SMB constructs a read reply and sends a scatter gather list denoting that file stream to INIC and passes the reply to the INIC to send the data over the network according to the server CCB. The INIC employs the scatter gather list to read data packets from its file cache which are prepended with IP TCP NetBios SMB headers created by the INIC based on the server CCB and sends the resulting frames onto network . The remaining 36 KB of the file is sent by similar means. In this manner a file on a network storage unit may be transferred under control of the server without any of the data from the file encountering the I O bus or server protocol stack.

For the situation in which the data requested by client is stored on NAS storage unit the request may be forwarded from server to that storage unit which replies by sending the data with headers addressed to client with server serving as a router. For an embodiment in which server is implemented as a proxy server or as a web cache server the data from NAS storage unit may instead be sent to server which stores the data in its file cache to offer quicker response to future requests for that data. In this implementation the file system on server directs INIC to request the file data on NAS storage unit which responds by sending a number of approximately 1.5 KB packets containing the first 64 KB of file data. The packets containing the file data are received by INIC categorized by INIC receive sequencer and matched with the NAS CCB and a session layer header from an initial packet is processed by the host stack which obtains from the file system a scatter gather list of addresses in INIC file cache to store the data from the packets. The scatter gather list is sent by the host stack to the INIC and stored with the NAS CCB and the INIC begins to DMA data from any accumulated packets and subsequent packets corresponding to the NAS CCB into the INIC file cache as a file stream according to the scatter gather list. The host file system then directs the INIC to create headers based on the client CCB and prepend the headers to packets of data read from the file stream for sending the data to client . The remaining 36 KB of the file is sent by similar means and may be cached in the INIC file cache as another file stream. With the file streams maintained in the INIC file cache subsequent requests for the file from clients such as client may be processed more quickly.

For the situation in which the file requested by client was not present in a cache but instead stored as file blocks on the server attached storage unit the server file system instructs a host SCSI driver to fetch the 100 KB of data from server attached storage unit into the server file cache assuming the file system does not wish to cache the data on INIC file cache . The host SCSI driver then sends a SCSI request for the data over SCSI channel to server attached storage unit . A controller on the server attached storage unit responds to the commands by reading the requested blocks from its disk drive or drives and sending the blocks over SCSI channel to the SCSI driver which interacts with the cache manager under direction of the file system to store the blocks as file streams in the server file cache. A file systemredirector then directs SMB to send a scatter gather list of the file streams to INIC which is used by the INIC to read data packets from the server file streams. The INIC prepends the data packets with headers it created based on the server CCB and sends the resulting frames onto network .

With INIC operating on the client when this reply arrives the INIC recognizes from the first frame received that this connection is receiving fast path processing TCP IP NetBios matching a CCB and the SMB may use this first frame to acquire buffer space for the message. The allocation of buffers can be provided by passing the first 192 bytes of the of the frame including any NetBios SMB headers via the ATCP fast path directly to the client NetBios to give NetBios SMB the appropriate headers. NetBios SMB will analyze these headers realize by matching with a request ID that this is a reply to the original Read connection and give the ATCP command driver a 64K list of buffers in a client file cache into which to place the data. At this stage only one frame has arrived although more may arrive while this processing is occurring. As soon as the client buffer list is given to the ATCP command driver it passes that transfer information to the INIC and the INIC starts sending any frame data that has accumulated into those buffers by DMA.

Should the client wish to write an SMB file to a server a write request is sent over network which may be matched with a CCB held by INIC . Session layer headers from an initial packet of the file write are processed by server SMB to allocate buffersin the server or INIC file cache with a scatter gather list of addresses for those buffers passed back to INIC assuming fast path processing is appropriate. Packets containing SMB file data are received by INIC categorized by INIC receive sequencer and placed in a queue. The INIC processor recognizes that the packets correspond to the server CCB and DMAs the data from the packets into the INIC or server file cache buffers according to the scatter gather list to form a file stream.

The file system then orchestrates sending the file stream to server storage unit network storage unit or NAS storage unit . To send the file stream to server storage unit the file system commands a SCSI driver in the server to send the file stream as file blocks to the storage unit . To send the file stream to network storage unit the file system directs the INIC to create ISCSI or similar headers based on the SAN CCB and prepend those headers to packets read from the file stream according to the scatter gather list sending the resulting frames over network to storage unit . To send the file stream to NAS storage unit which may for example be useful in distributed file cache or proxy server implementations the file systemredirector prepends appropriate NetBios SMB headers and directs the INIC to create IP TCP headers based on the NAS CCB and prepend those headers to packets read from the file stream according to the scatter gather list sending the resulting frames over network to storage unit .

An audio video interface AVI or similar peripheral device is coupled to the I O bus of server and coupled to the AVI is a speaker a microphone a display and a camera which may be a video camera. Although shown as a single interface for clarity AVI may alternatively include separate interfaces such as a sound card and a video card that are each connected to I O bus . In another embodiment AVI interface is connected to a host bus instead of an I O bus and may be integrated with a memory controller or I O controller of the host CPU. AVI may include a memory that functions as a temporary storage device for audio or video data. Client also has an I O bus with an AVI that is coupled to I O bus . Coupled to the AVI is a speaker a microphone a display and a camera which may be a video camera. Although shown as a single interface for clarity AVI may alternatively include separate interfaces such as a sound card and a video card that are each connected to I O bus .

Unlike TCP UDP does not offer a dependable connection. Instead UDP packets are sent on a best efforts basis and packets that are missing or damaged are not resent unless a layer above UDP provides for such services. UDP provides a way for applications to send data via IP without having to establish a connection. A socket however may initially be designated in response to a request by UDP or another protocol such as network file system NFS TCP RTCP SIP or MGCP the socket allocating a port on the receiving device that is able to accept a message sent by UDP. A socket is an application programming interface API used by UDP that denotes the source and destination IP addresses and the source and destination UDP ports.

To conventionally transmit communications via UDP datagrams up to 64 KB which may include session or application layer headers and are commonly NFS default size datagrams of up to about 8 KB are prepended with a UDP header by a host computer and handed to the IP layer. The UDP header includes the source and destination ports and an optional checksum. For Ethernet transmission the UDP datagrams are divided if necessary into approximately 1.5 KB fragments by the IP layer prepended with an IP header and given to the MAC layer for encapsulation with an Ethernet header and trailer and sent onto the network. The IP headers contain an IP identification field IP ID that is unique to the UDP datagram a flags field that indicates whether more fragments of that datagram follow and a fragment offset field that indicates which 1.5 KB fragment of the datagram is attached to the IP header for reassembly of the fragments in correct order.

For fast path UDP data transfer from client to server file streams of data from client application or audio video interface for example may be stored on a memory of INIC under direction of the file system. The application can arrange the file streams to contain about 8 KB for example which may include an upper layer header acquired by INIC under direction of application each file stream received by INIC being prepended by INIC with a UDP header according to the socket that has been designated creating UDP datagrams. The UDP datagrams are divided by INIC into six 1.5 KB message fragments that are each prepended with IP and MAC layer headers to create IP packets that are transmitted on network .

INIC receives the Ethernet frames from network that were sent by INIC checksums and processes the headers to determine the protocols involved and sends the UDP and upper layer headers to the AUDP layer to obtain a list of destination addresses for the data from the packets of that UDP datagram. The UDP and upper layer headers are contained in one packet of the up to six packets from the UDP datagram and that packet is usually received prior to other packets from that datagram. For the case in which the packets arrive out of order the INIC queues the up to six packets from the UDP datagram in a reassembly buffer. The packets corresponding to the UDP datagram are identified based upon their IP ID and concatenated based upon the fragment offsets. After the packet containing the UDP and upper layer headers has been processed to obtain the destination addresses the queued data can be written to those addresses. To account for the possibility that not all packets from a UDP datagram arrive the INIC may use a timer that triggers dropping the received data.

For realtime voice or video communication a telecom connection is initially set up between server and client . For communications according to the International Telecommunications Union ITU H.323 standard the telecom connection setup is performed with a TCP dialog that designates source and destination UDP ports for data flow using RTP. Another TCP dialog for the telecom connection setup designates source and destination UDP ports for monitoring the data flow using RTCP. SIP provides another mechanism for initiating a telecom connection. After the UDP sockets have been designated transmission of voice or video data from server to client in accordance with the present invention may begin.

For example audio video interface can convert sounds and images from microphone and camera to audio video AV data that is available to INIC . Under direction of server file system INIC can acquire 8 KB file streams of the AV data including RTP headers and store that data in an INIC file cache. In accordance with the telecom connection INIC can prepend a UDP header to each file stream which is then fragmented into 1.5 KB fragments each of which is prepended with IP and Ethernet headers and transmitted on network . Alternatively the INIC can create 1.5 KB UDP datagrams from the AV data stored on INIC memory avoiding fragmentation and allowing UDP IP and MAC layer headers to be created simultaneously from a template held on INIC corresponding to the telecom connection. It is also possible to create UDP datagrams larger than 8 KB with the AV data which creates additional fragmenting but can transfer larger blocks of data per datagram. The RTP header contains a timestamp that indicates the relative times at which the AV data is packetized by the audio video interface .

In contrast with conventional protocol processing of AV data by a host CPU INIC can offload the tasks of UDP header creation IP fragmentation and IP header creation saving the host CPU multiple processing cycles multiple interrupts and greatly reducing host bus traffic. In addition the INIC can perform header creation tasks more efficiently by prepending plural headers at one time. Moreover the AV data may be accessed directly by INIC on I O bus rather than first being sent over I O bus to the protocol processing stack and then being sent back over I O bus as RTP UDP IP packets. These advantages in efficiency can also greatly reduce delay in transmitting the AV data as compared to conventional protocol processing.

An object of realtime voice and video communication systems is that delays and jitter in communication are substantially imperceptible to the people communicating via the system. Jitter is caused by variations in the delay at which packets are received which results in variations in the tempo at which sights or sounds are displayed compared to the tempo at which they were recorded. The above mentioned advantage of reduced delay in transmitting packetized voice and or video can also be used to reduce jitter as well via buffering at a receiving device because the reduced delay affords increased time at the receiving device for smoothing the jitter. To further reduce delay and jitter in communicating the AV data IP or MAC layer headers prepended to the UDP datagram fragments by INIC include a high quality of service QOS indication in their type of service field. This indication may be used by network to speed transmission of high QOS frames for example by preferentially allocating bandwidth and queuing for those frames.

When the frames containing the AV data are received by INIC the QOS indication is flagged by the INIC receive logic and the packets are buffered in a high priority receive queue of the INIC . Categorizing and validating the packet headers by hardware logic of INIC reduces delay in receiving the AV data compared with conventional processing of IP UDP headers. A UDP header for each datagram may be sent to AUDP layer which processes the header and indicates to the application the socket involved. The application then directs INIC to send the data corresponding to that UDP header to a destination in audio video interface . Alternatively after all the fragments corresponding to a UDP datagram have been concatenated in the receive queue the datagram is written as a file stream without the UDP header to a destination in audio video interface corresponding to the socket. Optionally the UDP datagram may first be stored in another portion of INIC memory. The AV data and RTP header is then sent to audio video interface where it is decoded and played on speaker and display .

Note that the entire AV data flow and most protocol processing of received IP UDP packets can be handled by the INIC saving the CPU of client multiple processing cycles and greatly reducing host bus traffic and interrupts. In addition the specialized hardware logic of INIC can categorize headers more efficiently than the general purpose CPU. The AV data may also be provided directly by INIC to audio video interface over I O bus rather than first being sent over I O bus to the protocol processing stack and then being sent back over I O bus to audio video interface .

The INIC is connected by network connectors to four network lines and which may transport data along a number of different conduits such as twisted pair coaxial cable or optical fiber each of the connections providing a media independent interface MII via commercially available physical layer chips and such as model 80220 80221 Ethernet Media Interface Adapter from SEEQ Technology Incorporated 47200 Bayside Parkway Fremont Calif. 94538. The lines preferably are 802.3 compliant and in connection with the INIC constitute four complete Ethernet nodes the INIC supporting 10Base T 10Base T2 100Base TX 100Base FX and 100Base T4 as well as future interface standards. Physical layer identification and initialization is accomplished through host driver initialization routines. The connection between the network lines and and the INIC is controlled by MAC units MAC A MAC B MAC C and MAC D which contain logic circuits for performing the basic functions of the MAC sublayer essentially controlling when the INIC accesses the network lines and . The MAC units and may act in promiscuous multicast or unicast modes allowing the INIC to function as a network monitor receive broadcast and multicast packets and implement multiple MAC addresses for each node. The MAC units and also provide statistical information that can be used for simple network management protocol SNMP .

The MAC units and are each connected to transmit and receive sequencers XMT RCV A XMT RCV B XMT RCV C and XMT RCV D . Each of the transmit and receive sequencers can perform several protocol processing steps on the fly as message frames pass through that sequencer. In combination with the MAC units the transmit and receive sequencers and can compile the packet status for the data link network transport session and if appropriate presentation and application layer protocols in hardware greatly reducing the time for such protocol processing compared to conventional sequential software engines. The transmit and receive sequencers and are connected to an SRAM and DMA controller which includes DMA controllers and SRAM controller which controls static random access memory SRAM buffers . The SRAM and DMA controllers interact with external memory control to send and receive frames via external memory bus to and from dynamic random access memory DRAM buffers which is located adjacent to the IC chip . The DRAM buffers may be configured as 4 MB 8 MB 16 MB or 32 MB and may optionally be disposed on the chip. The SRAM and DMA controllers are connected to an I O bridge that in this case is a PCI Bus Interface Unit BIU which manages the interface between the INIC and the PCI interface bus . The 64 bit multiplexed BIU provides a direct interface to the PCI bus for both slave and master functions. The INIC is capable of operating in either a 64 bit or 32 bit PCI environment while supporting 64 bit addressing in either configuration.

A microprocessor is connected to the SRAM and DMA controllers and to the PCI BIU . Microprocessor instructions and register files reside in an on chip control store which includes a writable on chip control store WCS of SRAM and a read only memory ROM . The microprocessor offers a programmable state machine which is capable of processing incoming frames processing host commands directing network traffic and directing PCI bus traffic. Three processors are implemented using shared hardware in a three level pipelined architecture that launches and completes a single instruction for every clock cycle. A receive processor is primarily used for receiving communications while a transmit processor is primarily used for transmitting communications in order to facilitate full duplex communication while a utility processor offers various functions including overseeing and controlling PCI register access.

Since instructions for processors and reside in the on chip control store the functions of the three processors can be easily redefined so that the microprocessor can be adapted for a given environment. For instance the amount of processing required for receive functions may outweigh that required for either transmit or utility functions. In this situation some receive functions may be performed by the transmit processor and or the utility processor . Alternatively an additional level of pipelining can be created to yield four or more virtual processors instead of three with the additional level devoted to receive functions.

The INIC in this embodiment can support up to 256 CCBs which are maintained in a table in the DRAM . There is also however a CCB index in hash order in the SRAM to save sequential searching. Once a hash has been generated the CCB is cached in SRAM with up to sixteen cached CCBs in SRAM in this example. Allocation of the sixteen CCBs cached in SRAM is handled by a least recently used register described below. These cache locations are shared between the transmit and receive processors so that the processor with the heavier load is able to use more cache buffers. There are also eight header buffers and eight command buffers to be shared between the sequencers. A given header or command buffer is not statically linked to a specific CCB buffer as the link is dynamic on a per frame basis.

In general a first instruction phase of the pipelined microprocessors completes an instruction and stores the result in a destination operand fetches the next instruction and stores that next instruction in an instruction register. A first register set provides a number of registers including the instruction register and a set of controls for the first register set provides the controls for storage to the first register set . Some items pass through the first phase without modification by the controls and instead are simply copied into the first register set or a RAM file register . A second instruction phase has an instruction decoder and operand multiplexer that generally decodes the instruction that was stored in the instruction register of the first register set and gathers any operands which have been generated which are then stored in a decode register of a second register set . The first register set second register set and a third register set which is employed in a third instruction phase include many of the same registers as will be seen in the more detailed views of . The instruction decoder and operand multiplexer can read from two address and data ports of the RAM file register which operates in both the first phase and second phase . A third phase of the processor has an arithmetic logic unit ALU which generally performs any ALU operations on the operands from the second register set storing the results in a results register included in the third register set . A stack exchange can reorder register stacks and a queue manager can arrange queues for the processor the results of which are stored in the third register set.

The instructions continue with the first phase then following the third phase as depicted by a circular pipeline . Note that various functions have been distributed across the three phases of the instruction execution in order to minimize the combinatorial delays within any given phase. With a frequency in this embodiment of 66 MHz each Clock increment takes 15 nanoseconds to complete for a total of 45 nanoseconds to complete one instruction for each of the three processors. The rotating instruction phases are depicted in more detail in in which each phase is shown in a different figure.

More particularly shows some specific hardware functions of the first phase which generally includes the first register set and related controls . The controls for the first register set includes an SRAM control which is a logical control for loading address and write data into SRAM address and data registers . Thus the output of the ALU from the third phase may be placed by SRAM control into an address register or data register of SRAM address and data registers . A load control similarly provides controls for writing a context for a file to file context register and another load control provides controls for storing a variety of miscellaneous data to flip flop registers . ALU condition codes such as whether a carried bit is set get clocked into ALU condition codes register without an operation performed in the first phase . Flag decodes can perform various functions such as setting locks that get stored in flag registers .

The RAM file register has a single write port for addresses and data and two read ports for addresses and data so that more than one register can be read from at one time. As noted above the RAM file register essentially straddles the first and second phases as it is written in the first phase and read from in the second phase . A control store instruction allows the reprogramming of the processors due to new data in from the control store not shown in this figure the instructions stored in an instruction register . The address for this is generated in a fetch control register which determines which address to fetch the address stored in fetch address register . Load control provides instructions for a program counter which operates much like the fetch address for the control store. A last in first out stack of three registers is copied to the first register set without undergoing other operations in this phase. Finally a load control for a debug address is optionally included which allows correction of errors that may occur.

The SRAM FIFOS and are both connected to DRAM which allows virtually unlimited expansion of those FIFOS to handle situations in which the SRAM head and tail are full. For example a first of the thirty two queues labeled Q zero may queue an entry in DRAM as shown by arrow by DMA units acting under direction of the queue manager instead of being queued in the head or tail of FIFO . Entries stored in DRAM return to SRAM unit as shown by arrow extending the length and fall through time of that FIFO. Diversion from SRAM to DRAM is typically reserved for when the SRAM is full since DRAM is slower and DMA movement causes additional latency. Thus Q zero may comprise the entries stored by queue manager in both the FIFO and the DRAM . Likewise information bound for FIFO which may correspond to Q twenty seven for example can be moved by DMA into DRAM as shown by arrow . The capacity for queuing in cost effective albeit slower DRAM is user definable during initialization allowing the queues to change in size as desired. Information queued in DRAM is returned to SRAM unit as shown by arrow .

Status for each of the thirty two hardware queues is conveniently maintained in and accessed from a set of four thirty two bit registers as shown in in which a specific bit in each register corresponds to a specific queue. The registers are labeled Q Out Ready Q In Ready Q Empty and Q Full . If a particular bit is set in the Q Out Ready register the queue corresponding to that bit contains information that is ready to be read while the setting of the same bit in the Q In Ready register means that the queue is ready to be written. Similarly a positive setting of a specific bit in the Q Empty register means that the queue corresponding to that bit is empty while a positive setting of a particular bit in the Q Full register means that the queue corresponding to that bit is full. Thus Q Out Ready contains bits zero through thirty one including bits twenty seven twenty eight twenty nine and thirty . Q In Ready contains bits zero through thirty one including bits twenty seven twenty eight twenty nine and thirty . Q Empty contains bits zero through thirty one including bits twenty seven twenty eight twenty nine and thirty and Q full contains bits zero through thirty one including bits twenty seven twenty eight twenty nine and thirty .

Q zero corresponding to FIFO is a free buffer queue which holds a list of addresses for all available buffers. This queue is addressed when the microprocessor or other devices need a free buffer address and so commonly includes appreciable DRAM . Thus a device needing a free buffer address would check with Q zero to obtain that address. Q twenty seven corresponding to FIFO is a receive buffer descriptor queue. After processing a received frame by the receive sequencer the sequencer looks to store a descriptor for the frame in Q twenty seven. If a location for such a descriptor is immediately available in SRAM bit twenty seven of Q In Ready will be set. If not the sequencer must wait for the queue manager to initiate a DMA move from SRAM to DRAM thereby freeing space to store the receive descriptor.

Operation of the queue manager which manages movement of queue entries between SRAM and the processor the transmit and receive sequencers and also between SRAM and DRAM is shown in more detail in . Requests that utilize the queues include Processor Request Transmit Sequencer Request and Receive Sequencer Request . Other requests for the queues are DRAM to SRAM Request and SRAM to DRAM Request which operate on behalf of the queue manager in moving data back and forth between the DRAM and the SRAM head or tail of the queues. Determining which of these various requests will get to use the queue manager in the next cycle is handled by priority logic Arbiter . To enable high frequency operation the queue manager is pipelined with Register A and Register B providing temporary storage while Status Register maintains status until the next update. The queue manager reserves even cycles for DMA receive and transmit sequencer requests and odd cycles for processor requests. Dual ported QRAM stores variables regarding each of the queues the variables for each queue including a Head Write Pointer Head Read Pointer Tail Write Pointer and Tail Read Pointer corresponding to the queue s SRAM condition and a Body Write Pointer and Body Read Pointer corresponding to the queue s DRAM condition and the queue s size.

After Arbiter has selected the next operation to be performed the variables of QRAM are fetched and modified according to the selected operation by a QALU and an SRAM Read Request or an SRAM Write Request may be generated. The variables are updated and the updated status is stored in Status Register as well as QRAM . The status is also fed to Arbiter to signal that the operation previously requested has been fulfilled inhibiting duplication of requests. The Status Register updates the four queue registers Q Out Ready Q In Ready Q Empty and Q Full to reflect the new status of the queue that was accessed. Similarly updated are SRAM Addresses Body Write Request and Body Read Requests which are accessed via DMA to and from SRAM head and tails for that queue. Alternatively various processes may wish to write to a queue as shown by Q Write Data which are selected by multiplexer and pipelined to SRAM Write Request . The SRAM controller services the read and write requests by writing the tail or reading the head of the accessed queue and returning an acknowledge. In this manner the various queues are utilized and their status updated. Structure and operation of queue manager is also described in U.S. patent application Ser. No. 09 416 925 entitled Queue System For Microprocessors filed Oct. 13 1999 by Daryl D. Starr and Clive M. Philbrick the subject matter of which is incorporated herein by reference .

In some cases a CCB being used is one that is not desirable to hold in the limited cache memory. For example it is preferable not to cache a CCB for a context that is known to be closing so that other cached CCBs can remain in SRAM longer. In this case the number representing the cache unit holding the decacheable CCB is stored in the LRU block R rather than the MRU block R so that the decacheable CCB will be replaced immediately upon employment of a new CCB that is cached in the SRAM unit corresponding to the number held in the LRU block R. shows the case for which number which had been in block R in corresponds to a CCB that will be used and then closed. In this case number has been removed from block R and stored in the LRU block R. All the numbers that had previously been stored to the left of block R R R are then shifted one block to the right.

An array of sixteen comparators each receives the value stored in the corresponding block of the least recently used register . Each comparator also receives a signal from processor along line so that the register block having a number matching that sent by processor outputs true to logic circuits while the other fifteen comparators output false. Logic circuits control a pair of select lines leading to each of the multiplexers for selecting inputs to the multiplexers and therefore controlling shifting of the register block numbers. Thus select lines control MUX select lines control MUX select lines control MUX select lines control MUX and select lines control MUX.

When a CCB is to be used processor checks to see whether the CCB matches a CCB currently held in one of the sixteen cache units. If a match is found the processor sends a signal along line with the block number corresponding to that cache unit for example number . Comparators compare the signal from that line with the block numbers and comparator C provides a true output for the block R that matches the signal while all the other comparators output false. Logic circuits under control from the processor use select lines to choose the input from line for MUX storing the number in the MRU block R. Logic circuits also send signals along the pairs of select lines for MUX and higher multiplexers aside from MUX to shift their output one block to the left by selecting as inputs to each multiplexer MUX and higher the value that had been stored in register blocks one block to the right R R . The outputs of multiplexers that are to the left of MUX are selected to be constant.

If processor does not find a match for the CCB among the sixteen cache units on the other hand the processor reads from LRU block R along line to identify the cache corresponding to the LRU block and writes the data stored in that cache to DRAM. The number that was stored in R in this case number is chosen by select lines as input to MUX for storage in MRU block R. The other fifteen multiplexers output to their respective register blocks the numbers that had been stored each register block immediately to the right.

For the situation in which the processor wishes to remove a CCB from the cache after use the LRU block R rather than the MRU block R is selected for placement of the number corresponding to the cache unit holding that CCB. The number corresponding to the CCB to be placed in the LRU block R for removal from SRAM for example number held in block R is sent by processor along line which is matched by comparator C. The processor instructs logic circuits to input the number to R by selecting with lines input to MUX. Select lines to MUX choose as input the number held in register block R so that the number from R is stored in R. The numbers held by the other register blocks between R and R are similarly shifted to the right whereas the numbers in register blocks to the right of R are left constant. This frees scarce cache memory from maintaining closed CCBs for many cycles while their identifying numbers move through register blocks from the MRU to the LRU blocks.

Operation of receive sequencer of is now described in connection with the receipt onto INIC of a TCP IP packet from network line . At initialization time processor partitions DRAM into buffers. Receive sequencer uses the buffers in DRAM to store incoming network packet data as well as status information for the packet. Processor creates a 32 bit buffer descriptor for each buffer. A buffer descriptor indicates the size and location in DRAM of its associated buffer. Processor places these buffer descriptors on a free buffer queue by writing the descriptors to the queue manager . Queue manager maintains multiple queues including the free buffer queue . In this implementation the heads and tails of the various queues are located in SRAM whereas the middle portion of the queues are located in DRAM .

Lines comprise a request mechanism involving a request line and address lines. Similarly lines comprise a request mechanism involving a request line and address lines. Queue manager uses lines and to issue requests to transfer queue information from DRAM to SRAM or from SRAM to DRAM.

The queue manager interface of the receive sequencer always attempts to maintain a free buffer descriptor for use by the packet processing sequencer . Bit is a ready bit that indicates that free buffer descriptor is available for use by the packet processing sequencer . If queue manager interface does not have a free buffer descriptor bit is not set then queue manager interface requests one from queue manager via request line . Request line is actually a bus that communicates the request a queue ID a read write signal and data if the operation is a write to the queue. 

In response queue manager retrieves a free buffer descriptor from the tail of the free buffer queue and then alerts the queue manager interface via an acknowledge signal on acknowledge line . When queue manager interface receives the acknowledge signal the queue manager interface loads the free buffer descriptor and sets the ready bit . Because the free buffer descriptor was in the tail of the free buffer queue in SRAM the queue manager interface actually receives the free buffer descriptor from the read data bus of the SRAM control block . Packet processing sequencer requests a free buffer descriptor via request line . When the queue manager interface retrieves the free buffer descriptor and the free buffer descriptor is available for use by the packet processing sequencer the queue manager interface informs the packet processing sequencer via grant line . By this process a free buffer descriptor is made available for use by the packet processing sequencer and the receive sequencer is ready to processes an incoming packet.

Next a TCP IP packet is received from the network line via network connector and Physical Layer Interface PHY . PHY supplies the packet to MAC via a Media Independent Interface MII parallel bus . MAC begins processing the packet and asserts a start of packet signal on line indicating that the beginning of a packet is being received. When a byte of data is received in the MAC and is available at the MAC outputs MAC asserts a data valid signal on line . Upon receiving the data valid signal the packet synchronization sequencer instructs the data synchronization buffer via load signal line to load the received byte from data lines . Data synchronization buffer is four bytes deep. The packet synchronization sequencer then increments a data synchronization buffer write pointer. This data synchronization buffer write pointer is made available to the packet processing sequencer via lines . Consecutive bytes of data from data lines are clocked into the data synchronization buffer in this way.

A data synchronization buffer read pointer available on lines is maintained by the packet processing sequencer . The packet processing sequencer determines that data is available in data synchronization buffer by comparing the data synchronization buffer write pointer on lines with the data synchronization buffer read pointer on lines .

Data assembly register contains a sixteen byte long shift register . This register is loaded serially a single byte at a time and is unloaded in parallel. When data is loaded into register a write pointer is incremented. This write pointer is made available to the packet processing sequencer via lines . Similarly when data is unloaded from register a read pointer maintained by packet processing sequencer is incremented. This read pointer is available to the data assembly register via lines . The packet processing sequencer can therefore determine whether room is available in register by comparing the write pointer on lines to the read pointer on lines .

If the packet processing sequencer determines that room is available in register then packet processing sequencer instructs data assembly register to load a byte of data from data synchronization buffer . The data assembly register increments the data assembly register write pointer on lines and the packet processing sequencer increments the data synchronization buffer read pointer on lines . Data shifted into register is examined at the register outputs by protocol analyzer which verifies checksums and generates status information .

DMA control block is responsible for moving information from register to buffer via a sixty four byte receive FIFO . DMA control block implements receive FIFO as two thirty two byte ping pong buffers using sixty four bytes of SRAM . DMA control block implements the receive FIFO using a write pointer and a read pointer. When data to be transferred is available in register and space is available in FIFO DMA control block asserts an SRAM write request to SRAM controller via lines . SRAM controller in turn moves data from register to FIFO and asserts an acknowledge signal back to DMA control block via lines . DMA control block then increments the receive FIFO write pointer and causes the data assembly register read pointer to be incremented.

When thirty two bytes of data has been deposited into receive FIFO DMA control block presents a DRAM write request to DRAM controller via lines . This write request consists of the free buffer descriptor ORed with a buffer load count for the DRAM request address and the receive FIFO read pointer for the SRAM read address. Using the receive FIFO read pointer the DRAM controller asserts a read request to SRAM controller . SRAM controller responds to DRAM controller by returning the indicated data from the receive FIFO in SRAM and asserting an acknowledge signal. DRAM controller stores the data in a DRAM write data register stores a DRAM request address in a DRAM address register and asserts an acknowledge to DMA control block . The DMA control block then decrements the receive FIFO read pointer. Then the DRAM controller moves the data from the DRAM write data register to buffer . In this way as consecutive thirty two byte chunks of data are stored in SRAM DRAM control block moves those thirty two byte chunks of data one at a time from SRAM to buffer in DRAM . Transferring thirty two byte chunks of data to the DRAM in this fashion allows data to be written into the DRAM using the relatively efficient burst mode of the DRAM.

Packet data continues to flow from network line to buffer until all packet data has been received. MAC then indicates that the incoming packet has completed by asserting an end of frame i.e. end of packet signal on line and by presenting final packet status MAC packet status to packet synchronization sequencer . The packet processing sequencer then moves the status also called protocol analyzer status and the MAC packet status to register for eventual transfer to buffer . After all the data of the packet has been placed in buffer status and the MAC packet status is transferred to buffer so that it is stored prepended to the associated data as shown in .

After all data and status has been transferred to buffer packet processing sequencer creates a summary also called a receive packet descriptor by concatenating the free buffer descriptor the buffer load count the MAC ID and a status bit also called an attention bit . If the attention bit is a one then the packet is not a fast path candidate whereas if the attention bit is a zero then the packet is a fast path candidate . The value of the attention bit represents the result of a significant amount of processing that processor would otherwise have to do to determine whether the packet is a fast path candidate . For example the attention bit being a zero indicates that the packet employs both TCP protocol and IP protocol. By carrying out this significant amount of processing in hardware beforehand and then encoding the result in the attention bit subsequent decision making by processor as to whether the packet is an actual fast path packet is accelerated.

Packet processing sequencer then sets a ready bit not shown associated with summary and presents summary to queue manager interface . Queue manager interface then requests a write to the head of a summary queue also called the receive descriptor queue . The queue manager receives the request writes the summary to the head of the summary queue and asserts an acknowledge signal back to queue manager interface via line . When queue manager interface receives the acknowledge queue manager interface informs packet processing sequencer that the summary is in summary queue by clearing the ready bit associated with the summary. Packet processing sequencer also generates additional status information also called a vector for the packet by concatenating the MAC packet status and the MAC ID. Packet processing sequencer sets a ready bit not shown associated with this vector and presents this vector to the queue manager interface . The queue manager interface and the queue manager then cooperate to write this vector to the head of a vector queue in similar fashion to the way summary was written to the head of summary queue as described above. When the vector for the packet has been written to vector queue queue manager interface resets the ready bit associated with the vector.

Once summary including a buffer descriptor that points to buffer has been placed in summary queue and the packet data has been placed in buffer processor can retrieve summary from summary queue and examine the attention bit .

If the attention bit from summary is a digital one then processor determines that the packet is not a fast path candidate and processor need not examine the packet headers. Only the status first sixteen bytes from buffer are DMA transferred to SRAM so processor can examine it. If the status indicates that the packet is a type of packet that is not to be transferred to the host for example a multicast frame that the host is not registered to receive then the packet is discarded i.e. not passed to the host . If status does not indicate that the packet is the type of packet that is not to be transferred to the host then the entire packet headers and data is passed to a buffer on host for slow path transport and network layer processing by the protocol stack of host .

If on the other hand the attention bit is a zero then processor determines that the packet is a fast path candidate . If processor determines that the packet is a fast path candidate then processor uses the buffer descriptor from the summary to DMA transfer the first approximately 96 bytes of information from buffer from DRAM into a portion of SRAM so processor can examine it. This first approximately 96 bytes contains status as well as the IP source address of the IP header the IP destination address of the IP header the TCP source address of the TCP header and the TCP destination address of the TCP header. The IP source address of the IP header the IP destination address of the IP header the TCP source address of the TCP header and the TCP destination address of the TCP header together uniquely define a single connection context TCB with which the packet is associated. Processor examines these addresses of the TCP and IP headers and determines the connection context of the packet. Processor then checks a list of connection contexts that are under the control of INIC and determines whether the packet is associated with a connection context TCB under the control of INIC .

If the connection context is not in the list then the fast path candidate packet is determined not to be a fast path packet. In such a case the entire packet headers and data is transferred to a buffer in host for slow path processing by the protocol stack of host .

If on the other hand the connection context is in the list then software executed by processor including software state machines and checks for one of numerous exception conditions and determines whether the packet is a fast path packet or is not a fast path packet . These exception conditions include 1 IP fragmentation is detected 2 an IP option is detected 3 an unexpected TCP flag urgent bit set reset bit set SYN bit set or FIN bit set is detected 4 the ACK field in the TCP header is before the TCP window or the ACK field in the TCP header is after the TCP window or the ACK field in the TCP header shrinks the TCP window 5 the ACK field in the TCP header is a duplicate ACK and the ACK field exceeds the duplicate ACK count the duplicate ACK count is a user settable value and 6 the sequence number of the TCP header is out of order packet is received out of sequence . If the software executed by processor detects one of these exception conditions then processor determines that the fast path candidate is not a fast path packet. In such a case the connection context for the packet is flushed the connection context is passed back to the host so that the connection context is no longer present in the list of connection contexts under control of INIC . The entire packet headers and data is transferred to a buffer in host for slow path transport layer and network layer processing by the protocol stack of host .

If on the other hand processor finds no such exception condition then the fast path candidate packet is determined to be an actual fast path packet . The receive state machine then processes of the packet through TCP. The data portion of the packet in buffer is then transferred by another DMA controller not shown in from buffer to a host allocated file cache in storage of host . In one embodiment host does no analysis of the TCP and IP headers of a fast path packet . All analysis of the TCP and IP headers of a fast path packet is done on INIC card .

In the case of a shorter single packet session layer message portions and of the session layer message are transferred to 256 byte buffer of host along with the connection context identifier as in the case of the longer session layer message described above. In the case of a single packet session layer message however the transfer is completed at this point. Host does not return a destination to INIC and INIC does not transfer subsequent data to such a destination.

Network interface device has a single port which is coupled to network via a single network cable . Cable may be a CAT5 four twisted pair network cable a fiber optic cable or any other cable suitable for coupling network interface device to network . ISCSI target is also coupled to network via a network cable . In the illustrated example bus is a parallel bus such as a PCI bus. Host computer and network interface device are described above in this patent document. Network interface device is often called an intelligent network interface card INIC in the description above.

Host computer in one embodiment is a Pentium based hardware platform that runs a Microsoft NT operating system. Software executing on the host computer also includes a network interface driver and a protocol stack . Protocol stack includes a network access layer labeled in MAC layer an internet protocol layer labeled in IP layer a transport layer labeled in TCP layer and an application layer labeled in ISCSI layer . The ISCSI layer is generally considered to involve the application layer in the TCP IP protocol stack model whereas the ISCSI layer is generally considered to involve the session presentation and application layers in the OSI protocol stack model.

To read data from the network storage device in a first step a file system of the host computer issues a read. The read is passed to a disc driver of the host computer and then to ISCSI layer where the SCSI Read is encapsulated in an ISCSI command. The encapsulated command is called an ISCSI read request command . The ISCSI read request command passes down through protocol stack through the network interface driver passes over PCI bus and to network interface device . The ISCSI read request command is accompanied by a list that indicates where requested data is to be placed once that data is retrieved from the ISCSI target. The read request command is sometimes called a solicited read command .

In the present example the list that indicates where requested data is to be placed includes a scatter gather list of pairs of physical addresses and lengths. This scatter gather list identifies a destination where the data is to be placed. Destination may be on the host computer or on another computer or on another device or elsewhere on network . In the example of destination is located in semiconductor memory on the motherboard of host computer . Network interface device forwards the ISCSI read request command to ISCSI target in the form of a TCP packet that contains the ISCSI read request command. In the present example the ISCSI read request command encapsulates a request for a particular 8 k bytes of data found on hard disk .

ISCSI target includes hardware for interfacing to network as well as storage where the requested data is stored. In the illustrated example ISCSI target includes components including a host bus adapter HBA and a hard disk . Although an HBA is shown in this example it is understood that other hardware can be used to interface the ISCSI target to the network. An intelligent network interface card INIC may for example be used. An ordinary network interface card NIC could be used.

In a second step ISCSI target receives the ISCSI read request command extracts the encapsulated SCSI read command retrieves the requested data and starts sending that data back to the requesting device via network . In the present example the requested 8 k bytes of data is sent back as an ISCSI Data In response that is segmented into a stream of multiple TCP packets each TCP packet containing as much as 1460 bytes of data.

There is a communication control block CCB on host computer and a communication control block CCB on network interface device . These two CCBs are associated with the connection of the ISCSI transaction between computer and ISCSI target . Only one of the two CCBs is valid at a given time. If the CCB of the host computer is valid then the protocol stack of the host computer is handling the connection. This is called slow path processing and is described above in this patent document. If on the other hand the CCB is valid then network interface device is handling the connection such that protocol stack of the host performs little or no network layer or transport layer processing for packets received in association with the connection. This is called fast path processing and is described above in this patent document.

In the example of CCB is valid and network interface device is handling the connection as a fast path connection. Network interface device receives the first packet back from the ISCSI target. The first packet contains an ISCSI header and in that header an identifier that network interface device uses to identify the corresponding ISCSI read request command to which the Data In response applies. The network interface device s use of the identifier to identify the corresponding ISCSI read request command is session layer processing. Accordingly some session layer processing ISCSI is considered session layer is done on network interface device . Network interface device extracts any data in the packet and causes the data to be written into the destination appropriate for the ISCSI read request command. Subsequent ones of the packets are thereafter received and their respective data payloads are placed into the same destination .

In one embodiment network interface device stores the list associated with the ISCSI read request command that indicates where requested data is to be placed. This list may be a scatter gather list that identifies the destination. When packets containing the requested data are received back from the ISCSI target then the network interface device uses the stored information to write the data to the destination. The ISCSI identifier initiator task tag in the first packet of the ISCSI response is used to identify the scatter gather list of the appropriate ISCSI read request command. The network interface device does not cause the protocol stack to supply an address of the destination once the fast path connection has been set up. Network interface device uses the stored information for example the scatter gather list to determine where to write the data.

In another embodiment network interface device receives the first packet and forwards it or a piece of it to the protocol stack of the host. More particularly the first packet or piece of the first packet is supplied directly to the session layer portion of the protocol stack and as such is hardware assisted. The host then returns to the network interface device the address or addresses of the destination as described above in connection with . Once the network interface device has the address or addresses of the destination the network interface device receives subsequent packets extracts their data payloads and places the data payloads of the subsequent packets directly into the destination such that the protocol stack of the host does not do any additional network layer or transport layer processing.

In the example of the data payloads of the first four packets are transferred across PCI bus one at a time and are written into destination without the protocol stack of the host doing any network layer or transport layer processing. Network interface device includes a DMA controller that takes control of PCI bus and writes the data directly into destination destination being a semiconductor memory on the motherboard of the host computer.

Error conditions can cause in a third step processing of the ISCSI read request command by computer to switch from fast path processing to slow path processing. In such case network layer and transport layer and session layer processing occur on the host computer. In one example each of the TCP packets received on network interface device packets for the connection of the ISCSI read request command has a sequence number. The packets are expected to be received with sequentially increasing sequence numbers. If a packet is received with a sequence number that is out of sequence then an error may have occurred. If for example packets numbered 1 2 3 and then 5 are received in that order by network interface device then it is likely that packet number was dropped somewhere in the network. Under certain conditions including the out of sequence situation in this example network interface device determines that a condition has occurred that warrants the connection being flushed back to the host computer for slow path processing. Flushing of the connection entails making CCB on the network interface device invalid and making the shadow CCB on the host computer valid. Network interface device stops fast path processing of packets for the flushed connection and packets for the flushed connection are passed to the protocol stack of the host computer for slow path processing. Error handling and or exception handling are therefore done by the protocol stack in software.

In accordance with the example of network interface device detects the out of sequence condition and in response thereto sends a command status message sometimes called a command complete to host computer . is a simplified diagram of one possible command status message. The particular command status message of includes a pointer portion a status portion and a residual indication portion . Status portion includes in one embodiment an ISCSI command sent bit and a flushed bit . Pointer portion is a pointer or other indication that the host computer can use to identify the particular command to which the command status message pertains. The ISCSI command sent bit if set indicates that the ISCSI command was sent by the network interface device to the target device. The flushed bit if set indicates that the connection is to be flushed. For example a condition has been detected on the network interface device that will result in slow path protocol processing by the host protocol stack. In some embodiments the flushed bit is called an error bit because its being set indicates an error has occurred that requires slow path processing. The residual indication portion of the command status message can be used by the host to determine which part of destination that should have been filled still remains to be filled due to the flushing of the connection.

In the example of the out of sequence packet results in both the ISCSI command sent bit and the flushed bit of command status message being set. Host computer receives this command status message via PCI bus and from the flushed bit being set learns that CCB is to be valid and that the connection is to be flushed. Host computer therefore performs slow path processing on any subsequent packet information received from ISCSI target . In this manner fast path processing is performed on an initial portion of an ISCSI response from ISCSI target whereas slow path processing is performed on a subsequent portion of the ISCSI response from ISCSI target .

There may for example be multiple ISCSI read request commands pending where the multiple ISCSI read request commands are all associated with a single connection. If for example one of the ISCSI read request commands results in an error condition that causes the connection to be flushed and if there was another ISCSI read request command pending for that connection then the other ISCSI read request command either could have been sent to the target before the flushing or the other ISCSI read request command could not have been sent before the flushing. In order to properly handle the other ISCSI read request command host computer should know whether the other ISCSI read request command was sent or not. This information is supplied by network interface device sending host computer a command status message for the other ISCSI read request command where the command status message includes the ISCSI command sent bit set in the appropriate manner.

There may in some embodiments be a great many pending ISCSI read request commands. If network interface device were to store the indication of the destination for example a scatter gather list for each such pending ISCSI read request command for each of the many pending ISCSI read request commands then an undesirably large amount of memory space may be required on network interface device . It may be undesirably expensive to provide the necessary memory on network interface device . In accordance with one embodiment of the present invention network interface device stores only indications of destinations for a predetermined number of pending ISCSI read request commands. Once the predetermined number of pending ISCSI read request commands has been reached a subsequent ISCSI read request command will be handled without storing the indication of destination on network interface device . Rather the first packet or a part of the first packet received from ISCSI target is passed from network interface device to the protocol stack of host computer such that the protocol stack of host computer returns the address of the destination. Network interface device therefore receives the address of the destination in this manner after the first packet of the ISCSI response is received from the ISCSI target.

Although the connection involving the ISCSI read request command is described above as being flushed from network storage device to host computer the connection can be passed back from the host computer to the network storage device such that fast path processing by the network interface device resumes.

Although the command status message is described here in connection with an ISCSI read request command a command status message can be used with other types of commands. For example the command sent bit can be used to indicate that a command other than a read request command was sent from network interface device . A command status message with a command sent bit can be used to indicate that an SMB read command has been sent from network interface device where the SMB read command as received by network interface device was a solicited SMB read command. A command status message can be used with both solicited and non solicited commands and can be used with both ISCSI and non ISCSI commands.

Although network interface device is explained above as writing individual packet data payloads into destination one at a time network interface device in other embodiments writes into destination only once per ISCSI read from disk . In such embodiments individual data payloads of packets are collected on the network interface device until all of the requested data has been properly received onto network interface device . The collected data is then written into destination across PCI bus as a block in a substantially contiguous manner. Alternatively the data from a first group of packets can be collected and then written into the destination at one time and then data for a subsequent group of packets can be collected and then written into the destination at a second time and so forth. In this way a predetermined amount of data is collected before it is written into the destination.

In the embodiment of computer receives and sends over the same single cable both 1 network communications with other devices for example SMB communications with another device on the internet as well as 2 SAN communications with network storage device for example ISCSI communications with network storage device . The same single port on network interface device and the same single cable simultaneously communicate both the network and SAN communications. Both the network and the SAN communications are accelerated using the same fast path processing hardware disposed on a single network interface device .

In some embodiments host computer is coupled to multiple identical network interface devices each device involving its own separate port and cable connection to network . Host computer maintains control over TCP connections but gives temporary control of these connections to appropriate one of its network interface devices. In this way TCP connections float migrate between multiple ISCSI interfaces thereby allowing network failover and link aggregation over multiple network interface devices.

In some embodiments a host computer whose protocol stack includes a session layer for example an ISCSI layer is coupled to a network interface device. The network interface device is for example sometimes called an intelligent network interface card INIC and sometimes called a host bus adapter HBA . A solicited session layer read request command for example an ISCSI solicited read request command is sent from the network interface device to a network storage device and the network storage device sends a response back. The network interface device fast path processes the response such that a data portion of the response is placed into a destination memory on the host computer without the protocol stack of the host computer doing any network layer or transport layer processing. All session layer processing of the response does not occur on the network interface device. Rather some of the session layer processing occurs on the network interface device and the remainder of the session layer processing occurs on the host. See the above description for details. The present patent document suggests applying known techniques for splitting a protocol stack to the objective of realizing the above described network interface device and host computer. Applying teachings and information contained in IP storage standards documentation ISCSI documentation and HBA documentation is also suggested herein.

All told the above described devices and systems for processing of data communication provide dramatic reductions in the time and host resources required for processing large connection based messages. Protocol processing speed and efficiency is tremendously accelerated by an intelligent network interface card INIC containing specially designed protocol processing hardware as compared with a general purpose CPU running conventional protocol software and interrupts to the host CPU are also substantially reduced. These advantages are magnified for network storage applications in which case data from file transfers may also avoid both the host memory bus and host I O bus with control of the file transfers maintained by the host.


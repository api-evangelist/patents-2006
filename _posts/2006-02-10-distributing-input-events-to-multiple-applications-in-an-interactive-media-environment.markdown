---

title: Distributing input events to multiple applications in an interactive media environment
abstract: In an interactive media environment, input events are distributed to a plurality of applications where each application includes zero or more script components and zero or more markup files and has a Z order which corresponds to the position of the applications' visual elements on a display. An input event processing order is utilized where the application that has focus in an interactive media environment (by virtue of its receipt of user events) is provided with the first opportunity to consume the input event. If the focused application does not consume the input event, it is then delivered to the remaining applications in top down, inverse Z order. Each application is enabled with the ability to consume an input event, pass it on to lower Z ordered applications, hide it from lower Z ordered applications, or otherwise handle it. Input event processing stops when an application consumes the input event.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08108787&OS=08108787&RS=08108787
owner: Microsoft Corporation
number: 08108787
owner_city: Redmond
owner_country: US
publication_date: 20060210
---
This application claims the benefit of provisional application No. 60 695 944 filed Jul. 1 2005 which is incorporated by reference herein.

The described arrangements systems and methods relate generally to interactive media and more particularly to distributing input events to multiple applications in an interactive media environment.

Interactive media environments are typically resource constrained in terms of available processing power memory and other resources that are available to applications running in the environment. One common example of interactive media is video encoded on DVD digital versatile disc where users can interact with graphical menus or other controls to navigate to specific video content or invoke special features that are authored into the DVD.

In a typical interactive media environment visual elements such as graphics and menus are given a Z order that provides a visual order for the elements on a display screen. The Z order controls how visual elements appear to stack on top of one another along an imaginary z axis which extends outwardly from the display screen. Visual elements with a lower Z order appear to be at the bottom of display and farther away from a viewer while visual elements with a higher Z order appear to be on top of the lower Z ordered elements and thus closer to the viewer .

When multiple applications used to generate visual elements are running in an interactive media environment input events must be distributed to the applications. Input events must be handled in a resource efficient manner. In addition applications must be enabled to generate graphical output responsively to input from the environment to dynamically create a rich interactive experience for users.

In an interactive media environment input events are distributed to a plurality of applications. Each application includes zero or more script components and zero or more markup files and has a Z order which corresponds to the position of the application s visual elements on a display. An input event processing order is utilized where the application that has focus by virtue of its receipt of user events is provided with the first opportunity to consume the input event. If the focused application does not consume the input event it is then delivered to the remaining applications in top down inverse Z order. Each application is enabled with the ability to consume an input event pass it on to lower Z ordered applications hide it from lower Z ordered applications or otherwise handle it. Input event processing stops when an application consumes the input event.

Advantageously input event delivery in priority order i.e. focused application followed by remaining applications in inverse Z order gives preference to the most interactive applications so that input events are efficiently processed in an environment where hardware resources including processor cycles and memory are limited. In addition a predictable platform from which interactive media authors may create applications with highly responsive interactive behavior is provided.

Referring to an illustrative block diagram of the elements making up an application used in an interactive media environment is shown. Applications are typically used in the interactive media environment to enable interaction between a user and an interactive media player rendering graphics and video on a coupled display device such as a television or monitor through a user interface such as a remote control. More specifically applications control presentation behavior of various content objects including video playback in the environment. Presentation of graphic objects such as menus and interactive buttons over the video is also realized using applications. Applications further manage and control audio playback and sounds in the environment. It is contemplated that multiple applications will generally be running simultaneously in most interactive media settings. However there is no requirement the multiple applications run simultaneously and the decision to divide or aggregate applications in a particular setting is a design choice of the interactive media author. Applications may also be logically subdivided into application pages depending on the requirements of a specific setting.

The application comprises a script host containing zero or more script files and and zero or more markup documents that is used to generate a document object model DOM . The markup documents include information relating for example to content style timing and layout of graphic objects. Thus the markup context is used generally to provide graphics on a graphics plane in the interactive media environment.

In this illustrative example the markup documents are XML document files in accordance with W3C standards. As indicated in multiple physical XML files may be accessed using the element in the section of the markup. In some settings it may be preferable for an application to not have more than one active markup at a time. However an application may switch its markup by using a element in the markup. Alternatively an application may switch its markup by utilizing an application programming interface API that enables applications to gain access to functional objects within a current application. Using a loadMarkup call through the API an application may switch markup files by passing the Uniform Resource Identifier URI of the new markup through an API.

In cases where an application accesses a new markup the API call takes effect only after a current event handler in the application finishes executing its current task. Any current markup related event handlers that are pending are also cancelled as the new markup once loaded will invalidate those event handlers.

In this illustrative example script host contains script files and which are used along with the markup to implement interactive media experiences. Script files and may be implemented for example using ECMAScript as defined by Ecma International in the ECMA 262 specification. Common scripting programming languages falling under ECMA 262 include JavaScript and JScript. In some settings it may be desirable to implement scripts and using a subset of ECMAScript 262 in particular ECMA 327 along with a host environment and a set of common APIs. Script context in most settings is utilized to deal with interactive control issues from user along with system events graphics control video playback resource management e.g. use of caching or persistent store resources and other issues that are not readily or efficiently implemented using solely markup .

The availability of APIs and resources to application is indicated by reference numeral in . Resources include for example audio and video files fonts pictures and images e.g. in common file formats including PNG JPEG GIF BMP TIFF etc. and other resources as may be required by an application according to the circumstances of a specific setting.

Each application maintains its own script host that maintains the context for the script s variables functions and other states. In most settings variables and functions in one application are not visible to another application unless the applications are specifically set up to enable such cross application visibility for example by using an object that is shared across all applications. For example in this illustrative example the interactive media player object has a single instance that is shared across all applications. Optionally therefore special objects may be placed inside script host for example using a C object to implement singletons i.e. a objects having limited instantiation where the special objects all reference the same internal function for example of the player. This optional aspect enables interactive media script authors to logically treat common objects as singletons while still allowing the script host to implement the functionality necessary to expose an object to the single script host.

Referring now to an illustrative diagram showing the relationship among multiple markup documents and script is provided. An application manifest interacts with applications which as noted above are defined generally by resources script and markup documents and as shown. Each application typically uses a single application manifest file in most settings but the application manifest is not part of the runtime state of the application. In this illustrative example the application manifest is encoded as an XML document file.

The application manifest describes the initial markup file to be used by the application as well as the script files collectively indicated by the rectangle with reference numeral in FIG. contained in script host . If the application manifest lists more than one script as in this illustrative example then all the scripts are loaded into a script handling engine in the interactive media player. Thus the multiple script files are treated and behave as if the script author had concatenated all of the script files into a single large file in the order listed in the application manifest .

As shown in the application manifest refers to resources . The resources available to an application in an interactive media environment form a directed graph rooted by the resources referenced in the application manifest . The allowed extent of the graph for each application is proscribed by the application manifest .

The progression of context execution by applications in the interactive media environment is guided by a playlist which describes among other things the relationship among objects in the environment including presentation objects that are rendered by the player onto the display device. These presentation objects typically include video which may include multiple streams as described in more detail below and graphics produced by the applications.

Playlist further manages resources across the interactive media environment as a single management entity in order to efficiently allocate and control the consumption of resources by applications. As with the application manifest the playlist may be advantageously embodied as an XML document file in most settings.

The markup pages in may be used in some settings to fire events into an execution context created by the script files and in . The execution context then manipulates the DOM created by the current application markup. As the markup is used in the interactive media environment to specify style content timing and layout of graphical objects in the environment as represented by elements and in the combination of script and markup enables the creation of a comprehensive set of capabilities.

VCP manages one or more media streams that may be received from multiple sources including a local optical drives such as a DVD drive or a high definition DVD HD DVD drive a local memory or a remote broadband source over a network. VCP in this illustrative example includes one or more media processors . . . N as indicated by elements and in . Media processors and process the received media streams which typically include audio and video to decode and render the corresponding images and sound which are output as an audio video stream on line . Audio video stream may represent a plurality of video elements for example to render multiple separate video windows using a picture in picture type configuration.

Media processors and each comprise a media source interface demultiplexer and decoder. Media processors and may optionally include decryption capabilities as well. A display device is coupled to receive and display the audio video stream.

A media clock is utilized so that each received media has an associated Media Time. When a video stream is paused on the interactive media player then the media clock is paused as well. When the video stream is set by a user to go faster or slower than real time for example when the video is put into fast forward rewind or slow motion modes using any of these modes is referred to as trick play then the media clock speeds up or slows down accordingly. The Media Time is thus derived from the media clock and the operation of the media processors and . The Media Time is passed to the playlist manager in ICP over line . Time in the interactive media environment including Media Time is typically counted in units of ticks. 

ICP performs all application related processing and may be arranged from several components that may be realized in hardware software firmware or a combination thereof. The components of ICP include for example a markup engine script language interpreter and an XML parsing component not shown . ICP outputs a graphics stream on line which is synchronous with the audio video stream . Mixer takes the graphics stream on line and the audio video stream on line so that the graphics are rendered in a graphics layer over the video stream to implement an interactive media session for a user.

In most settings ICP outputs graphics that are synchronized on a frame by frame basis with the video stream. However such synchronization may be performed using other bases including for example time including Title Time and Media time as defined below content in the video or other metadata embedded in the video that is used to indicate or mark a particular point in the stream.

ICP includes a playlist manager and a task manager . The playlist manager is responsible for controlling presentation objects in the environment. These objects include video playback on the player along with applications that are running to generate interactive graphics. Playlist manager manages the playlist which is described above in the text accompanying .

The playlist manager also computes the Title Time associated with each portion of content in a media stream. A title is a unique sequence of video and audio content with a start and end time that is typically defined by the DVD author. However what such author defines as a title can be arbitrary. Thus particular content which is perceived in a video may be part of one title a complete title or run across multiple titles.

One example of a title is the copyright warning that precedes all pre recorded video in both analog and digital format in the United States. The featured attraction e.g. the main movie on a DVD is another example and is often the longest title. In some settings individual chapters in a movie might be designated as separates titles by the DVD author. For all such titles Title Time is defined as the time elapsed since a given title started playing as shown on the media clock .

A presentation clock is coupled to the playlist manager on line . The presentation clock is a clock whose time changes at the same pace as a real world clock i.e. it takes one second of real time for the presentation clock to advance by one second . In contrast to the media clock the presentation clock never stops and cannot be sped up or slowed down. The Presentation Time from the presentation clock is passed to the task manager which uses it to calculate Application Time and application Page Time. 

Application Time is the time elapsed since an application started or enters an Active state as described in more detail below . When multiple applications are in runtime each application has a notion of its own Application Time. For each application Application Time always starts at zero when an application is started in the environment.

For example if an application App starts at Presentation Time of 20 arbitrary time units which is 0 time units for App and application App starts at Presentation Time of 25 time units which is 0 time units for App then at Presentation Time of 35 time units App s Application Time is 15 time units and App s Application Time is 10 time units. For applications that are logically subdivided into pages the Page Time is the time elapsed since a page of an application has been loaded.

The audio video feeds and along with the synchronous graphics stream from ICP are mixed in mixer and output on line to a display device . The other elements in including ICP comprising playlist manager and task manager media clock in VCP and presentation clock are configured and function in a similar manner as their counterparts shown in and described in the accompanying text.

Each of the event queues and are arranged to feed into application thread from their head ends located at the right side of . A plurality of applications App App . . . AppN as designated by reference numerals and respectively are arranged to post workitems representatively designated by reference numeral into the queues and from their tail ends on the left side of .

Application events are events which are fired by an application. These may include events fired by either script e.g script host in or by markup e.g. markup in . Application events in most scenarios are handled only by script. However applications and do not invoke script or markup functionality directly. Instead all such functionality is posted to the applications respective event queues in the form of workitems which are invoked when the application thread processes the workitem.

In alternative arrangements events from sources other than applications are also scheduled using event queues. For example user events are fired by user interaction with a remote control. System events are events fired by the interactive media player such as player shown in and described in the accompanying text and include for example a chapter change in a video title.

Each workitem in events queues and contains fields as shown in . These fields include an application association field a method field a BeginTime field an EndTime field and an optional ClockSelector field .

The application association field indicates the particular application to which a workitem applies. The method field contains a method that is invoked when the workitem is processed by the application thread . Method field also includes arguments for the method.

The BeginTime field and EndTime field are used respectively to indicate when the workitem s method begins and ends. In this illustrative example time is expressed using Application Time. However in alternative examples the BeginTime field and EndTime field contain values which may be alternatively expressed in Title Time Application Time or Page Time depending on the requirements of a particular setting. In such cases the particular timeframe used by a workitem is indicated in the ClockSelector field . Regardless of the timeframe utilized a BeginTime for a workitem must always be less than the EndTime.

Event queue includes workitems . . . N as indicated by reference numerals and respectively. Each workitem includes the fields shown in and described in the accompanying text.

Workitem includes a BeginTimeand an associated time of insertion tinto the event queue as indicated in block in . Similarly workitem includes a BeginTimeand an associated time of insertion tinto the event queue as indicated in block . Workitem includes a BeginTimeand an associated time of insertion tinto the event queue as indicated in block . And workitem includes a BeginTimeand an associated time of insertion tinto the event queue as indicated in block .

In this illustrative example workitems are ordered in the event queue first by BeginTime and then by the time in which workitems were inserted into the event queue. Such ordering results in the application thread processing workitems in order of BeginTime or when two workitems have the same begin time then in FIFO first in first out order.

The ordering of workitems in an event queue is performed using two alternative methods workitems may be ordered when inserted into an event queue or when workitems are extracted from the event queue prior to processing. Either arrangement is equally usable so long as the processing of workitems from the event queue is performed by BeginTime followed by queue insertion time.

Workitem includes calls into the markup engine e g a markup engine disposed in ICP in to process timing for a page in application App as indicated in block . In block workitem includes calls into the markup to reflow application App s markup to reflect processed events and then render the markup on the display device e.g. display in . Workitems and are always the last workitems processed in an application s tick by application thread .

The process starts at block . At block when the application thread is free to process workitems it first marks each workitem in the event queue whose BeginTime corresponds to the current or previous ticks. Application thread will only process marked workitems. Thus a workitem in event queue will never be processed before its BeginTime.

At decision block if a marked workitem s EndTime has already been passed then it is dropped from event queue as indicated in block . No processing on that workitem will be performed in such a case. Should application App reloads its page the application s page clock is reset to zero and all outstanding i.e. queued workitems based on the application s page clock are dropped from event queue just as if they had reached their EndTime.

If at decision block a marked workitem s EndTime has not been passed then control is passed to block and the application thread processes the workitem. As noted above in the description accompanying each workitem is processed in order from the event queue first by BeginTime followed by the time each workitem was inserted into the event queue .

Both repetitive events and one shot i.e. single occurrence non repetitive events are managed using the method shown in . A repetitive event may include a periodic event where the associated workitem has an EndTime that is equal to the next scheduled BeginTime. That is each periodic event has a duration equal to the event s period.

Periodic events typically include events like timer events and application drawing events. For example if an application s script e.g. in script host in creates a timer that will call back once every 10 seconds it will add a timer workitem to the event queue with a BeginTime equal to the current time plus 10 seconds. The EndTime will be set to the BeginTime plus 10 seconds. Once the timer workitem is executed out of the event queue the BeginTimes and EndTimes will be adjusted by adding another 10 seconds and the workitem will be reinserted into the event queue at the proper location based on the new BeginTime.

Periodic events are invoked whenever possible. But if they cannot be processed by the application thread before the EndTime in their associated workitems expires then that particular invocation is dropped and the next invocation is scheduled with a new workitem. Thus periodic events are subject to workitem timeout.

Advantageously the event queuing method enables a parameter may be passed to timer events to indicate the time that the event is to be invoked. This parameter must be the same as the BeginTime in the associated workitem. Script associated with a periodic timer event might not be run exactly at the invoked time as noted above. However as each workitem includes a method field that specifies arguments to the method the argument s value will reflect an intended time of invocation and not the actual time. Accordingly the handler for a timer event will know what time i.e. tick it is handling.

A one shot event has a corresponding workitem with an EndTime of INFINITE. Therefore a one shot event will never be dropped from the event queue . For example if a one shot event is an input event then that event s handler is scheduled as a workitem in the event queue with an EndTime of INFINITE.

As indicated in block the processing is performed on a committed basis. That is once the application thread begins processing a workitem from the event queue it does not stop processing. For example script which may be long running is not aborted nor are exceptions injected into the script in order to throw it out. While such a scheme can tie up the application thread while it processes script as noted above the ICP e.g. ICP in may be arranged to include other threads which continue to run during the committed processing of workitems.

At block any new workitems that are created during the processing of marked workitems are inserted into the event queue after the marked workitems regardless of their BeginTime. The process of marking workitems committing to them and inserting new workitems after the committed workitems in an event queue as shown in blocks and ensures that the applications are always afforded some visible progress.

As indicated at block and in the application thread automatically inserts two workitems into each application s event queue for each tick as shown in and described in the accompanying text. These workitems call into the markup engine for each application to evaluate application timing and then reflow and render the markup on a display device. As noted above the workitems are inserted upon application start and a rescheduled after each tick. In addition the two workitems are always the last two to be processed for an application s tick and are treated as periodic events that may be dropped from the event queue .

All planes in the graphics plane set use a common xy coordinate system called a canvas. A third dimension is described by a z axis which projects outwardly from the display as indicated by reference numeral in . Applications running in an interactive media environment belong to specific planes. Applications cannot be explicitly or arbitrarily assigned to planes the association is made implicitly from the type of application. For example subtitle applications render in the subpicture plane while most other applications render in the graphics plane .

The graphics plane is the second plane of the graphics plane set and is generated by the presentation engine as described below. As noted above applications that generate interactive content such as graphics and menus in an interactive media environment are typically rendered into the graphics plane .

The subpicture plane is the third plane of graphics plane set and is typically used to display subtitles and or captions produced by respective applications. The subvideo plane is the fourth plane in graphics plane set and is typically used as a secondary video display in a picture in picture PIP arrangement. A PIP window like that indicated by reference numeral is often smaller than the main video display and may have other differing characteristics such as reduced resolution different aspect ratio etc.

The main video plane is the fifth plane in the graphics plane set and is positioned at the bottom of the stack of planes. The main video plane is typically used to display video content in the interactive media environment including video titles as described above in the description accompanying . As shown in all the planes in graphics plane set are mixed and composited into a single display by a graphics rendering engine as described below.

Application AppA generates three visual elements including element cross element star and element arrow with Z order of 0 1 and 2 respectively that are in a stacked arrangement as shown. Thus for a set of N elements a set of Z order values starts at 0 for the lowest element in the stack and ends at N 1 for the highest element in the stack. Elements and are drawn on a display . Window in shows the Z order of the visual elements produced by AppA as if looking at the display edgewise. Display and window are oriented with respect to each other as indicated by xyz coordinates .

Five applications are running in an interactive media environment including AppA AppB AppC AppD and AppE as indicated by reference numerals and respectively. The applications are composited bottoms up in Z order in window as indicated by arrow . Compositing starts with application AppA with a Z order 0 and end with application AppD with a Z order 4.

In this illustrative example as shown at the instant in time shown by FIG application AppC is the focused application producing visual elements that receives user events. However by virtue of running in a dynamic environment the particular application having focus typically changes over time by design and all applications can move up and down in Z order typically as a video title progresses although it is also emphasized that applications are able to change Z order as they run and operate even in cases when the main video is stopped paused or frozen . In addition the focused application can be in any position in the stack i.e have any Z order . In many scenarios however the focused application will be towards the top of the display order so that it can interact with the user and receive user events such as button pushes e.g. from remote control mouse clicks etc. in a manner that is consistent with user expectations.

As indicated by arrow applications are rendered in priority order starting with the focused application e.g. application AppD followed by the remaining applications in top down or inverse Z order.

Applications are enabled to invoke methods to manipulate their relative Z ordering as the video title progresses in block . Such methods include for example a moving the application to the top of the Z order within a plane b moving the application to the bottom of the Z order within a plane and c changing an application s Z order to be immediately above the application above it in the display stack.

As noted above applications belong to specific planes which places a limit on how far an application can move in Z order through invocation of a Z order manipulation method. For example calling a method to move a subtitle application to the top will make it the topmost application subtitle application in the subpicture plane but visual elements generated therein might still be occluded i.e. obscured by visual elements rendered into the graphics plane by applications in that plane. Similarly calling a method to move a regular interactive application to the bottom will move make it the bottom most application in the graphics plane but will still leave it above all subtitle applications.

At block an application receives an event to provide a notification that the application s Z order has changed. Such notification is alternatively arranged to be received when an application transitions to or away from the topmost application. Such notification would enable for example a game application to pause itself when a menu produced by menu generating application moves to a position with the highest Z order.

The illustrative method continues at block where visual elements from applications are rendered into one or more graphics buffers. The rendering is performed in priority order in most scenarios which starts with the focused application followed by the remaining applications in inverse Z order from the top of the display downwards. Drawing i.e. two dimensional 2D drawing is handled by drawing into a canvas object in the application s markeup e.g. markup in which is then rendered into the graphics buffer in much the same way as rendering a PNG Portable Network Graphics file.

The buffering step is arranged in the illustrative method shown in so that applications are rendered by posting workitems corresponding to periodic render events in respective application event queues. Event queues and associated methods that may be used with this arrangement are shown in and described in the accompanying text.

The posting of workitems means that the rendering of application frames is subject to workitem timeout. Doubled buffering of applications is accordingly performed to accommodate the possibility that new application frames may not always be rendered for each video frame if priority is placed elsewhere in the playback system. In such a case the most recently available frame will continue to be shown until the new one has been completed.

At block prior to compositing the display each application is checked to see if it includes a element in its markup e.g. markup in . Such element is included to produce an opening i.e. a hole in the graphics produced by the application through which the main video plane may be seen. The element produces a rectangular shaped opening but other regular geometric shapes irregular shapes and other arbitrary shapes are also usable depending on requirements of a specific setting.

Elements such as and other such opening producing elements are not included in the graphics buffers at such time the applications are rendered in block of the method. Instead the holes are cleared in applications with lower Z order than the application with the element during compositing so that the main video plane is seen through the holes in the applications. Since rendering is performed in a prioritized manner it is not practical to implement the element at the time the applications are rendered into the buffer. If it were so implemented and a lower priority application did not get immediately updated then there would be a possibility that a hole could inappropriately appear. This would occur for example if Z order changed or the application with the element terminated while the lower priority application was waiting to be updated while in timeout.

At block applications are composited from the bottom up in Z order as shown in and described in the accompanying text. In most settings the simple Painter s Algorithm is useable where each application s markup in rendered into the graphics plane using the most recent buffered frame for each application. In addition as described above or other opening producing elements are implemented during the compositing step. The illustrative method ends at block .

The navigation manager is coupled to a presentation engine and a graphics rendering engine . The presentation engine is arranged in most settings to present each of the planes in the graphics plane set to the graphics rendering engine which performs compositing of the planes and applications therein onto the display .

Disposed between the presentation engine and the graphics rendering engine are one or more graphics buffers . . . N as indicated by reference numerals and respectively. The N graphics buffers are arranged to map to respective applications on a one on one basis. That is N N. In alternative arrangements a single buffer indicated by reference numeral is utilized for all applications or graphics buffers are mapped to applications on an unequal basis.

Input events are first delivered to the focused application followed by all remaining applications in the plurality of N applications in inverse Z order. Accordingly at block an input event is delivered to the focused application. Input events in this illustrative method include user events system events application events and navigation events. User events system events and application events are described above in the text accompanying . Navigation events typically include events from a remote control e.g. remote control in that correspond to the button pushes for up down left right etc.

At decision block if the focused application consumes the input event then input event processing stops at block and the method terminates. If the focused application does not consume the input event then at block the input event is delivered to the application having the highest Z order among the plurality of applications. If the highest Z ordered application consumes the input event then input event processing stops at block and the method terminates.

If the highest Z ordered application does not consume the event then delivery of the input event is made to the next highest Z ordered application. Z order is counted down in inverse order at block and the method continues iteratively through block until either an application consumes the input event or there are no more applications to which the input event is deliverable. In this latter case input event processing stops at block and the method terminates.

Input event processing within an application proceeds as if the application and markup objects for example from markup in are all arranged in a single DOM with the application object at the top. Consistent with the W3C eventing model such arrangement enables the application object to capture input events before they are passed to the application s markup. At each object that object is afforded an opportunity to capture the input event. Once a capture occurs the input event handler has the option to stop further input event processing. Input events that are not handled in the markup bubble back up through the application object. In this way event handling within an application is unified with the model for event handling across applications. Note that in most settings markup may only handle user and navigation events. Application and system events are typically handled only through script.

Many visual elements in an application s markup are arranged to accept an accesskey attribute that assigns a specified key to a particular visual element. Accordingly any visual element in an application that has a matching accesskey attribute will consume the input event after handling it. Navigation events are also processed in a similar manner.

Application events implemented using an element in markup create and schedule corresponding workitems into an event queue as shown in and described in the accompanying text. Accordingly the element will not appear as a function call to its event handler. This occurs because new workitems are inserted into the event queue and handled after all marked workitems as discussed above which will most likely occur in the next tick. In addition as events are delivered to all in applications in priority order i.e. focused application followed by the remaining application in inverse Z order an application event may be handled by an application other than the one that created it. System events are also created and scheduled using the workitem paradigm.

Input events handled by script present a situation that differs from that with markup because an ICP such as ICP in does not know in advance whether script will handle a particular input event. Thus input events handlers for script must return a flag to indicate whether an input event was consumed returning true indicates that the input was consumed returning false indicates that the input event was not consumed.

Application script is provided with the first opportunity to process input events that have not been handled. Thus script may override the default player behavior implemented by markup for accesskey and navigation input events.

It is noted that consumed and handled as used herein can often mean the same thing. However in some scenarios they are not exactly the same. For example an application may want to consume an input event even though it does do anything with it in order to hide the input event from lower Z ordered applications. Alternatively an application may take some action to handle an event yet return a false flag indicating that the input event was not handled so that it can propagate further down the Z order.

An example of the illustrative method shown in is now described. A user is running an interactive game that responds to the up down left and right buttons on a remote control e.g. remote control in . The game is the topmost application on a display i.e. it has a Z order N 1 . The game is not designed by its author to handle a menu button on the remote control.

A menu application is running simultaneously with the game. It has a lower Z order and either appears below the game or perhaps it has no visual elements on the display and hence does not appear at all . As the menu application has a lower Z order than the game it only receives input events that are not handled by the game. As the user is pressing the up down left and right keys on the remote control the game handles those keys either in markup or though a script handler that returns a true flag for those keys and false for everything else.

When the user decides to stop the game the menu button on the remote control is pressed. This input event is first delivered to the game application as the focused application which does not handle it. The input event is then delivered to the menu application which responds by invoking a method to move itself to the top of the display with the highest Z order this method is described in the description accompanying . Such movement ensures that future up down left and right input events from the remote control will go first to the menu application so that the user can navigate through the menus.

It is noted that for the sake of clarity and ease of illustration in the description above that data programs and other executable program components such as operating systems are shown is discrete blocks boxes or other elements although it is recognized and emphasized that such programs and components may reside at various times in different storage memory or processing components of any hardware host used and are executed by one or more processors in such host hardware.

Although various illustrative arrangements and methods for managing application states in an interactive media environment have been shown and described it should be understood that the scope of the claims appended hereto shall not necessarily be limited to the specific features arrangements or methods described. Instead the specific features arrangements or methods are disclosed as illustrative forms of implementing managed applications states in an interactive media environment as more particularly claimed below.


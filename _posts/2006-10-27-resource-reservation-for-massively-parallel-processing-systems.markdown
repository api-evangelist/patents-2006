---

title: Resource reservation for massively parallel processing systems
abstract: Disclosed are a method, information processing system, and computer readable medium for reserving resources in a massively parallel processing system comprising. The method includes receiving a reservation request for reserving at least one resource within a massively parallel processing system. The massively parallel processing system includes compute nodes, base partitions, switches, wires, and node cards. The reservation type associated with the reservation is determined. The reservation type is at least one of a number of compute nodes, a base partition list, a particular shape configuration, a particular job, and a particular partition. A set of requirements is generated based on the reservation type. A set of resources is identified for satisfying the set of requirements and the set of resources is reserved.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07716336&OS=07716336&RS=07716336
owner: International Business Machines Corporation
number: 07716336
owner_city: Armonk
owner_country: US
publication_date: 20061027
---
This application is a continuation in part from prior U.S. patent application Ser. No. 11 414 029 entitled RESOURCE RESERVATION SYSTEM METHOD AND PROGRAM PRODUCT USED IN DISTRIBUTED CLUSTER ENVIRONMENTS filed on Apr. 28 2006 now abandoned the entire disclosure of which is herein incorporated by reference.

The present invention generally relates to the field of scheduling jobs in a computing environment and more particularly relates to scheduling jobs in a massively parallel processing system.

In a distributed cluster of computers a job scheduler is often used to manage when and where jobs run for more efficiently utilizing the resources in the cluster. A job scheduler receives job requests from end users and then dispatches the jobs to various computers. Job scheduling needs to be performed in such a way that critical workload can run when desired and on the machine resources required. Additionally a job scheduling scheme should be able to efficiently set aside time on a set of machines for maintenance purposes.

One job scheduling function is advance reservation which can enable computing resources to be reserved in advance so that they can be dedicated to run a certain workload or be used for maintenance purposes. However current job schedulers support advance reservations are not compatible with massively parallel processing systems such as a Blue Gene L system of International Business Machines Corporation IBM of White Plains N.Y. United States of America. A job associated with a Blue Gene L system is run on a subset of Blue Gene L hardware components. These components are exclusively allocated to that job. Current job schedulers incorporating advance reservation are not capable of taking into account the specific resources of a massively parallel processing system and cannot perform advance reservation for these systems.

Briefly in accordance with the present invention disclosed are a system method and computer readable medium for reserving resources in a massively parallel processing system. The method includes receiving a reservation request for reserving at least one resource within a massively parallel processing system The massively parallel processing system includes compute nodes base partitions switches wires and node cards. The reservation type associated with the reservation is determined. The reservation type is at least one of a number of compute nodes a base partition list a particular shape configuration a particular job and a particular partition. A set of requirements is generated based on the reservation type. A set of resources is identified for satisfying the set of requirements and the set of resources is reserved.

In another embodiment an information processing system for reserving resources In a massively parallel processing system is disclosed. The information processing system includes a memory and a processor that is communicatively coupled to the memory. An advance reservation module is communicatively coupled to the memory and the processor. The advance reservation module is for receiving a reservation request for reserving at least one resource within a massively parallel processing system. The massively parallel processing system includes compute nodes base partitions switches wires and node cards. The reservation type associated with the reservation is determined. The reservation type is at least one of a number of compute nodes a base partition list a particular shape configuration a particular job and a particular partition. A set of requirements is generated based on the reservation type. A set of resources is identified for satisfying the set of requirements and the set of resources is reserved.

In another embodiment a computer readable medium for reserving resources in a massively parallel processing system is disclosed. The computer readable medium comprises instructions for receiving a reservation request for reserving at least one resource within a massively parallel processing system. The massively parallel processing system includes compute nodes base partitions switches wires and node cards. The reservation type associated with the reservation is determined. The reservation type is at least one of a number of compute nodes a base partition list a particular shape configuration a particular job and a particular partition. A set of requirements is generated based on the reservation type. A set of resources is identified for satisfying the set of requirements and the set of resources is reserved.

One advantage of the present invention is that an advance reservation system ARS for a massively parallel processing system is provided. The advance reservation system reserves resources for special purposes such as running a high priority workload or performing maintenance during a specified period of time and optimizes reservation related balancing for the massively parallel processing system. The ARS is able to handle advance reservation of resources for the massively parallel processing system.

As required detailed embodiments of the present invention are disclosed herein however it is to be understood that the disclosed embodiments are merely exemplary of the invention which can be embodied in various forms. Therefore specific structural and functional details disclosed herein are not to be interpreted as limiting but merely as a basis for the claims and as a representative basis for teaching one skilled in the art to variously employ the present invention in virtually any appropriately detailed structure. Further the terms and phrases used herein are not intended to be limiting but rather to provide an understandable description of the invention.

The terms a or an as used herein are defined as one or more than one. The term plurality as used herein is defined as two or more than two. The term another as used herein is defined as at least a second or more. The terms including and or having as used herein are defined as comprising i.e. open language . The term coupled as used herein is defined as connected although not necessarily directly and not necessarily mechanically. The terms program software application and the like as used herein are defined as a sequence of instructions designed for execution on a computer system. A program computer program or software application may include a subroutine a function a procedure an object method an object implementation an executable application an applet a servlet a source code an object code a shared library dynamic load library and or other sequence of instructions designed for execution on a computer system.

According to an embodiment of the present invention as shown in an exemplary massively parallel processing system is illustrated. The example of is directed towards a Blue Gene L system. However the present invention is applicable to any massively parallel processing system comprising compute nodes base partitions switches wires and node cards. The massively parallel processing system in this embodiment includes a plurality of compute nodes . Each compute node comprises a processor. A compute node kernel CNK resides within each compute node . The CNK is a lightweight kernel that forwards system calls off the compute node for service.

The compute nodes are the computing engines of the massively parallel processing system . The massively parallel processing system also includes I O nodes which are similar to the compute nodes . For example an I O node also includes a processor. However an I O node can comprise additional memory and includes network hardware such as an Ethernet port. The compute nodes communicate with the I O nodes for receiving data to process. The I O nodes include a control and I O daemon CIOD . The CIOD is a process that receives the I O request from the CNK .

The Ethernet port allows the I O node to connect to a network such as the gigabit functional network for performing file I O operations. The gigabit network provides the massively parallel processing system with a means for exchanging data and to load programs. For example a file server communicatively coupled to the gigabit network provides a file system that is accessible by the I O nodes . The programs and data to be processed by the massively parallel processing system are prepared outside of the system . In other words the massively parallel processing system does not comprise local disks within the system . Therefore the file server comprises a global file system that is shared by the massively parallel processing system via the I O nodes with a service node and front end nodes .

A node card in the massively parallel processing system can hold 32 compute nodes and optional I O nodes. A midplane or base partition BP can include 16 compute cards or 512 compute nodes plus a number of I O nodes. A rack can hold two midplanes and the massively parallel processing system in one embodiment can connect up to 64 racks.

The service node is communicatively coupled to the massively parallel processing system . The service node controls and manages the system . In one embodiment the service node monitors the configuration of the system and provides a user administrator the ability to initiate any action on the system . For example through the service node the system can be managed and partitioned. Additionally compute nodes and I O nodes in any of the partitions can be booted and have jobs submitted to them through the service node .

The massively parallel processing system is also communicatively coupled to front end nodes which are information processing systems. The front end nodes allow users to interact with the massively parallel processing system . For example a user can log on to a font end node and submit a job to the massively parallel processing system . The service node allocates the necessary resources on the system for the submitted jobs to run. The service node and the front end nodes in this embodiment are not part of the system . They form a cluster with the system referred to as a massively parallel computing cluster. A massively parallel computing cluster may include other information processing systems which are not a service node or a front end node to the massively parallel processing system .

To run an application or job on the system an initiating program in one embodiment is started on a front end node and the application executable is started on a set of massively parallel computing resources called a partition. A partition can be defined by the system administrator or the job scheduler for the massively parallel computing cluster. In one embodiment one partition is entirely dedicated to a user job. Other jobs do not have access the partition while the current job is running. Once the current job is finished the partition may be freed for another user.

The communication networks inside a partition are isolated from the rest of the massively parallel processing system . A partition usually contains multiple BPs connected via switches and wires to form a TORUS or MESH network and a 3D shape. Each partition also includes at least one system I O node . In one embodiment the I O node is required because without it communication between the compute nodes and the external file system is not possible. Small partitions of 32 or 128 compute nodes are also possible. As a job has to be run within a partition and only certain sizes are possible for a partition a partition may include more resources than actually required by a job. The massively parallel processing system and its components is discussed in greater detail in the Published Application No. 2004 0103218 entitled Novel Massively Parallel Supercomputer which is commonly owned by International Business Machines Corporation IBM of White Plains N.Y. United States of America and is hereby incorporated by reference in its entirety.

In one embodiment the massively parallel processing system utilizes separate networks for different types of communications as compared to implementing a single type of network capable of transmitting all protocols needed in such an environment. The first network is a three dimensional torus network that allows for point to point messaging between the compute nodes. For example shows a small portion of a three dimensional torus geometry interconnecting eight computing nodes out of a typical base torus configuration of 512 computing nodes. shows a compute node being connected to its six neighbors by bidirectional torus link . It is understood however that other architectures comprising more processing nodes in different torus configurations e.g. torus in 4 or more dimensions are contemplated including those described in commonly owned co pending U.S. patent application Ser. No. 10 468 993 Publication Number US 2004 0103218 entitled Novel Massively Parallel Supercomputer .

As can be seen from the torus architecture connects the nodes along logical x y and z axes. A torus can be created from a mesh by connecting the opposite cubes in a closing loop. With respect to the torus network it is preferably of 3 dimensional design supporting cuboidal or rectangular communication and partitioning. A 3 Dimensional design allows a direct mapping of computational simulations of many physical phenomena to the Torus network. However higher dimensionality i.e. 4 5 or 6 dimensional Toroids allows shorter and lower latency paths at the expense of more chip to chip connections and significantly higher cabling costs. Lower dimensions give longer latency and fewer chip to chip communications. The torus network provides both adaptive and deterministic minimal path routing and is deadlock free.

The next network is referred to as a collective network. The three dimensional torus is an efficient network for communicating with neighbors. However during program run some calls are more global than others such as all to one one to all and all to all. For these the massively parallel processing system provides the collective network. The collective network connects all the compute nodes in the shape of a tree. In a collective network any node can be the tree root originating point .

Another network included in the massively parallel processing system is a barrier global interrupt network. The barrier network is used by the system for coordinating tasks. For example the work being performed by multiple tasks running in parallel need to be coordinated by parallel applications. The barrier network provides a network for this coordination.

As discussed above all interactions between the compute nodes and the outside world are carried through the I O nodes under the control of the service node . Therefore the massively parallel processing system provides two networks connecting the service node to the I O nodes. These networks are the gigabit network and the service network which is essentially another Ethernet network but is converted to the internal JTAG network via service cards. The gigabit functional network is used to mount the global file system for allowing the system access to file I O. The I O nodes further communicate to compute nodes through the collective network. The service network JTAG network grants the service node direct access to the massively parallel processing system nodes. The service network is used to boot the nodes initialize the hardware load the kernel and the like . Each node card has a chip that converts the JTAG connections coming from both compute and I O nodes into a 100 Mbps Ethernet network which is further connected to the service node .

When a user or an information processing system coupled to the massively parallel processing system wants to run a job it must be scheduled by the job scheduler for workload balance and resource usage optimization. The scheduler of the present invention provides an advance reservation system ARS for the massively parallel processing system . In one embodiment components of the service node such as the scheduler comprise the ARS . It should be noted that the ARS can be a separate module residing within the service node . The advance reservation system manages resource reservation for the system . ARS provides for resource management in advance by granting resource reservation requests when possible. Only jobs designated to be eligible are allowed to run on reserved resources or in certain cases when resource availability is not an issue or when special or preemptive conditions allow it other jobs can also run.

The ARS is able to handle advance reservation of resources for the massively parallel processing system . A massively parallel processing system job executes on a partition which is a collection of c nodes compute nodes BPs base partitions switches wires and node cards. Furthermore a corresponding initiating and monitoring job which is called mpirun executes on a machine which is called the FEN Front End Node . Advance reservation on Blue Gene allows the users to specify one of the following 

As discussed above a massively parallel computing cluster is a distributed cluster of information processing systems including a massively parallel processing system and a number of other information processing systems referred to from hereon in as regular systems. The regular systems may act as the service node or the front end node of the massively parallel processing system . A job to be run on the system is called a massively parallel computing job. A job that is run on the regular systems is referred to as a regular job. A reservation used for running only regular jobs is referred to as a regular reservation. A reservation used for running a massively parallel computing job is referred to as a massively parallel computing reservation. A reservation that can be used to run both regular jobs and massively parallel computing jobs is referred to as an all purpose reservation.

The most common use of a reservation is to run jobs therefore the massively parallel computing resources that are reserved by the ARS must be able to run at least one job. In other words there must be enough resources to form at least one massively parallel computing partition. A partition includes compute nodes I O nodes switches and wires. The ARS reserves all of these components for a massively parallel computing reservation In one embodiment the most direct method for specifying what to reserve for a massively parallel computing reservation is to specify a list for each type of the system components. However this is only plausible for small reservations like a node card or a few BPs. For a large massively parallel computing reservation this is very difficult to do as there are too many massively parallel computing components to specify. In addition this direct method requires expert knowledge to make a sensible selection of the massively parallel computing components to avoid wasting resources resources must be related in certain ways to be able to form a partition for running a job .

Therefore the ARS of the present invention provides more practical ways to specify what resources to reserve. For example the ARS allows a user or information processing system to reserve resources by specifying a predefined partition specifying a job specifying the number of compute nodes specifying the shape of the BPs to reserve and specifying a list of BPs. If a job is specified for a resource reservation request the ARS reserves enough resources to form a partition which can satisfy the requirements specified in the job. The scheduler schedules the job so that the required resources for the job can be placed in the reservation. Specifying the number of computes nodes can be considered a special case of specifying a job. The ARS can create a pseudo job and let it go through job scheduling to find the resources needed for the reservation. The network connection can be defaulted to be torus or mesh or be taken as additional input.

Specifying the shape of the BPs to reserve is similar to specifying the number of compute nodes. When the shape is specified the specification includes both the number of BPs to reserve and what kind of 3D shape the required partition shape. When a list of BPs is specified the ARS reserves the entire set of BPs that were specified. Additional switches and wires can also be added to a reservation. However this is allowed only if those switches and wires are used by the BPs in the reservation and not by any BPs outside of the reservation. A reservation may be made to run a plurality of jobs and the additional wires allow for more jobs to run at the same time in the reservation. Also a torus network connection needs more wires than the mesh network connection.

In one embodiment a front end node is needed to run a massively parallel computing job. Therefore a massively parallel computing reservation can include a front end node . In most cases a massively parallel computing job only uses a very small amount of the resources on the front end node. In some installations only one or two front end nodes may be available to handle all of the massively parallel computing jobs. In such cases it is more advantageous to not include a front end node in the massively parallel computing reservation. Instead one or more of the front end nodes are marked as reservation not permitted and allow all massively parallel computing jobs whether running inside a reservation or not to share the front end nodes .

In a massively parallel computing cluster reservations can still be made either for running jobs or for maintenance purposes. Generally the front end nodes and service node are not be used for regular reservations. This prevents the massively parallel processing system from being under used. A general purpose reservation can be thought of as the combination of two reservations a massively parallel computing reservation and a regular reservation. A general purpose reservation is not always necessary and can he made if there is a need for such kind of reservation.

A job must be bound to a reservation in order for it to be run in the reservation. In one embodiment a regular job can be bound to a regular reservation or a general purpose reservation. A massively parallel computing job can be bound to a massively parallel computing or a general purpose reservation. If a massively parallel computing reservation includes a front end node the jobs bound to the reservation only use resources reserved by the reservation. If a massively parallel computing reservation does not include a front end node a front end node outside of the reservation is shared in order to run the bound massively parallel computing jobs.

Reservations cannot overlap that is a resource can not be reserved by two reservations at the same time. Policies can be set for not allowing a job and a reservation to overlap. In the process of making a reservation checks are made for ensuring that the resource to be reserved is not already in use by another reservation or a job during the requested reservation time period. A list or a hash table can be used to gather either the free resources or the used resources to help with the resource availability checking. A massively parallel processing system includes a large number of wires and switches and other components. Therefore a more advantageous way to manage the availability checking is to mark a component as free or used before checking each reservation request. A resource component is considered free if it is free not reserved during the requested reservation time period.

All the features and policies available for a regular reservation can also be shared by a massively parallel computing reservation. A massively parallel computing reservation can have the option of being automatically removed by the job scheduler if it is idle. The massively parallel computing reservation can have its resources shared with outside jobs if all of its bound jobs have found resources to run. A bound job can be allowed to run beyond the end time of the reservation or only jobs that can finish before the reservation end will be allowed to run.

The service node includes a computer . The computer has a processor that is communicatively connected to a main memory e.g. volatile memory non volatile storage interface a terminal interface and a network adapter hardware . A system bus interconnects these system components. The non volatile storage interface is used to connect mass storage devices such as data storage device to the service node . One specific type of data storage device is a computer readable medium such as a CD drive which may be used to store data to and read data from a CD or DVD or floppy diskette not shown . Another type of data storage device is a data storage device configured to support for example NTFS type file system operations.

The main memory includes the scheduler the database and the MMCS . The scheduler in one embodiment includes the ARS module . These components were discussed in greater detail above. Although illustrated as concurrently resident in the main memory it is clear that respective components of the main memory are not required to be completely resident in the main memory at all times or even at the same time. In one embodiment the service node utilizes conventional virtual addressing mechanisms to allow programs to behave as if they have access to a large single storage entity referred to herein as a computer system memory instead of access to multiple smaller storage entities such as the main memory and data storage device . Note that the term computer system memory is used herein to generically refer to the entire virtual memory of the service node .

Although only one CPU is illustrated for computer computer systems with multiple CPUs can be used equally effectively. Embodiments of the present invention further incorporate interfaces that each includes separate fully programmed microprocessors that are used to off load processing from the CPU . Terminal interface is used to directly connect one or more terminals to computer to provide a user interface to the computer . These terminals which are able to be non intelligent or fully programmable workstations are used to allow system administrators and users to communicate with the thin client. The terminal is also able to consist of user interface and peripheral devices that are connected to computer and controlled by terminal interface hardware included in the terminal I F that includes video adapters and interfaces for keyboards pointing devices and the like.

An operating system not shown according to an embodiment can be included in the main memory and is a suitable multitasking operating system such as the Linux UNIX Windows XP and Windows Server operating system Embodiments of the present invention are able to use any other suitable operating system or kernel or other suitable control software. Some embodiments of the present invention utilize architectures such as an object oriented framework mechanism that allows instructions of the components of operating system not shown to be executed on any processor located within the client. The network adapter hardware is used to provide an interface to various networks . Embodiments of the present invention are able to be adapted to work with any data communications connections including present day analog and or digital techniques or via a future networking mechanism.

Although the exemplary embodiments of the present invention are described in the context of a fully functional computer system those skilled in the art will appreciate that embodiments are capable of being distributed as a program product via CD ROM or other form of recordable media or via any type of electronic transmission mechanism.

The ARS at step determines whether the reservation requires a front end node to be reserved. If the result of this determination is positive the ARS at step adds the front end node request to the requirements. The control then flows to step . If the result of the above determination is negative the ARS at step tries to locate resources that satisfy the requirements at reservation start time and for the entire duration of the reservation. The ARS at step determines if resources have been located. If the result of this determination is negative the ARS at step determines that the reservation cannot be granted. If this occurs the user or system is notified. The control flow then exits at step .

If the result of the determination at step is positive the located resource s is reserved for reservation. The control flow then exits at step . Returning to step if the result of the determination is negative the ARS at step determines if the reservation request specified a BP list. If the result of this determination is positive the ARS at step creates a set of requirements based on the requested BP list. The ARS then performs the processes discussed above with respect to steps and . If the result of the determination at step is negative the ARS at step determines if the reservation specifies a particular shape. If the result of this determination is positive the ARS at step creates a set of requirements based on the specified shape.

The ARS then performs the processes discussed above with respect to steps and . If the result of the determination at step is negative the ARS at step determines if the request specified a particular job. If the result of this determination is positive the ARS at step creates a set of requirements based on the specified job. The ARS then performs the processes discussed above with respect to steps and . If the result of the determination at step is negative the ARS at step determines if the request specified a particular partition. If the result of this determination is positive the ARS at step creates a set of requirements based on the specified partition. The ARS then performs the processes discussed above with respect to steps and . If the result of the determination at step is negative the ARS at step determines that the reservation request cannot be granted and the control flow exits at step 

If the result of the determination at step is negative the ARS at step determines if jobs that cannot overlap with the request reservation exist. Generally a reservation is not allowed to reserve resources expected to be used by a currently running job. In special situations administrators are allowed to create a reservation overlapping with a running job. If the result of this determination is positive the ARS at step identifies the next job J that will be running at some point during the requested reservation time period and that cannot overlap with the request reservation. The resources assigned to the job J at step are marked as in use . The control flows back to step where the ARS performs steps and for each remaining job that cannot be cut by reservation. If no more of these jobs exist the control flow continues at step .

If the result of the determination at step is negative the ARS at step identifies resources to satisfy the set of requirements in the current reservation request. The ARS in one embodiment only selects resources that are marked as free and resources that are marked as in use are considered unavailable. The ARS at step determines if enough resources have been identified to satisfy the reservation request. If the result of this determination is negative the ARS at step determines that enough resources have not been identified. The control flow then exits at step . If the result of the determination at step is positive the ARS at step determines that the process was a success and the control flow exits at step .

The ARS at step clears include and exclude flags from massively parallel computing resources. The resources that are part of the job s reservation at step are marked. The ARS at step retrieves the next reservation R from a reservation list that overlaps with the job excluding the job s reservation. The ARS at step marks exclude flags of resources that are part of the overlapping reservation R. The ARS at step determines if any more overlapping reservations exist. If the result of this determination is positive the control flows back to step where the ARS performs steps and until no more overlapping reservations exist.

If the determination at step is negative the ARS at step schedules the job within resources that have include flags marked. Resources with the exclude flag marked are ignored. The ARS at step determines if a solution for the job was found. If the result of this determination is negative the ARS at step determines that the job cannot be scheduled. The control flow then exits at step . If the result of the determination at step is positive the ARS at step starts the job on the selected resources. The control flow then exits at step .

If the result of the determination at step is positive the request at step is rejected and the job is not bound to the reservation. The control flow then exits at step . Returning to step if the result of this determination is positive the ARS at step determines if the reservation includes massively parallel computing resources. If the result of this determination is positive the ARS at step binds the job to the reservation. The control flow then exits at step . If the result of the determination at step is negative the request at step is rejected and the job is not bound to the reservation. The control flow then exits at step .

The present invention as would be known to one of ordinary skill in the art could be produced in hardware or software or in a combination of hardware and software. However in one embodiment the invention is implemented in software. The system or method according to the inventive principles as disclosed in connection with the preferred embodiment may be produced in a single computer system having separate elements or means for performing the individual functions or steps described or claimed or one or more elements or means combining the performance of any of the functions or steps disclosed or claimed or may be arranged in a distributed computer system interconnected by any suitable means as would be known by one of ordinary skill in the art.

According to the inventive principles as disclosed in connection with the preferred embodiment the invention and the inventive principles are not limited to any particular kind of computer system but may be used with any general purpose computer as would be known to one of ordinary skill in the art arranged to perform the functions described and the method steps described. The operations of such a computer as described above may be according to a computer program contained on a medium for use in the operation or control of the computer as would be known to one of ordinary skill in the art. The computer medium which may be used to hold or contain the computer program product may be a fixture of the computer such as an embedded memory or may be on a transportable medium such as a disk as would be known to one of ordinary skill in the art.

The invention is not limited to any particular computer program or logic or language or instruction but may be practiced with any such suitable program logic or language or instructions as would be known to one of ordinary skill in the art. Without limiting the principles of the disclosed invention any such computing system can include inter alia at least a computer readable medium allowing a computer to read data instructions messages or message packets and other computer readable information from the computer readable medium. The computer readable medium may include non volatile memory such as ROM Flash memory floppy disk Disk drive memory CD ROM and other permanent storage. Additionally a computer readable medium may include for example volatile storage such as RAM buffers cache memory and network circuits.

Although specific embodiments of the invention have been disclosed those having ordinary skill in the art will understand that changes can be made to the specific embodiments without departing from the spirit and scope of the invention. The scope of the invention is not to be restricted therefore to the specific embodiments and it is intended that the appended claims cover any and all such applications modifications and embodiments within the scope of the present invention.


---

title: System and method of building and using hierarchical knowledge structures
abstract: The present disclosure includes systems and techniques relating to building and using hierarchical knowledge structures. In general, embodiments of the invention feature a computer program product and a method including receiving a first ontology including initial categories, an indication of sample data for a given category of the initial categories, and an indication of symbolic knowledge for the given category; and populating the first ontology with new features to form a second ontology, the populating comprising determining the new features from the sample data using a statistical machine learning process and retaining the new features and the symbolic knowledge within the second ontology in association with the given category. In another aspect, embodiments of the invention feature a knowledge management system including a hierarchical knowledge structure that categorizes information according to cognitive and semantic qualities within a knowledge domain.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07644052&OS=07644052&RS=07644052
owner: Adobe Systems Incorporated
number: 07644052
owner_city: San Jose
owner_country: US
publication_date: 20060303
---
This application is related to U.S. Patent Application No. 60 778 869 entitled SMERSDAGSD to Walter Chang Nadia Ghamrawi and Arun Swami which was filed on Mar. 3 2006. The disclosure of the above application is incorporated herein by reference in its entirety.

This patent application is supplemented by a technical appendix submitted on compact disc which conforms to the International Standards Organization ISO 9660 standard showing example ontologies saved as plain text files in American Standard Code for Information Interchange ASCII format the submitted technical appendix includes two files entitled DocTypes SeedOnto.txt created Feb. 16 2006 and being 2 589 bytes in size 4 096 bytes used and DocTypes AugmOnto.txt created Feb. 16 2006 and being 56 119 bytes in size 57 344 bytes used the submitted technical appendix is hereby incorporated by reference in its entirety.

The present disclosure relates to content management systems and in particular to creating and using hierarchical knowledge structures.

The ability to automatically classify and categorize content is an important problem in content management systems. The need to categorize content occurs in both consumer and enterprise related content work flows. Numerous methods have been developed to address this problem. These methods typically use either symbolic knowledge representation or statistical machine learning techniques.

A symbolic knowledge representation is typically referred to as an ontology. In computer science an ontology generally refers to a hierarchical knowledge structure that contains a vocabulary of terms and concepts for a specific knowledge domain such as Bioinformation and contains relevant interrelationships between those terms and concepts. Symbolic knowledge generally refers to knowledge represented explicitly through precise domain vocabulary words e.g. Gene and their explicit relationship to other words e.g. has subtype recessive gene .

A traditional symbolic knowledge ontology is typically constructed by hand by one or more domain experts e.g. biologists and such ontologies are often very detailed and precise which can present difficulties in search and categorization applications. In a symbolic ontology a team of human domain experts will typically define the top level categories which form the structure of the ontology and then manually fill in this structure. Human knowledge engineers also maintain and update this structure as new categories are created or discovered.

For large symbolic ontologies a tree structure of ontology nodes is frequently created and stored in a database. A database structure called an Adjacency List is normally used. The Adjacency List typically consists of pairs of nodes each pair representing a parent child connection between nodes.

Another approach used in content management systems involves machine learning techniques. In computer science machine learning typically refers to a class of algorithms that generally employ statistical and probabilistic analysis methods to learn information from designated sample data typically example documents also known as document training sets . In contrast with symbolic knowledge methods machine learning methods represent knowledge in fuzzier and less precise ways which can provide benefits in terms of scalability and ease of document classification.

In a machine learning system which may or may not use an ontology a set of training documents is identified for each category and the system learns the relevant features keywords and phrases for each category. When a new document is presented to the system the document s features are extracted and statistically compared to training document features previously extracted by the machine learning system. The result of the comparison is a set of categories and scores that best match the likely topics in the new document. This approach is scalable but can be very sensitive to the data in the document training set.

There are numerous ontology standards building and editing tools and ontology based document classification systems. Existing ontology products build and edit symbolic ontologies and various standards exist that describe the semantics of these ontologies. In particular ISO 39.19 and W3C OWL DAML using RDF Resource Description Framework are common methods for specifying symbolic ontologies. Existing ontology products include those from Ontology Works Inc. of Odenton Md. Semio Corporation of San Mateo Calif. International Business Machines IBM Corporation of Armonk N.Y. Oracle Corporation of Redwood City Calif. Autonomy Corporation of San Francisco Calif. ClearForest Corporation of Waltham Mass. and Stratify Inc. of Mountain View Calif. . In addition existing classification systems use machine learning techniques such as Latent Semantic Indexing or Bayesian Networks.

This specification discloses various embodiments of technologies for creating hybrid hierarchical knowledge structures that combine both symbolic and machine learned knowledge. In general embodiments of the invention feature a computer program product and a method including receiving a first ontology including initial categories an indication of sample data for a given category of the initial categories and an indication of symbolic knowledge for the given category and populating the first ontology with new features to form a second ontology the populating including determining the new features from the sample data using a statistical machine learning process and retaining the new features and the symbolic knowledge within the second ontology in association with the given category.

These and other embodiments can optionally include one or more of the following features. The sample data can include sample documents. The determining can include extracting attributes from the sample documents calculating a statistical concept distance metric for the attributes selecting a first subset of the attributes that are more distinguishing with respect to the sample documents based on the calculated statistical concept distance metric for the attributes and a first user controllable input and selecting a second subset of the first attribute subset based on the given category and a relevance measure for attributes in the first attribute subset with respect to the given category the relevance measure being affected by a second user controllable input wherein the new features include attributes in the second attribute subset.

The calculating can include calculating information gain for an attribute A in relation to documents S and categories C by which the documents S are grouped and calculating the information gain can include handling separately a subset of the documents S for which the attribute A is absent to improve performance with respect to populating sub concepts in the second ontology. The handling separately can include using a fraction of entropy associated with the document subset for which the attribute A is absent. Moreover calculating the information gain can include calculating a smoothed zero value information gain SZVIG in accordance with an equation 

The determining can include discretizing frequency values Vfor the attribute A in the documents S based on a statistical variance of the frequency values V and wherein input for the calculating the information gain includes the discretized frequency values. The discretizing can include grouping the frequency values Vbased on a maximum per group variance determined according to a third user controllable input.

The determining can include determining a variance in the calculated statistical concept distance metric for the attributes and selecting the first subset can include selecting the first subset based on the first user controllable input combined with the determined variance in the calculated statistical concept distance metric for the attributes. The determining can include determining a variance in frequency values for the attributes and the relevance measure can be affected by the second user controllable input combined with the determined variance in the frequency values for the attributes.

The indication of symbolic knowledge can include a tag and a keyword the tag indicating an existing symbolic ontology and the populating can include mining the existing symbolic ontology based on the keyword to obtain the symbolic knowledge. The existing symbolic ontology can include a public ontology of an online lexical reference system and the mining can include accessing the online lexical reference system over a network. The indication of sample data can include a second tag and references to sample documents the second tag indicating the statistical machine learning process selected from multiple available statistical machine learning processes.

Operations effected by the computer program product and the method can further include receiving a query and retrieving information from the second ontology based on the query where the retrieving includes combining contributions of pertinence with respect to the query from the machine learned new features and the symbolic knowledge. The query can include a balancing factor and the combining can include adjusting the contributions from the machine learned new features and the symbolic knowledge based on the balancing factor. The query can include a document and the retrieving can include identifying a category for the document. In addition the query can include a search string and the retrieving can include identifying a document related to the search string and obtaining information associated with the identified document.

In another aspect embodiments of the invention feature a knowledge management system including a hierarchical knowledge structure that categorizes information according to cognitive and semantic qualities within a knowledge domain the hierarchical knowledge structure including discrete knowledge types included within a common information category of the hierarchical knowledge structure the discrete knowledge types including knowledge represented explicitly through domain vocabulary words and relationships among the domain vocabulary words and the discrete knowledge types including knowledge represented as designated sample data to be processed using statistical machine learning analysis wherein the knowledge management system includes a computer program product operable to cause data processing apparatus to process the discrete knowledge types and to effect a programming interface used to access the hierarchical knowledge structure and a document handling system configured to use the programming interface to access and obtain information from the knowledge management system.

The computer program product can be operable to cause data processing apparatus to perform operations including extracting attributes from the designated sample data calculating a statistical concept distance metric for the attributes selecting a first subset of the attributes that are more distinguishing with respect to the designated sample data based on the calculated statistical concept distance metric for the attributes and a first user controllable input selecting a second subset of the first attribute subset based on the given category and a relevance measure for attributes in the first attribute subset with respect to the given category the relevance measure being affected by a second user controllable input and augmenting the hierarchical knowledge structure with the second attribute subset.

The computer program product can be operable to cause data processing apparatus to perform operations including mining an existing symbolic knowledge resource based on a keyword and augmenting the hierarchical knowledge structure with results of the mining to add to the knowledge represented explicitly through domain vocabulary words and relationships among the domain vocabulary words. Moreover the computer program product can be operable to cause data processing apparatus to perform operations including retrieving information from the hierarchical knowledge structure based on a received query the retrieving can include combining contributions of pertinence with respect to the received query from the discrete knowledge types.

The document handling system can include an enterprise workflow system. The document handling system can include a resource management system. The document handling system can include a content management system. Moreover other system implementations are also possible.

Particular embodiments of the invention can be implemented to realize one or more of the following advantages. A knowledge domain can be more efficiently and comprehensively represented. Significant improvements in both the precision and recall of user queries to find relevant content can be realized. Moreover the ability of a knowledge management system to classify and categorize new content can also be significantly improved.

An ontology can include both symbolic knowledge and machine learned knowledge where the ontology employs a common ontology representation for both symbolic and machine learned features. This hybrid ontology can be more readily scalable and useful in search and categorization applications and can also capture explicit semantics e.g. that a recessive gene is a type of gene . The ontology system presented in this document can provide a framework or language for several kinds of knowledge aggregation within the same ontology and the process of evaluation of query results can then exploit this framework language. The framework can provide a reusable infrastructure for interpreting text documents several ways. The process for growing and extending an ontology can use statistical and symbolic methods including new statistical methods described herein. Information gain can be used as a statistical method together with feature discretization and feature relevance selection to populate the ontology with new concepts. The feature discretization can include a variance based attribute discretization that can be used in preprocessing feature vectors to improve accuracy of information gain calculations. Moreover the attribute discretization described can provide a way of discretizing a set of real numbers based on a user controllable clustering of their values with reduced computationally requirements and without dependence on observing the assignments of the feature values to specific categories.

The described information theoretic and statistical methods can be particularly amenable to supporting the construction and simultaneous use of multiple disparate ontologies constructed in a similar way using the same data but different initial groupings of documents. The constructed ontologies can be used to rank and categorize query documents by comparing the terms in the query documents to individual nodes and their surrounding nodes. Thus appropriate categories can be discovered for newly received documents using an ontology knowledge structure built using both symbolic and machine learned knowledge.

Multiple approaches to building and maintaining ontologies can be combined into one knowledge management system. A hybrid ontology system and associated language for incorporating multiple kinds of symbolic and statistical methods as described can represent varying symbolic and statistical relationships in one structure. The traditionally labor intensive nature of building and maintaining ontologies can be reduced a customized information gain technique can be employed to improve identification of sub categories within an ontology and a variance threshold approach to selecting and assigning features to ontology node categories can result improved machine learned features in an ontology. Learning from examples can assist the identification of sub categories. Moreover using symbolic knowledge from public ontologies to augment statistical knowledge can further assist the development of sub categories which may not otherwise be represented sufficiently by the examples in a given context.

Both explicit symbolic and relational knowledge can be imparted from formal taxonomy or ontology structures as well as machine knowledge acquired through statistical machine learning techniques. This can enable various implementations across a broad range of classification applications requiring automatic organization of information or documents. The ability to capture symbolic knowledge into an ontology structure can allow explicit concepts and terms and their interrelationships to be directly incorporated into the ontology to assist in classification and query tasks.

Machine learning algorithms typically learn from examples via statistical processing resulting in models of a domain that can be used to make predictions about that domain. However such models often include considerable noise and errors. Such models also tend to be abstract models for which augmentation with other knowledge is difficult if not impossible to accomplish in a meaningful way. The present systems and techniques can be used to deduce meaningful relations from statistically trained models and incorporate that into a single ontology representing one common model. By exploiting suitable learning methods such as the information gain and the variance threshold methods described in building an ontology structure the present systems and techniques can gain the advantages of statistical and probabilistic knowledge that has been gleaned from examples as well as the advantages of a model representation that allows for augmentation of external knowledge such as symbolic knowledge or other knowledge also deduced from statistically trained models.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features aspects and advantages of the invention will become apparent from the description the drawings and the claims.

As used herein the word document refers to an electronic document. An electronic document which for brevity will simply be referred to as a document does not necessarily correspond to a file. A document may be stored in a portion of a file that holds other documents in a single file dedicated to the document in question or in multiple coordinated files.

The present application generally involves combining aspects of both symbolic and machine learning categorization techniques to allow machine learning processes to be more selective while still working well with employed symbolic methods and to allow automatic creation of knowledge structures called ontologies which represent domain specific knowledge. These knowledge structures can be used to automatically classify and organize content such as text documents image documents or any documents containing metadata any tagged media .

During run time a query is received at . This query can be a document or a user entered search string. Then output from the populated ontology can be generated at based on the query. For example a new document can be submitted to a knowledge management system which can automatically search and classify the new document against the created ontology to produce a set of ranked categories that correspond to the most likely topics in the new document. User entered search strings can be treated in a similar manner to identify a set of ranked categories that correspond to the most likely topics in the search string. Moreover the output can be the set of ranked categories a highest ranked category or information derived from such category information e.g. a set of keywords that can be used in archiving a new document or additional information obtained by drawing an inference from an identified category to information being sought by the user who entered the search string .

The example documents can be processed using statistical machine learning techniques e.g. by the information gain and feature selection learning algorithms described to discover vocabulary and terms relevant to a given category. Text data can be extracted at from the documents . This text data can be pulled from the body of the documents from metadata for the documents or both. Feature extraction can then be performed at on the text data using relatively simple lexical tokenization algorithms e.g. a feature extractor consisting of a natural language tokenizer and or shallow parser or more sophisticated commercial semantic entity extractors. A document feature extractor is used to identify and extract semantic entities such as person company product city names or logical noun groups. Various known commercial products can provide this feature extractor capability e.g. ThingFinder software provided by Inxight of Sunnyvale Calif. or Machinese software provided by Connexor Oy of Helsinki Finland .

A feature is an attribute of a content object that is atomic and potentially useful in distinguishing one content object from another note that a feature attribute can be a single word or a phrase . A collection of features from a single document can be used to form a feature vector a collection of features associated with a content object . As used herein the word features encompasses the possible use of a feature vector for a document.

The keywords can be used to populate symbolic knowledge subtrees at from public ontologies such as WN 2.1 available from the Cognitive Science Laboratory at Princeton University of Princeton N.J. . The symbolic knowledge subtrees can be provided along with the extracted document features to populate the seed ontology . In addition the extracted document features can be initially stored in a database and then retrieved by a feature selection process at to identify and rank the most distinguishing features. The most significant valuable and distinguishing features can then be selected by an augmentation process at feature assignment and ontology construction which uses selected features to build a new ontology . This new ontology can be stored in a database such as a relational or object oriented database.

The ontology can simultaneously represent both symbolic and machine learned features. The ontology can be represented as a directed acyclic graph DAG of nodes where an ontology node describes or signifies a concept which can be recorded by a body of text. An ontology node can include references to both symbolic and statistical knowledge sources for the concept of the node. For example the ontology node can include sub nodes which can be sub trees together with data from the original seed ontology and data acquired during the ontology building process. A sub node can be identified by a tag that indicates a type for the sub node and thus also indicates an extraction method or point of view for the knowledge contained in the sub node.

The Applicant Resume category includes a STATISTICAL M1 tag and associated references to example documents resume 0001.acro.txt resume 0002.acro.txt resume 0003.acro.txt and resume 0004.acro.txt. The STATISTICAL M1 tag indicates that a first statistical machine learning technique is to be used to glean knowledge for the category from the listed example documents. Additional tags can also be provided and thus one or more statistical machine learning techniques can be used in building a single ontology .

The Applicant Resume category also includes a SYMBOLIC M1 tag and a SYMBOLIC M2 tag and associated keywords to be used in generating symbolic knowledge subtrees from different symbolic knowledge ontologies. Thus one or more symbolic knowledge sources can also be used in building the single ontology . These tags thus represent indications of the type of ontology aggregation to be performed within a given node of the ontology and a user is free to mix and match when constructing the seed ontology the types of ontology augmentation to be used at each node.

This structure allows concepts to be described in multiple ways and allows a given concept node in the ontology to include multi faceted descriptions of the concept at hand. Once the seed ontology has been processed to form the populated ontology a single concept node can include a wealth of information that has been extracted from different knowledge sources using very different methods. In the given example the Applicant Resume node now includes STATISTICAL M1 FEATURES which have been derived from the sample documents using statistical machine learning and also includes a SYMBOLIC M1 SUBTREE which has been derived from a public symbolic ontology such as WN 2.1.

This approach allows the representation and implementation of a hybrid of different kinds of ontology augmentations using a combination of statistical and symbolic methods. Furthermore it provides a framework for extending the augmentation to use other symbolic and statistical techniques by simply adding more tag types and associated algorithms for performing the augmentation. Thus multiple aspects of domain knowledge which are learned and recorded in multiple and varied ways can be represented and processed in a single hierarchical knowledge structure. This structure can be viewed as a hybrid ontology structure that provides a consistent and uniform way of representing both symbolic and machine learned knowledge.

It should be appreciated that the example ontologies shown in are only portions of the whole. In typical applications the seed ontology will be larger than that shown and the populated ontology will be much larger than that shown. In addition the populated ontology can include additional information useful for diagnostic or archival purposes such as build parameters used for both the machine learning and symbolic knowledge building processes and output measurements from the machine learning. Information included in the constructed ontology can include the number of best features found by information gain the number of features per category and values for user controllable training parameters a detailed example of such is provided in the technical appendix. Moreover the machine learning and symbolic ontology building methods used by the system can be known methods or the new methods described herein.

These frequency values can be retrieved along with the features and then grouped into value range buckets to decrease the number of discrete frequency values. This process is also referred to herein as attribute discretization or attribute hash. In general discretization is the process of mapping an array of number often including duplicates to a discrete set of values or ranges. Thus numbers that are close enough to be considered identical can be treated as being identical given a set of k numbers in increasing order n . . . n discretization involves identifying m values r . . . rin increasing order that correspond to adjacent intervals r r r r . . . r r such that each nfalls into an interval. Moreover the discretization of feature values at can be based on a statistical variance of the values and a user controllable variable that affects the number of buckets used for a particular ontology e.g. based on the AttributeHash parameter described below .

A statistical concept distance metric can be calculated at using the discretized values as input to produce a score and ranking for candidate feature values to be included in the ontology. The statistical concept distance metric can be a traditional metric such as information gain mutual information or chi squared. Alternatively the statistical concept distance metric can be a modified version of information gain as is described below.

The most distinguishing features can be selected at based on the calculated statistical concept distance metric and a user controllable input. Selecting distinguishing features is important since proper selection of these features can significantly help differentiate which categories a new unknown document should be categorized under. The most relevant significant features can be selected at based on a user controllable input and features can be assigned to the ontology at according to the intersection of the distinguishing features and the relevant features note that the identification and selection of most relevant features can be performed in parallel with the identification and selection of most distinguishing features . Detailed examples of these operations are now described including the modified version of information gain.

Information gain is a function that exploits the groupings of documents under different nodes in a seeded ontology. A vocabulary is built where the vocabulary includes all terms words or phrases in the documents excepting any stoplisted words and potentially using word truncation and stemming . Documents that are used to describe different nodes which represent distinct categories are considered to be in distinct groups a group of training documents for a given category . In the example discussed above in connection with there are two categories Applicant Resumes and Product Datasheets. The vocabulary is the set of terms that occur in the corresponding documents of the categories e.g. the set of words found in the documents prodbroch 0003.acro.txt prodbroch 0004.acro.txt prodbroch 0006.acro.txt resume 0001.acro.txt resume 0002.acro.txt resume 0003.acro.txt and resume 0004.acro.txt .

Information gain can be used to produce a ranking of the vocabulary terms where the highest ranked terms are most distinguishing between the various groups. In other words a term is ranked high if knowing its relevance or frequency within a document can help identify the appropriate group for the document with a certain degree of confidence. For instance the term experience may not occur in a product datasheet at all but may occur frequently in applicant resumes. Lower ranked terms may occur in multiple groups with equal probability such as the term Java or may occur in one group very infrequently e.g. a specific person s name may be in only one resume .

Documents can be represented as a set of features where a feature corresponds to a term and that term s frequency within the document. Across a corpus the set of documents forming the training sets for the various categories the same term may occur with varying frequency in different groups. For instance a term may occur with frequency of 0.3 in one group and 0.7 in another. Similarly another term might occur with frequency 0.35 in one group and frequency 0.36 in another.

Traditional information gain techniques typically identify the terms that render the greatest reduction in entropy. This is usually accomplished by using the term s frequency across a corpus and identifying how well the frequency the associated feature value identifies the category. However in typical corpora attributes that are otherwise informative may occur often in one or few categories but not at all in most other categories. In other words the term helps to distinguish subgroups of categories from the remaining categories.

Unfortunately the term has lower information gain if there are too many categories in which the term does not occur because knowing that the term is absent does not readily help in identify the groups with which the term is associated. This behavior is generally acceptable in traditional machine learning approaches that rely on feature selection but this behavior may not be ideal for populating an ontology with sub concepts because this approach may filter out sub concepts that are relevant and important. To overcome this when computing the information gain of a term t the present techniques can involve computing and taking only a fraction of the entropy of the set of documents for which t is absent. The modified function can be understood as a Smoothed Zero Value Information Gain SZVIG and can be defined by the following equation 

The information gain of an attribute can be significantly affected by the distribution of values in V. As addressed above documents can be represented as frequency feature vectors vectors of features and their frequency in the document. The frequency distribution can form natural clusters of individual frequencies for which two frequencies in one cluster can be considered the same such as the occurrence of the term software with frequencies of 0.35 and 0.36 in two document groups respectively .

To assist in identifying meaningful clusters of term values across a corpus various term value discretization techniques can be used. These techniques can be controlled by parameters that can be adjusted by a user. Moreover the selections of these parameters can depend on computed statistical characteristics of the clusters of attribute values. For example one important characteristic of the cluster can be the variance of values within a cluster.

The attribute discretization technique can have a significant impact on the final results because the discretization determines what difference in attribute value is significant enough to be considered informative. In other words if the discretization is too selective then the two term frequencies of 0.35 and 0.36 in two different document groups may be considered distinct and therefore the term would be more likely considered informative because knowing its frequency in a new document would indicate which group the new document belongs to. In some implementations the following parameter AttributeHash can be used to control the attribute discretization behavior.

The AttributeHash parameter can be used to control the degree of granularity of the feature value discretization that takes place in computation of the information gain of a feature. The AttributeHash parameter can control the degree to which two different feature values are considered the same. For a given feature A in the vocabulary each document x has an associated value f x in this case the frequency of A . The discretization of the attribute values of A can depend on the variance of f x across all documents x in the corpus together with the AttributeHash parameter value v which can be supplied by the user.

For each attribute A the attribute values f x can be sorted and subsequently grouped by variance. The maximum variance of each group is a function of the variance across the corpus 

The information gain IG function e.g. SZVIG as described above can produce a ranking of the vocabulary terms ranked by their ability to distinguish between categories. The machine learning process that augments or aggregates the ontology with new concepts in this case vocabulary features can select the features with the highest information gain value that is those ranking above some cutoff. This cutoff can be selected in various ways and the cutoff can be partly controlled by a parameter called the Overall parameter which is described further below.

For a node that is associated with a set of training documents subsets of the collection of most distinguishing overall terms can be selected to associate with the node. The decision about which vocabulary terms to use in this augmentation can depend on the value of the term to the associated group of documents represented by the frequency and on a third tuning parameter referred to as the BestFeatures parameter. This tuning parameter can be used to select the set of terms that are most relevant to the group of documents at hand and the relevance can be determined by the object feature vector. Note that the use of the term BestFeatures and the phrase best overall features herein does not imply that these features are the best possible in some absolute sense.

A term is relevant if it occurs frequently in the set of documents. The term can be included as a sub concept of the document seeded concept at hand if the term is among the concept s most relevant terms AND the term is among the most distinguishing overall terms. For example if the terms education and platform are terms selected as most distinguishing overall terms then those terms that have high relevance to the Product Datasheet category e.g. platform can be added as a sub concept of Product Datasheet. In contrast the term education may have low value to the Product Datasheet category and in such case would not become a sub concept.

In general the behavior and quality of results for a data driven machine learning method of building an ontology depends on the characteristics of the data such as the relative number of terms in documents and the degree of overlap between document groups. By providing the three primary parameters described AttributeHash Overall and BestFeatures the behavior of the statistical machine learning portion of the ontology aggregation process can be readily controlled and the quality of the results can be significantly improved. These three parameters can be user controllable and can also have default settings. The statistical ontology aggregation process is now described in detail in connection with these three parameters.

The features can also be ranked by their value to the category or categories in which they are found. Those features that are valuable to their category and also are among the most distinguishing features e.g. as determined by a threshold represent the best overall features . These features can then be used for ontology augmentation.

As discussed above the computation of information gain can involve feature value discretization and this discretization can employ varying degrees of granularity using the AttributeHash parameter. An attribute may be associated with many different values across a corpus of documents the word Java may occur with frequency 0.070 in one document and 0.065 in another and 0.030 in a third. The AttributeHash parameter influences the number of different values that an attribute can actually take on in a corpus by influencing the number of buckets into which the attribute values can be placed. Thus the AttributeHash parameter can control the degree to which two different feature values are considered the same for purposes of computing the information gain. In the above example a low AttributeHash may take the values 0.070 and 0.065 to be the same whereas a higher AttributeHash may take these values to be different. The shrinkage in the number of value buckets can be controlled according to equation 2 which can be pictorially understood as follows.

In the low AttributeHash two features and are considered to be close enough as to have the same value with respect to information gain represented by dashed circles . A third feature has been discretized to a value of zero in the document group and thus this feature does not appear in the overlap region. In the high AttributeHash features that are close in size are more likely to be considered different. In this case the feature is considered to have the same value in the two document groups and but the features and are not. Thus increasing the value of the AttributeHash parameter causes the creation of more buckets into which attribute values are placed with less difference between values in different buckets and decreasing the value of the AttributeHash parameter causes the creation of fewer buckets eventually causing information gain to interpret feature vectors as if they are binary valued.

In general when the feature overlap between groups of documents which can be understood as noise in the training sets is minimal the AttributeHash should be set to a lower value so that the few features that do occur in multiple document groups will correspond to feature values that belong to the same group thus the word will be considered less informative. Otherwise the AttributeHash can be set higher so that knowing the value of a word on a document will help to identify the document class. Additionally if one category has significantly more vocabulary words than another such as with a large collection of full news articles verses a smaller collection of just headlines then the AttributeHash parameter value should be set higher. Typical values for the AttributeHash parameter can be in the range of 3 to 10 and the default value can be 3.

As shown in the feature which the AttributeHash placed into discrete buckets for the two document groups and has a high information gain. Knowing the value of the feature reduces the total entropy thus increasing information gain since the feature occurs with a different value in the two groups and . The feature has low information gain because the AttributeHash caused the information gain function to treat the two frequencies of the attribute as the same value in the two groups and . Thus knowing the attribute value does not provide a gain in information.

As mentioned above variance and proportion are different heuristics that can be used for setting the Overall threshold . The variance option for the Overall parameter can involve computing the variance over all of the information gain values and selecting the top most informative features having some limited variance. For example the variance setting for the Overall parameter can be computed according to the following equation 

The proportion setting for the Overall parameter allows the user greater control over the set of best features selected by selecting a proportion of features. That is if the user supplies a proportion such as 0.5 then the 50 of vocabulary words that have the highest information gain are selected as the set of best overall features. This setting for the Overall parameter can provide more flexibility and the default setting can be a proportion of 0.1.

Typical values for the proportion setting of the Overall parameter can be in the range of 0.05 to 0.40 for large noisy corpora such as with corpora of research papers or news articles and higher for smaller or less noisy corpora. A generally optimal setting for this parameter for a given data set can produce an ontology that is as dense as possible with minimal shared child nodes across categories. Decreasing the proportion reduces the number of features that are considered distinguishing and is recommended if there is a large disparity between the number of features in different classes or if the distinguishing features are likely to be spurious. The proportion should be increased if a more connected graph with more shared descendent nodes is desired.

Given a node and a vocabulary the BestFeatures parameter governs the set of features that are considered most valuable where the value of a feature is the value in the feature vector e.g. the frequency of the feature in the set of documents . This ranking can be completely independent of the AttributeHash and Overall parameters. However in a way much like that of the Overall parameter the BestFeatures parameter governs a threshold and features whose value is above the threshold are considered to be semantically valuable to the ontology node at hand and are considered for aggregation.

The BestFeatures parameter can also have variance and proportion settings. In the case of the variance setting for the BestFeatures parameter the discretization of features values can use the variance of the feature vector values and the terms that are in the highest rank groups can then be selected. The variance setting for the BestFeatures parameter can be a nonnegative integer where larger values cause fewer terms to be used. The variance can be defined as follows 

The proportion setting for the BestFeatures parameter can be used to control the proportion of the number of terms that are selected the highest ranked terms . If this setting is 0.3 then 30 of the features are considered best . Use of the proportion setting for the BestFeatures parameter can allow greater control over the features that are used to populate the ontology. Note that this is different from distinguishability since a feature can be relevant but have the same relevance in all categories and conversely a feature can be distinguishing in general but relevant to a specific category far more than other categories.

In the context of increasing the proportion setting for the BestFeatures parameter lowers the threshold and thus increases the number of features that are considered valuable. The proportion is maximal when all of the features are considered valuable enough to include in the ontology and in such case the inclusion of features in the ontology then depends entirely on the selectivity of the Overall parameter subject to the AttributeHash parameter . Recommended settings for the proportion setting for the BestFeatures parameter are between 0.25 and 0.60. Lower values are recommended for more sparse ontologies or for when there is more spurious data while higher values are better suited for ontologies which have less noisy data.

Two additional parameters can also be used to constrain the features selected by limiting the proportion of overall features that can be associated with a node or limiting the number of features to associate with a node. The latter parameter setting if low enough allows all categories to be treated equally regardless of the number of terms and number of documents. The former achieves a similar result but depends on the vocabulary size.

Note that the proportion and variance settings described above for the Overall and BestFeatures parameters can be used in various combinations either within a given implementation or with different implementations of the described systems and techniques. In addition it should be noted that explicit discretization in order to facilitate selection of features has been used previously to facilitate na ve Bayes and decision tree learning classifiers.

The above described techniques for aggregating an ontology can also be understood in the context of the following pseudo code 

Furthermore the attribute discretization described above can be global and unsupervised. This attribute discretization can be influenced by an input controlled factor and be based on the statistical variance of attributes such as described above. This attribute discretization can be called Discretization Variance DVar and can also be understood in the context of the following pseudo code 

This approach can have the following noteworthy properties. The variance of any portion of the array cannot be larger than the variance of the entire array. In general this is because the values in that portion of the array must be at least as close to the average of their values as they are to the average of the entire array. Therefore the variance of a subgroup of items within a group of items is an effective way of measuring how relatively close the items are. Moreover a simple extension of this approach can factor in the number of attributes to be discretized as well by taking as a factor f some reducing function of the number of attributes k such as ln k sqrt k etc. . The ontology builder can implement this as an option as well.

DVar can map a continuous valued finite set of numbers into a finite set of intervals. In contrast with many prior discretization approaches DVar can discretize attribute values independent of any class memberships of those values. Moreover DVar makes use of variance in discretization and the granularity of discretization in DVar can be influenced or controlled by an input parameter.

Implementations of the present systems and techniques can employ a directed acyclic graph DAG to represent a generated ontology. Moreover a DAG can be stored in a relational database in a manner now described which can in general allow applications that employ hierarchical knowledge structures to efficiently represent search and retrieve DAGs. The following DAG representation systems and techniques can provide significant performance improvements during knowledge retrieval and querying functions and can have direct application to large Bayesian Belief Networks by providing a means to efficiently locate dependent evidence nodes for computing the conditional probability of any particular event node in the network. The DAG representation systems and techniques now described are the subject of another patent application U.S. Patent Application No. 60 778 869 entitled SMERSDAGSD which was filed on Mar. 3 2006 hereinafter referred to as the DAG application .

While semantic network DAGs can provide a general framework for representing knowledge performing machine reasoning and inferencing they are often inefficient when these structures are large. The DAG application can overcome several efficiency problems inherent to the other approaches. For example the DAG application can allow inference chains in the knowledge structure to be quickly located materialized and followed for machine reasoning applications can allow DAG sub structures such as a topic sub ontology to be quickly retrieved and the DAG application can provide a mechanism for logically and physically sharing knowledge node values within an ontology DAG and across ontology DAGs.

A traditional method for representing trees and DAGs in a database involves the use of Adjacency Lists. Adjacency Lists typically require navigation of the stored DAG structure and are frequently inefficient for larger graphs. The use of Materialized Paths is also a known idea for representing and searching tree data structures in a database. For DAGs this technique can also be used but without the indirection mechanism described in the DAG application significantly storage is required for node values within each path list. This method is typically not used due to inefficiency of string operations required to search and locate node data values.

Interval encoding methods have also been used but are generally restricted to strict tree structures. Furthermore interval encoding methods can have drawbacks in that for normal node insertion operations a large subset of the records that contain interval values are typically accessed and updated. In worst case update scenarios that result from common node insertion operations up to half of the total interval values may have to be accessed and updated. The DAG application need not suffer from this problem and may require at most M rows to be added to the DAG Path table described below where M is the total number of new paths formed by the addition of the new node to the DAG.

Fractional methods such as Farey Fractions can suffer from having to perform a large number of arithmetic operations on a potentially large subset of nodes within the DAG. In the case of Continued Fractions problems will often begin to be encountered due to limitations with the numeric precision used to represent the range. These algorithms can run out of precision at around four levels in the DAG which is typically insufficient for most large knowledge structures which often may be dozens of levels deep.

Particular embodiments of the DAG application can be implemented to realize one or more of the following advantages. DAGs can be efficiently represented in relational databases and subsets of DAGs represented in this manner can be efficiently retrieved. Sub areas of knowledge structures stored using the DAG application can be readily located and accessed and rapid inferencing can be realized using the DAG application.

The following description of the DAG application addresses four major elements the algorithms used to insert and delete nodes from the DAG and how the logical and physical data value sharing mechanism works which can allow symbolic knowledge to be shared both within a single ontology and across different ontologies. The four major elements are as follows 1 a DAG PATH table used to enumerate all possible paths form the root node to each node in the DAG 2 an indexing technique used to rapidly locate any DAG node in the DAG and all relevant paths that the node participates in which can allow inference chains to be quickly followed and materialized 3 a common knowledge operation involving the retrieval and materialization of a sub DAG of the knowledge structure where the path entries can be used to rapidly fetch a sub DAG and 4 an indirection mechanism that allows DAG node data values to be shared within a DAG and in addition Data Values can also be shared across different DAGs via the Data Value nodes.

The basic operations used in include inserting deleting and fetching nodes from the DAG structure and updating the DAG Path table to reflect these operations. The operations on the DAG can be expressed by using a combination of the primitives A H defined below 

As shown in given a knowledge DAG with nodes connected as shown the DAG Path table is as shown in . As each node is added the DAG Path Table lists all possible paths to each node indicated as a Leaf node using LeafID . E.g. for Node all possible paths are enumerated and a second path . As shown in the example of a new Node is to be added to the DAG. The addition of this node results in exactly one new DAG Path entry . Finally shows an example of how multiple DAG Path entries are created. Here a new Node is added. All possible paths to Node are indicated and the second path .

Data node value sharing In the description above Node has two parent DAG nodes Node and Node . The concept associated with Node is physically and logically shared by it s parent nodes. Node sharing is important in semantic networks when changes are made to the concept associated with Node all updates can be made consistently in one place and will ensure that the knowledge semantics are correctly expressed. Further since concepts in the knowledge structure can be arbitrarily large an important feature of the DAG application is the separation of the structure of the DAG and the actual information contained in each of the nodes of the DAG.

The DAG application includes a mechanism by which a data value can be associated with each DAG node such that the data value can be separately maintained and shared. A data value can be a topic category a concept one or more text terms or a larger text object. In most cases e.g. for knowledge taxonomies and knowledge ontologies the data value node is shared. In addition the DAG node used to form the structure of the total DAG can also be shared. This can be accomplished by associating a Data Node with each DAG Node and then using bi directional indexing to determine what data value concept is associated with which DAG Nodes and the inverse given a concept determining all DAG Nodes that use the specified concept in either a single DAG or across DAGs .

In DAGs where data values need to be shared the basic insertion and deletion algorithms described above can be extended in the following manner 

Consider the following node sharing example where two content taxonomies are to be constructed. The first taxonomy organizes people by job roles. In some cases an individual may have multiple roles. The second taxonomy indicates which individuals are working on which product. DAG Nodes are indicated by circles DAG Node IDs are indicated by a number. Data Nodes are indicated by rectangles data node values are shown as text.

In the left side of Karen Mary and John have roles as Graphic Designers. John Dave Travis and Karen have roles as Programmers. Additionally Dave John and Karen work on Photoshop. Karen Travis and Mary work on Acrobat. The two different taxonomies present different views or perspectives over the same people entities. The ability to support multiple hierarchical views of information is an important aspect of the DAG application and directly supports the concept of faceted taxonomies and ontologies.

The DAG systems and techniques support two separate policies for sharing DAG nodes. Each policy is material for constructing and representing a semantic network or arbitrary DAG. The first policy considers that DAG nodes are shared within the same DAG only. The second policy assumes that DAG nodes are potentially shared with other DAGs. By enforcing the first policy the invention can ensure that a DAG that is not a tree can be represented efficiently as a DAG. Furthermore DAG Nodes can be represented independently of other DAG Nodes that refer to the same Data Node preserving the distinct semantic relationships between other nodes in the graph. The second policy reduces the number of nodes created and moreover allows networks of DAGs to be interconnected.

Given two types of nodes DAG and Data nodes and given that each node type can be either shared or non shared this gives rise to the following truth table which enumerates the various sharing models presented by the DAG application.

In addition rather than enumerating a full node list for each respective path as described above DAG path compression can be employed in a DAG Path Table by factoring out common prefix paths. This DAG path compression can involve shortening the Path Node List by referring to sub paths using the Path ID in the Path Node List. Alternatively this DAG path compression can involve shortening the Path Node List and having fewer entries in the DAG Path Table by referring to arbitrary sub paths by a placeholder such as C although this can potentially result in more expensive queries .

Referring to the graph of path entries and can be compressed as follows. shows an example DAG path table with path compression implemented using references to the Path ID in the Path Node List rather than fully enumerating the path. Thus of entry is replaced by P where the P token indicates the Path Node List of entry and the token indicates the addition of Node to the referenced sub path P. Likewise of entry is replaced by P where the P token indicates the Path Node List of entry and the token indicates the addition of Node to the referenced sub path P.

This approach to path compression can result in considerably shorter path lists which can result in smaller tables e.g. reduced string lengths with the same number of rows particularly when the fan out is large and also when a node with descendants may have multiple parents and many paths too the root . However in this case a new token index entry is created for each path ID that is compressed and the additional complexity may require multiple roundtrip operations for inference chain retrieval and sub graph retrieval. Reducing these additional round trip requests to query the database can be accomplished by using a caching mechanism to store previously seen path IDs and their expansions and then periodically updating this cache.

Updates and deletions to the graph can continue operate as described above. The retrieval algorithms can be as follows 

Entry has been removed because path is already represented by path above. Path in this table represents the original path and the original path in the uncompressed table. Referring to the retrieval algorithms any path that has 4 as a leaf will prefix any other path that has as tokens in it s path string C followed by 4 . Thus the C can be visualized as being a signifier that the nodes following it connect to another path and in this example the nodes following C connect to paths and by Node 

This second approach can result in greater compression of the path table. However this second approach may also make the queries more expensive or require more filtering step 5a in retrieving a sub graph for example imposes a requirement on the second element of the path node list which is a more expensive query operation than a query on an individual indexed token. In both path compression examples above only select paths in the graph have been compressed paths for which the leaf node had multiple parents. Thus only the paths and in the original table in were compressed.

As before the basic operations used are inserting deleting and fetching nodes from the DAG structure and updating the DAG Path table to reflect these operations. All operations on the DAG can be expressed using a combination of the primitives A H defined below 

Typical uses of DAGs in semantic networks apply to DAGs that are mostly trees. That is most of the nodes have only one parent while some may have more but likely fewer than three. Thus a data structure that represents generalized DAGs is needed the implementation of this data structure that is most convenient for DAGs that are nearly trees is likely preferred. The path compression approaches described above are generally well suited for this. Because typical DAGs in many implementations are mostly trees the advantage of the second compression approach over the first compression approach may be small. Moreover the second compression approach may involve more expensive queries since the database token indexing method may not account for the ordinal value of the tokens. Therefore in implementations where the DAGs are mostly trees the first compression approach may be preferred over the second compression approach.

In addition it should be noted that path compression offers several potential space saving advantages. Important considerations include the decision about when to compress paths upon any update or in batch modes offline and the decision about which paths to compress. Regarding the latter consideration one could compress all paths in a DAG so that for example a path node list never has more than two node IDs and always has a compressed path except at the root . However this may be undesirable as it tends to negate the advantages of path enumeration and lexical indexing described herein. With that being observed suitable heuristics for compression can be tailored to the kinds of graphs and the kinds of updates expected to be made to the graph in order to optimize the queries and the space required.

One heuristic is to always compress paths that have at least four nodes. Another heuristic is to compress based on in degree and out degree. For instance two children of a node share all the same paths of a node to the root so a good candidate heuristic for path compression is at nodes that have more than 4 children. If those children have descendants that may be an even better candidate for path compression. Another important consideration is to compress paths at nodes that have more than one parent.

Path compression can be integrated with the insertion and deletion algorithms to ensure constraints on path length and depth as well as in degree are enforced. Alternately path compression can be applied offline in batch mode to enforce constraints. Path compression may be a combination of both approaches consider for example the insertion of a sub graph as a child of some other node N paths from N to the root can be compressed online.

In many implementations graphs are largely created in memory during augmentation or augmented by adding multi node sub graphs. Therefore path compression over the entire DAG can be deferred to batch mode or occur during persistence of the DAG in order to allow optimal path compression. For the DAGs described in herein it is expected that the updates edge and node insertions and deletions will likely alter the graph slowly over time so the compression can occur in batch mode unless a multi node sub graph is inserted. Moreover since the graphs used in many implementations are mostly trees it may preferable to use the first compression approach and compresses paths for paths that have four or more nodes and for which the leaf node has either out degree at least four or in degree at least two.

Leaving the DAG systems and techniques and once again referring to the present application the described techniques can be implemented in many different types of systems. is a block diagram showing an example system implementation. A knowledge management system can employ the techniques described above to build and maintain a hierarchical knowledge structure that categorizes information according to cognitive and semantic qualities within a knowledge domain.

The hierarchical knowledge structure can include discrete knowledge types included within a common information category of the hierarchical knowledge structure where the discrete knowledge types include knowledge represented explicitly through domain vocabulary words and relationships among the domain vocabulary words and the discrete knowledge types include knowledge represented as designated sample data to be processed using statistical machine learning analysis. As will be appreciated these discrete knowledge types e.g. symbolic knowledge and statistical based machine learned knowledge represent different points of view of knowledge in the knowledge domain and employ discrete knowledge extraction methods. Thus implementations of this hierarchical knowledge structure can be viewed as a multi faceted ontology contained within a single data structure.

The knowledge management system can include a computer program product operable to cause data processing apparatus to process the discrete knowledge types and to effect a programming interface used to access the hierarchical knowledge structure. The knowledge management system can be coupled with a document handling system such as through a network . The network can be an enterprise network e.g. a local area network a public network e.g. the Internet or combination of these or other computer networks e.g. a mobile device network .

The document handling system can be configured to use the programming interface of the knowledge management system to access and obtain information from the knowledge management system . The document handling system can be an enterprise workflow system a resource management system e.g. an enterprise relationship management ERM system or a customer relationship management CRM system or a content management system e.g. a document repository or document archiving system .

In general terms nearly any application that requires the storage access and organization of domain knowledge can use the present systems and techniques to implement such application s knowledge management functionality. Implementations are not limited to large enterprise systems such as those listed above but can also include personal computer programs where there is a need to organize related help information best practices and resource libraries etc. Examples include various software products provided by Adobe Systems Incorporated of San Jose Calif. such as P software e.g. for creating ontologies of image resources such as tools palettes brushes textures filters ADOBE A software e.g. for creating ontologies of audio special effects filters and complex audio mix workflows and video editing products such as AP software e.g. for creating ontologies of video effects backgrounds and stock video content .

Output from the semantic processing can be provided to metadata and feature persistence which can include one or more databases. The persistence services provided by the metadata and feature persistence can be Extensible Markup Language XML based and can also employ Extensible Metadata Platform XMP metadata processing services. The metadata and feature persistence can be coupled with multiple ontology management components including an ontology builder . The ontology builder can employ the techniques described herein to build a hybrid ontology for use by other system components.

One or more ontology parsers can mine symbolic knowledge from existing symbolic ontologies. These can include one or more generic ontologies e.g. WN and one or more enterprise ontologies e.g. an enterprise document retention policy ontology . The mined symbolic knowledge can be provided to the ontology builder for use in augmenting a seed ontology as described above.

One or more hybrid ontologies can be created by the ontology builder and an ontology engine can provide access to these hybrid ontologies. The ontology engine can provide query e.g. document or search string processing services to various enterprise tools . The enterprise tools can include document workflow management tools document retention policy tools summary and reporting tools digital rights management DRM and document security tools document archiving tools resource locator tools XMP metadata generator tools etc.

In general the enterprise tools can use the ontology engine to access a hybrid ontology to discover information relating to a query. The discovered information can benefit from discrete knowledge types in the knowledge domain in that the ontology engine can retrieving information from the hybrid ontology based on the query where this retrieving involves combining contributions of pertinence with respect to the query from both statistically machine learned features and symbolic knowledge in the hybrid ontology. Moreover the query can include a balancing factor and combining contributions of pertinence can involve adjusting the contributions from the machine learned features and the symbolic knowledge based on the balancing factor which can be a user controlled input.

The ontology engine can support information discovery in a hybrid ontology for various purposes including resource mining e.g. identifying an author of documents related to a search string to find an expert in a particular area and document classification e.g. identifying an appropriate document category in a content management system for a new document .

New documents can be submitted to the system for categorization. During the run time query processing a new unknown document can be submitted to the system. Document text can be extracted at lexical or semantic features can be extracted from the text at and these extracted feature can be used to form a feature vector for the new document to be classified. Features can be extracted from a query e.g. the document to be classified in the same way that they are extracted from the example documents used to grow the ontology. Using the same example ontology described above in connection with if a new unknown query document is a resume document then the terms and their frequencies are extracted.

Once features from the new unknown resume document are extracted and available the system can rank the features at . The ranked features can then be used at to query the augmented ontology data structure to find matches with feature terms that were assigned to the ontology categories during the earlier symbolic and machine learning operations used to initially build the ontology. The search results can be ranked at by the most relevant ontology terms that match the terms or features of the query document. Each ontology node that matches can be associated with a cumulative score that depends on the value of the feature in the query document. Moreover inference chains can be followed at to return the root categories or other information sought such as document authors who may be experts in the technical area of the query document .

Query results can then be provided as output to a user or to another system component. For instance all matches and rank scores can be reported back to indicate the best corresponding categories under which the new document should be categorized. All of the query document s features can be used in the ontology query but the scores of individual ontology nodes can be used to determine the confidence of the classification. For example if the most relevant term is experience and it has a rank of 0.3 then if the Resumes concept has been associated with sub concept experience then the Resumes node can receive a rank based on the value 0.3 in particular the experience node can receive the rank 0.3 and the Resumes node receives rank 0.3 3 0.1 . The query results can have the following form category C 0.40 category C 0.35 category C 0.02. Moreover if the variance of the classification scores is above a variance threshold then the classification can be deemed unknown . This allows the classification system to determine when it does not know into which class a query document belongs rather than misclassifying the document into an incorrect category.

Embodiments of the invention and all of the functional operations described in this specification can be implemented in digital electronic circuitry or in computer software firmware or hardware including the structures disclosed in this specification and their structural equivalents or in combinations of one or more of them. Embodiments of the invention can be implemented as one or more computer program products i.e. one or more modules of computer program instructions encoded on a computer readable medium for execution by or to control the operation of data processing apparatus. The computer readable medium can be a machine readable storage device a machine readable storage substrate a memory device a composition of matter effecting a machine readable propagated signal or a combination of one or more them. The term data processing apparatus encompasses all apparatus devices and machines for processing data including by way of example a programmable processor a computer or multiple processors or computers. The apparatus can include in addition to hardware code that creates an execution environment for the computer program in question e.g. code that constitutes processor firmware a protocol stack a database management system an operating system or a combination of one or more of them. A propagated signal is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.

A computer program also known as a program software software application script or code can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data e.g. one or more scripts stored in a markup language document in a single file dedicated to the program in question or in multiple coordinated files e.g. files that store one or more modules sub programs or portions of code . A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.

The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by and apparatus can also be implemented as special purpose logic circuitry e.g. an FPGA field programmable gate array or an ASIC application specific integrated circuit .

Processors suitable for the execution of a computer program include by way of example both general and special purpose microprocessors and any one or more processors of any kind of digital computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Generally a computer will also include or be operatively coupled to receive data from or transfer data to or both one or more mass storage devices for storing data e.g. magnetic magneto optical disks or optical disks. However a computer need not have such devices. Moreover a computer can be embedded in another device e.g. a mobile telephone a personal digital assistant PDA a mobile audio player a Global Positioning System GPS receiver to name just a few. Information carriers suitable for storing computer program instructions and data include all forms of non volatile memory media and memory devices including by way of example semiconductor memory devices e.g. EPROM EEPROM and flash memory devices magnetic disks e.g. internal hard disks or removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in special purpose logic circuitry.

To provide for interaction with a user embodiments of the invention can be implemented on a computer having a display device e.g. a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device e.g. a mouse or a trackball by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well for example feedback provided to the user can be any form of sensory feedback e.g. visual feedback auditory feedback or tactile feedback and input from the user can be received in any form including acoustic speech or tactile input.

Embodiments of the invention can be implemented in a computing system that includes a back end component e.g. as a data server or that includes a middleware component e.g. an application server or that includes a front end component e.g. a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the invention or any combination of one or more such back end middleware or front end components. The components of the system can be interconnected by any form or medium of digital data communication e.g. a communication network. Examples of communication networks include a local area network LAN and a wide area network WAN e.g. the Internet.

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

While this specification contains many specifics these should not be construed as limitations on the scope of the invention or of what may be claimed but rather as descriptions of features specific to particular embodiments of the invention. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover although features may be described above as acting in certain combinations and even initially claimed as such one or more features from a claimed combination can in some cases be excised from the combination and the claimed combination may be directed to a subcombination or variation of a subcombination.

Similarly while operations are depicted in the drawings in a particular order this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order or that all illustrated operations be performed to achieve desirable results. In certain circumstances multitasking and parallel processing may be advantageous. Moreover the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.

Thus particular embodiments of the invention have been described. Other embodiments are within the scope of the following claims. For example the actions recited in the claims can be performed in a different order and still achieve desirable results.

Moreover the described systems and techniques can be implemented using a variety of ontology and machine learning techniques. The described infrastructure allows different ontology and machine learning methods to be incorporated into one knowledge hierarchy. Thus additional implementations can involve the use of decision tree constructors Conditional Random Field CRF techniques belief networks and other kinds of undirected graphical models given similar heuristics for mapping learned relationships to augment the ontology. In addition a hybrid ontology can be augmented directly by identifying concepts or features in text and clustering them in a flat or hierarchical fashion this can be accomplished using statistical machine learning methods.


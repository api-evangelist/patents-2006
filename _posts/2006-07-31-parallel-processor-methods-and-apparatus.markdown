---

title: Parallel processor methods and apparatus
abstract: Methods and apparatus for parallel processors are disclosed. A policy module is loaded from a main memory of a processor into the local memory of a selected secondary processing element under control of a policy module manager running on the secondary processing element. A selected one or more work queues are assigned from a main memory to a selected one or more of the secondary processing elements according to a hierarchy of precedence. A policy module for the selected one or more work queues is loaded to the selected one or more secondary processing elements. The policy module interprets the selected one or more of the selected one or more work queues. Under control of the policy module, work from one or more of the selected one or more work queues is loaded into the local memory of the selected secondary processing element. The work is performed with the selected secondary processing element. After completing the work or upon a pre-emption, control of the selected secondary processing element is returned to the policy module manager.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07975269&OS=07975269&RS=07975269
owner: Sony Computer Entertainment Inc.
number: 07975269
owner_city: Tokyo
owner_country: JP
publication_date: 20060731
---
This application is a continuation in part of and claims priority from U.S. patent application Ser. No. 11 238 077 to John P. Bates et al filed Sep. 27 2005 and entitled CELL PROCESSOR METHODS AND APPARATUS the entire disclosures of which are incorporated herein by reference.

This application is also a continuation in part of and claims priority from U.S. patent application Ser. No. 11 238 087 entitled SPU TASK MANAGER FOR CELL PROCESSOR to John P. Bates Payton R. White Richard Stenson Howard Berkey Attila Vass and Mark Cerny filed Sep. 27 2005 the entire disclosures of which are incorporated herein by reference.

This application is also a continuation in part of and claims priority from U.S. patent application Ser. No. 11 238 095 entitled CELL PROCESSOR TASK AND DATA MANAGEMENT to Richard B. Stenson and John P. Bates Sep. 27 2005 now U.S. Pat. No. 7 522 168 the entire disclosures of which are incorporated herein by reference.

This application is also a continuation in part of and claims priority from U.S. patent application Ser. No. 11 238 085 entitled METHOD AND SYSTEM FOR PERFORMING MEMORY COPY FUNCTION ON A CELL PROCESSOR to Antoine Labour John P. Bates and Richard B. Stenson filed Sep. 27 2005 now U.S. Pat. No. 7 506 123 the entire disclosures of which are incorporated herein by reference.

This application is also related to commonly assigned U.S. patent application Ser. No. 11 238 086 entitled OPERATING CELL PROCESSORS OVER A NETWORK to Tatsuya Iwamoto filed Sep. 27 2005 the entire disclosures of which are incorporated herein by reference.

This application is also related to commonly assigned U.S. patent application Ser. No. 11 257 761 entitled SECURE OPERATION OF CELL PROCESSORS to Tatsuya Iwamoto filed Oct. 24 2005 the entire disclosures of which are incorporated herein by reference.

This invention generally relates to parallel processing and more particularly to scheduling across various programming models in parallel processors.

A major advance in electronic computation has been the development of systems that can perform multiple operations simultaneously. Such systems are said to perform parallel processing. Recently cell processors have been developed to implement parallel processing on electronic devices ranging from handheld game devices to main frame computers. A typical Cell processor has a power processor unit PPU and up to 8 additional processors referred to as synergistic processing units SPU . Each SPU is typically a single chip or part of a single chip containing a main processor and a co processor. All of the SPUs and the PPU can access a main memory e.g. through a memory flow controller MFC . The SPUs can perform parallel processing of operations in conjunction with a program running on the main processor. The SPUs have small local memories typically about 256 kilobytes that must be managed by software code and data must be manually transferred to from the local SPU memories. For high performance this code and data must be managed from SPU software PPU software involvement must be minimized . There are many techniques for managing code and data from the SPU. Often different techniques for managing code and data from the SPU need to operate simultaneously on a cell processor. There are many programming models for SPU driven task management. Unfortunately no single task system is right for all applications.

One prior art task management system used for cell processors is known as SPU Threads. A thread generally refers to a part of a program that can execute independently of other parts. Operating systems that support multithreading enable programmers to design programs whose threaded parts can execute concurrently. SPU Threads operates by regarding the SPUs in a cell as processors for threads. A context switch may swap out the contents of an SPU s local storage to the main memory and substitute 256 kilobytes of data and or code into the local storage from the main memory where the substitute data and code are processed by the SPU. A context switch is the computing process of storing and restoring the state of a SPU or PPU the context such that multiple processes can share a single resource. Context switches are usually computationally intensive and much of the design of operating systems is to optimize the use of context switches.

Unfortunately interoperating with SPU Threads is not an option for high performance applications. Applications based on SPU Threads have large bandwidth requirements and are processed from the PPU. Consequently SPU threads based applications are not autonomous and tend to be slow. Because SPU Threads are managed from the PPU SPU context switching swapping out the current running process on an SPU to another waiting process takes too long. Avoiding PPU involvement in SPU management can lead to much better performance for certain applications

To overcome these problems a system referred to as SPU Runtime System SPURS was developed. In SPURS the memory of each SPU has loaded into it a kernel that performs scheduling of tasks handled by the SPU. Groups of these tasks are referred to as Tasksets. SPURS is described in PCT Application PCT JP2006 310907 to Keisuke Inoue and Seiji Murata filed May 31 2006 entitled METHOD AND APPARATUS FOR SCHEDULING IN A MULTI PROCESSOR SYSTEM and in U.S. Patent Application Publication No. 20050188373 to Keisuke Inoue Tatsuya Iwamoto and Masahiro Yasue Filed Feb. 20 2004 and entitled METHOD AND APPARATUS FOR TASK MANAGEMENT IN A MULTI PROCESSOR SYSTEM and in U.S. Patent Application Publication No. 20050188372 to Keisuke Inoue and Tatsuya Iwamoto filed Feb. 20 2004 and entitled METHOD AND APPARATUS FOR PROCESSOR TASK MIGRATION IN A MULTI PROCESSOR SYSTEM and in US Provisional Patent Application No. 60 650 153 to Keisuke Inoue and Masahiro Yasue filed Feb. 4 2005 and entitled PROCESSOR TASK MIGRATION OVER A NETWORK IN A MULTI PROCESSOR SYSTEM the disclosures of all four of which are incorporated herein by reference. Unfortunately SPURS like SPU Threads uses context switches to swap work in and out of the SPUs. The work is performed on the SPUs rather than the PPU so that unlike in SPU Threads there is autonomy of processing. However SPURS suffers from the same overhead of context switches as SPU Threads. Thus although SPURS provides autonomy it is not suitable for many use cases.

SPURS is just one example of an SPU task system. Middleware and applications will require various task systems for various purposes. Currently SPURS runs as a group of SPU Threads so that it can interoperate with other SPU Threads. Unfortunately as stated above SPU Threads has undesirable overhead so using it for the interoperation of SPU task systems is not an option for certain high performance applications.

In cell processing it is desirable for middleware and applications to share SPUs using various task systems. It is desirable to provide resources to many task classes e.g. audio graphics artificial intelligence AI or for physics such as cloth modeling fluid modeling or rigid body dynamics. To do this efficiently the programming model needs to manage both code and data. It is a challenge to get SPU middleware to interoperate with no common task system. Unfortunately SPU Threads and SPURS follow the same programming model and neither model provides enough performance for many use cases. Thus application developers still have to figure out how to share limited memory space on the SPUs between code and data.

Thus there is a need in the art for a cell processor method and apparatus that overcomes the above disadvantages.

To overcome the above disadvantages embodiments of the invention are directed to methods and apparatus for cell processors having one or more central processors and one or more synergistic processing units SPU each SPU having a processor and a local memory. According to an embodiment of the invention managing code and data on one or more of the SPUs can be implemented by an inventive method. According to this method a policy module is loaded from a main memory into the local memory of a selected SPU under control of an SPU policy module manager SPMM running on one or more of the SPUs. The policy module may be configured to load a work queue from the main memory into the local memory of the SPU. Under control of the policy module one or more tasks are loaded from the main memory into the local memory of the selected SPU. The policy module may be configured to interpret and process one or more tasks from the work queue on the SPU. The selected SPU performs the task s and after completing the tasks or upon a pre emption returns control of the SPU to the SPMM.

Embodiments of the present invention provide solutions for efficient interoperation of SPU policy modules.

Although the following detailed description contains many specific details for the purposes of illustration anyone of ordinary skill in the art will appreciate that many variations and alterations to the following details are within the scope of the invention. Accordingly the exemplary embodiments of the invention described below are set forth without any loss of generality to and without imposing limitations upon the claimed invention.

The PPU acts as a controller for the SPUs which handle most of the computational workload. The PPU may also be used to run conventional operating systems if it is sufficiently similar to other 64 bit PowerPC processors and if the SPUs are designed for vectorized floating point code execution. By way of example the PPU may contain a 32 KiB instruction and data Level 1 cache and a 512 KiB Level 2 cache.

The PPU and SPUs can exchange code and data with each other over an exchange interface bus EIB . The PPU and SPUS can also exchange code and data stored in a main memory e.g. via the EIB and a memory flow controller MFC such as a digital memory access DMA unit or the like. The EIB may be a circular bus having two channels in opposite directions. The EIB may also be connected to the Level 2 cache the MFC and a system interface such as a FlexIO for external communications.

Each SPU includes a local memory . Code and data obtained from the main memory can be loaded into the local memory so that the SPU can process tasks. As shown in the inset a small software manager referred to herein as an SPU Policy Module Manager SPMM resides in the local memory of each SPU . Preferably the SPMM takes up only a small fraction of the total memory space available in each local memory e.g. less than about 1 of each SPU memory . The heart of SPMM is referred to as an SPMM Kernel which typically takes up about 2 KB resident on each SPU. For a 256K local storage this represents about 0.8 SPU Local Store usage.

The SPMM manages policy modules. The SPMM Kernel provides Priority based Work Queue scheduling. As used herein the term Work Queue sometimes also called a Work Load refers to work defined at some location in a memory such as SPU work defined in main memory . This is often a queue of task definitions however other arrangements of work definitions may be used. A Policy Module associated with the work queue determines how this work is interpreted and executed. Thus the policy module interprets the Work Queue . A Work Queue is usually a group of tasks or jobs that can be processed by multiple SPUs. SPURS Tasksets or SPU Task Management STM job lists are examples of Workloads.

As used herein Policy Module refers to a small manager object on an SPU that defines a programming model and task execution scheme. A policy module may be implemented as an SPU binary code for processing Work Queues . The software development kit SDK for a given application implemented in accordance with embodiments of the present invention may use different policies depending how a given work queue is to be implemented. For example one policy may be used for multi tasking and another policy may be used for job streaming. Job streaming is a popular term for the STM processing model. A given Policy Module can manage code and or data in the remaining SPU memory to execute SPU work. Policy Modules may be transferred from main RAM to SPU local storage to execute as needed by the current Work Queue. Other schemes of transferring a Policy Module from SPU to SPU are also recognized by the present inventors. The SPMM Kernel typically chooses a new Work Queue every time it runs.

The SPMM implements the simplest set of features to enable scheduling of SPU work. Therefore Policy Modules must manage their own context data. Policy Modules must determine their context data from a pointer to SPU work. A Policy Module image may be preprocessed with state data. For example initialize some global data in the Policy Module ELF image based on runtime options. During execution of a Policy Module state data may be changed by the SPU and passed from a Policy Module running in the SPU to main memory. The PPU may also change the state of state data in a Policy Module. The Policy Module is typically stored as an ELF image which refers to executable linkable file format. After an ELF has been processed and linked to an image that image is ready to execute.

Embodiments of the present invention are able to avoid context switches because the work is loaded under control of the policy module running on the SPU . Although context switches are generally not performed by the SPMM Kernel policy Modules may perform context switches because their implementation of is completely up to developers. However many task systems will not need their context switched out because they already manage code data in the SPU Local Store . For example SPURS will context switch SPURS Tasks to from Local Store but the SPURS Kernel does not need its own context to be saved restored.

By way of example policy modules and work queues may be associated as follows. As shown in the lower inset in the main memory may contain a work queue array having a set of work definitions . The work queue array may include any number of work definitions for any number of corresponding work queues . The number of work queues that the work queue array can accommodate may depend on the availability for processing the work queues. In some embodiments the work queues may be very efficiently scheduled amongst the SPUs if there are definitions for sixteen work queues in the work queue array . Scheduling of sixteen work queues may be particularly efficient e.g. where the SPMM kernel has a limited amount of overhead e.g. about 2 kilobytes and the cell processor has eight SPUs.

Table I illustrates an example of a data structure for a work definition stored in the work queue array .

Table I represents one possible work definition among others. The particular contents of work definitions data structures may vary from that of Table I. In general each of the work definitions includes a pointer to a memory address for the corresponding work queue WQ . The memory address includes SPU work defined in RAM which may contain both code and data for the work queue . Examples of work queues include tasks characterized e.g. Tasksets or Task Queues and jobs characterized by job chains. Both tasks and jobs may contain code and or data associated with performing associated work. Tasks may be distinguished from jobs as follows. As used herein a task refers to work that is similar to a thread in that it has an associated context. As such a task can be swapped in and out of an SPU at some intermediate stage of completion. A job by contrast has no associated context. Consequently a job runs complete i.e. once an SPU starts working on a job the SPU runs until the job is complete. The data associated with a job may define how much space in SPU local memory is needed to complete the job.

The Work Queue definition may also include a value for the state of the corresponding WQ . Work Queue states can be set from either the PPU or SPU . Examples of states include a SPM READY which means that the WQ contains ready work b SPM EMPTY WQ which means that the WQ contains no ready work and c SPM FREE WQ which means that the WQ is no longer in use.

Contention refers to the number of SPU that are working on a given WQ . Depending on the type of work there may be a maximum number of SPUs that can work on a given WQ . This number may be stored as part of the work definition . Certain types of work require two or more SPUs for efficient processing. In such cases the output of one SPU may be forwarded to a second SPU for further processing. To address such cases it is useful to define a minimum contention i.e. a minimum number of SPUs needed to process the work. This number can also be stored as part of the work definition . The choice of the value of minimum contention for a particular work queue may depend on the nature of the work within the work queue. For example task work queues often have interdependencies amongst tasks and may benefit from assignment of more than one SPU to their work. Consequently it may be desirable to set a minimum contention value for a task work queue equal to the total number of SPUs in the cell processor e.g. eight for the system shown in . Job work queues by contrast tend to work better if contention for their work is balanced amongst the SPUs . Consequently it may be more desirable to set the minimum contention value for a job work queue equal to 1.

Each work queue is associated with a Policy Module. In the case represented by Table I the association between a work queue and its corresponding policy module is by way of a pointer to the memory address of a policy module definition . A Policy Module may be associated with multiple work queues. Table II illustrates an example of a policy module definition.

In Table II the Code image offset refers to the entry point address for the first function call. In this example the policy module definition includes a pointer to a memory address containing the code image of the policy module. It is this code image that is loaded into the local storage by the SPMM . The loaded image then runs the SPU as a policy module . The policy module controls the loading of work e.g. context data in the form of code or data into the local storage from the work queue in main memory .

If the WQ s contention is less than its max contention value the SPMM then checks whether the value of CHOICE is valid at step . For example if the initial value of CHOICE none it is invalid. If CHOICE is invalid the value of CHOICE is set equal to the value of W at step and the work corresponding to the value of W is selected for processing by the SPU . The value of W is set to point to the next WQ definition in the WQ array at step and the algorithm returns to step .

Referring again to once a WQ is selected from among the Work definitions in the WQ array the SPMM loads the corresponding policy module from the main memory into the local storage of the SPU at step . As described above loading a policy module from the main memory may include reading from the work definition in a work queue array a memory address of the policy module code image in the main memory . The SPMM implements the simplest set of features to enable scheduling of work on the SPU . The SPMM does not assume anything about the contents of the WQ . Instead the Policy Module determines WQ contents and usage and manages its own context data.

When a WQ is chosen its contention value is incremented its policy module code image is transferred to local storage unless it is already resident and the policy entry function is called with a pointer to the WQ . At step under control of the policy module the selected WQ is processed e.g. one or more tasks from a work queue are loaded from the main memory into the local memory .

In practice Work Queue contents will often be either an atomic mutex that protects a shared task queue or lock free data related to a set of tasks. WQ array access may be serialized using an atomic lock to ensure that WQ contention is balanced and or ensures that max contention rules are followed. Lock free access to work queues is also possible however the code becomes more complex. In some cases only 128 byte sections of Work Queue definitions can be examined at a time. In addition it is not clear whether performance of the cell processor would be any different. Performance may be better with few Work Queues but may be worse with many Work Queues.

At step the SPU performs one or more tasks from the work queue . Work Queues can be processed by one or more SPUs simultaneously. As described above a maximum number of contending SPUs max contention can be set for each WQ . The Policy Modules returns control of the SPU to the SPMM Kernel at step if either the previous work was completed or previous work was preempted.

The SPMM schedules multiple SPU work queues based on priorities and readiness. When higher priority work becomes available the SPMM can assign high priority tasks to available SPUs. Embodiments of the present invention are said to be semi preemptive in that they can also implement cooperative preemption. Specifically the Policy Module can periodically check an SPU event channel to determine preemption. If and when preemption occurs the Policy Module can clean up and return control of the SPU to the SPMM . When higher priority work becomes ready preemption events are sent to lower priority SPUs Policy Modules return to allow processing of higher priority work. Priority and contention of a given task or work queue may be stored as part of the work definition in the work queue array.

The Policy Modules have one required function execute EA work queue . . . where EA stands for effective address the main memory address of the WQ from the chosen WQ definition. The function does not have to be called execute but it must take a single 32 bit parameter. The size of the parameter e.g. 32 bits may vary based on the implementation but the parameter is the address of the chosen Work Queue. The execute function represents the entry point for the Policy Module. This function should process the Work Queue passed to it via the work queue parameter. The execute function may be configured to return state information which can be passed to main memory in the PPU depending on the particular configuration of the Policy Module. However the execute function to process a work queue will typically not return a value.

To end processing of a WQ the policy module calls a function referred to herein as spmm release work new state which decrements the contention value for the WQ and sets the value of the WQ state to new state typically SPMM READY or SPMM EMPTY depending on whether processing ends due to completion or pre emption . To return control of the SPU to SPMM the policy module calls a function referred to herein as spmm exit that resets a stack pointer to top of LS and chooses a new WQ. The policy module may be reused by the next chosen WQ so policy module s stack size is typically at least 4 KB.

In embodiments of the present invention spmm release work is usually called immediately before spmm exit. There is an important reason why they are not a single function. Often WQs will allow new work to be added at any time. When new work is added to a WQ its state must be set to SPM READY. The Policy Module must prevent undesirable race conditions flaws in a process where the output exhibits unexpected critical dependence on the relative timing of events with proper atomics.

For example after the SPU processes the last task in WQ it may check the state of WQ . The PPU may add work to the WQ and set its state to SPM READY before the Policy module calls spmm release work new state . If the Policy Module subsequently calls spmm release work SPM EMPTY the state of WQ will be incorrect when the policy module subsequently calls spmm exit .

A preferred technique to prevent such invalid WQ states proceeds as follows. After the SPU processes the last task in WQ the Policy Module locks an atomic mutex for the WQ and then checks for more tasks in the WQ . If there are no more tasks the Policy Module calls spmm release work SPM EMPTY and the state of the WQ is set to SPM EMPTY. The policy module then unlocks the atomic mutex for the WQ . If the PPU wants to add work to the WQ the PPU locks the atomic mutex for the WQ adds the work sets the state of the WQ to SPM READY and unlocks the atomic mutex for the WQ . Even if the work is added before the Policy Module can call spmm exit the state of the WQ will be correct.

It should be noted that even if the WQ is never reused state is set to SPM FREE by PMs upon completion of tasks there is still a potential race condition without atomics. Specifically suppose a policy module running on a first SPU processes the last task in the WQ checks for more tasks in the WQ and determines that there are none. The SPU then signals the PPU that the WQ is complete. After the PPU has received the signal it frees or reuses the memory space allotted for the WQ . Either way the memory space allotted for the WQ may contain garbage data. If before the policy module on the first SPU can call spmm release work SPM FREE a second SPU under the control of the SPMM then chooses the WQ the second SPU may crash when it attempts to process the garbage data from the memory space allotted for WQ .

In avoiding this race condition with atomics it is important that the first SPU not signal the PPU that the WQ is complete before calling spmm release work SPM FREE . To address the possibility that the second SPU may choose the WQ after the mutex is locked but before calling spmm release work the policy module on the first SPU can check the contention of the WQ . If a second SPU has chosen the WQ and loaded its policy module the contention will not be equal to zero which would be expected for completed work . Instead of signaling the PPU that the work is complete the first SPU unlocks the mutex and calls spmm exit. The policy module running on the second SPU checks for more work on the WQ and upon finding none calls spmm release work SPM FREE sets the contention of the WQ equal to zero signals the PPU that the WQ is complete and unlocks the mutex.

Although the above discussion addresses atomic mutex locking to avoid race condition it may also be possible to avoid such conditions in a lock free manner. In general once the policy module determines that a given WQ contains no more tasks the policy module needs to prevent task states from being changed while the WQ state is set to empty. Alternatively the processes of adding work or finishing work must be atomic. In the context of embodiments of the present invention adding work includes setting the task state to ready in the policy module space and setting the SPMM WQ state to ready. Finishing work refers to a condition in which the last task is either taken or set to a not ready state. Finishing work includes updating the task state to not ready and setting the SPMM WQ state to not ready .

The SPMM may enable scheduling of multiple work Queues with different priorities. Higher priority queues can be processed before lower. When processing queues of equal priority SPUs can balance contention. If the highest priority queue does not allow any more contention other SPUs will work on the next highest priorities. Examples of pre emption by the SPMM can be understood by again referring to . The policy module processing the WQ on the SPU continues to analyze the other WQ s represented by pointer W in the Work Queue array . At this point the steps of the algorithm may be implemented as part of the policy module running the process on the SPU . Recall that at step the SPMM checked whether the value of CHOICE which indicates the WQ currently being processed by the SPU is valid. If at step the value of CHOICE is still valid the SPMM compares the priority of the chosen WQ to the priority of W. If W s priority is higher this represents a pre emption event and control of the SPU returns to the SPMM . The value of CHOICE is set to the value of W which represents a different work queue at step and the algorithm returns to step to increment the pointer W.

Pre emption may also occur under certain circumstances when W and CHOICE are of equal priority at step . For example if at step the policy module finds that W and CHOICE are of equal priority and W s contention is greater than zero but less than a minimum contention value min contention control of the SPU may be returned to the SPMM at step . In such a case W is said to be urgent . The concept of minimum contention is useful in situations where work cascades from one SPU to one or more other SPUs. For example in many physics applications such as cloth simulation the output of one SPU serves as input for another SPU running a different part of a process implementing a given work queue. When such a work queue is of equal priority to other work being processed the algorithm attempts to balance the contention by making SPUs available for such a process. If enough SPUs are available to process the work the output may be loaded directly into the next SPU. Alternatively the output may be temporarily stored in the main memory until additional SPUs become available to process the output.

If at step it is determined that W is not urgent the policy module can check at step whether CHOICE represents urgent work e.g. by determining whether CHOICE s contention is greater than zero but less than its min contention. If CHOICE is urgent the policy module continues to check other work e.g. by returning the algorithm to step . If CHOICE is not urgent the policy module can balance contention by checking whether W s contention is less than CHOICE s contention at step and if so setting CHOICE equal to W at step and returning control to the SPMM . If W s contention is not less than CHOICE s contention the policy module can check whether W and CHOICE are of equal contention at step . If not the policy module continues to check other work e.g. by returning the algorithm to step . If W and CHOICE are of equal contention the policy module may check at step whether there is affinity i.e. if W s policy is already loaded into SPU . If there is affinity the policy module may set CHOICE equal to W at step and return control of the SPU to the SPMM .

From the above discussion of it may be seen that work queues may be assigned to SPUs at step of according to a hierarchy of precedence. By way of example a fiver tiered hierarchy for the order of precedence of assigning work described with respect to may be summarized as follows.

In embodiments of the present invention additional considerations in determining precedence may enhance performance through a more even distribution of work amongst the various SPUs . Specifically a work definition may be structured as shown in Table III below.

In Table III the Ready Count refers to a number of SPUs requested by a work queue . A Ready Count value of zero may be used to mean there is nothing to process in a particular work queue . For example if a policy module determines there is nothing to do in a particular work queue it may reset the Ready Count to zero. A Ready Count value greater than zero is an estimate of the number SPUs that would be productive for processing a given work queue. The Ready Count value need not be an optimal number of SPUs for working on a particular work queue. Instead the Ready Count value may represent an acceptable number of SPUs for working on a particular work queue .

In some situations a particular workload may benefit To further optimize the number of SPUs assigned to a particular work queue may benefit if its work could be assigned to some number of available SPUs in addition to the Ready Count value. To address such a situation the work queue definition may include an Idle SPU Request Count also known as an Idle Count . If there are idle SPUs the Idle Count represents a number of SPUs in addition to the Ready Count value that can help out with processing a work queue . The advantage of the idle count is that it allows SPUs to begin processing a work queue even if the number of available SPUs is less than ideal. For example suppose a work queue would ideally be processed by four SPUs but could acceptably be processed by one SPU. The work definition for such a work queue may have a Ready Count value of 1 and an idle count value of 3. Use of the idle count allows such a work queue to begin processing if a single SPU becomes available but keeps open the possibility of assigning up to three additional SPUs as they become available.

It is noted that the work definition may have more than one level of idle count. For example the work definition may have first second and third level idle counts of 3 2 and 1 respectively. In such a case three additional SPUs are quested if available if not two are requested if available and if not one is requested if available.

As shown in Table III the work definition may also take into account the type of work within a given work queue when assigning precedence. For example all other considerations being equal certain types of work queues may take precedence over others based on the nature of the tasks or jobs that are to be performed. To address this situation the work definition may include a Work Queue ID that is associated with the type of work to be performed. The Work Queue ID categorizes the Work Queues by the type of work to be performed e.g. memory management work versus application specific calculations.

When Ready Count Idle Count and Work Queue ID are taken into account the hierarchy of precedence for assigning work from the work queues to a particular SPU as described above may be modified to form an eight tiered hierarchy which may be summarized as follows.

The above scheduling precedence is an example of many possible hierarchies of scheduling precedence. The order of precedence may be rearranged and items may be removed to achieve different behavior. For example removing item number 4 Work Queue Affinity may result in work queues being scheduled in a manner that is more preferred by a developer but it may also result in more work queue thrashing when work queues yield to other work queues that go in and out of ready state .

According to embodiments of the invention different developers working on particular programs may be responsible for different scheduling parameters. For example a Policy Module Developer may be responsible for implementing Policy Module PM code and or libraries a PM application programming interface API to create and manipulate Work Queues for using PM code. The Policy Module Developer may determine the Minimum Contention values for policy modules that are to be associated with the Work Queues . The Policy Module Developer may optionally determine the Ready Count and Idle Count values.

A Work Queue Developer may create and manipulate Work Queues through the PM API developed by the Policy Module Developer. The Work Queue Developer may determine the Ready Count and or Idle Count for the Work Queues to the extent allowed by the corresponding policy modules as determined by the Policy Module Developer. The Work Queue Developer may implement the work Queues in the form code and or data libraries.

An Integrator takes code and or libraries from the above developers and combines them to form an application that shares a single SPU Task System Instance. As used herein an SPU Task System Instance refers to the context of one instantiation of an SPU Task System within an application. The SPU Task System instance may include an associated SPU Thread Group e.g. a particular assignment of Work Queues amongst the SPUs in the cell processor . For typical applications the cell processor usually but not invariably implements one SPU Task System Instance at a time. The Integrator may determine Maximum Contention values for each Work Queue and develops an SPU Priority List containing Per SPU priorities for each Work Queue . The SPU Priority List provides the integrator with fine control over Workload scheduling.

Operation of a cell processor using the precedence hierarchy of Table III may be understood with reference to . As shown in multiple Work Queues . . . may be scheduled for implementation by eight SPUs . In only six SPUs are depicted in the SPU Thread Group for the purpose of illustration. By way of example and without limitation SPMM Kernel may use an Atomic Notifier to schedule the Work Queues . . . . The Atomic Notifier may be e.g. a 128 byte data structure that contains relevant Work Queue scheduling parameters. The Work Queues . . . may include a task Work Queue having a task policy module task mod and work defined by a task set . In some embodiments there may be an upper limit on the number of tasks in the task Work Queue . For example there may be an upper limit of 128 tasks for a given tasks Work Queue even if the Main Memory could accommodate more. The Work Queues . . . may further include a job Work Queue having a job policy module job mod and work defined by a job chain . The number of jobs in the job chain may be almost unlimited e.g. limited only by considerations of memory space available in the Main Memory . The Work Queues . . . may additionally include a custom Work Queue having a custom policy module x mod and custom work e.g. code and data of some kind. The Custom Work Queue may fall into a category that does not fit the description of either a task or a job as defined herein.

Each of the Work Queues . . . includes corresponding work definitions . . . that include per SPU priority values and stored values of Ready Count Ready Maximum Contention Max and Minimum Contention Min. The combined priority values for the contending Work Queues . . . form a priority table . In each column of the priority table corresponds to a particular SPU in the cell processor . Each row of the priority table corresponds to a particular Work Queue. In the example depicted in all the Work Queues Work Queues . . . have equal Readiness e.g. Contention

Based on criteria 1 and 2 of the above hierarchy initially SPUs are assigned to Work Queues for which Ready Count is less than contention. Once all three Work Queues are equally Schedulable and Ready criterion 3 Priority takes precedence. However in this example all Work Queues have equal priority for all SPUs. Furthermore since all Work Queues have equal policy module affinity and equal urgency the remaining SPUs are assigned to the Work Queues according to criterion 6 i.e. in a manner that balances contention. Thus SPU0 and SPU2 are assigned to work on Work Queue SPU3 and SPU5 are assigned to work on Work Queue and SPU4 and SPU6 are assigned to work on Work Queue . Thus in this example the priority table has been configured such that contention is balanced as much as possible amongst the Work Queues competing for available SPUs.

The position of Ready Count in the hierarchy may be used assure that no particular Work Queue can monopolize all the available SPUs in the cell processor . For example in the priority table is configured such that Work Queue has priority 1 for all SPUs and Work Queues have priority 2. In addition Work Queues all have Ready Count 1 Maximum Contention 8 Minimum Contention 1 and Idle Request Count 8. Initially SPUs are assigned to the Work Queues based on readiness i.e. on whether Contention is less than or equal to Ready Count. Since initially Readiness takes precedence over Priority SPU0 is assigned to Work Queue SPU1 is assigned to Work Queue and SPU2 is assigned to Work Queue . At this point all three Work Queues have equal Readiness . Consequently based on the hierarchy described above SPU3 SPU4 and SPU5 are subsequently assigned to Work Queue based on its lower priority value. It is noted that this assignment does not violate the Schedulability requirement since Contention 4 for Work Queue which is still less than Ready Count Idle Request Count 9.

It is noted that depending on the Ready Count value for the Work Queues it is possible for more SPUs to be assigned to work of lower priority. For example depicts a situation For example in the priority table is configured such that Work Queue has priority 1 for all SPUs and Work Queues have priority 2. However Work Queue has a Ready Count of 1 which Work Queues have Ready Counts of 8 . Thus initially SPU0 is assigned to Work Queue SPU1 is assigned to Work Queue and SPU2 is assigned to Work Queue . At this point Readiness has been satisfied for Work Queue but not for Work Queues . Thus based on the hierarchy described above the remaining available SPUs will be assigned to Work Queues for which Contention is less than Ready Count even if they are less preferable in terms of priority. It is further noted that assuming equal policy module affinity the assignment of the available SPUs is done in a way that attempts to balance contention between Work Queue and Work Queue . This is because given equal readiness equal priority equal policy module affinity and equal urgency criterion 6 balanced contention applies.

As described above the hierarchy parameters in general and the priority table in particular may be configured to prevent monopolization of SPUs by a single work queue. However it is also possible within the context of this hierarchy to configure the parameters and priority table so that one Work Queue does monopolize all available SPUs. For example as shown in the priority table may be configured as follows. Work Queues all have Ready Count 8 Max Contention 8 Min Contention 1 and Idle Request Count 8. Work Queue has Priority 1 for all SPUs while Work Queues have Priority 2 for all SPUs. Note that in this example all Work Queues have equal readiness until Ready Count Contention. However because the ready count is greater than or equal to the number of available SPUs this is true for all three Work Queues . Since no Work Queue is more Ready than any other SPUs are assigned based on priority according to criterion 3 and all available SPUs are assigned to Work Queue .

In embodiments of the present invention the SPMM may be optimized for interoperability. Examples of such interoperability are depicted diagrammatically in . For example the SPMM may be implemented as a particularly advantageous type of normal SPU thread as depicted in . As such a processing system that implements SPMM can interoperate with SPU threads or vice versa. In such an embodiment SPU Threads can be useful to accommodate rare high priority preemptions.

Furthermore as depicted in SPURS may be implemented as one possible policy module within the context of SPMM. Thus SPURS can operate on certain tasks where SPMM can replace SPURS code with something else for work on other tasks. SPMM can schedule multiple SPURS tasksets next to other SPU task systems obeying priorities. Furthermore it is possible for SPU Threads to implement both SPURS and SPMM as different possible SPU Threads. As such embodiments of the present invention can be fully interoperable with both SPURS and SPU threads. The feature of scheduling across various programming models is important. This feature is especially useful for the Cell processor and other parallel processor devices with similar features.

As may be seen from SPMM may be incorporated into an SPU Task System such as SPURS. In such a case the SPU Task System Instance referred to above may be designated a SPURS Instance i.e. the context of one instantiation of SPURS including an associated SPU Thread Group. The SPMM may be regarded as a SPURS Kernel e.g. a relatively small amount of binary code that is resident on all SPURS SPU Threads that schedules the Work Queues and loads Policy Modules to the SPU Local Store .

The advantage of SPMM is that SPURS and other policies can be easily switched between as the nature of the work requires. For example depicts a memory map for a local storage in which SPU Task Manager STM is implemented as a policy under SPMM . The STM policy manages a STM taskset having a task code and one or more sets of task data The STM policy reads one or more task definitions stored in the main memory into the local storage . Based on information contained in the task definitions the SPU loads code and or data related to the task definitions from the main memory into the local memory associated with the selected SPU. The selected SPU then performs one or more tasks using the code and or data. STM can be modified to run on SPMM with same program start address for STM tasks. When running under SPMM the STM policy does not need to manage multiple tasksets. STM is described in detail in commonly assigned U.S. patent application Ser. No. 11 238 087 entitled SPU TASK MANAGER FOR CELL PROCESSOR to John P. Bates Payton R. White Richard Stenson Howard Berkey Attila Vass and Mark Cerny which is filed the same day as the present application the entire disclosures of which are incorporated herein by reference.

Another possible policy that may be loaded under SPMM is known as Cell Processor Task and Data Management CTDM . depicts a memory map for a local storage in which a CTDM is implemented as a policy under SPMM . The CTDM policy allows the SPU to break up sets of data that are too large to fit in the local storage into smaller segments than can be processed by code running on the SPU. The data can also be divided up into groups of a size suitable for processing on a subsequent processor such as a graphics card. CTDM is described in detail in commonly assigned U.S. patent application Ser. No. 11 238 095 entitled CELL PROCESSOR TASK AND DATA MANAGEMENT to Richard B. Stenson and John P. Bates which is filed the same day as the present application the entire disclosures of which are incorporated herein by reference.

Embodiments of the present invention are the result of a broad understanding of existing SPU programming models. It is often the case that one model is never perfect for all applications. Consequently engineers tend to develop custom programming models. Interoperability is not a problem when all the SPU code is written by the same company. However interoperability can be a problem when middleware is needed that SPU code must interoperate efficiently.

Parallel processor units of the type depicted in operating as described above may be implemented as part of a larger processing system as depicted in . The system may include a cell processor module and a memory e.g. RAM DRAM ROM and the like . In addition the processing system may have multiple cell processor modules . The cell processor module generally includes one or more main processors PPU and one or more SPUs SPU SPU. . . SPUwhich may be configured to operate under the control of an SPMM as described above. The processor module may also include a memory flow controller MFC. The cell processor module may be a cell processor e.g. of the type depicted in . The memory includes data and code configured as described above. Specifically the memory includes a work queue array work queues and policy modules each of which may include code data or some combination of both code and data as described above.

The system may also include well known support functions such as input output I O elements power supplies P S a clock CLK and cache . The system may optionally include a mass storage device such as a disk drive CD ROM drive tape drive or the like to store programs and or data. The controller may also optionally include a display unit and user interface unit to facilitate interaction between the controller and a user. The display unit may be in the form of a cathode ray tube CRT or flat panel screen that displays text numerals graphical symbols or images. The user interface may include a keyboard mouse joystick light pen or other device. The cell processor module memory and other components of the system may exchange signals e.g. code instructions and data with each other via a system bus as shown in .

As used herein the term I O generally refers to any program operation or device that transfers data to or from the system and to or from a peripheral device. Every transfer is an output from one device and an input into another. Peripheral devices include input only devices such as keyboards and mouses output only devices such as printers as well as devices such as a writable CD ROM that can act as both an input and an output device. The term peripheral device includes external devices such as a mouse keyboard printer monitor external Zip drive or scanner as well as internal devices such as a CD ROM drive CD R drive or internal modem or other peripheral such as a flash memory reader writer hard drive.

By way of example and without loss of generality the user interface may be configured e.g. by suitable programming to allow a user to adjust relevant parameters of the work definitions in the work queue array . Specifically the user interface may allow the user to adjust values of Ready Count Idle Request Count Priority Maximum Contention and Minimum Contention so that the user may optimize performance of the program as it runs on the processor module . Such a capability may be particularly useful to users who are developers of the program or portions thereof.

The processor module may manage the performance of tasks in the work queues in response to data and program code instructions of a main program stored and retrieved by the memory and executed by the processor module . Code portions of the program may conform to any one of a number of different programming languages such as Assembly C JAVA or a number of other languages. The processor module forms a general purpose computer that becomes a specific purpose computer when executing programs such as the program code . Although the program code is described herein as being implemented in software and executed upon a general purpose computer those skilled in the art will realize that the method of task management could alternatively be implemented using hardware such as an application specific integrated circuit ASIC or other hardware circuitry. As such it should be understood that embodiments of the invention can be implemented in whole or in part in software hardware or some combination of both. In one embodiment among others the program code may include a set of processor readable instructions that implement a method having features in common with the method of and or the algorithm of .

Although the above discussion addresses cell processors embodiments of the present invention may be implemented using any multi processor scheme. Specifically embodiments of the invention may be implemented in various configurations of parallel processors. For example the invention herein may be implemented in a configuration with multiple SPU like processors but no PPU like processor. The embodiments may be implemented in a parallel processor environment having one or more main processors and more or fewer than eight SPUs or similar processors with local memories . Embodiments of the present invention provide a high performance SPU management solution that is SPU driven with low usage of SPU local storage space. As described above embodiments of the present invention enable parallel SPU processing of work on multiple SPUs. Furthermore the modular nature of the SPMM model is desirable in that it allows various programming models and task systems to interoperate.

While the above is a complete description of the preferred embodiment of the present invention it is possible to use various alternatives modifications and equivalents. Therefore the scope of the present invention should be determined not with reference to the above description but should instead be determined with reference to the appended claims along with their full scope of equivalents. Any feature described herein whether preferred or not may be combined with any other feature described herein whether preferred or not. In the claims that follow the indefinite article A or An refers to a quantity of one or more of the item following the article except where expressly stated otherwise. The appended claims are not to be interpreted as including means plus function limitations unless such a limitation is explicitly recited in a given claim using the phrase means for. 


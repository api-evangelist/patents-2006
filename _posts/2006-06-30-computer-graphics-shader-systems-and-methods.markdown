---

title: Computer graphics shader systems and methods
abstract: Methods and systems are described that unite various shading applications under a single language, enable the simple re-use and re-purposing of shaders, facilitate the design and construction of shaders without need for computer programming, and enable the graphical debugging of shaders.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07548238&OS=07548238&RS=07548238
owner: NVIDIA Corporation
number: 07548238
owner_city: Santa Clara
owner_country: US
publication_date: 20060630
---
This application for patent is a Continuation In Part of commonly owned U.S. application for patent Ser. No. 10 462 500 filed Jun. 16 2003 now U.S. Pat. No. 7 173 617 which is a Divisional of application Ser. No. 10 232 069 filed on Aug. 30 2002 now U.S. Pat. No. 6 606 092 which is a Divisional of U.S. patent application Ser. No. 09 108 596 filed Jul. 1 1998 now U.S. Pat. No. 6 496 190 issued Dec. 17 2002 which claims the benefit of U.S. Provisional Patent Application Ser. No. 60 051 507 filed Jul. 2 1997 each and all of which are incorporated herein by reference.

The invention relates generally to the field of computer graphics computer aided design and the like and more particularly to systems and methods for generating shader systems and using the shader systems so generated in rendering an image of a scene. The invention in particular provides a new type of component useful in a computer graphics system identified herein as a phenomenon which comprises a system including a packaged and encapsulated shader DAG directed acyclic graph or set of cooperating shader DAGs each of which can include one or more shaders which is generated and encapsulated to assist in defining at least a portion of a scene in a manner which will ensure that the shaders can correctly cooperate during rendering.

In computer graphics computer aided geometric design and the like an artist draftsman or other user generally referred to herein as an operator attempts to generate a three dimensional representation of objects in a scene as maintained by a computer and thereafter render respective two dimensional images of the objects in the scene from one or more orientations. In the first representation generation phase conventionally computer graphics systems generate a three dimensional representation from for example various two dimensional line drawings comprising contours and or cross sections of the objects in the scene and by applying a number of operations to such lines which will result in two dimensional surfaces in three dimensional space and subsequent modification of parameters and control points of such surfaces to correct or otherwise modify the shape of the resulting representation of the object.

During this process the operator also defines various properties of the surfaces of the objects the structure and characteristics of light sources which illuminate the scene and the structure and characteristics of one or more simulated cameras which generate the images. After the structure and characteristics of the scene light source s and camera s have been defined in the second phase an operator enables the computer to render an image of the scene from a particular viewing direction.

The objects in the scene light source s and camera s are defined in the first scene definition phase by respective multiple dimensional mathematical representations including at least the three spatial dimensions and possibly one time dimension. The mathematical representations are typically stored in a tree structured data structure. The properties of the surfaces of the objects in turn are defined by shade trees each of which includes one or more shaders which during the second scene rendering phase enables the computer to render the respective surfaces essentially providing color values representative of colors of the respective surfaces. The shaders of a shade tree are generated by an operator or are provided a priori by a computer graphics system in a high level language such as C or C which together enable the computer to render an image of a respective surface in the second scene rendering phase.

A number of problems arise from the generation and use of shaders and shade trees as typically provided in computer graphics arrangements. First shaders generally cannot cooperate with each other unless they are programmed to do so. Typically input values provided to shaders are constant values which limits the shaders flexibility and ability to render features in an interesting and life like manner. In addition it is generally difficult to set up systems of cooperating shaders which can get their input values from a common source.

In order to provide solutions to such problems the above cited U.S. Pat. No. 6 496 190 described a computer graphics system in which a new type of entity referred to as a phenomenon can be created instantiated and used in rendering an image of a scene. A phenomenon is an encapsulated shader DAG directed acyclic graph comprising one or more nodes each comprising a shader or an encapsulated set of such DAGs which are interconnected so as to cooperate which are instantiated and attached to entities in the scene which are created during the scene definition process to define diverse types of features of a scene including color and textural features of surfaces of objects in the scene characteristics of volumes and geometries in the scene features of light sources illuminating the scene features of simulated cameras which will be simulated during rendering and numerous other features which are useful in rendering.

Phenomena selected for use by an operator in connection with a scene may be predefined or they may be constructed from base shader nodes by an operator using a phenomenon creator. The phenomenon creator ensures that phenomena are constructed so that the shaders in the DAG or cooperating DAGs can correctly cooperate during rendering of an image of the scene.

Prior to being attached to a scene a phenomenon is instantiated by providing values or functions which are used to define the values for each of the phenomenon s parameters using a phenomenon editor.

After a representation of a scene has been defined and phenomena attached a scene image generator can generate an image of the scene. In that operation the scene image generator operates in a series of phases including a pre processing phase a rendering phase and a post processing phase. During a pre processing phase the scene image generator can perform pre processing operations such as shadow and photon mapping multiple inheritance resolution and the like. The scene image generator may perform pre processing operations if for example a phenomenon attached to the scene includes a geometry shader to generate geometry defined thereby for the scene. During the rendering phase the scene image generator renders the image. During the post processing phase the scene image generator may perform post processing operations if for example a phenomenon attached to the scene includes a shader that defines post processing operations such as depth of field or motion blur calculations which are dependent on velocity and depth information stored in connection with each pixel value in the rendered image.

The phenomena system described in U.S. Pat. No. 6 496 190 is extremely useful. However in recent years many shading platforms and languages have been developed such that currently existing shader languages are narrowly focused on specific platforms and applications contexts whether hardware shading for video games or software shading for visual effects in motion pictures. This platform dependence typical of conventional shader systems and languages can be a significant limitation.

It would be desirable to provide shader methods and systems that are platform independent and which can unite various shading tools and applications under a single language or system construct.

It would also be desirable to provide such methods and systems which enable the efficient and simple re use and re purposing of shaders such as may be useful in the convergence of video games and feature films an increasingly common occurrence e.g. Lara Croft Tomb Raider .

It would also be desirable to provide methods and systems that facilitate the design and construction of shaders without the need for computer programming as may be useful for artists.

Still further it would be desirable to provide such methods and systems that enable the graphical debugging of shaders allowing shader creators to find and resolve defects in shaders.

The present invention various aspects of which are herein collectively termed Mental Mill addresses the above mentioned limitations of the prior art and provides platform independent methods and systems that can unite various shading applications under a single language herein termed the MetaSL shading language enable the simple re use and re purposing of shaders facilitate the design and construction of shaders without need for computer programming enable the graphical debugging of shaders and accomplish many other useful functions.

One aspect of the invention involves methods and systems that facilitate the creation of simple and compact componentized shaders referred to herein as Metanodes that can be combined in shader networks to build more complicated and visually interesting shaders.

A further aspect of the invention is a Mental Mill shading language referred to herein as MetaSL Meta Shading Language . MetaSL is designed as a simple yet expressive language specifically for implementing shaders. It is also designed to unify existing shading applications which previously were focused on specific platforms and contexts e.g. hardware shading for games software shading for feature film visual effects under a single language and management structure.

The Mental Mill thus enables the creation of Metanodes i.e. shader blocks written in MetaSL that can be attached and combined to form sophisticated shader graphs and phenomena.

The shader graphs provide intuitive graphical user interfaces for creating shaders which are accessible even to users lacking technical expertise to write shader software code.

In a further aspect of the invention the Mental Mill GUI libraries harness the shader graph paradigm to provide a complete GUI for building shader graphs and phenomena.

In another aspect of the invention because the MetaSL shading language is effectively configurable as a superset of all currently existing and future shading languages for specific hardware platforms and hence independent of such instantiations of special purpose graphics hardware it enables the use of dedicated compilers in the Mental Mill system for generating optimized software code for a specific target platform in a specific target shader language such as Cg HLSL or the like from a single re usable MetaSL description of a Phenomenon which in turn is comprised of Metanodes in MetaSL .

The platform language optimized code for the specific target platform language can then be converted to machine code for specific hardware integrated circuit chip instantiations by the native compiler for the shading language at issue. That native compiler need not be and in the general case will not be part of Mental Mill. This can be especially useful for example where a particular chip is available at a given point in time but may soon be superseded by the next generation of that chip.

In this regard it is noted that prior attempts to represent complex visual effects i.e. Phenomena in such shading languages as Cg have led to code that was not optimal for the compilation process to machine code and in such cases the compilation process is extremely slow thereby defeating the desired real time performance or the hardware level shader code was not as efficient as it could be so that the code executed much too slowly. The foregoing aspect of the present invention addresses and resolves this problem associated with such prior efforts.

A further aspect of the invention relates to a novel interactive visual real time debugger for the shader programmer writer i.e. the programmer in the MetaSL shading language in the Phenomenon creation environment. This debugger described in greater detail below allows the effect of a change in even a single line of code to be immediately apparent from visual feedback in a viewport where a test scene with the shader Metanode or Phenomenon under development is constantly rendered.

Further aspects examples details embodiments and practices of the invention are set forth below in the Detailed Description of the Invention.

The present invention provides improvements to the computer graphics entity referred to as a phenomenon which was described in commonly owned U.S. Pat. No. 6 496 190 incorporated herein by reference. Accordingly we first discuss in Section I below the various aspects of the computer graphics phenomenon described in U.S. Pat. No. 6 496 190 and then in Section II which is subdivided into four subsections we discuss the present improvements to the phenomenon entity. 

U.S. Pat. No. 6 496 190 described a new computer graphics system and method that provided enhanced cooperation among shaders by facilitating generation of packaged and encapsulated shader DAGS each of which can include one or more shaders generated in a manner so as to ensure that the shaders in the shader DAGs can correctly cooperate during rendering.

In brief summary a computer graphics system is provided in which a new type of entity referred to as a phenomenon can be created instantiated and used in rendering an image of a scene. A phenomenon is an encapsulated shader DAG comprising one or more nodes each comprising a shader or an encapsulated set of such DAGs which are interconnected so as to cooperate which are instantiated and attached to entities in the scene which are created during the scene definition process to define diverse types of features of a scene including color and textural features of surfaces of objects in the scene characteristics of volumes and geometries in the scene features of light sources illuminating the scene features of simulated cameras which will be simulated during rendering and numerous other features which are useful in rendering.

Phenomena selected for use by an operator in connection with a scene may be predefined or they may be constructed from base shader nodes by an operator using a phenomenon creator. The phenomenon creator ensures that phenomena are constructed so that the shaders in the DAG or cooperating DAGs can correctly cooperate during rendering of an image of the scene.

Prior to being attached to a scene a phenomenon is instantiated by providing values or functions which are used to define the values for each of the phenomenon s parameters using a phenomenon editor.

After a representation of a scene has been defined and phenomena attached a scene image generator can generate an image of the scene. In that operation the scene image generator operates in a series of phases including a pre processing phase a rendering phase and a post processing phase. During a pre processing phase the scene image generator can perform pre processing operations such as shadow and photon mapping multiple inheritance resolution and the like. The scene image generator may perform pre processing operations if for example a phenomenon attached to the scene includes a geometry shader to generate geometry defined thereby for the scene. During the rendering phase the scene image generator renders the image. During the post processing phase the scene image generator may perform post processing operations if for example a phenomenon attached to the scene includes a shader that defines post processing operations such as depth of field or motion blur calculations which are dependent on velocity and depth information stored in connection with each pixel value in the rendered image.

With reference to the computer graphics system in one embodiment includes a computer including a processor module and operator interface elements comprising operator input components such as a keyboard A and or a mouse B generally identified as operator input element s and an operator output element such as a video display device . The illustrative computer system is of the conventional stored program computer architecture. The processor module includes for example processor memory and mass storage devices such as disk and or tape storage elements not separately shown which perform processing and storage operations in connection with digital data provided thereto. The operator input element s are provided to permit an operator to input information for processing. The video display device is provided to display output information generated by the processor module on a screen to the operator including data that the operator may input for processing information that the operator may input to control processing as well as information generated during processing. The processor module generates information for display by the video display device using a so called graphical user interface GUI in which information for various applications programs is displayed using various windows. Although the computer system is shown as comprising particular components such as the keyboard A and mouse B for receiving input information from an operator and a video display device for displaying output information to the operator it will be appreciated that the computer system may include a variety of components in addition to or instead of those depicted in .

In addition the processor module may include one or more network ports generally identified by reference numeral which are connected to communication links which connect the computer system in a computer network. The network ports enable the computer system to transmit information to and receive information from other computer systems and other devices in the network. In a typical network organized according to for example the client server paradigm certain computer systems in the network are designated as servers which store data and programs generally information for processing by the other client computer systems thereby to enable the client computer systems to conveniently share the information. A client computer system which needs access to information maintained by a particular server will enable the server to download the information to it over the network. After processing the data the client computer system may also return the processed data to the server for storage. In addition to computer systems including the above described servers and clients a network may also include for example printers and facsimile devices digital audio or video storage and distribution devices and the like which may be shared among the various computer systems connected in the network. The communication links interconnecting the computer systems in the network may as is conventional comprise any convenient information carrying medium including wires optical fibers or other media for carrying signals among the computer systems. Computer systems transfer information over the network by means of messages transferred over the communication links with each message including information and an identifier identifying the device to receive the message.

As noted above computer graphics system provides for enhanced cooperation among shaders by facilitating generation of phenomena comprising packaged and encapsulated shader DAGs or cooperating shader DAGs with each shader DAG comprising at least one shader which define features of a three dimensional scene. Phenomena can be used to define diverse types of features of a scene including color and textural features of surfaces of objects in the scene characteristics of volumes and geometries in the scene features of light sources illuminating the scene features of simulated cameras or other image recording devices which will be simulated during rendering and numerous other features which are useful in rendering as will be apparent from the following description. The phenomena are constructed so as to ensure that the shaders in the DAG or cooperating DAGs can correctly cooperate during rendering of an image of the scene.

More particularly the scene structure generation portion is used by the operator to generate a mathematical representation defining comprising the geometric structures of the objects in the scene the locations and geometric characteristics of light sources illuminating the scene and the locations geometric and optical characteristics of the cameras to be simulated in generating the images that are to be rendered. The mathematical representation preferably defines the three spatial dimensions and thus identifies the locations of the object in the scene and the features of the objects. The objects may be defined in terms of their one two or three dimensional features including straight or curved lines embedded in a three dimensional space two dimensional surfaces embedded in a three dimensional space one or more bounded and or closed three dimensional surfaces or any combination thereof. In addition the mathematical representations may also define a temporal dimension which may be particularly useful in connection with computer animation in which the objects and their respective features are considered to move as a function of time.

In addition to the mathematical representation of the geometrical structure of the object s in the scene to be rendered the mathematical representation further defines the one or more light sources which illuminate the scene and a camera. The mathematical representation of a light source particularly defines the location and or the direction of the light source relative to the scene and the structural characteristics of the light source including whether the light source is a point source a straight or curved line a flat or curved surface or the like. The mathematical representation of the camera particularly defines the conventional camera parameters including the lens or lenses focal length orientation of the image plane and so forth.

The scene structure generation portion also facilitates generation of phenomena which will be described in detail below and association of the phenomena to respective elements of the scene. Phenomena generally define other information that is required for the completion of the definition of the scene which will be used in rendering. This information includes but is not limited to characteristics of the colors textures and so forth of the surfaces of the geometrical entities defined by the scene structure generation portion . A phenomenon may include mathematical representations or other objects which when evaluated during the rendering operation will enable the computer generating the rendered image to display the respective surfaces in the desired manner. The scene structure generation portion under control of the operator effectively associates the phenomena to the mathematical representations for the respective elements that is objects surfaces volumes and the like with which they are to be used effectively attaching the phenomena to the respective elements.

After the mathematical representations have been generated by the scene structure generation portion and stored in the scene representation database the scene image generation portion is used by an operator during a rendering phase to generate an image of the scene on for example the video display unit .

The scene structure generation portion includes several elements including an entity geometrical representation generator a phenomenon creator a phenomenon database a phenomenon editor a base shader node database a phenomenon instance database and a scene assembler all of which operate under control of operator input information entered through an operator interface . The operator interface may generally include the operator input devices and the video display unit of computer graphics system as described above in connection with . The entity geometrical representation generator under control of operator input from the operator interface facilitates the generation of the mathematical representation of the objects in the scene and the light source s and camera as described above. The phenomenon creator provides a mechanism whereby the operator using the operator interface and base shader nodes from the base shader node database can generate phenomena which can be used in connection with the scene or otherwise as will be described below . After a phenomenon is generated by the phenomenon creator it that is the phenomenon will be stored in the phenomenon database . After a phenomenon has been stored in the phenomenon database an instance of the phenomenon can be created by the phenomenon editor . In that operation the operator will use the phenomenon editor to provide values for the phenomenon s various parameters if any . For example if the phenomenon has been created so as to provide features such as color balance texture graininess glossiness or the like which may be established adjusted or modified based on input from the operator at attachment time or thereafter the phenomenon editor allows the operator through the operator interface to establish adjust or modify the particular feature. The values for the parameters may be either fixed or they may vary according to a function of a variable illustratively time . The operator using the scene assembler can attach phenomenon instances generated using the phenomenon editor to elements of the scene as generated by the entity geometrical representation generator .

Although the phenomenon editor has been described as retrieving phenomena from the phenomenon database which have been generated by the phenomenon creator of the scene structure generation portion of computer graphics system it will be appreciated that one or more and perhaps all of the phenomena provided in the computer graphics system may be predefined and created by other devices not shown and stored in the phenomenon database for use by the phenomenon editor . In such a case the operator controlling the phenomenon editor through the operator interface can select appropriate predefined phenomena for attachment to the scene.

The scene image generation portion includes several components including an image generator and an operator interface . If the scene image generation portion forms part of the same computer as the scene structure generation portion the operator interface may but need not comprise the same components as operator interface . On the other hand if the scene image generation portion forms part of a different computer from the computer of which the scene structure generation portion the operator interface will generally comprise different components as operator interface although the components of the two operator interfaces and may be similar. The image generator under control of the operator interface retrieves the representation of the scene to be rendered from the scene representation database and generates a rendered image for display on the video display unit of the operator interface .

Before proceeding further it would be helpful to further describe a phenomenon used in connection with the invention. A phenomenon provides information that in addition to the mathematical representation generated by the entity geometrical representation generator is used to complete the definition of the scene which will be used in rendering including but not limited to characteristics of the colors textures and closed volumes and so forth of the surfaces of the geometrical entities defined by the scene structure generation portion . A phenomenon comprises one or more nodes interconnected in the form of a directed acyclic graph DAG or a plurality of cooperating DAGs. One of the nodes is a primary root node which is used to attach the phenomenon to an entity in a scene or more specifically to a mathematical representation of the entity. Other types of nodes which can be used in a phenomenon comprise optional root nodes and shader nodes. The shader nodes can comprise any of a plurality of conventional shaders including conventional simple shaders as well as texture shaders material shaders volume shaders environmental shaders shadow shaders and displacement shaders and material shaders which can be used in connection with generating a representation to be rendered. In addition a number of other types of shader nodes can be used in a phenomenon including i Geometry shaders which can be used to add geometric objects to the scene. Geometry shaders essentially comprise pre defined static or procedural mathematical representations of entities in three dimensional space similar to representations that are generated by the entity geometrical representation generator in connection with in connection with entities in the scene except that they can be provided at pre processing time to for example define respective regions in which other shaders used in the respective phenomenon are to be delimited. A geometry shader essentially has access to the scene construction elements of the entity geometrical representation generator so that it can alter the scene representation as stored in the scene object database to for example modify or create new geometric elements of the scene in either a static or a procedural manner. It should be noted that a Phenomenon that consists entirely of a geometry shader DAG or of a set of cooperating geometry shader DAGs can be used to represent objects in a scene in a procedural manner. This is in contrast to typical modeling which is accomplished in a modeling system by a human operator by performing a sequence of modeling operations to obtain the desired representation of an object in the computer. Hence in the essence a geometry phenomenon represents an encapsulated and automated parameterized abstract modeling operation. An instance of a geometry phenomenon that is a geometry phenomenon associated with a set of parameter values which are either fixed or which vary in a predetermined manner with time or the like will result in a specific geometric scene extension when it is evaluated by the scene image generator at runtime during a pre processing phase. ii Photon shaders which can be used to control the paths of photons in the scene and the characteristics of interaction of photons with surfaces of objects in the scene such as absorption reflection and the like. Photon shaders facilitate the physically correct simulation of global illumination and caustics in connection with rendering. In one embodiment photon shaders are used during rendering by the scene image generator during a pre processing operation. iii Photon volume shaders which are similar to photon shaders except that they operate in connection with a three dimensional volume of space in the scene instead of on the surface of an object. This allows simulation of caustics and global illumination to be extended to volumes and accompanying enclosed participating media such as scattering of photons by dust or fog particles in the air by water vapor such as in clouds or the like. iv Photon emitter shaders which are also similar to photon shaders except that they are related to light sources and hence to emission of photons. The simulated photons for which emission is simulated in connection with photon emitter shaders may then be processed in connection with the photon shaders which can be used to simulate path and surface interaction characteristics of the simulated photons and photon volume shaders which can be used to simulate path and other characteristics in three dimensional volumes in particular along the respective paths. v Contour shaders which are used in connection with generation of contour lines during rendering. In one embodiment there are three sub types of contour shaders namely contour store shaders contour contrast shaders and contour generation shaders. A contour store shader is used to collect contour sampling information for for example a surface. A contour contrast shader is used to compare two sets of the sampling information which is collected by use of a contour store shader. Finally a contour generation shader is used to generation contour dot information for storage in a buffer which is then used by an output shader described below in generating contour lines. vi Output shaders which are used to process information in buffers generated by the scene image generator during rendering. An output shader can access pixel information generated during rendering to in one embodiment perform compositing operations complex convolutions and contour line drawing from contour dot information generated by contour generation shaders as described above. vii Three dimensional volume shaders which are used to control how light other visible rays and the like pass through part or all of the empty three dimensional space in a scene. A three dimensional volume shader may be used for any of a number of types of volume effects including for example fog and procedural effects such as smoke flames fur and particle clouds. In addition since a three dimensional volume shader is used in connection with light they are also useful in connection with shadows which would arise from the procedural effects and viii Light shaders which are used to control emission characteristics of light sources including for example color direction and attenuation characteristics which can result from properties such as the shapes of respective light sources texture projection shadowing and other light properties.

Other types of shaders which may be useful in connection with definition of a scene may also be used in a phenomenon.

A phenomenon is defined by i a description of the phenomenon s externally controllable parameters ii one primary root node and optionally one or more optional root nodes iii a description of the internal structure of the phenomenon including the identification of the shaders that are to be used as nodes and how they are interconnected to form a DAG or a plurality of cooperating DAGs and iv optionally a description of dialog boxes and the like which may be defined by the phenomenon for use by the phenomenon editor to allow the operator to provide values for parameters or properties that will be used in evaluation of the respective phenomenon. In addition a phenomenon may include external declarations and link executable code from libraries as is standard in programming.

As noted above a phenomenon may include a plurality of cooperating DAGs. In such a phenomenon during rendering information generated from processing of one or more nodes of a first DAG in the phenomenon may be used in processing in connection with one or more nodes of a second DAG in the phenomenon. The two DAGs are nonetheless processed independently and may be processed at different stages in the rendering process. The information generated by a respective node in the first DAG which may be cooperating with a node in the second DAG that is which may be used by the node in the second DAG in its processing may be transferred from the respective node in the first DAG to the node in the second DAG over any convenient communication channel such as a buffer which may be allocated therefor. Providing all of the DAGs which may need to cooperate in this manner in a single phenomenon ensures that all of the conditions for cooperation will be satisfied which may not be the case if the DAGs are provided unencapsulated or separated in distinct phenomena or other entities.

As an example of a phenomenon including several cooperating DAGs a phenomenon may include several DAGs including a material shader DAG an output shader DAG and instructions for generating a label frame buffer. The material shader DAG includes at least one material shader for generating a color value for a material and also stores label information about the objects which are encountered during processing of the material shader DAG in the label frame buffer which is established in connection with processing of the label frame buffer generation instructions. The output shader DAG in turn includes at least one output shader which retrieves the label information from the label frame buffer to facilitate performing object specific compositing operations. In addition to the label frame buffer generation instructions the phenomenon may also have instructions for controlling operating modes of the scene image generator such that both DAGs can function and cooperate. For example such instructions may control the minimum sample density required for the two DAGs to be evaluated.

As a second example of a phenomenon including multiple cooperating shader DAGs a material phenomenon may represent a material that is simulated by both a photon shader DAG which includes at least one photon shader and a material shader DAG which includes at least one material shader. During rendering the photon shader DAG will be evaluated during caustics and global illumination pre processing and the material shader DAG will be evaluated later during rendering of an image. During processing of the photon shader DAG information representing simulated photons will be stored in such a way that it can be used during later processing of the material shader DAG to add lighting contributions from the caustic or global illumination pre processing stage. In one embodiment the photon shader DAG stores the simulated photon information in a photon map which is used by the photon shader DAG to communicate the simulated photon information to the material shader DAG.

As a third example of a phenomenon including multiple cooperating shader DAGs a phenomenon may include a contour shader DAG which includes at least one shader of the contour shader type and an output shader DAG which includes at least one output shader. The contour shader DAG is used to determine how to draw contour lines by storing dots of a selected color transparency width and other attributes. The output shader DAG is used to collect all cells created during rendering and when the rendering is completed join them into contour lines. The contour shader DAG includes a contour store shader a contour contrast shader and a contour generation shader. The contour store shader is used to collect sampling information for later use by a contour contrast shader. The contour contrast shader in turn is used to determine whether the sampling information collected by the contour store shader is such that a contour dot is to be placed in the image and if so the contour generation shader actually places the contour dot. This illustrative phenomenon illustrates four stage cooperation including 1 a first stage in which sampling information is collected by the contour store shader 2 a second stage in which the decision as to whether a contour cell is to be placed by the contour contrast shader 3 a third stage in which the contour dot is created by the contour generation shader and 4 a fourth stage in which created contour dots are created by the output shader DAG .

None of the shaders in any stage makes use of another shader in another stage but instead are processed and evaluated individually at different times but they cooperate to enable the generation of the final result.

As a fourth example of a phenomenon including multiple cooperating shader DAGs a phenomenon may include a volume shader DAG and a geometry shader DAG. The volume shader DAG includes at least one volume shader that defines properties of a bounded volume for example a fur shader that simulates fur within the bounded volume. The geometry shader DAG includes at least one geometry shader that is used to include an outer boundary surface as a new geometry into the scene before rendering begins with appropriate material and volume shader DAGs attached to the outer boundary surface to define the calculations that are to be performed in connection with hair in connection with the original volume shader DAG. In this illustrative phenomenon the cooperation is between the geometry shader DAG and the volume shader DAG with the geometry shader DAG introducing a procedural geometry in which the geometry shader DAG supports the volume shader DAG. The volume shader DAG makes use of this geometry but it would not be able to create the geometry itself since the geometry is generated using the geometry shader DAG during a pre processing operation prior to rendering whereas the volume shader DAG is used during rendering. The cooperation illustrated in connection with this fourth illustrative example differs from that illustrated in connection with the first through third illustrative examples since the shader or shaders comprising the geometry shader procedurally provide elements that are used by the volume shader DAG and do not just store data as is the case in connection with the cooperation in connection with the first through third illustrative examples.

All of these examples illustrate computer graphic effects in which an image of a scene can be rendered using multiple cooperating but independent shader DAGs which are bundled and encapsulated into a single phenomenon.

With this background the operations performed in connection with the phenomenon creator and phenomenon editor will be described in connection with respectively. In addition an illustrative phenomenon created in connection with the phenomenon creator will be described in connection with and details of the operations performed by the phenomenon editor in connection with the phenomenon depicted in connection with will be described in connection with . depicts a phenomenon creator window which the phenomenon creator enables the operator interface to display to the operator to enable the operator to define a new phenomenon and modify the definition of an existing phenomenon. The phenomenon creator window includes a plurality of frames including a shelf frame a supported graph node frame a controls frame and a phenomenon graph canvas frame . The shelf frame can include one or more phenomenon icons generally identified by reference numeral each of which represents a phenomenon which has been at least partially defined for use in the scene structure generation portion . The supported graph node frame includes one or more icons generally identified by reference numeral which represent entities such as interfaces the various types of shaders which can be used in a phenomenon and the like which can the operator can select for use in a phenomenon. As will be described below the icons depicted in the supported graph node frame can be used by an operator to form the nodes of the directed acyclic graph defining a phenomenon to be created or modified. In one embodiment there are a number of types of nodes including i A primary root node which forms the root of the directed acyclic graph and forms the connection to the scene and typically provides a color value during rendering. ii Several types of optional root nodes which may be used as anchor points in a phenomenon DAG to support the main root node item i above . Illustrative types of optional root nodes include a A lens root node which can be used to insert lens shaders or lens shader DAGs into a camera for use during rendering b A volume root node which can be used to insert global volume or atmosphere shaders or shader DAGs into a camera for use during rendering c An environment root node which can be used to insert global environment shader or shader DAGs into a camera for use during rendering d A geometry root node which can be used to specify geometry shaders or shader DAGs that may be pre processed during rendering to enable procedural supporting geometry or other elements of a scene to be added to the scene database e A contour store root node which can be used to insert a contour store shader into a scene options data structure f An output root node which can be used in connection with post processing after a rendering phase and g A contour contrast root which can be used to insert a contour contrast shader into the scene options data structure. iii A shader node which represents a shader that is a function written in a high level language such as C or C . iv A light node which is used in conjunction with a light source. A light node provides the light source with a light shader color intensity origin and or direction and optionally a photon emitter shader. v A material node which is used in conjunction with a surface. A material node provides a surface with a color value and has inputs for an opaque indication indicating whether the surface is opaque and for material volume environment shadow displacement photon photon volume and contour shaders. vi A phenomenon node which is a phenomenon instance. vii A constant node which provides a constant value which may be an input to any of the other nodes. The constant value may be most types of data types in the programming language used for the entities such as shaders represented by any of the other nodes such as scalar vector logical Boolean color transformation and so forth and viii A dialog node which represents dialog boxes which may be displayed by the phenomenon editor to the operator and which may be used by the operator to provide input information to control the phenomenon before or during rendering. The dialog nodes may enable the phenomenon editor to enable pushbuttons sliders wheels and so forth to be displayed to allow the operator to specify for example color and other values to be used in connection with the surface to which the phenomenon including the dialog node is connected. As shown in the shelf frame and the supported graph node frame both include left and right arrow icons generally identified by reference numeral which allow the icons shown in the respective frame to be shifted to the left or right as shown in to shift icons to be displayed in the phenomenon creator window if there are more entities than could be displayed at one time.

The controls frame contains icons not shown which represent buttons which the operator can use to perform control operations including for example deleting or duplicating nodes in the shelf frame or supported graph node frame beginning construction of a new phenomenon starting an on line help system exiting the phenomenon creator and so forth.

The phenomenon graph canvas provides an area in which a phenomenon can be created or modified by an operator. If the operator wishes to modify an existing phenomenon he or she can using a drag and drop methodology using a pointing device such as a mouse select and drag the icon from the shelf frame representing the phenomenon to the phenomenon graph canvas . After the selected icon associated with the phenomenon to be modified has been dragged to the phenomenon graph canvas the operator can enable the icon to be expanded to show one or more nodes interconnected by arrows representing the graph defining the phenomenon. A graph representing an illustrative phenomenon is depicted in . As shown in the graph includes a plurality of graph nodes comprising circles and blocks each of which is associated with an entity which can be used in a phenomenon which nodes are interconnected by arrows to define the graph associated with the phenomenon.

After the graph associated with the icon which has been dragged to the phenomenon graph canvas has been expanded to show the graph defining the phenomenon associated with the icon the operator can modify the graph defining the phenomenon. In that operation the operator can using a corresponding drag and drop methodology select and drag icons from the supported graph nodes frames representing the entities to be added to the graph to the phenomenon graph canvass thereby to establish a new node for the graph. After the new node has been established the operator can interconnect it to a node in the existing graph by clicking on both nodes in an appropriate manner so as to enable an arrow to be displayed therebetween. Nodes in the graph can also be disconnected from other nodes by deleting arrows extending between the respective nodes and deleted from the graph by appropriate actuation of a delete pushbutton in the controls frame .

Similarly if the operator wishes to create a new phenomenon he or she can using the corresponding drag and drop methodology select and drag icons from the supported graph nodes frames representing the entities to be added to the graph to the phenomenon graph canvas thereby to establish a new node for the graph to be created. After the new node has been established in the phenomenon graph canvas the operator can interconnect it to a node in the existing graph by clicking on both nodes in an appropriate manner so as to enable an arrow to be displayed therebetween. Nodes in the graph can also be disconnected from other nodes by deleting arrows extending between the respective nodes and deleted from the graph by appropriate actuation of a delete pushbutton in the controls frame .

After the operator has specified the DAG or set of cooperating DAGs for the phenomenon either for a new phenomenon or for a modified phenomenon and before the phenomenon represented by the graph is stored in the phenomenon database the phenomenon creator will examine the phenomenon graph to verify that it is consistent and can be processed during rendering. In that operation the phenomenon creator will ensure that the interconnections between graph nodes do not form a cycle thereby ensuring that the graph or graphs associated with the phenomenon form directed acyclic graphs and that interconnections between graph nodes represent respective input and output data types which are consistent. It will be appreciated that if the phenomenon creator determines that the graph nodes do form a cycle the phenomenon will essentially form an endless loop that generally cannot be properly processed. These operations will ensure that the phenomenon so created or modified can be processed by the scene image generation portion when an image of a scene to which the phenomenon is attached is being rendered.

After the operator has created or modified a phenomenon it will be stored in the phenomenon database .

Details of a material shader a texture shader and a coherent noise shader are known to those skilled in the art and will not be described further herein. Generally the material shader has one or more outputs represented by result which are provided to the root node . The material shader in turn has several inputs including a glossiness input an ambient color input a diffuse color input a transparency input and a lights input and the material shader node represented thereby is shown as receiving inputs therefor from the dialog node in the case of the glossiness input from the texture shader node in the case of the ambient and diffuse color inputs from a hard wired constant in the case of the transparency input and from a lights list in the case of the lights input . The hard wired constant value indicated as 0.0 provided to the transparency input indicates that the material is opaque. The glossiness input is connected to a glossiness output provided by the dialog node and when the material shader represented by node is processed during rendering it will obtain the glossiness input value therefor from the dialog box represented by the dialog node as will be described below in connection with .

The ambient and diffuse inputs of the material shader represented by node are provided by the output of the texture shader as indicated by the connection of the result output of node to the respective inputs of node . When the wood material phenomenon is processed during the rendering operation and in particular when the material shader represented by node is processed it will enable the texture shader represented by node to be processed to provide the ambient and diffuse color input values. The texture shader in turn has three inputs including ambient and diffuse color inputs represented by color and color inputs shown on node and a blend input. The values for the ambient and diffuse color inputs are provided by the operator using the dialog box represented by the dialog node as represented by the connections from the respective diffuse and ambient color outputs from the dialog node to the texture shader node in .

In addition the input value for the input of the texture shader represented by node is provided by the coherent noise shader represented by node . Thus when the texture shader represented by node is processed during the rendering operation it will enable the coherent noise shader represented by node to be processed to provide the blend input value. The coherent noise shader has two inputs including a turbulence input and a cylindrical input. The value for the turbulence input is provided by the operator using the dialog box represented by the dialog node as represented by the connections from the turbulence output from the dialog node to the coherent noise shader node . The input value for the cylindrical input which is shown as a logical value TRUE is hard wired into the phenomenon .

Operations performed by the phenomenon editor will be described in connection with . depicts a phenomenon editor window which the phenomenon editor enables to be displayed by the operator interface for use by an operator in one embodiment of the invention to establish and adjust input values for phenomena which have been attached to a scene. In particular the operator can use the phenomenon editor window to establish values for phenomena which are provided by dialog boxes associated with dialog nodes such as dialog node established for the respective phenomena during the creation or modification as described above in connection with . The phenomenon editor window includes a plurality of frames including a shelf frame and a controls frame and also includes a phenomenon dialog window and a phenomenon preview window . The shelf frame depicts icons representing the various phenomena which are available for attachment to a scene. As with the phenomenon creator window the shelf frame includes left and right arrow icons generally identified by reference numeral which allow the icons shown in the respective frame to be shifted to the left or right as shown in to shift icons to be displayed in the phenomenon editor window if there are more icons than could be displayed at one time.

The controls frame contains icons not shown which represent buttons which the operator can use to perform control operations including for example deleting or duplicating icons in the shelf frame starting an on line help system exiting the phenomenon editor and so forth.

The operator can select a phenomenon whose parameter values are to be established by suitable manipulation of a pointing device such as a mouse in order to create an instance of a phenomenon. An instance of a phenomenon corresponds to a phenomenon whose parameter values have been fixed. After the operator has selected a phenomenon the phenomenon editor will enable the operator interface to display the dialog box associated with its dialog node in the phenomenon dialog window. An illustrative dialog box used in connection with one embodiment of the wood material phenomenon described above in connection with will be described below in connection with . As the operator provides and adjusts the input values that can be provided through the dialog box the phenomenon editor effectively processes the phenomenon and displays the resulting output in the phenomenon preview window . Thus the operator can use the phenomenon editor window to view the result of the values which he or she establishes using the inputs available through the dialog box displayed in the phenomenon dialog window.

Returning to after the operator using the phenomenon editor has established the values for the various phenomena and phenomena instances associated with a scene those values are stored with the scene in the scene object database . Thereafter an image of scene can be rendered by the scene image generation portion in particular by the scene image generator for display by the operator interface . Operations performed by the scene image generator will generally be described in connection with the flowchart depicted in . With reference to the scene image generator operates in a series of phases including a pre processing phase a rendering phase and a post processing phase. In the pre processing phase the scene image generator will examine the phenomena which are attached to a scene to determine whether it will need to perform pre processing and or post processing operations in connection therewith step . The scene image generator then determines whether the operations in step indicated that pre processing operations are required in connection with at least one phenomenon attached to the scene to step and if so will perform the pre processing operations step . Illustrative pre processing operations include for example generation of geometry for the scene if a phenomenon attached to the scene includes a geometry shader to generate geometry defined thereby for the scene. Other illustrative pre processing operations include for example shadow and photon mapping multiple inheritance resolution and the like. Following step or step if the scene image generator makes a negative determination in that step the scene image generator can perform further pre processing operations which may be required in connection with the scene representation prior to rendering which are not related to phenomena attached to the scene step .

Following step the scene image generator will perform the rendering phase in which it performs rendering operations in connection with the pre processed scene representation to generate a rendered image step . In that operation the scene image generator will identify the phenomena stored in the scene object database which are to be attached to the various components of the scene as generated by the entity geometric representation generator and attach all primary and optional root nodes of the respective phenomena to the scene components appropriate to the type of the root node. Thereafter the scene image generator will render the image. In addition the scene image generator will generate information as necessary which may be used in post processing operations during the post processing phase.

Following the rendering phase step the scene image generator will perform the post processing phase. In that operation the scene image generator will determine whether operations performed in step indicated that post processing operations are required in connection with phenomena attached to the scene step . If the scene image generator makes a positive determination in step it will perform the post processing operations required in connection with the phenomena attached to the scene step . In addition the scene image generator may also perform other post processing operations which are not related to phenomena in step . The scene image generator may perform post processing operations in connection with manipulate pixel values for color correction filtering to provide various optical effects. In addition the scene image generator may perform post processing operations if for example a phenomenon attached to the scene includes an output shader that defines post processing operations such as depth of field or motion blur calculations that can be in one embodiment entirely done in an output shader for example dependent on the velocity and depth information stored in connection with each pixel value in connection with the rendered image.

The invention provides a number of advantages. In particular the invention provides a computer graphics system providing arrangements for creating reference the phenomenon creator and manipulating reference the phenomenon editor phenomena. The phenomena so created are processed by the phenomenon creator to ensure that they are consistent and can be processed during rendering. Since the phenomena are created prior to being attached to a scene it will be appreciated that they can be created by programmers or others who are expert in the development in computer programs thereby alleviating others such as artists draftsmen and the like of the necessity developing them. Also phenomena relieve the artist from the complexity of instrumenting the scene with many different and inter related shaders by separating it that is the complexity into an independent task performed by a phenomenon creator expert user in advance. With phenomena the instrumentation becomes largely automated. Once a phenomenon or phenomenon instance has been created it is scene independent and can be re used in many scenes thus avoiding repetitive work.

It will be appreciated that a number of changes and modifications may be made to the invention. As noted above since phenomena may be created separately from their use in connection with a scene the phenomenon creator used to create and modify phenomena and the phenomenon editor used to create phenomenon instances may be provided in separate computer graphics systems. For example a computer graphics system which includes a phenomenon editor need not include a phenomenon creator if for example the phenomenon database includes appropriate previously created phenomena and the operator will not need to create or modify phenomena.

Furthermore as noted above the values of parameters of a phenomenon may be fixed or they may vary based on a function of one or more variables. For example if one or more values of respective parameters vary in accordance with time as a variable the phenomenon instance can made time dependent or animated. This is normally discretized in time intervals that are labeled by the frame numbers of a series of frames comprising an animation but the time dependency may nevertheless take on the form of any phenomenon parameter valued function over the time each of which can be tagged with an absolute time value so that even if an image is rendered at successive frame numbers the shaders are not bound to discrete intervals.

In this connection the phenomenon editor is used to select time dependent values for one or more parameters of a phenomenon creating a time dependent phenomenon instance. The selection of time dependent values for the parameters of a phenomenon is achieved in one particular embodiment by the graphically interactive attachment of what will be referred to herein as phenomenon property control trees to an phenomenon. A phenomenon property control tree which may be in the form of a tree or a DAG is attached to phenomenon parameters effectively outside of the phenomenon and is stored with the phenomenon in the phenomenon instance database. A phenomenon property control tree consists of one or more nodes each of which is a shader in the sense of the functions that it provides for example motion curves data look up functions and the like. A phenomenon property control tree preferably can remain shallow and will normally have only very few branching levels. A phenomenon property control tree can consist of only one shader which defines a function to compute the value for the parameter associated with it at run time. A phenomenon property control tree can remain shallow because the phenomenon allows and encourages encapsulation of the complicated shader trees or DAGs facilitating evaluation in an optimized manner during the rendering step by for example storing data for re use. Allowing an operator to attach such phenomenon property control trees to control the phenomenon s parameters greatly increases the flexibility of the user to achieve custom effects based on his use of a predefined and packaged phenomenon. The number of distinct phenomenon instances that may be created this way is therefore greatly increased while the ease of use is not compromised thanks to the encapsulation of all complexity in the phenomenon.

In addition it will be appreciated that the appearance and structures of the windows used in connection with the phenomenon creator and phenomenon editor described in connection with may differ from those described herein.

It will be appreciated that a system in accordance with the invention can be constructed in whole or in part from special purpose hardware or a general purpose computer system or any combination thereof any portion of which may be controlled by a suitable program. Any program may in whole or in part comprise part of or be stored on the system in a conventional manner or it may in whole or in part be provided in to the system over a network or other mechanism for transferring information in a conventional manner. In addition it will be appreciated that the system may be operated and or otherwise controlled by means of information provided by an operator using operator input elements not shown which may be connected directly to the system or which may transfer the information to the system over a network or other mechanism for transferring information in a conventional manner.

While the phenomena system described in U.S. Pat. No. 6 496 190 and discussed above has proven extremely useful in recent years many shading platforms and languages have been developed such that currently existing shader languages are narrowly focused on specific platforms and applications contexts whether hardware shading for video games or software shading for visual effects in motion pictures. This platform dependence typical of conventional shader systems and languages can be a significant limitation.

Accordingly the following section and its subsections describe 1 shader methods and systems that are platform independent and that can unite various shading tools and applications under a single language or system construct 2 methods and systems that enable the efficient and simple re use and re purposing of shaders such as may be useful in the convergence of video games and feature films an increasingly common occurrence e.g. Lara Croft Tomb Raider 3 methods and systems that facilitate the design and construction of shaders without the need for computer programming as may be useful for artists and 4 methods and systems that enable the graphical debugging of shaders allowing shader creators to find and resolve defects in shaders.

In step a metanode environment is configured that is operable for the creation of metanodes the metanodes comprising component shaders that can be combined in networks to build more complex shaders.

In step a graphical user interface GUI is configured that is in communication with the metanode environment and is operable to manage the metanode environment to enable a user to construct shader graphs and phenomena using the metanode environment.

In step a software language is provided as an interface usable by a human operator and operable to manage the metanode environment implement shaders and unify discrete shading applications. The software language is configurable as a superset of a plurality of selected shader languages for selected hardware platforms and operable to enable a compiler function to generate from a single re usable description of a phenomenon expressed in the software language optimized software code for a selected hardware platform in a selected shader language.

In step at least one GUI library is provided that is usable in connection with the metanode environment to generate a GUI operable to construct shader graphs and phenomena.

In step an interactive visual real time debugging environment is configured that is in communication with the GUI and that is operable to 1 enable the user to detect and correct potential flaws in shaders and 2 provide a viewing window in which a test scene with a shader metanode or phenomenon under test is constantly rendered.

In step a facility is configured that is in communication with the compiler function and that is operable to convert the optimized software code for the selected hardware platform and selected shader language to machine code for selected integrated circuit instantiations using a native compiler function for the selected shader language.

The mental mill technology provides an improved approach to the creation of shaders for visual effects. The mental mill solves many problems facing shader writers today and future proofs shaders from the changes and evolutions of tomorrow s shader platforms.

In addition to providing a user interface for standalone operation the mental mill further includes a library providing APIs to manage shader creation. This library can be integrated into third party applications in a componentized fashion allowing the application to use only the components of mental mill it requires.

The foundation of mental mill shading is the mental mill shading language MetaSL . MetaSL is a simple yet expressive language designed specifically for implementing shaders. The mental mill encourages the creation of simple and compact componentized shaders referred to as Metanodes which can be combined in shader networks to build more complicated and visually interesting shaders.

The goal of MetaSL is not to introduce yet another shading language but to leverage the power of existing languages through a single meta language MetaSL. Currently existing shader languages focus on relatively specific platforms or contexts for example hardware shading for games or software shading for feature film visual effects. MetaSL unifies these shading applications into a single language.

The mental mill allows the creation of shader blocks called metanodes which are written in MetaSL to be attached and combined in order to form sophisticated shader graphs and Phenomena . Shader graphs provide intuitive graphical user interfaces for creating shaders that are accessible to users who lack the technical expertise to write shader code. The mental mill graphical user interface libraries harness the shader graph paradigm to provide the user a complete graphical user interface for building shader graphs and Phenomena. As discussed in detail below the present invention provides a metanode environment i.e. an environment that is operable for the creation and manipulation of metanodes. As further discussed below the described metanode environment may be implemented as software or as a combination of software and hardware.

A standalone application is included as part of mental mill however since mental mill provides a cross platform componentized library it is also designed to be integrated into third party applications. The standalone mental mill application simply uses these libraries in the same way any other application would. The mental mill library can be broken down into the following pieces 1 Phenomenon creator graphical user interface GUI 2 Phenomenon shader graph compiler and 3 MetaSL shading language compiler.

The mental mill Phenomenon creator GUI library provides a collection of GUI components that allow the creation of complex shaders and Phenomenon by users with a wide range of technical expertise.

The primary GUI component is the shader graph view. This view allows the user to construct Phenomena by creating shader nodes Metanodes or other Phenomena and attaching them together in a graphs described. The shader graph provides a clear visual representation of the shader program that is not found when looking at shader code. This makes shader creation accessible to those users without the technical expertise to write shader code. The GUI library also provides other user interface components summarized here 

The mental mill GUI library is both componentized and cross platform. The library has been developed without dependencies on the user interface libraries of any particular operating system or platform.

Furthermore the mental mill GUI library is designed for integration into third party applications. While the components of the GUI library have default appearances and behaviors plug in interfaces are provided to allow the look and feel of the Phenomenon creator GUI to be customized to match the look and feel of the host application.

The MetaSL shading language unites the many shading languages available today and is extensible to support new languages and platforms as they appear in the future. This allows MetaSL to provide insulation from platform dependencies.

MetaSL is a simple yet powerful language targeted at the needs of shader writers. It allows shaders to be written in a compact and highly readable syntax that is approachable by users that might not otherwise feel comfortable programming.

Because a MetaSL shader is written without dependencies on particular platforms MetaSL shaders can be used in a variety of different ways. A single shader can be used when rendering offline in software or real time in hardware. The same shader can be used across different platforms such as those used by the next generation of video game consoles.

By writing shaders in MetaSL the time invested in developing shaders is protected from the obsolescence of any particular language and is leveraged by the potential to be re used on many different platforms.

The MetaSL compiler that is part of the mental mill library is itself extendable. The front end of the compiler is a plug in so that parsers for other languages or syntaxes can replace the MetaSL front end. Similarly the back end of the compiler is also a plug in so new target platforms can easily be supported in the future. This extensibility to both ends of the mental mill compiler library allows it to become the hub of shader generation.

Shader writers typically face difficulties on several fronts. The following sections outline these issues and the rationale behind the creation of the mental mill technology which is designed to provide a complete solution set.

Shaders developed with mental mill are platform independent. This is a key feature of mental mill and insures that the effort invested in developing shaders is not wasted as target platforms evolve. This platform independence is provided for both shaders written in MetaSL and shader graphs of Metanodes.

The mental mill libraries provide application programming interfaces APIs to generate shaders for a particular platform dynamically on demand from either a Phenomenon shader graph or a monolithic MetaSL shader. Alternatively mental mill makes it possible to export a shader in the format required by a target platform to a static file. This allows the shader to be used without requiring the mental mill library.

A component of platform independence is insulation from particular rendering algorithms. For example hardware rendering often employs a different rendering algorithm as compared to software rendering. Hardware rendering is very fast for rendering complex geometry but may not directly support advanced lighting algorithms such as global illumination.

MetaSL can be considered to be divided into three subsets or levels with each level differing in both the amount of expressiveness and suitability for different rendering algorithms. shows a diagram illustrating the levels of MetaSL as subsets. The dotted ellipse region shows C as a subset for reference.

Level This is the most general subset of MetaSL. Shaders written within this subset can easily be targeted to a wide variety of platforms. Many types of shaders will be able to be written entirely within this subset.

Level A superset of Level Level adds features typically only available with software rendering algorithms such as ray tracing and global illumination. Like Level Level is still relatively simplified language and shaders written within Level may still be able to be partially rendered on hardware platforms. This makes it possible to achieve a blending of rendering algorithms where part of the rendering takes place on hardware and part on software.

Level This is a superset of both Levels and . In addition Level is also a superset of the popular C language. While Level shaders can only ever execute in software Level is the most expressive of the three levels since it includes all the features of C . However few shaders need the complexity of C and given that Level has the least general set of possible targets most shaders will likely be written using only Levels and .

While Level appears to be the smallest subset of MetaSL it is also the most general in the types of platforms it will support. MetaSL Level is the largest superset containing even all of C making it extremely powerful and expressive. For the applications that require it Level allows complex shaders to be written that use all the features of C . The cost of using Level comes from the limited targets that support it. is a bar chart illustrating the levels of MetaSL and their applicability to hardware and software rendering.

Level and shaders have a high degree of compatibility with the only difference being that Level shaders utilize advanced algorithms not capable of running on a GPU. However the MetaSL compiler can use a Level shader as if it were a Level shader and target hardware platforms by removing functions not supported by Level and replacing them with no ops. This feature and the ability of the MetaSL compiler to also detect the level of a given shader allows the MetaSL compiler to simultaneously generate a hardware and software version of a shader or only generate a software shader when it is required . The hardware shader can be used for immediate feedback to the user through hardware rendering. A software rendering can then follow up with a more precise image.

Another useful feature of mental mill is the ability to easily repurpose shaders. One key example of this comes from the convergence of video games and feature films. It is not uncommon to see video games developed with licenses to use content from successful films. Increasingly feature films are produced based on successful video games as well. It makes sense to use the same art assets for a video game and the movie it was based on but in the past this has been a challenge for shaders since the film is rendered using an entirely different rendering algorithm than the video game. The mental mill overcomes this obstacle by allowing the same MetaSL shader to be used in both contexts.

The shader graph model for constructing shaders also encourages the re use of shaders. Shader graphs inherently encourage the construction of shaders in a componentized fashion. A single Metanode implemented by a MetaSL shader can be used in different ways in many different shaders. In fact entire sub trees of a graph can be packaged into a Phenomenon and re used as a single node.

The mental mill graphical user interface provides a method to construct shaders that doesn t necessarily involve programming. Therefore an artist or someone who is not comfortable writing code will now have the ability to create shaders for themselves. In the past an artist needed to rely on a programmer to create shaders which is a slow process possibly involving many iterations between the programmer and the artists. Giving the artist control over the shader creation process not only frees up programmers for other tasks but allows the artist to more freely explore the possibilities enabled through custom shaders.

The mental mill user interface also provides a development environment for programmers and technical directors. Programmers can create custom Metanodes written in MetaSL and artists can then use these nodes to create new shaders. Technical directors can create complex custom shader graphs which implement units of functionality and package those graphs into Phenomenon. These different levels of flexibility and complexity give users extensive control over the shader creation process and can involve users of widely ranging technical and artistic expertise.

An important aspect of the creation of shaders is the ability to analyze flaws determine their cause and find solutions. In other words the shader creator must be able to debug their shader. Finding and resolving defects in shaders is necessary regardless of whether the shader is created by attaching Metanodes to form a graph or writing MetaSL code or both. The mental mill provides functionality for users to debug their shaders using a high level visual technique. This allows shader creators to visually analyze the states of their shader to quickly isolate the source of problems. A prototype application has been created as a proof of concept of this shader debugging system.

Nearly all applications of shaders such as offline or real time interactive rendering require shaders to achieve the highest level of performance possible. Typically shaders are invoked in the most performance critical section of the renderer and therefore can have a significant impact on overall performance. Because of this it is crucial for shader creators to be able to analyze the performance of their shaders at a fine granularity to isolate the computationally expensive portions of their shaders.

The mental mill provides such analysis referred to as profiling through an intuitive graphical representation. This allows the mental mill user to receive visual feedback indicating the relative performance of portions of their shaders. This profiling information is provided at both the node level for nodes that are part of a graph or Phenomenon and at the statement level for the MetaSL code contained in a Metanode.

The performance timing of a shader can be dependent on the particular input values driving that shader. For example a shader may contain a loop where the number of iterations through the loop is a function of a particular input parameter value. The mental mill graphical profiler allows shader performance to be analyzed in the context of the shader graph where the node resides which makes the performance results relative to the particular input values driving the node in that context.

The performance information at any particular granularity is normalized to the overall performance cost of a node the entire shader or the cost to render an entire scene with multiple shaders. For example the execution time of a MetaSL statement within a Metanode can be expressed as a percentage of the total execution time of that Metanode or the total execution time of the entire shader if the Metanode is a member of a graph.

The graphical representation of performance results can be provided using multiple visualization techniques. For example one technique is to present the normalized performance cost by mapping the percentage to a color gradient.

When a shader is part of an animation its performance characteristics may change over time either because the overall cost to render the scene changes over time or the shader itself is a function of time. The graphical representation of the performance results will update as the animation progresses to reflect these changes in the performance profile. In the screen the colored bars next to each statement of code will change color to reflect changes in measured performance as an animation is played back.

The mental mill can also display performance results in tabular form. shows a table in which the performance timings of each node of a Phenomenon are displayed with respect to the overall performance cost of the entire shader.

The graphical profiling technique provided by mental mill like other features of mental mill is platform independent. This means that performance timings can be generated for any supported target platform. As new platforms emerge and new back end plug ins to the mental mill compiler are provided these new platforms can be profiled in the same way. However any particular timing is measured with respect to some selected target platform. For example the same shader can be profiled when executed on hardware versus software or on different hardware platforms. Different platforms have individual characteristics and so the performance profile of a particular shader may look quite different when comparing platforms. The ability for a shader creator to analyze their shader on different platforms is critical in order to develop a shader that executes with reasonable performance on all target platforms.

According to a further aspect of the invention mental image s next generation renderers and the Reality Server will be based on MetaSL and monolithic C shaders. For this purpose they contain a copy of the MetaSL compiler that generates executable C and hardware shader code from MetaSL shaders and Phenomena. shows a diagram of a renderer according to this aspect of the invention.

In the configuration no MetaSL or other shader code is exported. Note that although all language paths are shown in this diagram typical renderers will not use all five rendering units shown at the bottom typically only the most appropriate two one software and one hardware are used at any one time.

The extensibility of the MetaSL compiler allows multiple target platforms and shading languages to be supported. New targets can be supported in the future as they emerge. This extensibility is accomplished through plug ins to the back end of the compiler. The MetaSL compiler handles much of the processing and provides the back end plug in with a high level representation of the shader which it can use to generate shader code. The MetaSL compiler currently targets high level languages however the potential exists to target GPUs directly and generate machine code from the high level representation. This would allow particular hardware to take advantage of unique optimizations available only because the code generator is working from this high level representation directly and bypassing the native compiler.

The mental mill GUI library provides an intuitive easy to use interface for building sophisticated shader graphs and Phenomenon. The library is implemented in a componentized and platform independent method to allow integration of some or all of the UI components into third party applications.

A standalone Phenomenon creator application is also provided which utilizes the same GUI components available for integration directly into other applications.

The Phenomenon graph editor allows users to build shaders and Phenomena by clicking and dragging with the mouse to place nodes and attach them together to form a graph. An extensive toolset aids the user in this task allowing them to easily navigate and maintain a complex shader graph. shows a diagram of the graph editor user interface .

Graph nodes are presented at various levels of detail to allow the user to zoom out to get a big picture of their entire graph or zoom in to see all the details of any particular node.

Portions of the shader graph can easily be organized into Phenomena that appear as a single node when closed. This allows the user to better deal with large complex graphs of many nodes by grouping subgraphs into single nodes. A Phenomenon can be opened allowing the user to edit its internal graph.

Each node in the shader graph has a preview window to show the state of the shader at that point in the graph. This provides a visual debugging mechanism for the creation of shaders. A user can follow the dataflow of the graph and see the result so far at each node. At a glance the user can see a visual representation of the construction of their shader.

Attachments can also be made from within the parameter view and users can follow attachment paths from one node to another within this view. This provides an alternate method for shader graph creation and editing that can be useful in some contexts.

A sizeable render preview window allows the user to interactively visualize the result of their shaders. This preview window can provide real time hardware accelerated previews of shaders as well as high quality software rendered results involving sophisticated rendering algorithms such as ray tracing and global illumination.

The Phenomenon Metanode library explorer view allows the user to browse and organize collections of Phenomena and Metanodes. shows a thumbnail view and shows a list view . To create a new node in the current graph from one of these libraries the user simply drags a node and drops it in their graph.

The libraries can be sorted and categorized for organization. The user can view the libraries in a list view or icon view to see a sample swatch illustrating the function of each node.

The code editor provides the ability for the user to author new Metanodes by writing MetaSL code. The code editor is integrated into the rest of the mental mill GUI so as a user edits shader code the rest of the user interface interactively updates to reflect their changes.

The mental mill MetaSL debugger presents the user with a source code listing containing the MetaSL code for the shader node in question. The user can then step through the shader s instructions and inspect the values of variables as they change throughout the program s execution. However instead of just presenting the user with a single numeric value the debugger displays multiple values simultaneously as colors mapped over the surface of an object.

Representing a variable s values as an image rather than a single number has several advantages. First the user can immediately recognize characteristics of the function driving the variable s value and spot areas that are behaving incorrectly. For example the rate of change of a variable across the surface is visible in an intuitive way by observing how the color changes over the surface. If the user was using the traditional method of debugging a shader one pixel at a time this would be difficult to recognize.

The user can also use the visual debugging paradigm to quickly locate the input conditions that produce an undesirable result. A shader bug may only appear when certain input parameters take on specific values and such a scenario may only occur on specific parts of the geometry s surface. The mental mill debugger allows the user to navigate in 3D space using the mouse to find and orient the view around the location on the surface that is symptomatic of the problem.

Traditional debugging techniques allow the user to step through a program line by line and inspect the program state at each statement. Typically the user can only step forward in the direction of program execution by one or more lines but jumping to an arbitrary statement in general requires the program to be restarted.

The mental mill MetaSL debugger allows the user to jump to any statement in their shader code in any order. One particularly nice aspect of this feature is when a code statement modifies the value of a variable of interest. The shader writer can easily step backward and forward across this statement to toggle between the variable s value before and after the statement is executed. This makes it easier for the user to analyze the effect of any particular statement on a variable s value.

Displaying a variable s value as a color mapped over the surface of an object obviously works well when the variable is a color type. This method also works reasonably well for scalar and vector values with three or less components but the value must be mapped into the range 0 1 in order to produce a legitimate color. The mental mill UI will allow the user specify a range for scalars and vectors that will be used to map those values to colors. Alternatively mental mill can automatically compute the range for any given viewpoint by determining the minimum and maximum values of the variable over the surface as seen from that viewpoint.

In addition to viewing a variable as colors mapped over the surface of an object the user can utilize other visualization techniques provided by mental mill. One such technique for vector values allows the user to sweep the mouse over the surface of an object and the mental mill debugger will draw an arrow pointing in the direction specified by the variable at that location on the surface. The debugger will also display the numeric value for a variable at a pixel location which can be selected by the mouse or specified by the user by providing the pixel coordinates. This technique is illustrated in which are a series of screen images displayed in response to different mouse positions at the surface of the image.

The mental mill shader debugger illustrates another benefit of the platform independence of mental mill. The debugger can operate in either hardware or software mode and works independently of any particular rendering algorithm or platform. The fact that the shader debugger is tightly integrated into mental mill s Phenomenon creation environment further reduces the create test cycle and allows the shader creator to continue to work at a high level insulated from platform dependencies.

The mental mill GUI library is implemented in a platform independent manner. The shader graph editor component uses a graphics API to insure smooth performance when editing complex shader graphs. A graphics abstraction layer prevents a dependency on any particular API. For example some applications may prefer the use of DirectX over OpenGL to simplify integration issues when their application also uses DirectX.

The rest of the GUI also uses an abstraction layer to prevent dependencies on the user interface libraries of any particular platform operating system. shows a diagram of a GUI library architecture according to this aspect of the invention. illustrates how these abstraction layers insulate the application and the mental mill user interface from platform dependencies.

When integrating the mental mill GUI components into a third party application it is possible to customize the look and feel of the components to better match the standards of the host application. This could involve purely superficial customization such as using specific colors or fonts or could involve customizing the specific appearance of elements and their behaviors as the user interacts with them.

The mental mill shading language MetaSL is simple intuitive and yet still expressive enough to represent the full spectrum of shaders required for the broad range of platforms supported by mental mill.

The MetaSL language uses concepts found in other standard shading languages as well as programming languages in general however MetaSL is designed for efficient shader programming. Users familiar with other languages will be able to quickly learn MetaSL while users without programming technical expertise will likely be able to understand many parts of a MetaSL shader due to its readability.

The shader class The MetaSL shader class declaration describes the shader s interface to the outside world. Shader declarations include the specification of input and output parameters as well as other member variables.

The shader declaration also contains the declaration of the shader s entry point a method called main. In addition an optional event method allows the shader to respond to initialization and exit events.

Shader classes can also include declarations of other member variables and methods. Other member variables can hold data used by the shading calculation and are initialized from the shader s event method. Other member methods can serve as helper methods called by the shader s main or event methods.

A standard set of math functions and operators are provided to work with vectors and matrices. Arithmetic operators are supported to multiply divide add and subtract matrices and vectors. This allows for compact and readable expressions such as Vector3 result mat 

A concept called swizzling is also supported. This allows components of a vector to be read or written to while simultaneously re ordering or duplicating components. For example 

Vector types can also be implicitly converted from one type to another as long as the conversion doesn t result in a loss of data. The Color type is provided primarily for code readability and is otherwise synonymous with Vector4.

In addition to the built in types provided by MetaSL custom structure types can be defined. Structures can be used for both input and output parameters as well as other variables. Both structures and built in types can be declared as arrays. Arrays can have either a fixed or dynamic size. Array elements are accessed with bracket syntax.

MetaSL supports the familiar programming constructs that control the flow of a shader s execution. Specifically these are for while do while if else switch case.

The task of iterating over scene lights and summing their illumination is abstracted in MetaSL by a light loop and iterator. An instance of a light iterator is declared and a for each statement iterates over each scene light. Inside the loop the light iterator variable provides access to the resulting illumination from each light.

The shader writer doesn t need to be concerned with which lights contribute to each part of the scene or how many times any given light needs to be sampled. The light loop automates that process.

Within a shader s main method a set of special state variables are implicitly declared and available for the shader code to reference. These variables hold values describing both the current state of the renderer as well as information about the intersection that led to the shader call. For example normal refers to the interpolated normal at the point of intersection. State variables are described in greater detail below.

There are two constructs for handling lights and illumination within MetaSL. The first is the BRDF bidirectional reflectance distribution function shader type which allows a surface s illumination model to be abstracted and rendered in a highly efficient manner. Alternatively light iterators provide a more traditional method for iterating over scene lights and samples within each light.

The BRDF shader approach is often more desirable for several reasons. It allows for efficient sampling and global illumination without the need to create a separate photon shader. In general a single BRDF implementation can be used unchanged by different rendering algorithms. It facilitates the ability to perform certain lighting computations such as tracing shadow rays in a delayed manner which allows for significant rendering optimizations. It also provides a unified description of analytical and acquired illumination models.

In MetaSL BRDFs are first class shader objects. They have the same event method as regular shaders. However instead of a main method several other methods must be supplied to implement BRDFs.

The in dir and out dir vectors in these methods are specified in terms of a local coordinate system. This coordinate system is defined by the surface normal as the z axis and the tangent vectors as the x and y axis.

A BRDF is declared like a regular shader except the brdf keyword is used in place of the shader keyword. Also BRDFs differ from regular shaders by the fact that they have no output variables the BRDF itself is its own output. The following is an example implementation for a Phong BRDF 

The combination of a surface shader and direct and indirect BRDF shaders together define the shading for a particular surface. MetaSL supplies two functions which a surface shader can use to perform illumination computations direct lighting which loops over some or all lights and evaluates the given BRDF and indirect lighting which computes the lighting contribution from global illumination. These two functions compute the illumination as separate diffuse glossy and specular components and store the results in variables passed to them as out arguments.

In most cases the variables passed to the lighting functions as out parameters can be the actual output parameters of the root surface shader node. In other words the calls to the lighting functions produce the final result of the shader. As long as there are no other immediate dependencies on these output values the renderer is free to defer their computation which allows for significant optimizations.

Another possibility is that the outputs of the surface shader are attached to the inputs of other nodes. This places an immediate dependency on the results of the lighting functions which is allowed but removes the possibility for deferred lighting computations and the potential performance gains that go with it.

To avoid placing dependencies on the results of the surface shader most operations that might have been performed with the surface shader outputs can be applied to the BRDF nodes themselves. A limited set of math operations can be applied to BRDF nodes but these are usually enough to accomplish the most common use cases. These operations are 

The ability to add BRDFs after scaling them with a scalar or color makes it possible to blend multiple illumination models represented by BRDF nodes. The scalar or color factor doesn t have to be constant and can be driven by the output of other shaders for example to blend two BRDFs as a function of a texture of Fresnel falloff. shows a diagram of an example configuration in which two BRDFs are mixed 

In the the Mix BRDF node is a composite BRDF that is implemented by scaling its two BRDF inputs by amount and 1 amount respectively. In this example the amount parameter is attached to the output of a texture which controls the blending between the two BRDFs. The Phong BRDF s specular reflection is attenuated by a Fresnel falloff function.

The material Phenomenon collects together the surface shader which itself may be represented by a shader graph and the direct and indirect BRDF shaders. When the surface shader invokes the lighting functions the BRDF shaders in the material Phenomenon are used to iterate over light samples to compute the result of the lighting functions. Since there are no dependencies on the result of the surface shader in this case the lighting calculation can be deferred by the renderer to an optimal time.

The BRDF shader type unifies the representation of BRDFs represented by an analytical model such as Phong with acquired BRDFs which are represented by data generated by a measuring device. The direct lighting and indirect lighting functions are not concerned with the implementation of the BRDFs they are given and thus operate equally well with acquired or analytical BRDFs.

The raw data representing acquired BRDFs may be provided in many different forms and is usually sparse and unstructured. Typically the raw data is given to a standalone utility application where it is preprocessed. This application can organize the data into a regular grid factor the data and or compress the data into a more practical size. By storing the data in a floating point texture measured BRDFs can be used with hardware shading.

Storing the data in a floating point texture allows a data based BRDF to operate in hardware. In this case the texture holding the data and any other parameter to describe the data model can be made explicit parameters of the BRDF node.

For software shading with measured BRDFs there are two options to load the data. The first is to implement a native C function that reads the data from a file into an array. This native function can then be called by a Level MetaSL BRDF shader. The other option is to implement the entire BRDF shader as a Level MetaSL shader which gives the shader complete access to all the features of C . This shader can read the data file directly but loses some of the flexibility of Level shaders. As long as the data can be loaded into a Level compatible representation such as an array the first option of loading the data from a native C function is preferable. If the data must be represented by a structure requiring pointers such as a kd tree then the part of the implementation which requires the use of pointers will need to be a Level shader.

A technique is a variation of a shader implementation. While some shaders may only require a single technique there are situations where it is desirable to implement multiple techniques. The language provides a mechanism to declare multiple techniques within a shader.

Techniques can be used when it is desirable to execute different code in hardware or software contexts although often the same shader can be used for both hardware and software. Another use for techniques is to describe alternate versions of the same shader with differing quality levels.

A technique is declared within the shader class. Each technique has its own version of the main and event methods but shares parameters and other member variables or methods with other techniques.

The language includes a mechanism to allow material shaders to express their result as a series of components instead of a single color value. This allows the components to be stored to separate image buffers for later compositing. Individual passes can also render a subset of all components and combine those with the remaining components that have been previously rendered.

A material shader factors its result into components by declaring a separate output for each component. The names of the output variable define the names of layers in the current rendering.

This example shows a material shader that specifies three components for diffuse specular and indirect lighting.

When multiple material shaders exist in a scene that factor their result into different layers the total number of layers could be large. A user may not wish to allocate separate image buffers for each of these layers. A mechanism in the scene definition file will allow the user to specify compositing rules for combining layers into image buffers. The user will specify how many image buffers are to be created and for each buffer they would specify an expression which determines what color to place in that buffer when a pixel is rendered. The expression can be a function of layer values such as 

In this example the three layers from the shader result structure in the previous example are routed to two image buffers.

In order for users of shaders to interact with them in a GUI and set their parameter values an application must know some additional information about the shader parameters and the shader itself. MetaSL provides functionality to annotate shader parameters techniques and the shader itself with additional metadata.

Shader annotations can describes parameter ranges default values and tooltip descriptions among other things. Custom annotation types can be used to attach arbitrary data to shaders as well.

MetaSL includes a comprehensive collection of built in functions. These include math geometric and texture lookup functions to name a few. In addition functions that may only be supported by software rendering platforms are also included. Some examples are functions to cast reflection rays or compute the amount of global illumination at a point in space.

The mental mill Phenomenon creation tool allows users to construct shaders interactively without programming. Users work primarily in a shader graph view where Metanodes are attached to other shader nodes to build up complex effects. Metanodes are simplistic shaders that form the building blocks for constructing more complicated Phenomena .

The definition of Phenomena was introduced with the mental ray renderer to completely capture the notion of visual effects. In short a Phenomenon can be a shader a shader tree or a set of cooperating shader trees DAGs including geometry shaders resulting in a single parameterized function with a domain of definition and a set of boundary conditions in 3D space which include those boundary conditions which are created at run time of the renderer as well as those boundary conditions which are given by the geometric objects in the scene.

A Phenomenon is a structure containing one or more shaders or shader DAGs and various miscellaneous requirement options that control rendering. To the outside a Phenomenon looks exactly like a shader with input parameters and outputs but internally its function is not implemented with a programming language but as a set of shader DAGs that have special access to the Phenomenon interface parameters. Additional shaders or shader DAGs for auxiliary purposes can be enclosed as well. Phenomena are attached at a unique root node that serves as an attachment point to the scene. The internal structure is hidden from the outside user but can be accessed with the mental mill Phenomenon creation tool.

For users that wish to develop shaders by writing code mental mill will also provide an integrated development environment IDE for creating Metanodes using mental images shader language MetaSL . Users may develop complete monolithic shaders by writing code or Metanodes which provide specific functionality with the intention that they will be components of Phenomenon shader graphs.

The mental mill tool also provides an automatically generated graphical user interface GUI for Phenomena and Metanodes. This GUI allows the user to select values for parameters and interactively preview the result of their settings.

Prior to being attached to a scene parameter values must be specified to instantiate the Phenomenon. There are two primary types of Phenomena which a user edits. A Phenomenon whose parameter values have not been specified referred to as free valued Phenomena and Phenomena whose parameters have been fixed or partially fixed referred to as fixed Phenomena . When a user creates a new Phenomenon by building a shader graph or writing MetaSL code or a combination of both they are creating a new type of Phenomenon with free parameter values. The user can then create Phenomena with fixed parameter values based on this new Phenomena type. Typically many fixed value Phenomena will exist based on a particular Phenomenon. If the user changes a Phenomenon all fixed Phenomena based on it will inherit that change. Changes to a fixed Phenomenon are isolated to that particular Phenomenon.

When the user chooses to create a new Phenomenon type mental mill begins by creating a new empty Phenomenon leaving the user to construct its shader graph. The user will also be able to specify the Phenomenon interface parameters which form the public interface for their shader. In addition they will be able to specify the number of Phenomenon roots and other options.

The mental mill application UI is comprised of several different views with each view containing different sets of controls. The view panels are separated by four movable splitter bars. These allow the relative sizes of the views to be adjusted by the user.

The Phenomenon graph view allows the user to create new Phenomena by connecting Metanodes or Phenomenon nodes together to form graphs. An output of a node can be connected to one or more inputs which allow the connected nodes to provide values for the input parameters they are connected to.

The Phenomenon graph view area can be virtually infinitely large to hold arbitrarily complex shader graphs. The user can navigate around this area using the mouse by holding down the middle mouse button to pan and the right mouse button to zoom button assignments are remappable . The navigation control described in a following section provides more methods to control the Phenomenon view.

The user can create nodes by dragging them from the toolbox as described below into the Phenomenon graph view . Once in the graph view nodes can be positioned by the user. A layout command will also perform an automatic layout of the graph nodes.

Some input or output parameters may be structures of sub parameters. In this case the node s input parameter will have a or button to open or close the structure. Attachments can be made to individual elements of the structure. shows a graph node including sub parameters.

Phenomenon nodes themselves contain shader graphs. At the top level the user can create multiple Phenomenon nodes each representing a new shader. A command will let the user dive into a Phenomenon node which causes the Phenomenon graph view to be replaced with the graph present inside the Phenomenon. Alternatively a Phenomenon can be opened directly in the graph in which it resides. This allows the user to see the nodes outside the Phenomenon and possibly connected to it as well as the contents of the Phenomenon itself.

Inside the Phenomenon the user has access to the Phenomenon interface parameters as well as the Phenomenon output and auxiliary roots. The inputs to a Phenomenon the interface parameters appear in the upper left corner of the Phenomenon and behave like outputs when viewed from the inside of the Phenomenon. The outputs of a Phenomenon appear in the upper right corner and behave as inputs when viewed from inside.

A shader graph inside a Phenomenon can also contain other Phenomenon nodes. The user can dive into these Phenomenon nodes in the same way and repeat the process as long as Phenomena are nested.

When a Phenomenon is opened inside a graph it can either be maximized in which case it takes over the entire graph view or it can be opened in place. When opened in place the user is able to see the graph outside the Phenomenon as well as the graph inside as shown in the graph view in .

Since Phenomena can be nested inside other Phenomena it s possible to open a Phenomenon inside another open Phenomenon and create new nested Phenomenon. shows a graph view illustrating such a case.

If the user drags a node into the top level they will create a Phenomenon with fixed values based on the Phenomenon type that they chose to drag. The top level fixed Phenomenon node has parameters which may be edited but not attached to other nodes. The fixed Phenomenon refers back to the Phenomenon from which it was created and inherits any changes to that Phenomenon. A command is available that converts a fixed Phenomenon into a free valued Phenomenon which allows the user to modify the Phenomenon without affecting other instances.

If the user drags a node into a Phenomenon a fixed valued Phenomenon or Metanode will be created inside the Phenomenon depending on the type of the node created. Nodes inside Phenomena can be wired to other nodes or Phenomenon interface parameters. If the node the user dragged into a Phenomenon was itself a Phenomenon node then a Phenomenon with fixed values is created. Its parameter values can be set or attached to other nodes but because it is a fixed Phenomenon that refers back to the original the user can not dive into the Phenomenon node and change it. Also any changes to the original will affect the node. If the user wishes to change the Phenomenon a command is available that converts the node into a new free valued Phenomenon which the user can enter and modify.

To create shader attachments the user clicks on the output area of one node and drags to position the mouse cursor over the input of another node. When they release the mouse a connection line is drawn which represents the shader connection. If the connection is not a valid one the cursor will indicate this to the user when the mouse is placed over a potential input during the attachment process.

A type checking system will ensure that shaders can only be attached to inputs that match their output type. In some cases an attachment can be made between two parameters of different types if an adapter shader is present to handle the conversion. For example a scalar value can be attached to a color input using an adapter shader. The adapter shader may convert the scalar to a gray color or perform some other conversion depending on settings selected by the user. Adapter shaders are inserted automatically when they are available. When the user attaches parameters that require an adapter the adapter will automatically be inserted when the user completes the attachment. In addition mental mill will ensure that the user doesn t inadvertently create cycles in their graphs when making attachments.

Both nodes and connection lines can be selected and deleted. When deleting a node all connections to that node are also deleted. When deleting a connection only the connection itself is deleted.

As a shader graph becomes more complex the user can organize the graph by boxing up parts of the graph into Phenomenon nodes. A command is available that takes the currently selected subgraph and converts it to a Phenomenon node. The result is a new Phenomenon with interface parameters for each input of selected nodes that are attached to an unselected node. The Phenomenon will have an output for each selected node whose output is attached to an unselected node. The new Phenomenon will be attached in place of the old subgraph which is moved inside the Phenomenon. The result is no change in behavior of the shader graph but the graph will appear simplified since several nodes will be replaced by a single node. The ability of a Phenomenon to encapsulate complex behavior in a single node is an important and powerful feature of mental mill.

A preview window displays a sample rendering of the currently selected Phenomenon. The image is the same image shown in the preview window of the Phenomenon node but can be sized larger to show more detail.

The preview will always show the result of the topmost Phenomenon node. Once the user enters a Phenomenon the preview will show the result of that Phenomenon regardless of which node is selected. This allows the user to work on the shader graph inside a Phenomenon while still previewing the final result.

The navigation window provides controls to allow the user to navigate the Phenomenon graph. Buttons will allow the user to zoom to fit the selected portion of the graph within the Phenomenon graph view or fit the entire graph to the view.

A bird s eye control shows a small representation of the entire shader graph with a rectangle that indicates the portion of the graph shown in the Phenomenon graph view. The user can click and drag on this control to position the rectangle on the portion of the graph they wish to see. There are also zoom in and zoom out buttons and a slider to allow the user to control the size of the view rectangle. As the user changes the zoom level the rectangle becomes larger or smaller. Conformal views are also being considered.

When the user zooms in and out shader nodes in the graph view become larger or smaller. As the user zooms further out and the nodes become smaller certain element of the node disappear to simplify the node. shows a series of views and illustrating the progression of node levels of detail.

When the inputs or outputs collapse the user can still make attachments. When dragging an attachment to a node a popup list will let the user select the actual input when they release the mouse.

As described in the Phenomenon graph section the user can dive into a Phenomenon node which causes the graph view to be replaced with the graph of the Phenomenon they entered. This process can continue as long as Phenomena are nested in other Phenomena. The navigation window provides back and forward buttons to allow users to retrace their path as they navigate through nested Phenomenon.

The toolbox window contains the shader nodes which make up the building blocks that shader graphs are built from. The user can click and drag nodes from the toolbox into the Phenomenon graph view to add nodes to the shader graph.

Nodes appear in the toolbox as an icon and a name. Typically the icon will be a sphere rendered using the shader but in some cases other icons may be used. The list of nodes can also be viewed in a condensed list without icons to allow more nodes to fit in the view. Some nodes may be Phenomenon nodes i.e. nodes defined by a shader graph and other nodes may be Metanode i.e. nodes defined by MetaSL code. This often is not important to the user creating shader graphs since both types of nodes can be used interchangeably. Phenomenon nodes will be colored differently from Metanodes or otherwise visually distinct allowing the user to differentiate between the two. Phenomenon nodes can be edited graphically by editing their shader graphs while Metanodes can only be edited by changing their MetaSL source code.

Nodes are sorted by category and the user can choose to view a single category or all categories by selecting from a drop down list of categories. There is also a command to bring up a dialog which allows the user to organize their categories. In this dialog the user can create or delete categories and control the category assigned to each node type.

In addition to Phenomena or Metanodes the toolbox also contains actions. Actions are fragments of a complete shader graph that the user can use when building new shader graphs. It is common for patterns of shader nodes and attachments to appear in different shaders. A user can select a portion of a shader graph and use it to create a new action. In the future if they wish to create the same configuration of nodes they can simply drag the action from the toolbox into the shader graph to create those nodes.

The toolbox is populated with nodes defined in designated shader description files. The user can select one or more shader description files to use as the shader library that is accessible through the toolbox. There are commands to add node types to this library and remove nodes. The mental mill tool will provide an initial library of Metanodes as well.

When editing free valued Phenomenon interface parameters it is the default value that is being edited. Fixed valued Phenomena may override those parameter values.

When shader attachments are allowed a column of buttons is present that allows the user to pick an attachment source from inside the parameter view. It should be noted that as currently implemented shader attachments are not allowed when editing a top level Phenomenon. This button will cause a popup list to appear that allows the user to pick a new node or choose from other available nodes currently in the graph. A none option is provided to remove an attachment. When an attachment is made the normal control for the parameter is replaced by a label indicating the name of the attached node.

Some parameters are structures in which case controls will be created for each element of the structure. According to a further aspect the structures are displayed by the UI in collapsed form and are opened with a button to open them recursively for nested structures . Alternatively the structures may be always displayed in expanded form.

In the UI parameters will appear in the order in which they are declared in the Phenomenon or Metanode however attributes in the node can also control the order and grouping of parameters. When editing the default parameters for a free valued Phenomenon there are commands available to change the order of parameters as well as organize parameters into groups. These commands edit attributes associated with the node.

Most parameters can have hard or soft limits associated with them. Hard limits are ranges for which the parameter is not allowed to exceed. Soft limits specify a range for the parameter that is generally useful but the parameter is not strictly limited to that range. The extents of a slider control will be set to the soft limits of a parameter. A command in the parameter view will allow the user to change the extents of a slider past the soft limits as long as they do not exceed the hard limits.

Controls in the parameter view will display tooltips when the user hovers the mouse over a control for a short period of time. The text displayed in the tooltip is a short description of the parameter that comes from an attribute associated with the shader. Similarly a button at the top of all controls will display a relatively short description of the function of the shader as a whole. This description is also taken from an attribute associated with the node.

A command allows the user to create a new Metanode. The result is a Metanode at the top level of the graph not inside a Phenomenon that represents a new type of Metanode. The user can always create an instance of this new Metanode inside a Phenomenon if they wish to.

When the user creates a new Metanode mental mill will create the initial skeleton code for a minimal compile able shader. The user can then immediately focus on implementing the specific functionality their shader is intended to provide.

When a top level Metanode outside of a Phenomenon is selected the corresponding MetaSL code will appear in the code editor view for the user to edit. After making changes to the code a command is available to compile the shader. The MetaSL compiler and a C compiler for the user s native platform are invoked by mental mill to compile the shader. Cross compilation for other platforms is also possible. Any errors are routed back to mental mill which in turn displays them to the user. Clicking on errors will take the user to the corresponding line in the editor. If the shader compiles successfully then the Metanode will update to show the current set of input parameters and outputs. The preview window will also update to give visual feedback on the look of the shader. This integration of the MetaSL and C compilers will greatly simplify the development of Metanodes and monolithic shaders.

The parameter view will also display controls for selected top level Metanodes allowing the user to edit the default value for the node s input parameters. This is analogous to editing a free valued Phenomenon s interface parameters.

Many users especially non technical users may not be interested in writing code and want to only use the graph editing method of building Phenomena. The UI will allow the code editing window to be closed with the extra space relinquished to the Phenomenon graph view.

A toolbar contains tool buttons which provide easy access to common commands. In general toolbar commands operate on the shader graph selection. Some toolbar commands replicate commands found in the main menu. The list of toolbar items may include the following commands Open file Save file New shader graph Phenomenon New shader code based Undo Redo Copy Paste Close.

An important aspect of the creation of shaders is the ability to analyze flaws determine their cause and find solutions. In other words the shader creator must be able to debug their shader. Finding and resolving defects in shaders is necessary regardless of whether the shader is created by attaching Metanodes to form a graph or writing MetaSL code or both. The mental mill provides functionality for users to debug their shaders using a high level visual technique. This allows shader creators to visually analyze the states of their shader to quickly isolate the source of problems.

The present aspect of the invention provides structures for debugging Phenomena. The mental mill GUI allows users to construct Phenomena by attaching Metanodes or other Phenomena to form a graph. Each Metanode has a representation in the UI that includes a preview image describing the result produced by that node. Taken as a whole this network of images provides an illustration of the process the shader uses to compute its result. shows a partial screenshot illustrating this aspect of the invention. A first Metanode might compute the illumination over a surface while another Metanode computes a textured pattern. A third node combines the results of the first two to produce its result.

By visually traversing the Metanode network a shader creator can inspect their shading algorithm and spot the location where a result is not what they expected. A node in the network might be a Phenomenon in which case it contains one or more networks of its own. The mental mill allows the user to navigate into Phenomenon nodes and inspect their shader graphs visually using the same technique.

In some cases viewing the results of each node in a Phenomenon does not provide enough information for the user to analyze a problem with their shader. For example all of the inputs to a particular Metanode may appear to have the correct value and yet the result of that Metanode might not appear to be as the user is expecting. Also when authoring a new Metanode by writing MetaSL code a user may wish to analyze variable values within the Metanode as the Metanode computes its result value.

Traditional models for debugging generic programs are not perfectly suited for debugging shaders. Generic programs that execute on a CPU are typically linear in nature where a sequence of instructions is executed that manipulate data. Some programs have multiple threads of execution but usually a relatively small number of threads often under a dozen with each thread representing a separate and independent sequence of instructions.

Shader programs on the other hand while also representing a linear sequence of instructions differ in that they operate on a large number of data points in parallel. While the shader program executes a sequential list of instructions it appears to do so simultaneously for each data point. In some cases shaders are processed using a SIMD single instruction multiple data model where each instruction is in fact applied to many data points simultaneously.

The traditional model of stepping through a program line by line and inspecting the value of variables must be modified to take into account for the fact that each variable potentially has many different values spread across each data point at any particular stage of execution of the shader. This makes the traditional method of inspecting the individual values of variables impractical much of the time.

The mental mill extends the visual debugging paradigm into the MetaSL code behind each Metanode. The mental mill MetaSL debugger presents the user with a source code listing containing the MetaSL code for the shader node in question. The user can then step through the shader s instructions and inspect the values of variables as they change throughout the program s execution. However instead of just presenting the user with a single numeric value the debugger displays multiple values simultaneously as colors mapped over the surface of an object.

Representing a variable s values as an image rather than a single number has several advantages. First the user can immediately recognize characteristics of the function driving the variable s value and spot areas that are behaving incorrectly. For example the rate of change of a variable across the surface is visible in an intuitive way by observing how the color changes over the surface. If the user was using the traditional method of debugging a shader one pixel at a time this would be difficult to recognize. Often defects in a shader are caused by discontinuities in the functions behind the shader. Observing a shader s rate of change allows the user to isolate such discontinuities.

The user can also use the visual debugging paradigm to quickly locate the input conditions that produce an undesirable result. A shader bug may only appear when certain input parameters take on specific values and such a scenario may only occur on specific parts of the geometry s surface. The mental mill debugger allows the user to navigate in 3D space using the mouse to find and orient the view around the location on the surface that is symptomatic of the problem.

The mental mill MetaSL debugger presents the user with a list of all variables that are in scope at the selected statement. shows a partial screenshot illustrating a variable list according to this aspect of the invention. As the user selects different statements new variables may come into scope and appear in the list while others will go out of scope and be removed from the list. Each variable in the list has a button next to its name that allows the user to open the variable and see additional information about it such as its type and a small preview image displaying its value over a surface. As the user steps past a statement that modifies a variable the preview image for that variable will update to reflect the modification.

In addition the user can select a variable from the list to display its value in a larger preview window.

Traditional debugging techniques allow the user to step through a program line by line and inspect the program state at each statement. Typically the user can only step forward in the direction of program execution by one or more lines. Jumping to an arbitrary statement in general requires the program to be restarted.

The mental mill MetaSL debugger allows the user to jump to any statement in their shader code in any order. One particularly nice aspect of this feature is that when a code statement modifies the value of a variable of interest the shader writer can easily step backward and forward across this statement to toggle between the variable s value before and after the statement is executed. This makes it easier for the user to analyze the effect of any particular statement on a variable s value.

Loops such as for foreach or while loops and conditional statements such as if and else create an interesting circumstance within this debugging model. Because the shader program is operating on multiple data points simultaneously the clause of an if else statement may or may not be executed for each data point.

The MetaSL debugger provides the user several options for viewing variable values inside a conditional statement. At issue is how to handle data points that do not execute the if or else clause containing the selected statement. These optional modes include the following 

Similarly a loop may execute a different number of times for each data point. Furthermore because the user can arbitrarily jump to any statement in the shader program if they select a statement inside a loop they must also specify which iteration of the loop they wish to consider. Given that the loop is potentially executed a different number of times for each data point some data points may have already exited the loop before the desired number of iterations is reached.

The mental mill debugger allows the user to specify a loop count value that specifies the desired number of iterations though a loop. The loop counter can be set to any value greater than zero. The higher the value the more data points will likely not reach the selected statement and in fact given a large enough value no data points will reach the selected statement. The same options that control the situation where the selected statement isn t reached for a conditional apply to loops as well.

Displaying a variable s value as a color mapped over the surface of an object obviously works well when the variable is a color type. This method also works reasonably well for scalar and vector values with three or less components but the value must be mapped into the range 0 1 in order to produce a legitimate color. The mental mill UI will allow the user specify a range for scalars and vectors that will be used to map those values to colors. Alternatively mental mill can automatically compute the range for any given viewpoint by determining the minimum and maximum values of the variable over the surface as seen from that viewpoint.

Mapping scalars and vectors to colors using user specified ranges can be effective however it still requires the user to deduce the value of the variable by looking at the surface colors. The mental mill UI provides other techniques for visualizing these data types. For vector types that represent direction not position one visualization technique is to draw the vector as an arrow positioned on the surface as the user drags the mouse over the surface as in the following illustration. This visualization technique is illustrated in a series of partial screenshots and shown in .

The numeric values of the variable are also displayed in a tooltip window as the user moves the mouse over the surface of the object.

Matrix type variables pose another challenge to visualize. Matrices with 4 rows and columns can be viewed as a grid of numeric values formatted into standard matrix notation. This visualization technique is illustrated in the partial screenshot shown in .

Matrix type variables with three rows and columns can be considered to be a set of three direction vectors that make up the rows of the matrix. A common example of this is the tangent space matrix which is comprised of the u derivative the v derivative and the normal. The three row vectors of the matrix can be drawn as arrows under the mouse pointer as shown below. In addition the individual values of the matrix can be displayed in a standard matrix layout. This visualization technique is illustrated in the partial screenshot shown in .

Vector type values that don t represent direction can be viewed using a gauge style display. The same user specified range that maps these values to colors can be used to set the extent of a gauge or set of gauges that appear as the user selects positions on the surface. As the user moves the mouse over the surface the gauges graphically display the values relative to the user specified range. This visualization technique is illustrated in the partial screenshot shown in .

When vector arrows gauges or tooltips containing numeric values are displayed the user can opt to view the final shader result on the object s surface instead of the variable value mapped to colors. This allows the user to locate portions of the surface that correspond to features in the final shader result while also monitoring the value of the selected variable.

Another useful feature the mental mill debugger provides is to lock onto a particular pixel location instead of using the mouse pointer to interactively sweep over the surface. The user can choose a pixel location either by selecting it with the mouse or providing the numeric pixel coordinates and the value of the variable at that pixel location will be displayed as the user steps through statements in the code.

The mental mill shader debugger illustrates another benefit of the platform independence of mental mill. A single MetaSL Metanode or an entire Phenomenon can be created and debugged once and yet targeted to multiple platforms.

The debugger can operate in either hardware or software mode and works independently of any particular rendering algorithm. The fact that the shader debugger is tightly integrated into mental mill s Phenomenon creation environment further reduces the create test cycle and allows the shader creator to continue to work at a high level insulated from platform dependencies.

The mental mill application is built on a modular library containing all the functionality utilized by the application. An API allows third party tools to integrate the mental mill technology into their applications.

Future generations of mental ray will access some subcomponents of the mental mill libraries however they will do so only to facilitate rendering with mental ray. The complete mental mill library will be licensed separately from mental ray.

When integrating mental mill technology into a third party application the mental mill GUI can be customized to match the look and feel of that application. There are several ways in which the mental mill API will allow GUI customization 

The mental mill shading language MetaSL provides a powerful interface for implementing custom shading effects and serves as an abstraction from any particular platform where shaders may be executed.

Rendering can be executed on either the CPU or on a graphics processor unit GPU and therefore shaders themselves operate on either the CPU or GPU. There is a significant amount of variation in both the capabilities of graphics hardware and the APIs that drives them. As such a shader written in a language directly targeted at graphics hardware will likely have to be rewritten as hardware and APIs change. Furthermore such a shader will not operate in a software renderer and will not support features such as ray tracing that are currently only available to software renderers.

MetaSL solves this problem by remaining independent of any target platform. A shader can be written once in MetaSL and the MetaSL compiler will automatically translate it to any supported platform. As new platforms emerge and the capabilities of graphics hardware changes shaders written in MetaSL will automatically take advantage of new capabilities without the need to be rewritten.

This insulation from target platforms also allows a MetaSL shader to automatically operate in software or hardware rendering modes. The same shader can be used to render in software mode on one machine and in hardware mode on others. Another use for this capability is the automatic re purposing of shaders for a different context. For example a shader created for use in a visual effect for film could also be used for a video game based on that film.

In many cases the same MetaSL shader can be used regardless of whether rendering takes place in software or hardware so the user isn t required to implement multiple shaders. In some cases however the user may wish to customize a shader for the GPU or CPU. The language provides a mechanism to do this while still implementing both techniques in MetaSL.

Hardware shaders generated by the MetaSL compiler are restricted such that they can be used only for rendering with the next generation of mental ray and the Reality Server based on neuray . The mental mill Phenomenon creation technology provides the capability of generating hardware shaders that can be used for either rendering with mental ray or externally by other applications such as games.

The following sections describe the MetaSL language specification. MetaSL has been designed to be easy to use with a focus on programming constructs needed for common shading algorithms rather than the extensive and sometime esoteric features found in generic programming languages.

All components of a MetaSL shader are grouped together in a shader class denoted with the shader keyword.

A single source file can have multiple shader definitions. A shader class can also inherit the definition of another shader by stating the name of the parent shader following the name of the child shader and separated by a colon. For example 

This allows variations of a shader type to be implemented while sharing parts of the shader that are common to all variations. A shader definition can only inherit from a single parent shader.

A shader can have zero or more input parameters which the shader uses to determine its result value. Parameters can use any of the built in types described below or custom structure types also described below.

When used in a scene input parameters may store literal values or be attached to the result of another shader however a shader doesn t need to be concerned with this possibility. A shader can refer to an input parameter as it would any another variable. Note though that input parameters can only be read and not written to.

A shader declares its input parameters in a section denoted by the input label followed by a declaration of each parameter.

An input parameter can also be a fixed or variable sized array. Since the size of a dynamic array isn t known in advance array input parameters include a built in count parameter. The length of an array named my array can be referred to as my array.count. An input parameter of fixed length will have the length indicated as part of the declaration while a dynamic array will not.

A shader must have at least one output parameter but may have more than one. Output parameters store a shader s result. The purpose of a shader is to compute some function of its input parameters and store the result of that function in its output parameters.

An output parameter can be of any type including a user defined structure type however it cannot contain a dynamic array.

A shader declares its output parameters in a section denoted by the output label followed by a declaration of each parameter.

This example declares a shader with two color outputs. Many shaders will only have a single output parameter which by convention is named result. 

Shaders can declare other variables which are not input or output parameters but instead store other values read by the shader. When initializing before rendering begins a shader can compute values and store them in member variables. Member variables are designed to hold values that are computed once before rendering begins but do not change thereafter. This avoids redundantly computing a value each time the shader is called.

The primary task of a shader is to compute one or more result values. This is implemented in the shader class by a method named main. This method is called when the renderer needs the result of the shader or when the shader is attached to an input parameter of another shader and that shader needs the parameter s value.

The return type of this method is always void which means it doesn t return a value. Instead the result of a shader should be placed in its output parameters.

The main method can be implemented as an inline method which may be convenient for simple shaders. In cases where the method is large it may be desirable to implement the method separately from the shader definition. To do this the method name must be prefixed with the shader name and separated by two colons. For example 

In addition to the main method other methods can be defined in the shader class that act as helper methods to the main method. These methods can return values of any type and accept calling parameters of any type. A method declaration is placed in the shader class and looks like the following example 

As is the case with the main method the implementation of a helper method can either occur directly in the shader class definition or outside it. When it is implemented outside of the shader class definition the method name must be prefixed with the name of the shader followed by two colons.

MetaSL also allows the definition of functions that are not directly associated with a particular shader class the way methods are. Functions are declared the same way as methods except the declaration appears outside of any shader class. Functions must have a declaration that appears before any reference to the function is made. The function body can be included as part of the declaration or a separate function definition can occur later in the source file after the function declaration.

Both methods and functions can be overloaded by defining another method or function with the same name but a different set of calling parameters. Overloaded versions of a function or method must have a different number of parameters or parameters that differ in type or both. It is not sufficient for overloaded functions to only differ by return type.

Parameters to functions and methods are always passed by value. Calling parameter declarations can be qualified with one of the following qualifiers to allow a function or method to modify a calling parameter and allow that modification to be visible to the caller 

It should also be noted that according to the present aspect of the invention neither functions nor shader methods can be called recursively.

Another specially named method in the shader class is a method called event. This method is called to allow the shader to perform tasks before and after shading operations take place sometimes these are referred to as init or exit functions.

The event method is passed a single Event type parameter that identifies the event. shows a table setting forth a list of Event type parameters according to this aspect of the invention.

In this example the Instance init event is handled and a noise table array is initialized to contain random values.

MetaSL includes a comprehensive set of fundamental types. These built in types cover most of the needs shaders will have but can also be used to define custom structures as described below.

MetaSL provides the capability to define an enumeration as a convenient way to represent a set of named integer constants. The enum keyword is used followed by a comma separated list of identifiers enclosed in brackets. For example 

An enumeration can also be named in which case it defines a new type. The enumerators can be explicitly assigned values as well. For example 

This example defines a new type called Detail with possible values of LOW MEDIUM and HIGH. Enumeration type values can be implicitly cast to integers which results in an integer with the explicitly assigned value. If explicit values are not specified values are assigned beginning with zero for the first enumerator and incremented by one thereafter.

All the vector types have components that can be accessed in a similar manner. The components can be referred to by appending a period to the variable name followed by one of x y z w . Vectors of length 2 only have x and y components vectors of length 3 have x y and z and vectors of length 4 have all 4 components.

When referring to vector components multiple components can be accessed at the same time the result being another vector of the same or different length. Also the order of the components can be arbitrary and the same component can be used more than once. For example given a vector V of length 3 

A similar syntax can be used as a mask when writing to a vector. The difference is that a component on the left side of the assignment can not be repeated.

In this example the y and z components are set to 0.0 while the x and y components are left unchanged.

Vectors can be constructed directly from other vectors or colors providing the total number of elements is greater than or equal to the number of elements in the vector being constructed. The elements are taken from the constructor parameters in the order they are listed.

All three vector constructor calls above are legal. The example would result in the three vectors initialized with the values set forth in the table set forth in .

The standard math operators apply to all vectors and operate in a component wise fashion. The standard math operators are overloaded to allow a mixture of scalars and vectors of different sizes in expressions however in any single expression all vectors must have the same size. When Scalars are mixed with vectors the scalar is promoted to a vector of the same size with each element set to the value of the scalar.

The standard Boolean logic operators also can be applied to individual or vectors of Booleans. When applied to vectors of Booleans they operate in a component wise fashion and produce a vector result. These operators are listed in the table set forth in .

Bitwise operators are not supported. Comparison operators are supported and operate on vectors in a component wise fashion and produce a Boolean vector result. A list of supported comparison operators is set forth in the table shown in .

A ternary conditional operator can be applied to vector operands as well in a component wise fashion. The conditional operand must be a single Boolean expression or a vector Boolean expression with the number of components equal to the number of components of the second and third operands.

Instances of the Color type are identical in structure to instances of Vector4 although their members are referred to by r g b a instead of x y z w to refer to the red green blue and alpha components respectively.

They can be used any place a Vector4 can be used and all operations that apply to Vector4 will work with Color as well. The primary purpose of this type is for code readability. Otherwise this type is logically synonymous with Vector4.

Matrices are defined with row and column sizes ranging from 2 to 4. All matrices are comprised of Scalar type elements. Matrix elements can also be referred to using array notation row major order with the array index selecting a row from the matrix. The resulting row is either a Vector2 Vector3 or Vector4 depending on the size of the original matrix. Since the result of indexing a matrix is a vector and vector types also support the index operator individual elements of a matrix can be accessed with syntax similar to a multidimensional array.

As illustrated by this example matrices also have a constructor which accepts element values in row major order. Matrices can also be constructed by specifying the row vectors as in the following example 

The number of elements of the vectors passed to the matrix constructor must match the number of elements in a row of the matrix being constructed.

The multiplication operator is supported to multiply two matrices or a matrix and a vector and will perform a linear algebra style multiplication between the two. As should be expected when multiplying two matrices the number of columns of the matrix on the left must equal the number of rows of the matrix on the right. The result of multiplying a N T matrix with a T M matrix is a N M matrix. A vector can be multiplied on the right or left side provided the number of elements equals the number of rows when the vector is on the left side of the matrix and the number of elements equals the number of columns when the vector is on the right.

Automatic type conversions are allowed to cast a variable of one type to a value of another type. The only restriction is that when implicitly converting a vector the conversion is to a vector of the same size. To convert between vectors of different sizes or scalars either a constructor can be used or the .xyzw notation can be used. For example 

Conversions that potentially result in a loss of precision such as converting a scalar to an integer are allowed but produce a compiler warning. This warning can be disabled if the shader writer desires.

MetaSL supports arrays of any of the built in types or user defined structures however only fixed length arrays can be declared in shader functions. There are two exceptions. As stated in the input parameter section shader inputs can be declared as dynamically sized arrays. The other exception is for parameters to functions or methods which can also be arrays of unspecified size. In both these cases by the time the shader is invoked during rendering the actual size of the array variable will be known. The shader code can refer to the size of an array as name.count where name is the array variable name.

This simple example loops over an array and sums the components. The code for this function was written without actual knowledge of the size of the array but when shading the size will be known. Either the array variable will come from an array shader parameter or a fixed size array declared in a calling function.

Custom structure types can be defined to extend the set of types used by MetaSL shaders. The syntax of a structure type definition looks like the following example 

Structure member variables can be of any built in type or another user defined structure type to produce a nested structure. Structure members can also be arrays.

Within a shader s main method a set of special state variables are implicitly declared and available for the shader code to reference. These variables hold values describing both the current state of the renderer as well as information about the intersection that led to the shader call. For example normal refers to the interpolated normal at the point of intersection.

These variables are only available inside the shader s main method. If a shader wishes to access one of these state variables within a helper method the variable must be explicitly passed to that method.

This set of state variables can be viewed as an implicit input to all shaders. The state input s data type is a struct containing all the state variables available to shaders. A special state shader can be connected to this implicit input. The state shader has an input for each state member and outputs the state struct.

Data members of the state cannot be directly modified however exposing the state as an input allows one shader to refer to state variables while allowing another shader to drive the state values used by that shader. When the state input of a shader is left un attached any references to state variables from within the shader revert to a reference to unmodified state values.

For example a shader that computes illumination will likely refer to the surface normal at the point of intersection. A bump map shader could produce a modified normal which it computes from a combination of the state normal and a perturbation derived from a gray scale image. A state shader can be attached to the illumination shader thus exposing the normal as an input. The output of the bump shader can then be attached to the state shader s normal input.

The illumination shader will most likely contain a light loop that iterates over scene lights and indirectly causes light shaders to be evaluated. The state values passed to the light shaders will be the same state values provided to the surface shader. If the state was modified by a state shader the modification will also affect the light shaders.

This system of implicit state input parameters simplifies shader writing. A shader can easily refer to a state variable while at the same time maintaining the possibility of attaching another shader to modify that state variable. Since the state itself isn t actually modified there is no danger of inadvertently affecting another shader.

The Perturb normal shader uses three samples of a gray scale image to produce a perturbation amount. The texture coordinate used to sample the bump map texture is offset in both the U and V directions allowing the slope of the gray scale image in the U and V directions to be computed.

An amount input scales the amount of the perturbation. The Perturb normal shader adds this perturbation to the state s normal to produce a new modified normal.

Three Texture lookup shaders drive the inputs of the Perturb normal shader. Two of these shaders are fed modified texture coordinates from the attached state shaders. The state shaders themselves are fed modified texture coordinates produced by Offset coordinate shaders.

The whole schematic is contained in a Phenomenon so not all users have to be concerned with the details. The bump map Phenomenon has an input of type Texture2d which is fed to all three texture lookups. An offset input allows the user to control the offset in the U and V direction with a single parameter. The amount input of the Perturb normal shader is also exposed as an input to the bump map Phenomenon.

The presence of the state shader makes it very clear to the user what is happening behind the scenes. An interested user can open up the bump map Phenomenon and see the graph visually depicting the bump map algorithm.

A set of state variables includes the following position normal origin direction distance texture coord n and screen position. This list can be supplemented to make it more comprehensive. Note that the preprocessor can be used to substitute common short name abbreviations often single characters for these longer names.

In an alternative configuration state variable parameters are added to nodes. According to this aspect of the invention a set of special state variables are implicitly declared within a shader s main method and are available for the shader code to reference. These variables hold values describing both the current state of the renderer as well as information about the intersection that led to the shader call. For example the normal variable refers to the interpolated normal at the point of intersection.

According to the present aspect of the invention these variables are only available inside the shader s main method. If a shader wishes to access one of these state variables within a helper method the variable must be explicitly passed to that method. Alternatively the state variable itself may be passed to another method in which case all the state variables are then available to that method.

This set of state variables can be viewed as implicit inputs to all shaders which by default are attached to the state itself. However one or more input parameters can be dynamically added to an instance of a shader that corresponds by name to a state variable. In that case these inputs override the state value and allow a connection to the result of another shader without modifying the original shader source code. In addition to modifying a state variable with an overriding input parameter a shader can also directly modify a state variable with an assignment statement in the MetaSL implementation.

Exposing state variables as inputs allows one shader to refer to state variables while allowing another shader to drive the state values used by that shader. If no input parameter is present for a particular referenced state variable that variable will continue to refer to the original state value.

For example a shader that computes illumination typically refers to the surface normal at the point of intersection. A bump map shader may produce a modified normal which it computes from a combination of the state normal and a perturbation derived from a gray scale image. A parameter called normal can be added to an instance of the illumination shader thus exposing the normal as an input just for that particular instance. The output of the bump shader can then be attached to the shader s normal input.

According to a further aspect of the invention the illumination shader contains a light loop that iterates over scene lights and indirectly causes light shaders to be evaluated. The state values passed to the light shaders will be the same state values provided to the surface shader. If a state variable was overridden by a parameter or modified within the shader that modification will also affect the light shaders. It is not possible however to make modifications to a state variable that will affect shaders attached to input parameters because all input parameters are evaluated before a shader begins execution.

This system of implicit state input parameters simplifies shader writing. A shader can easily refer to a state variable while at the same time maintaining the possibility of attaching another shader to modify that state variable.

The Perturb normal shader uses three samples of a gray scale image to produce a perturbation amount. The texture coordinate used to sample the bump map texture is offset in both the U and V directions allowing the slope of the gray scale image in the U and V directions to be computed. An amount input scales the amount of the perturbation. The Perturb normal shader adds this perturbation to the state s normal to produce a new modified normal.

Three Texture lookup shaders drive the inputs of the Perturb normal shader 004. Two of these shaders are fed modified texture coordinates from the attached Offset coord. shaders .

The whole schematic is contained in a Phenomenon so not all users have to be concerned with the details. The bump map Phenomenon has an input of type Texture2d which is fed to all three texture lookups. An offset input allows the user to control the offset in the U and V direction with a single parameter. The amount input of the Perturb normal shader is also exposed as an input to the bump map Phenomenon.

State vectors are always provided in internal space. Internal space is undefined and can vary across different platforms. If a shader can perform calculations independently of the coordinate system then it can operate with the state vectors directly otherwise it will need to transform state vectors into a known space.

There are several defined spaces and the state provides matrices and functions to transform vectors points and normals between these coordinate systems. shows a table listing the transformation matrices.

There are additional state variables available for light and volume shaders that provide access to properties of lights and the input value for volume shaders. A shader node that refers to light or volume shader state variable can only be used as a light or volume shader or in a graph which is itself used as a light or volume shader.

Light shaders can also call the state transformation functions and pass the value light as the from or to parameter.

The ray that is responsible for the current intersection state is described by the ray type ray shader is ray dispersal group and is ray history group state variables and functions. These variables and functions use the following strings to describe attributes of the ray 

A predefined Trace options class holds parameters used by the trace and occlusion functions described in the next section. A shader can declare an instance of this type once and pass it to multiple trace calls.

When a Trace options instance is declared all its member values are initialized to default values. The shader can then call various set methods to change the values to something other than the default. sets forth a table listing the methods of the Trace options class.

MetaSL supports the familiar programming constructs that control the flow of a shader s execution. Specifically these are 

The syntax for the flow control statements is identical to that used with the standard C programming language.

A Light iterator class facilitates light iteration and an explicit light list shader input parameter is not required. The light iterator implicitly refers to scene lights through the state. An instance of this iterator is declared and specified as part of the for each statement. The syntax looks like the following.

Most surface shaders will loop over light sources to sum up the direct illumination from lights in the scene. In addition area lights require multiple samples to be taken over points on the surface of the area light. The for each statement will enumerate both lights in the scene and sample points on the surface of an area light when appropriate.

Inside the foreach block members of the light iterator can be accessed that contain the results from invoking the light shader for each light. Members of the Light iterator class are listed in the table shown in .

The shader does not need to be aware of how many lights or how many samples it is accumulating it is only responsible for providing the BRDF for a single sample at a time with the renderer driving the enumeration of lights and samples. The shader will likely declare one or more variables outside the loop to store the result of the lighting. Each trip through the loop the shader will add the result of the BRDF to these variables.

A powerful feature of MetaSL is its ability to describe shaders independent of a particular target platform. This includes the ability to run MetaSL shaders in software with software based renderers and in hardware when a GPU is available.

Software rendering is typically more generalized and flexible allowing a variety of rendering algorithms including ray tracing and global illumination. At the time of this writing graphics hardware doesn t generally support these features. Further more different graphics hardware have different capabilities and resource limitations.

The MetaSL compiler will provide feedback to the shader writer indicating the requirements for any particular shader it compiles. This will let the user know if the shader they have written is capable of executing on a particular piece of graphics hardware. When possible the compiler will specifically indicate which part of the shader caused it to be incompatible with graphics hardware. For example if the shader called a ray tracing function the compiler may indicate that the presence of the ray tracing call forced the shader to be software compatible only. Alternatively the user may specify a switch that forces the compiler to produce a hardware shader. Calls to APIs that aren t supported by hardware will be removed from the shader automatically.

MetaSL includes support for the following preprocessor directives define undef if ifdef ifndef else elif endif.

These directives have the same meaning as their equivalents in the C programming language. Macros with arguments are also supported such as 

The include directive is also supported to add other MetaSL source files to the current file. This allows structure definitions and shader base classes to be shared across files.

A technique is a variation of a shader implementation. While some shaders may only require a single technique there are situations where it is desirable to implement multiple techniques. The language provides a mechanism to declare multiple techniques within a shader.

Often times a single shader implementation can map to both software and hardware so the exact same shader can be used regardless of whether rendering takes place on the CPU or GPU. In some cases though such as when the software shader uses features not supported by current graphics hardware a separate method for the shader needs to be implemented to allow the shader to also operate on the GPU. Different graphics processors have different capabilities and limitations as well so a shader that works on a particular GPU might be too complicated to work on another GPU. Techniques also allow multiple versions of a shader to support different classes of hardware.

Some shaders will also want to implement various shading methods that are used in different contexts. For example a material shader might implement a shadow technique that provides the amount of transparency at a surface point used when tracing shadow rays. Different techniques can also be used to implement shaders that are faster but lower quality or slower and higher quality.

While in a sense techniques are like different versions of a shader techniques of a shader are declared within the shader class and share shader parameters and other variables. This keeps techniques grouped within the class for organization. When a shader has only one technique it is not necessary for the shader class to formally declare the technique.

The technique declaration appears somewhat like a nested class definition inside the shader class definition. The technique declaration provides a name that can be used to refer to the technique. The technique must at least define the main method which performs the primary functionality of the shader technique. In addition the technique can implement an event method to handle init and exit events. The main and event methods are described in previous sections. In addition the technique can contain other local helper methods used by the two primary technique methods.

This example shows a shader that implements two separate techniques for hardware and software. The main and event methods of the techniques can be implemented inline in the class definition or separately as illustrated in this example.

A separate rule file accessible by the renderer at render time will inform the renderer how to select different techniques of a shader. A rule describes the criteria for selecting techniques based on the values of a predefined set of tokens. The token values describe the context in which the shader is to be used. Possible token values are 

Rules need to specify the name of the technique and an expression based on token values which defines when the particular technique should be selected. Multiple rules can match any particular set of token values. The process in which the renderer uses to select a technique for a shader is the following first only rules for techniques present in the shader are considered. Out of these rules each one is tested in order and the first matching rule selects the technique. If no rule matches then either an error warning is produced or a default technique is used for the shader.

The first three rules support software shaders that either have a single technique called standard to handle all shading quality levels or shaders that have two techniques beauty and fast to separately handle shading two different quality levels. Token values can also be available to shaders at runtime so the shader with a single standard technique could still perform optional calculations depending on the desired quality level.

The second three rules are an example of different techniques to support different classes of hardware. The fancy hardware technique might take advantage of functionality only available within shader model 3.0 or better. The nvidia hardware technique may use features specific to NVIDIA s nv30 chipset. Finally basic hardware could be a catchall technique for handling generic hardware.

The language includes a mechanism to allow material shaders to express their result as a series of components instead of a single color value. This allows the components to be stored to separate image buffers for later compositing. Individual passes can also render a subset of all components and combine those with the remaining components that have been previously rendered.

When a shader factors its result into multiple components it is possible for variations of the shader to be automatically generated that compute all components at the same time or a subset of all components. When only a subset of components are being calculated it s possible that some other computations on which the components don t depend on can be omitted from the automatically generated shader. For example if a shader used global illumination to compute indirect lighting and stored that indirect lighting in a separate layer other layers could be re rendered and composited with the indirect lighting layer without re computing the global illumination. The same shader source code can be used for each pass but the renderer will automatically generate individual shaders from the single source which compute only the necessary components. This obviously depends on having the source code available as well as a C compiler if any of the layers involve software rendering.

A material shader factors its result into components by declaring a separate output for each component. The names of the output variable define the names of layers in the current rendering.

This example shows a material shader that specifies three components for diffuse specular and indirect lighting.

When multiple material shaders exist in a scene that factor their result into different layers the total number of layers could be large. A user may not wish to allocate separate image buffers for each of these layers. A mechanism in the scene definition file will allow the user to specify compositing rules for combining layers into image buffers. The user will specify how many image buffers are to be created and for each buffer they would specify an expression which determines what color to place in that buffer when a pixel is rendered. The expression can be a function of layer values such as 

In this example the three layers from the shader result structure in the previous example are routed to two image buffers.

The standard MetaSL library provides API functions to allow shaders to cast rays. Ray tracing can be computationally intensive to optimize rendering times the renderer provides a mechanism to allow the delay of ray tracing so that multiple shader calls can be grouped together. This improves cache coherency and therefore overall shader performance. A shader has the option of calling a function to schedule a ray for ray tracing. This function returns immediately before the ray is actually traced allowing the shader and other shaders to continue processing.

When the shader schedules a ray trace it must also provide a factor to help control the compositing of the result from the ray trace with the shader s result. In addition it will provide a weight factor that describes the significance of the ray to the final image to allow for importance driven sampling. The factor could for example be the result of a fresnel falloff function combined with a user specified reflection amount. Ray scheduling implicitly defines a layer. The expressions in the layer compositing rules can refer to the factors provided when scheduling a ray. For example 

There are some cases where a shader needs to act on the result of tracing a ray immediately and cannot schedule a ray for later compositing. For these cases the synchronous ray trace functions will still be available.

Shader parameters are often set by users in an application using a graphical user interface GUI . In order for users to interact with shaders in a GUI an application must know some additional information about the shader parameters and the shader itself.

Informational attributes can be attached to shaders parameters or techniques by annotating the shader source code. Annotations are placed immediately after a shader parameter or technique declaration by enclosing a list of attributes in curly braces. An attribute instance is declared in a similar fashion to a class with optional parameters passed to its constructor. The syntax is 

Note that parameters to attribute constructors must be literals except for the special case of assigning state values as default values. Some standard attributes are predefined.

These attributes and other GUI related attributes can also be assigned to shaders through an external file. It may not always be desirable to clutter up the shader source code with a lot of annotations.

The following keywords are currently not used within MetaSL but are reserved for use in future versions class const private protected public template this typedef union virtual.

MetaSL includes a standard library of intrinsic functions. The following lists which may be expanded without departing from the scope of the invention do not include software only methods including lighting functions and ray tracing functions.

The compiler front end supports pluggable parser modules to support different input languages. While MetaSL is expected to be the primary input language other languages can be supported through an extension. This will allow for example an existing code base of shaders to be utilized if a parser is created for the language the shaders were written in.

The compiler back end is also extensible by plug in modules. The MetaSL compiler handles much of the processing and provides the back end plug in with a high level representation of the shader which it can use to generate shader code. Support is planned for several languages and platforms currently in use however in the future new platforms will almost certainly appear. A major benefit of the mill technology is to insulate shaders from these changes. As new platforms or languages become available new back end plug in modules can be implemented to support these targets.

The MetaSL compiler currently targets high level languages however the potential exists to target GPUs directly and generate machine code from the high level representation. This would allow particular hardware to take advantage of unique optimizations available only because the code generator is working from this high level representation directly and bypassing the native compiler.

Another component of the mill s MetaSL compiler is the Phenomenon shader graph compiler. The graph compiler processes shader graphs and compiles them into single shaders. These shaders avoid the overhead of shader attachments which makes it possible to build graphs from a greater number of simple nodes rather than a few complex nodes. This makes the internal structure of a shader more accessible to users that are not experienced programmers.

The phong specular function called in this example is a built in function provided by MetaSL. State parameters such as surface normal and ray direction are implicitly passed to the function.

There is now provided a description of the mental mill MetaSL shader debugger application which provides an implementation of the concept of images based shader debugging.

As shown in a statement is selected by clicking on the line of code click on a variable to display its value in the render window. In the screenshot the normal variable is selected which is of type Vector3 . The vector values are mapped to the respective colors. Lines that are statements have a white background. Lines that are not statements are gray.

As shown in when the selected statement is in a conditional only pixels where the conditional value evaluated to true display the debug value. The rest of the pixels display the original result.

The user can step through statements by using the left and right arrow keys to move forward and backward through the lines of code. The up and down arrow keys move through the variable list.

When vector values are mapped to colors they are set to wrap when their value exceeds one. In this example the selected texture coordinates repeat four times which is clearly visible when viewing the variable in the debugger.

The foregoing description has been limited to a specific embodiment of this invention. It will be apparent however that various variations and modifications may be made to the invention with the attainment of some or all of the advantages of the invention. It is the object of the appended claims to cover these and such other variations and modifications as come within the true spirit and scope of the invention.


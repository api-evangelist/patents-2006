---

title: Standardized natural language chunking utility
abstract: A method is disclosed for providing a chunking utility that supports robust natural language processing. A corpus is chunked in accordance with a draft chunking specification. Chunk inconsistencies in the corpus are automatically flagged for resolution, and a chunking utility is provided in which at least some of the flagged inconsistencies are resolved. The chunking utility provides a single, consistent global chunking standard, ensuring compatibility among various applications. The chunking utility is particularly advantageous for non-alphabetic languages, such as Chinese.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07672832&OS=07672832&RS=07672832
owner: Microsoft Corporation
number: 07672832
owner_city: Redmond
owner_country: US
publication_date: 20060201
---
There is a strong need felt in industry and academia for effective natural language processing NLP . Among the goals of natural language processing is to enable automated systems such as computers to perform functions on an input of natural human language. This would tremendously multiply the capabilities of computing environments in a broad range of applications. However despite substantial investigation by workers in artificial intelligence and linguistics effective natural language processing has remained elusive. Additionally different attempted solutions have been developed and applied from one application to another causing inconsistencies that prevent NLP interaction between applications.

Furthermore there are special problems in trying to develop NLP systems for certain languages that use non alphabetic writing systems. For example one such language is Chinese which uses a largely logographic writing system wherein thousands of characters are used each functioning as a logogram that is representing a concept rather than a particular sound as in an alphabetic writing system such as that used for English and other Western languages. A single character may represent a word or two or more characters may together represent a single word. Additionally the characters are traditionally written in a continuous string without spacing separating one word from the next as is typically in alphabetic writing systems. This adds an extra layer of ambiguity relative to languages written alphabetically the ambiguity in the proper boundaries between words from among a continuous string of logograms that may be one or several to a word. This ambiguity has posed a formidable additional obstacle to NLP systems in languages using logographic writing systems as opposed to those using alphabetic writing systems. Still other languages are written with a substantially syllabary writing system in which each character represents a syllable. For example Japanese is written with a mixture of logographic kanji and syllabary hiragana and katakana characters. The hiragana characters sometimes give hints on how to separate words and phrases while the kanji and katakana characters likely would not therefore also presenting an additional layer of ambiguity not encountered in NLP with Western writing systems.

Therefore there is a persistent need for better methods and systems of natural language processing particularly in non alphabetic languages.

The discussion above is merely provided for general background information and is not intended to be used as an aid in determining the scope of the claimed subject matter.

A method is disclosed for providing a chunking utility that supports robust natural language processing. A corpus is chunked in accordance with a draft chunking specification. Chunk inconsistencies in the corpus are automatically flagged for resolution and a chunking utility is provided in which at least some of the flagged inconsistencies are resolved. The chunking utility provides a single consistent global chunking standard ensuring compatibility among various applications. The chunking utility is particularly advantageous for non alphabetic languages such as Chinese.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter. The claimed subject matter is not limited to implementations that solve any or all disadvantages noted in the background.

Various embodiments provide a wealth of additional and unexpected advantages beyond the resolution of difficulties with current solutions. A variety of other variations and embodiments besides those illustrative examples specifically discussed herein are also contemplated and may be discerned by those skilled in the art from the entirety of the present disclosure.

Natural language processing NLP tasks can analyze text to identify syntactic and or semantic information contained therein. Syntax refers generally to the rules by which the symbols or words of a language may be combined independent of their meaning while semantics refers generally to the meaning of a grouping of symbols or words.

Such natural language processing tasks may include word segmentation part of speech tagging text chunking parsing and semantic labeling. Chunking a text is an intermediate step towards full parsing of text. Chunking is a useful and relatively tractable median stage of text analysis that is to divide sentences into non overlapping segments only based on superficial and local information. Chunking has been viewed as an intermediate step of parsing. While parsing typically involves identifying all linguistic structure of sentence such as the head of a sentence other components and relationships among components chunking is an intermediate step involving identifying phrase boundaries of sentences. Chunking results in the syntactic structure of a text becoming identifiable into e.g. noun phrases verb phrases and so forth. This also allows the relationships or dependencies between the phrases to become identifiable. For example one noun phrase is the subject of the verb phrase and a second noun phrase is the object of the verb phrase.

Chunking depends on a pre defined set of chunk types so a text can be divided into separate non overlapping chunks each of which is assigned a consistent chunk type. According to one illustrative embodiment as elaborated below eleven chunk types are defined one of which may appropriately cover most portions of a text while a few sections of text are left unchunked such as auxiliaries and conjunctions .

A chunking utility may be developed which may support additional natural language processing applications along with a variety of other kinds of applications. The chunking utility in one illustrative embodiment may include the definitions of the chunk types a set of unambiguous chunking rules such as to omit auxiliary words and conjunctions from the chunks and a large well refined chunking specification that has been refined through iterative chunking consistency feedback with a training corpus.

By providing a rich characterization of the phrase types and boundaries in a text chunking is also considerably useful in itself in applications in addition to those that involve further natural language processing such as voice user interface machine translation and search as a few illustrative examples. Chunking a text includes dividing the text into syntactically correlated groups of words which may be used by additional applications. This is illustrated in later sections with examples demonstrating certain embodiments that are illustrative of a broader range of methods.

Prior to discussing particular aspects of present embodiments in greater detail a few illustrative systems and environments with which various embodiments can be used are discussed. illustrates an example of a suitable computing system environment on which embodiments may be implemented. The computing system environment is only one example of a suitable computing environment and is not intended to suggest any limitation as to the scope of use or functionality of the claimed subject matter. Neither should the computing environment be interpreted as having any dependency or requirement relating to any one or combination of components illustrated in the exemplary operating environment .

Embodiments are operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with various embodiments include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputers mainframe computers telephony systems distributed computing environments that include any of the above systems or devices and the like.

Embodiments may be described in the general context of computer executable instructions such as program modules being executed by a computer. Generally program modules include routines programs objects components data structures etc. that perform particular tasks or implement particular abstract data types. Various embodiments may be implemented as instructions that are executable by a computing device which can be embodied on any form of computer readable media discussed below. Various additional embodiments may be implemented as data structures or databases that may be accessed by various computing devices and that may influence the function of such computing devices. Some embodiments are designed to be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment program modules may be located in both local and remote computer storage media including memory storage devices.

With reference to an exemplary system for implementing some embodiments includes a general purpose computing device in the form of a computer . Components of computer may include but are not limited to a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnect PCI bus also known as Mezzanine bus.

Computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer and includes both volatile and nonvolatile media removable and non removable media. By way of example and not limitation computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.

The system memory includes computer storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules that are immediately accessible to and or presently being operated on by processing unit . By way of example and not limitation illustrates operating system application programs other program modules and program data .

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example only illustrates a hard disk drive that reads from or writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from or writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from or writes to a removable nonvolatile optical disk such as a CD ROM or other optical media. Other removable non removable volatile nonvolatile computer storage media that can be used in the exemplary operating environment include but are not limited to magnetic tape cassettes flash memory cards digital versatile disks digital video tape solid state RAM solid state ROM and the like. The hard disk drive is typically connected to the system bus through a non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

The drives and their associated computer storage media discussed above and illustrated in provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing operating system application programs other program modules and program data . Note that these components can either be the same as or different from operating system application programs other program modules and program data . Operating system application programs other program modules and program data are given different numbers here to illustrate that at a minimum they are different copies.

A user may enter commands and information into the computer through input devices such as a keyboard a microphone and a pointing device such as a mouse trackball or touch pad. Other input devices not shown may include a joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . In addition to the monitor computers may also include other peripheral output devices such as speakers and printer which may be connected through an output peripheral interface .

The computer may be operated in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a hand held device a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or other appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. By way of example and not limitation illustrates remote application programs as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

Memory is implemented as non volatile electronic memory such as random access memory RAM with a battery back up module not shown such that information stored in memory is not lost when the general power to mobile device is shut down. A portion of memory is illustratively allocated as addressable memory for program execution while another portion of memory is illustratively used for storage such as to simulate storage on a disk drive.

Memory includes an operating system application programs as well as an object store . During operation operating system is illustratively executed by processor from memory . Operating system in one illustrative embodiment is a WINDOWS CE brand operating system commercially available from Microsoft Corporation. Operating system is illustratively designed for mobile devices and implements database features that can be utilized by applications through a set of exposed application programming interfaces and methods. The objects in object store are maintained by applications and operating system at least partially in response to calls to the exposed application programming interfaces and methods.

Communication interface represents numerous devices and technologies that allow mobile device to send and receive information. The devices include wired and wireless modems satellite receivers and broadcast tuners to name a few. Mobile device can also be directly connected to a computer to exchange data therewith. In such cases communication interface can be an infrared transceiver or a serial or parallel communication connection all of which are capable of transmitting streaming information.

Input output components include a variety of input devices such as a touch sensitive screen buttons rollers and a microphone as well as a variety of output devices including an audio generator a vibrating device and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition other input output devices may be attached to or found with mobile device .

As an overview method includes step of providing a draft chunking specification step of chunking one or more corpora in accordance with the draft chunking specification step of flagging chunk inconsistencies in the chunking of similar portions of the one or more corpora and step of providing a chunking utility comprising chunking assignments revised from the draft chunking specification such that the flagged inconsistencies are resolved. Method may also include additional steps such as step of providing access to the chunking utility to one or more external applications such as a natural language processing application associated with additional natural language processing tasks or additional applications such as a voice user interface a machine translation tool or a search tool among other potential applications.

Method includes step of providing a draft chunking specification. This may be for example a first iteration drafted based on computational linguistic principles. The draft chunking specification provides indications of how a text may have chunk annotations or indications associated with the elements of the text. This is elaborated in greater detail below.

Method further includes step of chunking one or more corpora in accordance with the draft chunking specification. For example two similarly sized and similarly sourced training corpora e.g. all articles from each of two separate major newspapers over the same long duration of time may be used for chunk training at the same time based on the same draft chunking specification. The scale of each training corpus may be for example in excess of one million words in one illustrative embodiment or in excess of half a million sentences in another embodiment. The corpora may be smaller or larger than this range in various embodiments. The chunk assigned results from both corpora may then be compared for inconsistencies in which similar or identical portions of text were chunked differently. The corpus or corpora used for this step may for example include large collections of fairly standard published examples of written text in the target language such as a collection of all articles appearing in a major newspaper over long periods of time. For example one prominent English corpus relies on a large collection of published material from the Wall Street Journal. A corpus in another language such as Chinese might similarly include a large collection of articles from a major Chinese language newspaper such as the People s Daily for example.

As the corpus is chunked in accordance with the draft chunking specification a large amount of chunked text is accumulated and made available for mutual comparison. This in turn provides a valuable way of evaluating the performance of the chunking specification as it is used by the chunking application performing the chunking. The application performing method continues storing chunked corpus in accordance with the chunking indications provided by the chunking specification and as the sample of chunked corpus grows larger chunking inconsistencies may be discovered. These chunking inconsistencies consist of identical or similar sections of text from different parts of the corpus that have been chunked differently from each other. This indicates an inconsistent set of indications from the chunking specification. Such inconsistent indications are difficult to eradicate because they often tend to stem from ambiguities in language that human speakers take for granted and typically interpret from context without a second thought but that resist bright line logical delineations of the kind that might more easily be comprehended by a software application.

As one illustrative example in English of such easily overlooked linguistic ambiguity the terms con and pen can both have a variety of meanings on their own or as parts or prefixes of larger words. However when someone says the con is in the pen a human listener fluent in English does not think twice about understanding this statement to mean that the convict is in the penitentiary . However such real world context as we take for granted in combining with the statement of itself to deduce the meaning is not available to a computer application and cannot be compensated for without a considerable complexity of different indications for how to chunk or otherwise interpret the terms con and pen using only the knowledge of their surrounding text. So even after significant development it might not be surprising to find a chunking utility having provided inconsistent chunking indications between two different occurrences of the phrase the con in different textual contexts perhaps between the con is in the pen the convent had to be evacuated and the conniving became too much for her . Similarly in Chinese for example a single sentence may read as transliterated into Roman alphabetic characters in the standard pinyin to take an egregious example m mhacek over a m ma m mhacek over a m ma which means Did mother curse the horse or did the horse curse mother 

While the Chinese characters in the former sentence provide more substantial differences to distinguish the meaning of the words Chinese is also noted for frequently having an identical character that may mean very different things in different contexts a more serious obstacle for chunking Chinese text. Referring again to mhacek over a or this is the word for horse when it is alone. The character also appears as one of two or more characters in literally dozens of other words. Many have to do more or less with a horse such as words for bridle horsepower cart and engine. However they also include mhacek over a bihacek over a o which means stopwatch though as two separate words mean horse surface mhacek over a f which means groom while as separate words mean horse husband mhacek over a hu which means careless while as separate words mean horse tiger mhacek over a sh ng which means immediately while as separate words mean horse up mhacek over a xgrave over l which means circus while as separate words mean horse play mhacek over a l khacek over o which means intersection while as separate words means horse road mouth mhacek over a l ingshhacek over u which means potato though as separate words mean horse bell yam .

To add still additional complexity and sensitivity to context a single character may mean completely different things each of which is pronounced differently. For example the character may be pronounced hu hu h or h . The word hu means soft or warm the word hu means to mix together to blend the word h may mean and with together with peace harmony or union and h means to respond within a poem or to respond in singing. Despite the different pronunciations in spoken Chinese each of these words is written as with only context to inform the reader which meaning is intended. This is in addition to the possibilities for to be part of a multi character word such as h feng breeze h hhacek over a o to become reconciled heshang Buddhist monk or h pingzhhacek over u yi pacifism among a wide variety of other examples.

There are several other examples but this provides some indication of the ambiguous sensitivity to context in Chinese exacerbated relative to alphabetic languages by the lack of spaces between words to give any clear indicator of the boundaries separating one word from the next. Furthermore Chinese involves additional characteristics that complicate chunking such as relatively little use of function words or inflections. All of these factors make it difficult to automatically assign portions of a text to their appropriate chunks.

As is apparent even after a substantial amount of development a chunking utility is likely to continue to experience at least a few inconsistencies in the different chunking indications it provides to an application chunking a corpus. This leads to the next step depicted in method the step of flagging chunk inconsistencies in the chunking of similar or identical portions of the one or more corpora. The application performing the method is in the present illustrative embodiment configured to search out and automatically flag such inconsistencies that accumulate in different sections of the chunked corpus. This may include for example an automated comparison of the chunking applied to similar or identical sets of characters from the one or more corpora. Flagging the chunk inconsistencies may also include automatically providing to an output device such as monitor of or input output components of indications of similar sets of characters that have been chunked differently.

After they are flagged these inconsistencies can be examined and a selection is made as to which chunking indication is more appropriate. Since identical portions of text may have legitimately different meanings in different contexts assigning chunk indications should be done in a way that prevents or minimizes interference with the precision of the automatic chunking. It sometimes happens that a sentence appears for example in the form ABC and both AB and BC can be legitimate chunked phrases but one or the other must be selected to ensure a globally consistent chunking specification in which case the more useful chunked phrase should be selected with care. The application then responds to this selection of chunking indications by revising the chunking utility with the additional information thereby adding to the sophistication and complexity it is able to apply to further chunked texts. This leads into step of providing a refined chunking utility comprising chunking assignments revised from the draft chunking specification such that the flagged inconsistencies are resolved. At some point perhaps after a threshold of consistency is reached as in one illustrative embodiment the iterative learning process for refining the chunking specification may be declared finished and the refined chunking utility comprising the results of refining the chunking specification may be made available. In one embodiment a chunk training application regularly or in response to a command calculates the consistency ratio of the chunking utility. For example this may be the ratio of consistently chunked text to the total amount of text chunked. This automatically calculated consistency ratio can be compared with a threshold level to determine if the chunking utility has become sufficiently refined for its purposes.

One advantageous purpose to which such a highly refined chunking utility can be applied is as a global standard for chunking later inputs in the subject language across a wide array of different applications platforms or environments. The inconsistency of different localized approaches to natural language processing have posed a significant barrier in the past to different applications interacting in ways that involve the exchange of natural language inputs. Such inconsistencies from one application to another often tend to result in unpredictable or nonsensical results. Instead as in step according to one embodiment of method access to the refined chunking utility may be provided to external applications examples of which include a voice user interface a machine translation tool and a search tool. These additional applications may be hosted locally on components shared on a computing device executing the chunking method or they may be hosted remotely and communicated with via a network or communication interface such as wide area network or local area network of or via communication interface of . With a wide variety of applications able to share in the single highly refined global standardized chunking utility provided according to method as opposed for example to two or more applications that have incorporated results from different inconsistent chunking specifications the interactions among various natural language processing applications and a wide variety of additional applications may become smooth and seamless with the assurance of as high a level of consistency as is desired. Some of these other applications may go on to perform functions incorporating results from the standardized chunking utility resulting in tangibly superior performance such as more valuable search results or more accurate responses in a voice user interface for example.

One example is further illustrated in . depicts a flowchart for a method of language chunking according to another illustrative embodiment. Method may be executed by a natural language processing program using the global standardized refined chunking utility as its reference. Method includes step receiving an input step assigning chunk types to portions of the input based at least in part on the chunking utility and step providing an output comprising the portions of the input with the assigned chunk types. The output may be provided to a user via a user output device or it may be provided to another application for that application to act on. Method may include a variety of other steps in different embodiments. Method may illustratively include step to convert a non text input such as from a user s spoken word or handwriting into a text format and step to segment the input into words and tag the words with parts of speech. The chunking may involve searching the refined chunking utility for chunked text portions similar to the portions of the input and assigning the portions of the input chunk types corresponding to chunk types associated with the chunked text portions. For example method might include a voice user interface running on a computing device that receives a speech signal from a user converts the speech signal to text chunks the text by comparing it to the refined global standardized chunking utility then using the syntactic structure indicated by the chunked text version of the user s spoken word input to carry out tasks indicated by the input and to respond to the user. In other applications the non text input may be a user s handwriting as written with a stylus on the monitor of a handheld or notebook device or otherwise input to a computing device. This input is non text in the sense that it is not input via a keyboard. In other applications the user may be entering natural language text inputs in some applications chunked texts derived from the user input may be applied to other applications such as a machine translation tool or a search tool for example.

Some other additional details involved in various illustrative embodiments are provided in further details below such as flagging chunk inconsistencies by comparing both chunking indications and tagged part of speech sequences of similar passages of text. Also elaborated below are some of the particular advantages provided by a chunking utility directed to chunking in a non alphabetic writing system such as Chinese or Japanese for example. In such a case providing the draft chunking specification may include providing indications for word segmenting the non alphabetic writing system to avoid the word boundary ambiguities particular to such non alphabetic languages as indicated with the horse example above.

Additional details on the chunking methods and using a training corpus to provide iterative feedback in growing the sophistication and optimizing the consistency ratio of the chunking utility specification are provided below.

As noted above one factor that makes an important difference in chunking is the type of writing system of the text being chunked. Some languages such as English and French are associated with an alphabetic writing system in which each character represents one sound while still other languages are associated with a syllabary writing system in which each character represents one syllable while still other languages such as Chinese are associated with a logographic writing system in which each character represents a concept or word while still other languages such as Japanese are associated with a mixture of these writing systems. Writing systems are distinguishable from languages in that a language can typically be represented in a writing system alien to the one with which that language is customarily associated. However some languages have a prominent history of being written in more than one writing system for example the Chinese language is traditionally written in Chinese logographic characters but can also be written in alphabetic characters. Several standards for transliterating Chinese into alphabetic writing have been developed perhaps the most widely used is the standard known as pinyin. Other languages have a strong tradition of being written in one writing system while a closely related language is written in a dramatically different writing system. For example Urdu is traditionally written in a variation of the Persian alphabet while Hindi a language very closely related to Urdu is traditionally written in the Devanagari script which is an alphasyllabary writing system a hybrid of an alphabet and a syllabary.

Whereas alphabetic writing systems typically clearly indicate the separation between adjacent words the same is often not true of syllabary and logographic writing systems in which characters are typically written in one long string with boundaries between words discernible only by context. Even in some largely alphabetic writing systems such as the alphasyllabaric Thai alphabet the letters are typically written in one continuous stream with spacing only between sentences and only context to guide the reader as to the proper boundaries between words within the sentence. Such lack of clear word boundaries poses an added layer of ambiguity for natural language processing systems and related systems such as those involving chunking. This is of particular concern because such non alphabetic languages include some such as Chinese for which demand and growth prospects for natural language processing applications are particularly high.

One embodiment directed to aspects of chunking in non alphabetic languages is illustrated below as applied to written Chinese texts which are used as an example. It is not necessary for the reader to understand Chinese to follow the steps and methods demonstrated to illustrate aspects of chunking and associated natural language processing methods in accordance with certain illustrative embodiments. And although Chinese is used extensively for illustrative examples herein this does not imply any limitations regarding other languages and writing systems additional embodiments use similar methods with regard to other languages and writing systems. And while some additional examples demonstrate particular advantages as applied to non alphabetic writing systems this covers a wide range of writing systems even the so called Thai alphabet though closer to being alphabetical than the Chinese writing system is still actually an alphasyllabary rather than a true alphabet and therefore non alphabetic. Additionally various embodiments are similarly advantageous as applied to true alphabetic writing systems.

Various embodiments of a chunking utility may be directed to any of these languages and writing systems and provide particular advantages in providing a standardized set of rules across writing systems that are logographic syllabic alphabetic and anything in between so that the chunking annotations will be represented in a standardized way regardless of the type of underlying writing system or language. Standardized chunking utilities according to various embodiments provide substantial advantages in machine translation as one illustrative example of the applications facilitated by such chunking utilities.

Examples of Chinese texts are therefore used below to demonstrate how Chinese texts may be for example parsed chunked and otherwise manipulated. In a few illustrative examples these characters are provided with Roman alphabet transliterations and English translations to aid in gaining familiarity with various methods related to chunking. In particular these examples are provided with three representations of each character first the original Chinese characters themselves in the standardized simplified form known as jihacek over a ntizi i.e. adopted by the governments of the People s Republic of China and Singapore as well as by the Library of Congress and the American Library Association second in the standardized format for representing transliterated Chinese words with Roman alphabet characters known as hanyu pinyin or simply pinyin h nyhacek over u p ny n i.e. and third in English translation. So for example a character may be provided as ni n year which provides first the Chinese jihacek over a nthacek over i z character then the pinyin representation ni n and finally the English translation year . Incidentally Chinese also commonly uses the same Arabic numerals that are used in English and other Western writing systems.

A first sample sentence in Chinese preceded by a reference marker for the sentence is used below to demonstrate first word segmenting and part of speech tagging and then chunking. The sample Chinese sentence reads 

This sentence is a sample chosen from the Jan. 1 1998 issue of People s Daily a major Chinese newspaper printed in the simplified jihacek over a nthacek over i z script. It is a statement attributed to Boris Yeltsin referring to the Russian economy. It reads roughly In 1997 production succeeded in halting the collapse and appears to be expanding for the year. 

Chunking operates on a passage of text that has already been word segmented and part of speech tagged in this embodiment. The sentence above is shown here after being word segmented 

It can be seen that the word segmenting has isolated some of the individual logograms by themselves these ones carry the meaning of a single word by themselves while other logograms are segmented into pairs indicating that the two concepts represented by the two separate logograms actually form part of a single word. The distinctions between such single logogram words and such double logogram words is not apparent from the original unspaced string of characters and indicates the added ambiguity in Chinese relative to English or other Western languages in which identifying the words of a text is relatively simpler involved in natural language processing tasks such as chunking.

Notice for example that 1 9 9 7 is lumped together as a word in Chinese the symbol pronounced ni n which means year is lumped together with and after the numerals to indicate that it is not just a reference to the numner 1 987 but rather that it means the year 1997 which represents a single concept and is treated as a single word. Notice also that the period is also segmented it also conveys its own meaning separately from any of the other words i.e. the meaning end of sentence and it too is treated and segmented as its own separate word.

The importance of the word segmenting can be appreciating by examining the differences between the two character words in the sentence above and the words that would be represented by the same characters as one character words. The two character words are provided with pinyin transliteration and English translation also provided for each character in the format Chinese character pinyin transliteration English translation such as in ni n year . This is in order to aid in gaining familiarity or as might be done to help English language developers become familiar with Chinese texts being processed although it is not a necessary part of a natural language processing or chunking process.

The first two character word is sh ngchhacek over a n to produce to manufacture. If interpreted as two separate words these characters would mean sheng to be born to give birth and chhacek over a n to reproduce. The next two character word is t ngzhhacek over i to stop to halt to cease if interpreted as two separate words this would mean t ng to stop to halt and zhhacek over i to stop toe. The next two character word is huapo rockslide landslip collapse if interpreted as two separate words this would mean hua comical cunning slippery smooth and p slope. The next two character word is ch xi n to appear to arise to emerge if interpreted as two separate words this would mean chu to go out to come out to occur to produce and xi n appear present now existing current. The final two character word in the above sample sentence is z ngzhhacek over a ng to grow to increase expand if interpreted as two separate words this would mean z ng to increase to expand to add and zhhacek over a ng length long forever always constantly. In each of these cases there is no blank space between the characters of the word or between them and the adjacent words and in isolation interpreting the pairs as a single two character word or as two separate one character words are equally valid only context is normally available to inform the reader of the true meaning. The word segmenting effectively substitutes for appreciation of context for an automated chunking utility.

Here each of the segmented words has been tagged i.e. has had a tag added to it of the format x with a one or two letter code in this illustrative embodiment indicating one of forty pre defined parts of speech. The part of speech tags shown in this example are t v u c m and w with the v appearing six times and the u appearing twice. The t indicates a temporal word v indicates a verb u indicates an auxiliary c indicates a conjunction m indicates a numeral and w indicates a punctuation mark.

A full list of the forty part of speech tags used in one illustrative embodiment is provided as follows with the Chinese name for the parts of speech also listed 

The same sentence as before now word segmented and part of speech tagged is next provided with pinyin transliteration and English translation also provided for each character in the format Chinese character pinyin transliteration English translation such as in ni n year . This is in order to aid in gaining familiarity or as might be done to help English language developers become familiar with Chinese texts being processed although it is not a necessary part of a natural language processing or chunking process in this embodiment 

 1 9 9 7 1997 ni n year 1997 t ch ng succeed v le particle indicating completed action u sh ngchhacek over a n produce v t ngzhhacek over i stop v hu p collapse v b ng and c ch xi n appear v z ngzhhacek over a ng expand v de of u y one m ni n year q j h o period w

After a chunking process in which the text is divided into syntactically correlated parts of words the same sentence now chunked appears as follows with brackets around each chunk first with Chinese characters only the at the beginning is just a reference marker for the sentence 

The chunk types along with the chunk assignment indicators such as TP VP etc. are described in more detail below. Notice that one portion of the chunked sentence i.e. w is not contained within a pair of brackets indicating a chunk. A punctuation mark though word segmented and part of speech tagged is not included in a chunk according to one of several rules applied in the chunking process according to the present illustrative embodiment. The same passage is again provided next with pinyin representation and English translation for each character 

 TP 1 9 9 7 1997 ni n year 1997 t VP ch ng succeed v le particle indicating completed action u VP sh ngchhacek over a n produce v VP t ngzhhacek over i stop v VP hu p collapse v CONJP b ng and c VP ch xi n appear v VP z ngzhhacek over a ng expand v de of u MP y one m ni n year q j h o period w

In the above sentence there are six verbs ch ng succeed sh ngchhacek over a n produces t ngzhhacek over i stops hu p collapse ch xian appear and z ngzhhacek over a ng expand a conjunction b g and and two auxiliaries le particle indicating completed action and de of . Hundreds of possible parses can be generated by a full parser due to structural ambiguities among these types of words. A chunking process can avoid syntactic ambiguities to provide consistency for natural language processing NLP tasks.

Many chunks have a modifier head structure. However verb object and subject object relations do not appear inside a chunk. In some applications the task of chunking can be implemented by a finite state transducer FST . For natural language processing applications such as information retrieval information extraction and question answering a full parsing result of a given sentence or portion of text is not required. Instead a chunking process can meet the need of most applications and also provide a solid basis for further incremental higher level syntactic and semantic analyzing of text.

When the chunking specification is being refined against the training corpus a semi automatic inconsistency control method may be used to evaluate and list all potential inconsistent annotations in one illustrative embodiment. A tool is used to check mechanical errors and detect those potential inconsistent annotations with part of speech and or lexicalized sequence. The part of speech tags are as listed above. For example one inputs a part of speech sequence n n n referring to a noun noun noun sequence e.g. computer network provider and an expected chunking annotation result B NP I NP E NP referring to a beginning of noun phrase intermediate part of noun phrase end of noun phrase and the tool will list all the consistent and inconsistent sentences in the chunk annotated text respectively.

The chunk types defined in the present embodiment of the chunking utility specification include NP noun chunk VP verb chunk ADJP adjective chunk ADVP adverb chunk PP prepositional chunk CONJP conjunction MP numerical chunk TP temporal chunk SP spatial chunk INTJP interjection and INDP independent chunk .

Many examples of chunked sentences or portions of text according to one illustrative embodiment of a chunking utility directed to chunking in the Chinese language as one particular example are provided below including both retained or correct chunk assignments and rejected or incorrect chunk assignments. The following example provides additional details of one particular embodiment to provide a better understanding of how this particular embodiment functions and to provide a jumping off point for understanding how a variety of other embodiments would also work by differentiation with the details of the present illustrative embodiment. A considerable breadth of variation from the particular details described below for the present illustrative embodiment may occur in other embodiments. The correctly chunked sentences according to the present illustrative embodiment of a chunking utility are indicated by being prefaced with a reference number in the format . The rejected passages are indicated by or by . While the not recommended sentences may be grammatically correct in some context they have not been able to be generalized and are determined likely to foster ambiguity in the present embodiment. Because different combinatations and definitions of chunk assignments may be used in various embodiments some sentences indicated as not recommended in the following which is only one illustrative example may be quite appropriate in another embodiment.

A noun chunk is a word sequence which consists of a single noun or a modifier head structure d ngzh ngji g u . Compounds formed by two or more adjacent nouns are particularly common in Chinese for example. Typically about 77 of the time a Chinese noun phrase takes the form of a modifier head structure where the right most noun in the phrase is the head and the one or more nouns preceding on the left are modifiers of the head noun. For example to illustrate the same structure in English in which it is not as common as in Chinese galaxy cluster image and dairy farm country are examples of three noun phrases in which the right most noun image and country respectively are the head and the nouns preceding on the left galaxy cluster dairy farm modify the meaning of the head by providing details to characterize the head. Galaxy cluster and dairy farm taken by themselves are also noun phrases in which the noun on the right is once again the head and the noun preceding on the left modifies the head. The modifier head structure consists of a head noun and one pre modifier whether the pre modifier has one or more nouns in it. In turn a pre modifier consists of a sequence having one or more of the following phrase structures and excluding an auxiliary de of numerical phrase adjective phrase or noun phrase. In other words the auxiliary de of remains outside of any chunk according to the chunking utility. Other types of noun phrases in Chinese have different structures for example such as coordination in which two nouns appear next to each other with an and implied between them.

The part of speech of the head words of NP are usually n vn r an Ng names entities nr nr nt nz and some nominal idioms. The part of speech vn and an should be treated as a noun only. For example 

Most NPs only have one head but for some noun coordination without conjunction it could be annotated as a single NP. The reason is that it is almost impossible to distinguish it from a modifier head construction. So there are more than one head in a NP when its inner structure is a noun coordination without conjunction for example 

Most NPs only have one modifier and it can be a numerical phrase adjective phrase or one noun. For example 

But there are some exceptions for the indistinguishable inner structures. If certain syntactic ambiguities exist inside a chunk and do not affect the boundary of the chunk then those ambiguities will be left without chunk annotating.

If the second noun is the head of the NP a n n no matter whether the a n or the n n form a noun phrase first then a n n should be chunk annotated as a single NP. For example 

If a a n n sequence as a NP follows either a numeral m or a MP then they should be chunk annotated as two chunks separately. For example 

If the second noun is the head of the NP m n n no matter whether the m n or the n n form a noun phrase first then m n n should be chunk annotated as a single NP. For example 

Compounds formed by more than two neighboring nouns are very common in Chinese and usually but not always all the nouns to the left of the head of the compound modify the head of the compound. Some compounds consist of several shorter sub compounds. Unfortunately sometimes it is impossible to distinguish the inner structures. In one embodiment the noun sequence may be grouped as a single NP if its head is the last noun of the sequence. This grouping into a single larger NP is less likely to result in a noun phrase being inaccurately severed among two different chunks the longer chunks also tend to make subsequent parsing easier. For example 

If two neighboring nouns belong to two different constituents respectively then they should be chunk annotated as two separated chunks.

If a v n pattern forms a VP which in turn modifies the second noun then these two nouns should be chunk annotated as two chunks separately. For example 

If the first noun and the preceding phrase de of possessive particle together form a noun phrase which in turn modifies the second noun then these two nouns should be chunk annotated as two separated NPs. For example 

If only the second noun and de of possessive particle including its following words form a noun phrase then these two nouns should be chunk annotated as two separated NPs. For example 

If the first noun and the conjunction or h and including its preceding words form a coordinate phrase which in turn modifies the second noun then these two nouns should be chunk annotated as two separated NPs. For example 

If the second noun and the conjunction or h and including its following words form a coordinate phrase then these two nouns should be chunk annotated as two separated NPs. For example 

 7 Punctuation between neighboring nouns is omitted in titles of news articles or poems in the present Chinese embodiment. But those neighboring nouns should be chunk annotated separately according to their context. For example 

The adjoining organization name the professional title and the person name should be chunk annotated as three chunks separately. But the person name followed with his her title should be chunk annotated as a single NP. For example 

A verb chunk is a word sequence which consists of a single verb including a verb particles structure refer to the morph lexicon or a modifier verb structure . Such particles consist of oriented verbs and auxiliaries A modifier verb structure is in turn formed by a head verb and a pre modifier. We define a pre modifier here as adverbial phrases and or auxiliary verbs . Auxiliary verbs include dhacek over e i etc. Post modifiers of a verb such as object and complement should not be included in the verb chunk. The following is an example of VP 

In addition particles of a verb particle structure include auxiliaries . In general the auxiliary dhacek over e i must is an indicator of a following complement. Therefore the auxiliary dhacek over e i must remains outside the verb chunk in most cases. One exception is the so called complement of possible mode . Since expressions in possible mode look like they are well constructed they should also be treated as verb particle structures including the auxiliary . For example VP v u VP v u VP v u a VP v d a . But in most cases the auxiliary dhacek over e i must remains outside any chunk.

If the head verb followed by an auxiliary should be also chunk annotated as a single VP and if it has pre modifiers they are grouped as a whole as a VP. For example 

A head verb with its preceding auxiliary verbs should be grouped together as a single VP. Following is a whole list of auxiliary verbs in Chinese 

Adverbials either before or after an auxiliary verb should be combined with the head verb to form a single VP. For example 

A head verb followed by oriented verbs should be treated as a single VP in whole. Following is a whole list of oriented verbs in Chinese 

Some sophisticated VP s include more than one auxiliary verbs oriented verbs adverbs or auxiliary particles. For example 

Verbs other than auxiliary verbs and oriented verbs in the above lists could not be grouped with a head verb to form a v v VP in this embodiment. For example 

For a splittable verb one auxiliary and or one classifier could be inserted into the splittable verb and form a single VP as a whole. For example 

A verb and its following preposition should be chunk annotated as a VP and a PP respectively. For example 

An adjective or adverb that acts as a post modifier of a head verb should not be grouped with its preceding verb. For example 

An adjective chunk is a word sequence which consists of a single adjective or a head adjective with its pre modifiers such as an adjective adverb phrase. As mentioned above if an ADJP was already included in a NP or VP as its pre modifier like NP a n then it should not be tagged as an ADJP explicitly. If an ADJP acts as the predicate of a sentence then it should be tagged separately. For example 

An adverb chunk is a word sequence which consists of a single adverb or a head with its pre modifiers. As mentioned above if an ADVP is already included in a VP or ADJP as its pre modifier then it should not be tagged as an ADVP explicitly. An ADVP example is as follows 

Most prepositions form a PP which consists of only the preposition itself. However certain prepositional phrases form a frame with explicit boundary such as PP p . . . f PP p . . . f so they could be chunked as a multiple word PP without ambiguity. The length of such kinds of PP frames can be limited to four words.

A frame like PP is formed by a p  . . .  f pattern in which the p and f are the left and right boundary respectively and between these two words at most two other words could be inserted in this embodiment. For example 

Most conjunctions excluding h and h o or and yhacek over u and form a CONJP which consists of only the single conjunction word alone. Conjunctions h and h o or and yhacek over u and remain out of any chunk. For example 

A temporal chunk consists of a temporal word shiji nci temporal word part of speech tag is t as its head. A TP always acts as a temporal adverbial constitute in a sentence. For example 

A spatial chunk consists of a localizer f ngw ici direction position word part of speech tag is f as its head. Note that in general due to structural ambiguity f could not chunked with its preceding n or v directly. For example 

A localizer f could be grouped with its preceding preposition p to form a frame like PP as referred to above.

If the left context of a localizer is the beginning of a sentence or a punctuation except and the number of words in front of the localizer is no more than three then those words and the localizer should be grouped together as a SP. Inside a SP a multi word TP or MP is treated as a single word. For example 

An interjection chunk consists of an interjection yhacek over u q c mood word part of speech tag is y . Frequently used interjections include 

All the meta data of a document and all the descriptions inside a pair of brackets including the opening and closing brackets should be tagged as an INDP. For example 

Certain inserted phrases which do not act as syntactic constituents in a sentence should be tagged as a INDP. Following are examples of those inserted phrases . . .

Although Chinese was used for these illustrative examples methods according to various embodiments are advantageously applicable to any of a wide variety of writing systems.

Similarly although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as illustrative examples of ways to implement the claimed subject matter. A wide variety of implementations of the claimed subject matter is contemplated including but not limited to implementation in stand alone applications portions of applications network based applications server hosted applications wireless delivered applications dynamic link libraries objects and a wide variety of other code data resources and combinations thereof.


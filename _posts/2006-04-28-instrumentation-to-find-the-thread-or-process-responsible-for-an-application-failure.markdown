---

title: Instrumentation to find the thread or process responsible for an application failure
abstract: An application programming interface (API) that leverages operating system instrumentation to provide a chain of threads and processes may alleviate some debugging complications. Specifically, the chain may start with the first thread in the process that experienced the original failure and end with the last thread upon which the first thread directly or indirectly depends. The API may aid debugging efforts by classifying all threads related or dependent upon an original failed thread into specific categories of failures, requesting further information from the originating OS concerning specific failed threads, and using that information to debug the failed application or process more thoroughly.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07958512&OS=07958512&RS=07958512
owner: Microsoft Corporation
number: 07958512
owner_city: Redmond
owner_country: US
publication_date: 20060428
---
This application is a continuation in part of prior application Ser. No. 11 263 318 DETECTION DIAGNOSIS AND RESOLUTION OF DEADLOCKS AND HANGS to Heddaya et al. filed Oct. 31 2005 the entire disclosure of which is incorporated by reference herein.

Operating systems OS are a key building block in the development of computing systems. Over the several decades since personal computing has become widespread operating systems have substantially increased in complexity. The ability to multi task and support concurrent processes has given even modest personal computers the appearance of simultaneously running a wide variety of programs from word processors to Internet browsers.

In fact though virtually all microprocessor based systems run one program at a time using a scheduler to guarantee that each running program is given processor time in sufficient quantities to keep running. This task can become quite complex. Each process running on a computer can spawn individual tasks called threads. Some threads can spawn subordinate threads. It is common to have dozens or even hundreds of threads active at a given time. On the other hand the computer may have a limited number of resources such as disk storage or network input output. Even though each resource can often support multiple threads in many cases a thread may have to wait for access to a given resource until a different thread releases it.

A thread can lock a resource it is using and make it unavailable for other threads. A common situation occurs where two or more threads require resources that are locked by another thread. When threads lock each other s resources a deadlock may occur. Typically a timeout timer will fire when inactivity is observed over a pre determined time period and kill one or more of the involved threads. Unfortunately most users are less patient than the timers and will intervene before the timeout period with a reset or other dramatic action. The timeout time can be shortened to beat user s impatience but at the risk of killing slow but not deadlocked threads.

Another way to address deadlocks is strict monitoring of every locking relationship. However in modern high clock rate systems locks can be placed and released in a matter of microseconds and it is not unusual for hundreds of locks to exist at any moment in time. Therefore strict monitoring may require more processor resources than those being monitored and the associated memory write times could slow processing to a crawl.

The standard approaches to diagnosing and solving deadlocks are live debugging of the application process or capturing detailed information about the processes involved in the failure from the memory at the time of the failure for post mortem analysis. Because a first thread s failure may be due to its dependency on a second thread s failure finding the root source of a failure may be complicated. To find the root cause the other thread or process which is responsible for the failure must be identified. However the root failure is difficult to obtain during post mortem because information to trace the root cause thread is not included in the process memory dump. Furthermore even if the root cause failed thread can be obtained through additional debugging using the process memory dump it may be impossible to debug further because information about the root cause process is not collected at the time of the failure.

An operating system may monitor verify and assess deadlock conditions by taking advantage of the fact that by definition deadlocks are persistent. A quick scan of locking relationships may be made building an approximation of locks and dependencies. It is an approximation because even several clock cycles after scanning the locking relationships those relationships are obsolete. Even between the beginning of the scan and the end the relationships may change. An analysis of the scan of locking relationships may show cyclical relationships as described above but in fact it may not be cyclical and only an artifact of a locking relationship that no longer exists.

However a real deadlock may exist. By examining the locking relationships a second time particularly targeting suspect locking relationships of the first scan a deadlock can be verified because it will persist over extended periods of time. When a deadlock is confirmed data corresponding to the threads and resources involved can be forwarded to a monitor or other process that can intervene to break the deadlock preferably before a user notices the incident.

Although not a deadlock by definition a similar situation called a hang where a thread or resource stops or becomes inaccessible and blocks predecessors with locking relationships can be monitored and verified in a similar fashion. Determining hangs can be useful for both resolving the hang and diagnosing root causes for the situation.

An application programming interface API that leverages operating system instrumentation to provide a chain of threads and processes may alleviate some debugging complications. Specifically the chain may start with the first thread in the process that experienced the original failure and end with the last thread upon which the first thread directly or indirectly depends. The API may aid debugging efforts by classifying all threads related or dependent upon an original failed thread into specific categories of failures requesting further information from the originating OS concerning specific failed threads and using that information to debug the failed application or process more thoroughly.

Although the following text sets forth a detailed description of numerous different embodiments it should be understood that the legal scope of the description is defined by the words of the claims set forth at the end of this patent. The detailed description is to be construed as exemplary only and does not describe every possible embodiment since describing every possible embodiment would be impractical if not impossible. Numerous alternative embodiments could be implemented using either current technology or technology developed after the filing date of this patent which would still fall within the scope of the claims.

It should also be understood that unless a term is expressly defined in this patent using the sentence As used herein the term is hereby defined to mean . . . or a similar sentence there is no intent to limit the meaning of that term either expressly or by implication beyond its plain or ordinary meaning and such term should not be interpreted to be limited in scope based on any statement made in any section of this patent other than the language of the claims . To the extent that any term recited in the claims at the end of this patent is referred to in this patent in a manner consistent with a single meaning that is done for sake of clarity only so as to not confuse the reader and it is not intended that such claim term by limited by implication or otherwise to that single meaning. Finally unless a claim element is defined by reciting the word means and a function without the recital of any structure it is not intended that the scope of any claim element be interpreted based on the application of 35 U.S.C. 112 sixth paragraph.

The steps of the claimed method and apparatus are operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with the methods or apparatus of the claims include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and the like.

The steps of the claimed method and apparatus may be described in the general context of computer executable instructions such as program modules being executed by a computer. Generally program modules include routines programs objects components data structures etc. that perform particular tasks or implement particular abstract data types. The methods and apparatus may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment program modules may be located in both local and remote computer storage media including memory storage devices.

With reference to an exemplary system for implementing the steps of the claimed method and apparatus includes a general purpose computing device in the form of a computer or PC . Components of computer may include but are not limited to a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnect PCI bus also known as Mezzanine bus.

Computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer and includes both volatile and nonvolatile media removable and non removable media. By way of example and not limitation computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can accessed by computer . Communication media typically embodies computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.

The system memory includes computer storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules that are immediately accessible to and or presently being operated on by processing unit . By way of example and not limitation illustrates operating system application programs other program modules and program data .

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example only illustrates a hard disk drive that reads from or writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from or writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from or writes to a removable nonvolatile optical disk such as a CD ROM or other optical media. Other removable non removable volatile nonvolatile computer storage media that can be used in the exemplary operating environment include but are not limited to magnetic tape cassettes flash memory cards digital versatile disks digital video tape solid state RAM solid state ROM and the like. The hard disk drive is typically connected to the system bus through a non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

The drives and their associated computer storage media discussed above and illustrated in provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing operating system application programs other program modules and program data . Note that these components can either be the same as or different from operating system application programs other program modules and program data . Operating system application programs other program modules and program data are given different numbers here to illustrate that at a minimum they are different copies. A user may enter commands and information into the computer through input devices such as a keyboard and pointing device commonly referred to as a mouse trackball or touch pad. Other input devices not shown may include a microphone joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . In addition to the monitor computers may also include other peripheral output devices which may be connected through an output peripheral interface or connected via the network interface .

The computer may operate in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer although only a memory storage device has been illustrated in . The logical connections depicted in include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or other appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. By way of example and not limitation illustrates remote application programs as residing on memory device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

Application failures may occur on a computing device and reported to a separate remote computing device . Generally application failures may describe any event occurring in one application that interrupts the predicted or normal functioning of other applications or hardware operating on either the computing device or the remote computing device . The application failure may relate to a wide variety of programs executing on the computers system memory and may include without limitation failures relating to the BIOS operating system application programs other program modules and the program data . Additionally the application failures may relate to problems associated with applications stored in non removable non volatile memory and may include without limitation the operating system application programs other program modules or the program data . Further the application failures may relate to applications stored on or executing from removable non volatile memory such as a floppy disk or an optical disk . The application failures may also relate to remote application programs executing on the remote computer . Additionally the application failures may relate to any hardware device interface network connection or internal connection associated with the computers .

Application failures may occur in a variety of situations. is a block diagram showing a wait chain relationship illustrating a condition in a computer known as a deadlock. Thread A owns the lock on Resource Thread A is also waiting for Resource . Thread A cannot access Resource because it is locked by Thread B . In turn Thread B is waiting for Resource . Resource is locked by Thread C . To this point this is not necessarily a problem. If Thread C releases Resource the dependencies will clear. However if Thread C is waiting for Resource a deadlock exists and the processing associated with each of the three threads will stop. This is one form of blockage caused by a wait chain among resources and threads.

A related situation a hang may occur when one thread for example Thread C may not be waiting for another resource but instead is slow or stopped. All the preceding elements threads and resources will be blocked until Thread C releases Resource . It is not just threads that can cause hangs but also resources. For example if Resource is a network connection it may itself be too slow or unable to make progress even though its owner Thread C is active and making progress. A third related wait chain relationship is an orphaned resource which occurs when one thread for example Thread C is simply non existent either because it terminated or was killed. A chain that contains an orphaned resource also represents a hang because all the threads that are waiting for the orphaned resource to be released are prevented indefinitely from making progress.

The resources of may be those traditionally thought of as resources such as memory locations print queues database queues etc. However with respect to their ability to play in wait chain relationships other functions may also fall into the category of resources that are atypical. For example a remote procedure call RPC may cause the caller thread to wait until the callee thread returns the result. In this case we say that the RPC is owned by the callee thread. When the result is slow in coming or lost the RPC may be part of a wait chain that represents a hang as discussed above. Other kinds of response oriented operations such as acquiring locks or semaphores waiting for network or I O connections or sending a message and waiting for the receiver to process it may also act in this manner.

As well the threads and resources involved in a wait chain are not necessarily restricted to one to one relationships. As shown in a thread can wait for multiple resources and a resource can be owned by multiple threads. In a graph of threads and resources shows Thread A owns Resource . Thread B is waiting for Resource and Resource . Resource is owned by Thread A . In the other branch of the graph Thread B is waiting for Resource and owns Resource . Resource is owned by Thread C . The deadlock cycle is completed by Thread C waiting for Resource . Even in the presence branches or multiple dependencies the techniques for identifying and verifying deadlocks hangs and orphans still apply.

The relationship data may be analyzed to determine if a potential wait chain relationship exists at block . For example a potential wait chain relationship may be a cycle that is a combination of threads waiting for resources and resources waiting for ownership such as a lock held by a thread that loops back on itself as shown in in other words a deadlock. Another wait chain relationship may be a hang as exemplified by a series of events that persist over a period of time with a common terminal point the source of the hang. A hang is different from a deadlock in that a deadlock having a cycle will never clear without outside intervention while a hang may be dependent on an extremely slow resource such as that available over a network. The hang may clear itself when the slow resource finally completes its task and returns data or sends an error. Another wait chain relationship that can cause processing failure is a non existent thread that is a thread in a wait chain relationship that has been killed or otherwise stopped. The surrounding threads and resources may be held by a thread that simply no longer exists. When no wait chain relationship is identified the no branch from block may be taken to block and the process started over in some cases after a delay period. If a wait chain relationship is found the yes branch from block may be taken to block . At block data corresponding to locked resources and threads may again be gathered. While a complete catalog similar to the first may be taken it may be more efficient to only catalog the elements identified as being of interest at block . At block the data may be analyzed to determine if the wait chain relationship identified at block still exists and the threads in the wait chain have not made progress. Depending on how the data was collected an exhaustive search may be required. If only data of interest is cataloged it may be quicker to compare only the elements of interest to the prior list. In one embodiment a way to verify that threads have not made progress is to check the context switch count that the operating system increments every time a thread runs on a processor. In an alternative embodiment the cataloging at block may be done in the reverse order of thread appearance at block starting from a thread that can be frozen prevented from making progress for the duration of the verification step. This can be done by requesting the OS to freeze the thread temporarily or by having the thread in question itself run the verification step since while it is running that step it cannot do anything else including releasing a locked resource. Matching in this fashion is especially valuable in verifying wait chains in situations where a context switch count may not be available. This method works because threads on a wait chain that is verified in the opposite order of the waiting relationship are guaranteed not to have made progress. Because each of the resources being analyzed may be more quickly identified as not having changed between measurements this method may allow easier verification of a deadlock or hang.

If the context switch count has changed and the change is not attributable to the analysis process then the Yes branch from block may be followed to block . When there are no changes in context switch count except those that can be accounted for then the No branch from block may be followed to block .

At block information about the threads involved in the wait chain relationship may be reported to a monitor or other process capable of breaking the deadlock. To break the deadlock a number of methods may be used including killing one of the threads. Another method may be to force an error from the resource that causes the thread to release the lock. By enabling an error to be returned other processes involved may be able to recover both more quickly and with fewer side effects than the more dramatic technique of simply killing the offending process.

Another application of this technique may be applied to preventing deadlocks from occurring in the first place. When a thread is about to place a lock the two step cataloging process may be initiated to see if the proposed new wait chain relationship will introduce a deadlock.

As mentioned above resource locks may be placed and released hundreds at a time and last as short as microseconds. By taking advantage of the persistent nature of a deadlock or hang the need to exhaustively catalog every resource lock particularly in real time may be eliminated. Thread dependencies and resource ownerships may be cataloged and analyzed on an intermittent basis over a relatively long periods of time for example a second or more. Deadlocks and hangs may be positively identified in the second measurement step simply by comparing the second measurement data to the approximation of resource ownerships and thread dependencies of earlier measurement. In contrast to a timeout scheme this method may allow the identification and resolution of the deadlock before a user is aware of any problem.

An application programming interface API which may facilitate debugging a failed process thread by identifying the most crucial point of a wait chain may improve the debugging process. For example an application such as Microsoft Word may become unresponsive while running on a Microsoft Windows type OS during the printing process and the user may forcibly terminate the application. The root cause of this particular failure may be that a Word process is waiting fora print spooler process that is executing a complicated or time consuming operation. Live debugging may obtain information about the spooler process but this method is not usually practical given the need for live support personnel. Also current Windows Error Reporting WER may only send post mortem process memory dump information regarding the failed application and not its dependencies. Debugging the root cause of the problem may be impossible using only the post mortem process memory dump information. The API however may determine all process thread dependencies on the client s machine at the time of the failure as a wait chain indicate this dependency to an automated diagnosis system or personnel and collect the process memory dumps of both Word the related print spooler processes and any or all processes included in or related to the wait chain.

The following methodology and API may present an approach to find the thread or process responsible for a hang a failure class when the application becomes unresponsive on a user s machine when the original failing thread is blocked on other threads processes through a variety of synchronization objects. Collection of other process dumps involved in the failure as well as quick identification and prioritization of classes of hangs such as those caused by deadlocks may also be possible through this approach.

As described above a major class of application failures are hangs. A large percentage of hangs are caused when the thread that became unresponsive and caused the hang was blocked waiting on other threads or processes. In this scenario blaming the original hung thread for the failure and using it for debugging analysis may be insufficient. Information may be collected on the specific threads and processes upon which the original failure thread is blocked so that root cause analysis may be performed on the thread or process at the end of the blocking chain.

When a hang occurs on a user s machine a unified API may be called that provides a wrapper around all the OS instrumentation and returns a wait chain consisting of a chain of threads and synchronization objects starting with the original hung thread. A synchronization object is an object whose handle may be specified to coordinate the execution of multiple threads. More than one process can have a handle to the same synchronization object making inter process synchronization possible.

Generally and with reference to a system hang may be reported by the illustrated API. Beginning at block the OS may detect a hang scenario as previously described. At block a user or system setting may determine whether or not the hang should be reported through WER to a centralized debugging service such as the Microsoft Watson remote application program a present user or any other destination. If at block the report is denied the API ends. If at block the report is acknowledged at block the API may open a memory module shared between Watson and the computer system . At block the API may create a set of process bucketing parameters which describe the system hang. The process bucketing parameters may include but are not limited to the name of the executable file or process that generated the hang the hung file or process version a timestamp of the hung file or process a hash of the hung thread s call stack and type code parameters indicating what kind of hang occurred i.e. in process mutex out of process mutex deadlock hung process has a modal dialog hang reporting was invoked via end task thread wait or process wait . Information about what kind of threads and resources were involved in the hang may be obtained through a call to the method described above in relation to . As previously described this method may provide a chain of nodes starting with the original hung thread and may indicate which threads processes and resources including synchronization objects are involved in the blocking call. At block the API may write the bucketing parameters to the shared memory opened at block . At block the API may then submit a report containing all bucketing parameters to the Watson remote application program or other destination.

Once a user or the Watson remote application program analyzes the bucketing parameter report submitted at block at block the user or application program may initiate a callback for additional information related to the hang. If at block no callbacks are initiated the API will end. Referring to callbacks from a user or the Watson remote application program may be described. At block a data collection callback may be sent to the API. At block the API may determine if the request of block is for a live kernel mini dump. A live kernel mini dump may consist of information related to the hung process and all the threads in that process and may include a header basic OS information information related to the processor control block PRCB an OS module list faulting thread or process stack and context data context data pages or any targeted or default kernel data. If the callback is for a live kernel minidump at block the API may add the live kernel minidump information to the error report. If the callback is not for a live kernel minidump then at block the API may determine if the callback is for a cross process dump. If at block the callback is not for a cross process dump then the API may collect the hung process dump at block . At block the API may add the dump to the error report to end the API. The hung process dump may include the bucketing parameters related to the hung process only. At block the callback may request a cross process dump and at block the user or the system may grant or deny access to information related to that data. Access to the cross process dump information may be set by the user or may be set by default. If at block access to the information is denied the API may at block proceed with protected process data collection. If at block access to the information is not denied the API may proceed with cross process data collection.

With reference to at block a per user throttle may be used to prevent users of the WER remote application program from repeatedly and unnecessarily collecting cross process dumps. If at block the callback does not meet this throttle then the API may not send a further report. If at block the throttle is met then at block the API may ask the user if it may collect further process information regarding the hang. If the user does not consent the API may not send a further report. If at block the user consents to further reporting and at block the user has access to the process then at block the API may collect all cross process dumps and any other files registered with the service. A cross process dump may include the bucketing parameters related to any process associated with the hung process or included in the original wait chain. The cross process dump may also include the application name and version of the process on which the hung application is blocked. This information may be sent in addition to the previously described bucketing parameters. This information may be particularly helpful when the hang is due to a process other than the one that originated the hang. At block the API may add the reports collected at block to the error report and send them to WER . At block the user may consent to further reporting but at block the user may not have access to the hung process and may initiate protected process data collection. If at block the hung process is not protected at block the API may launch elevated data collection and collect a cross process dump a live kernel minidump or any other protected information. Elevated data collection may allow the method to collect any information about the system. A process may be protected if it relates to any application or information which may be particularly sensitive to the system s stability and security or the user personally. Either the user by preference or the system by default may determine whether a process should be protected. At block the API may collect the cross process dump the live kernel minidump and any other registered files and at block may copy the dumps to the user s temporary folder. At block both dumps may be added to the error report and the API may end. If at block the process is protected at block the API may launch secure WER . Secure WER may be sensitive to the security and stability issues of protected processes and at block the API may collect the cross process dump and any other registered files and at block the API may encrypt the collected files before at block adding the files to the report.

Referring to at block the remote application program may also initiate a callback to recover and restart the hung process. An application may register for recovery and restart. To recover a hung application the API may inject a new Win32 thread into the hung process and initiate restart. For restart the API may retrieve a registered restart command line and at block may signal the OS to terminate the application. At block the API may wait a period of time for the OS to signal that the application has terminated. At block the API may then re launch the hung application and end.

Although the forgoing text sets forth a detailed description of numerous different embodiments it should be understood that the scope of the patent is defined by the words of the claims set forth at the end of this patent. The detailed description is to be construed as exemplary only and does not describe every possible embodiment because describing every possible embodiment would be impractical if not impossible. Numerous alternative embodiments could be implemented using either current technology or technology developed after the filing date of this patent which would still fall within the scope of the claims.

Thus many modifications and variations may be made in the techniques and structures described and illustrated herein without departing from the spirit and scope of the present claims. Accordingly it should be understood that the methods and apparatus described herein are illustrative only and are not limiting upon the scope of the claims.


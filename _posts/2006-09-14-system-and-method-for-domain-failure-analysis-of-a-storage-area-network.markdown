---

title: System and method for domain failure analysis of a storage area network
abstract: Systems, methods, apparatus and software can implement a SAN monitoring scheme for determining changes in SAN topology, such as device failure and state changes. These changes are recorded in a SAN topology data structure. Information in the SAN topology data structure is used, for example, to identify a suspect path or set of paths, and to make decisions about communications pathways used by a multipath device driver.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07930583&OS=07930583&RS=07930583
owner: Symantec Operating Corporation
number: 07930583
owner_city: Mountain View
owner_country: US
publication_date: 20060914
---
The present invention relates to storage devices in distributed computer systems and more particularly to performing failure analysis in storage area networks.

Distributed computing systems are an increasingly important part of research governmental and enterprise computing systems. Among the advantages of such computing systems are their ability to handle a variety of different computing scenarios including large computational problems high volume data processing situations and high availability situations. Such distributed computing systems typically utilize one or more storage devices in support of the computing systems operations. These storage devices can be quite numerous and or heterogeneous. In an effort to aggregate such storage devices and to make such storage devices more manageable and flexible storage virtualization techniques are often used. Storage virtualization techniques establish relationships between physical storage devices e.g. disk drives tape drives optical drives etc. and virtual or logical storage devices such as volumes virtual disks and virtual logical units sometimes referred to as virtual LUNs . In so doing virtualization techniques provide system wide features e.g. naming sizing and management better suited to the entire computing system than those features dictated by the physical characteristics of storage devices. Additionally virtualization techniques enable and or enhance certain computing system operations such as clustering and data backup and restore.

Other elements of computing system include storage area network SAN and storage devices such as tape library typically including one or more tape drives a group of disk drives i.e. just a bunch of disks or JBOD and intelligent storage array . As shown in both hosts and are coupled to SAN . SAN is conventionally a high speed network that allows the establishment of direct connections between storage devices and and hosts and . SAN can also include one or more SAN specific devices such as SAN switches SAN routers SAN hubs or some type of storage appliance. Thus SAN is shared between the hosts and allows for the sharing of storage devices between the hosts to provide greater availability and reliability of storage. Although hosts and are shown connected to storage devices and through SAN this need not be the case. Shared resources can be directly connected to some or all of the hosts in the computing system and computing system need not include a SAN. Alternatively hosts and can be connected to multiple SANs.

The DMP functionality enables greater reliability and performance by using path failover and load balancing. In general the multipathing policy used by DMP drivers and depends on the characteristics of the disk array in use. Active active disk arrays A A arrays permit several paths to be used concurrently for I O operations. Such arrays enable DMP to provide greater I O throughput by balancing the I O load uniformly across the multiple paths to the disk devices. In the event of a loss of one connection to an array the DMP driver automatically routes I O operations over the other available connections to the array. Active passive arrays in so called auto trespass mode A P arrays allow I O operations on a primary active path while a secondary passive path is used if the primary path fails. Failover occurs when I O is received or sent on the secondary path. Active passive arrays in explicit failover mode A PF arrays typically require a special command to be issued to the array for failover to occur. Active passive arrays with LUN group failover A PG arrays treat a group of LUNs that are connected through a controller as a single failover entity. Failover occurs at the controller level and not at the LUN level as would typically be the case for an A P array in auto trespass mode . The primary and secondary controller are each connected to a separate group of LUNs. If a single LUN in the primary controller s LUN group fails all LUNs in that group fail over to the secondary controller s passive LUN group.

When DMP functionality is extended to support SAN attached disks and storage arrays certain deficiencies can arise. The proliferation of storage arrays has placed higher demand on array supportability of DMP. Maturity of multipathing support in operating systems and third party driver software has increased the need for and complexity of DMP coexistence with these products. Moreover use of DMP in a SAN environment significantly changes the complexity of path management. The number of devices that can be connected to a host generally increases by one or two orders of magnitude. Similarly the number of paths to a particular device is often greater than two the number in basic DMP implementations. Both of these factors have contributed to a significantly longer recovery time when some error condition occurs.

With the larger number of path segments and devices in a given path between an application executing on a host computer system and target storage the overall chance of failure somewhere in the path increases. Because DMP functionality is typically one of the lowest elements in the software stack i.e. closest to the hardware its responsiveness is important to maintaining system wide high availability characteristics. Accordingly improved systems methods software and devices are needed to improve the error detection recovery and monitoring functions of DMP functionality.

It has been discovered that systems methods apparatus and software can implement a SAN monitoring scheme for determining changes in SAN topology such as device failure and state changes. These changes are recorded in a SAN topology data structure. Information in the SAN topology data structure is used for example to identify a suspect path or set of paths and to make decisions about communications pathways used by a multipath device driver.

Accordingly one aspect of the present invention provides a method. A message indicating occurrence of an event related to a change in topology of a storage area network SAN is received at a host bus adapter HBA . Information from the received message is obtained from the host bus adapter. The information describes a change in a SAN device. A SAN topology data structure is updated according to the information from the received message. Information in the SAN topology data structure is used to identify a suspect path for use by a multipath device driver.

In another aspect of the present invention a system includes a multipath driver and an event monitor in communication with the multipath driver. The multipath driver is configured to direct input output I O operations along at least one of a plurality of communication pathways to at least one storage device in a storage area network SAN . The event monitor is configured to obtain from a host bus adapter coupled to the SAN information about a change in topology of the SAN. The event monitor is also configured to update a SAN topology data structure according to the information about the change in topology of the SAN. At least one of the multipath driver and the event monitor is further configured to use information in the SAN topology data structure to identify a suspect path for use by the multipath device driver.

In another aspect of the present invention a computer readable medium comprising program includes instructions executable on a processor. The computer readable medium is one or more of an electronic storage medium a magnetic storage medium an optical storage medium or a communications medium conveying signals encoding the instructions. The program instructions are operable to implement each of obtain from a host bus adapter information about a change in topology of a storage area network SAN update a SAN topology data structure according to the information about the change in topology of the SAN and use information in the SAN topology data structure to identify a suspect path for use by a multipath device driver.

Yet another aspect of the present invention provides an apparatus including a means for receiving at a host bus adapter HBA a message indicating occurrence of an event related to a change in topology of a storage area network SAN a means for obtaining from the host bus adapter information from the received message wherein the information describes a change in a SAN device a means for updating a SAN topology data structure according to the information from the received message and a means for using information in the SAN topology data structure to identify a suspect path for use by a multipath device driver.

The foregoing is a summary and thus contains by necessity simplifications generalizations and omissions of detail consequently those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting. As will also be apparent to one of skill in the art the operations disclosed herein may be implemented in a number of ways and such changes and modifications may be made without departing from this invention and its broader aspects. Other aspects inventive features and advantages of the present invention as defined solely by the claims will become apparent in the non limiting detailed description set forth below.

The following sets forth a detailed description of at least the best contemplated mode for carrying out the one or more devices and or processes described herein. The description is intended to be illustrative and should not be taken to be limiting.

Device discovery layer stores information about various storage devices in database . Moreover since hardware and or software characteristics of storage devices can differ significantly e.g. because of differences among device manufacturers the procedures used to discover device attributes can differ among devices. Consequently device discovery layer can use a set of device support libraries that provide device information specific to the storage devices of particular vendors. In one implementation each device support library in the set of device support libraries is a storage device specific dynamically loadable library. Thus support for a particular type of storage device can be added or removed dynamically from without stopping volume manager or rebooting host system . Moreover if a disk array contains active volumes the disk volumes can remain available during and after the addition or removal of the support.

In order to perform the device discovery function device discovery layer uses code to scan an operating system device tree in a platform specific manner. In one embodiment device discovery layer discovers all storage devices available to host computer system . In yet another embodiment partial discovery of available storage devices is achieved using constraint based discovery. For example a set of predefined storage device attributes can be used to constrain the discovery process to a subset of the storage devices identifies by the host operating system.

Device discovery layer gathers attributes of the storage devices connected to a host and configures DMP driver for a particular storage device so that volume manager can access and use the device. Configuration of DMP driver enables multipathing features as described above within corresponding storage devices.

In one embodiment device discovery layer uses SCSI commands to discover attributes of different disk arrays. Use of these commands can be hard coded into device discovery layer or it can be dictated in whole or in part by information from one or more of the device support libraries . The device support libraries typically include one or more functions procedures and or object oriented methods for use in determining storage device attributes. Examples of the types of storage device attributes discovered by device discovery layer include vendor identification product identification enclosure serial number LUN serial number disk array type e.g. A A A P A PG A PF and LUN ownership. Still other storage device attributes will be well known to those having ordinary skill in the art. In still another example dynamic attributes e.g. storage device attributes that can change between two invocations of a device discovery operation are supported. In such a situation a device support library may declare to the device discovery layer that one or more attributes are dynamic. When one of these dynamic attribute is retrieved a function within the support library can be invoked to get the latest value of the attribute.

Some or all of the storage device attributes discovered by device discovery layer are typically stored in database . In one embodiment database is maintained as a database of name value pairs called a property list. The name is the property name and the value is one of the values of the associated property. This scheme allows a new property to be created with an associated value and further allows expanding or shrinking the set of values of a particular property. Another component of database can be an association list. This list maintains the association between a tuple with another tuple. The association list is typically used to maintain the property values associated with various device discovery layer objects. Access to and manipulation of database is typically handled through an application program interface API not shown that includes a number of functions procedures and or object oriented methods designed for accessing modifying and manipulating data stored in database such as the aforementioned property and association lists.

Device discovery module is responsible for passing storage device information to DMP driver . For example device discovery layer can transmit a stream of opcodes and operands to DMP driver . DMP driver and in particular core functions interprets these instructions and performs a variety of operations based on the instructions such as configuring or reconfiguring its device database . In addition to core functionality and device database DMP Driver can include one or more device policy modules DPMs managed by module management . Device discovery layer provides I O policy configuration information to DMP driver only for those storage device types or models which are applicable to the devices discovered by device discovery layer . For example a support library for a particular storage device may specify that the corresponding storage device can or should only use a particular array model e.g. A A A P A PG A PF . In response core functions will ensure that the proper I O policy is supported by DMP driver . This operation might involve loading certain default I O policy modules or routines enabling certain default I O policy modules or routines and or de selecting certain incompatible default I O policy modules or routines. As shown DMP core functions typically include software for path restoration e.g. failover error condition detection and general path probing and analysis.

DMP driver uses the storage device attributes received from device discovery layer along default I O policies to perform I O operation scheduling path failover and other I O operations e.g. SCSI reservation in the multipath environment of computing system . However because it is not practical for the core functionality of DMP driver to provide an unlimited number of different I O policies for every possible storage device for use in computing system DMP supports the use of dynamically loadable DPMs to modify augment or replace the fixed set of procedures in storage device specific manner. Thus DPMs such as DPM typically include one or more functions procedures or object oriented methods for performing various I O operations. This functionality is typically designed to be device specific i.e. the some or all of the functionality of the DPM takes advantage of specific features or attributes of a particular manufacturer s or provider s storage device. Examples of the I O policy functionality present in DPM include path select procedures failover procedures load balance procedures error detection procedures and path probing and analysis procedures.

Module management shown as a separate software entity but implementable as part of core functions provides an interface to one or more DPMs loaded in DMP Driver . Just as the case with storage device attributes information about various DPMs can also be stored in database . DPMs can come from respective device support libraries or loaded from some other repository. DPMs can be loaded into DMP driver as part of the normal configuration process e.g. a device is discovered its attributes determined and information is provided to the DMP driver or DPMs can be loaded upon specific request by an application such as application .

In order to monitor the state of SAN components and to use this information to make proactive decisions about path selection device discovery layer includes event monitor . Although the functionality of event monitor will be discussed in the context of device discovery layer it should be noted that one or more aspects of the event monitor can be implemented as programs or threads of programs that are separate from the various modules illustrated part of other modules illustrated e.g. DMP driver or some combination of the two. Event monitor is shown with three components HBA interface DMP logging and operating system OS event handler .

DMP logging is used to receive event information from DMP driver . This information does not typically include specific information about SAN topology but rather information and statistics about basic DMP operations e.g. path failover occurrence time for failover SCSI Inquiry times etc. This log of DMP events is particularly useful for system debugging. This information can be stored in any of a variety of data structures e.g. databases log files and the like. Such data structures can be part of database or some other data storage. Similarly OS event handler gathers information about various OS events that are related to device discovery and DMP functionality e.g. OS device reconfiguration events. As with DMP logging this information can be stored in a variety of different data structures and retained in various portions of system .

HBA interface is an event monitoring component designed to interface with computer system s HBAs and to monitor SAN events e.g. fiber channel events related to the SAN topology and by extension the paths used to implement DMP. HBA interface also maintains a data structure describing the SAN topology. That data structure is used by event monitor device discovery layer and or DMP driver to perform proactive functions in support of DMP. Although the examples of the present invention will emphasize SANs utilizing fibre channel protocols those having ordinary skill in the art will recognize that the systems software devices and techniques described in the present application can be extended to and implemented using other protocols such as those for iSCSI Infiniband and the like.

In one embodiment HBA interface gathers information about fibre channel registered state change notification RSCN events. RSCN events generally occur in the SAN fabric whenever a change in the fabric occurs. This could mean a hard drive is added removed bypassed or re inserted into a system a host rebooted or any other change in the topology. RSCN is intended to provide a timely indication of changes in nodes to avoid the considerable traffic that polling may generate. RSCN may be used to indicate a failed node allowing the release of resources tied up by the failed node. It may also be used to notify interested nodes of new devices coming online and of changes within an online node that affect the operation of the system e.g. more storage has become available . Moreover a sender of an RSCN request i.e. the event notification may coalesce several events into a single report. Thus when a suitable event occurs an RSCN message is sent to all the node devices in the same fabric zone or at least those node devices in the same zone that have registered to receive such messages . RSCN messages can be sent by a fabric controller e.g. a SAN switch or by the affected device port. The payload of an RSCN message includes a list containing the addresses of the affected devices or device ports. The RSCN message also includes a summary indication of the type of state change being reported to assist in analyzing the change. RSCN message contents and formats may change depending on the source and can be expanded beyond current fibre channel specifications.

When properly registered HBAs and will receive RSCN messages describing changes in the SAN. HBA interface gathers this information from the HBAs using an application programming interface API specific to the HBA. In order to operate upper level software applications require information that is not available from HBAs in a consistent manner across operating systems vendors and platforms and in some cases not at all. In some cases HBA vendors provide a proprietary API for interfacing with the HBA. In other cases vendors have developed HBA APIs that conform to a common HBA API specified by the Storage Networking Industry Association SNIA . This API provides a consistent low level HBA standard interface for accessing information in a fibre channel SAN that would be implemented across vendors as a standard C language API supported by vendor specific library instances.

More specifically at the upper level a common HBA library provides the ability to handle multiple vendor implementations of the HBA API through dynamic loading of libraries. At the intermediate level the functions of the common API invoke their respective functions in vendor specific libraries provided by each HBA vendor. For the most part there is a one to one correspondence between the functions of the common API library and the functions of the vendor specific libraries. Certain references will be made below to SNIA HBA API functions. However these are merely examples. Various other APIs can be implemented and utilized depending on for example the HBA hardware being used the underlying SAN protocols etc.

As will be discussed in greater detail below in the context of software such as HBA interface integrates the HBA API so that it can gather RSCN information from the HBA as well as probe the SAN and issue commands to various SAN devices.

In a typical implementation some or all of the components of DMP driver operate in a kernel portion of the host computer system s memory while some or all of the components of device discovery layer operate in a user portion of system memory. In general the software components shown in are divided into those components operating at the kernel level and those operating at the user level as is well known in the art. Kernel memory space is generally reserved for the computer operating system kernel and associated programs. Programs residing in kernel memory space typically have unrestricted privileges including the ability to write and overwrite in user memory space. By contrast programs residing in user space typically have limited privileges. Thus depending on the implementation of DMP driver DPM can be a kernel module or a user space module. However because of the nature of driver software e.g. the need to interface with low level portions of the operating system the need to protect the driver from other programs the handling of I O operations etc. DPMs are typically implemented as kernel modules. Moreover various aspects of the functionality can operate as single threaded programs or multithreaded programs either of which can be implemented as daemons. For example in one embodiment event monitor is implemented as a multi threaded daemon having a thread each for each of HBA interface DMP logging and operating system OS event handler .

Other system components illustrated in function in a manner similar to corresponding components shown in . For example host bus adapters and provide a hardware interface between the host bus of host computer system and SAN . Various other drivers e.g. storage device specific drivers OS drivers etc. are shown at and . Although DMP driver device discovery layer and various related features have been described in the context of a standard host computer system it should be noted that these features and functionality can be implemented in a variety of other architectures such as clustered computing systems and specialized storage devices e.g. SAN switches SAN routers SAN hubs or some type of storage appliance . Moreover the present systems methods devices and software can be implemented in conjunction with a variety of different virtualization schemes e.g. host based appliance based storage based in band out of band etc. and indeed with no virtualization scheme at all. Similarly a variety of different storage devices and indeed addressable storage objects generally can be used in conjunction with the methods devices and software disclosed.

In table illustrates one way in which various SAN devices corresponding ports and their corresponding port types can be organized and tracked. Here the SAN topology is represented using a two dimensional array. One dimension of the array represents inter connected elements e.g. switches bridges . Another dimension of the array represents the list of ports that each switch bridge has. Each element in the two dimensional array has a world wide name WWN which is a 64 bit address used in fibre channel networks to uniquely identify each element in the network. Here switch port is indicated as having a pWWN as its WWN. The record can also include relevant port information such as the type of the port to which this port is connected and a list of the one or more port WWNs to which this port has been connected. The type of port in this data structure will typically have values such as switch bridge port end point e.g. HBA or LUN etc. More specifically port type information can include explicit fibre channel port type classifications. These port type designations include 1 N port a node port or a port on a disk or computer these communicate only with other N ports or to a switch 2 F port a fabric port found on switches 3 L port the L in a port name implies that it is a port that can participate in an arbitrated loop e.g. NL port FL port etc. 4 E port an expansion port on a switch that is used to connect to other switches via their E ports to form a fabric and 5 G port a generic port on a switch that can act as an E port an FL port or an F port depending on what connects to it.

In either example SAN topology is initially ascertained stored and updated as changes occur in the SAN. This information can then be used to make more intelligent decisions related to failure analysis e.g. path failover marking suspect paths and the like.

Operation of systems and methods for gathering and using SAN topology information are illustrated in . In particular a process of building and using a SAN topology data structure is shown at .

In step the SAN topology data structure is initialized. This can include a number of sub operations including for example functionality verification data structure initialization and gathering initial data about the SAN. The initialization process can begin with verification that the HBA API library in use is available in the host computer system. Since this directly impacts both initial gathering of topology information and subsequent monitoring of SAN events e.g. via RSCN messages unavailability of certain HBA API function may require the entire operation to be aborted. Moreover if HBAs from two different vendors are present in the host system then separate APIs for both may be needed. Once this is confirmed the appropriate processes threads can be started.

Another component of operation is the initial population of the SAN topology data structure s . Data about the various SAN devices e.g. WWNs connectivity information status information etc. is gathered via calls to the HBA API. In the case of the aforementioned SNIA HBA API a call can be made to the HBA SendCTPassThru interface. A call to this function send a common transport CT pass through frame. In this case CT refers to an instance of the common transport protocol through which device services are accessed. An HBA should decode this request routing the CT frame in a fabric according to information within the CT frame. For example using HBA SendCTPassThru the software e.g. event monitor can send command packets to the HBA to access fibre channel generic services FC GS . Using FC GS the software can get a list of inter connected elements e.g. switches bridges hubs in the SAN. With that information each returned device can be queried to provide a list of its ports. Each port can be queried to get the attached port names list port type information and the like. Using this information the data structure describing the SAN topology can be populated. After collecting initial the topology information the software can use another function to collect mapping information between device names and WWNs. For example the HBA GetFcpTargetMapping interface can be used to collect the mapping between OS identification of SCSI logical units and fibre channel identification of logical units. Still other API functions can be used to gather device information. For example the functions HBA GetAdapterAttributes HBA GetAdapterPortAttributes and HBA GetDiscoveredPortAttributes can be used to determine the exact source and destination WWN of each path. With this information end devices within the topology are identified and these devices can be mapped to port WWN.

If the HBA is not already registered to receive certain messages and events the software causes the HBA to become registered within the SAN. Although RSCN messages events are emphasized in the present description registration can occur for various other fibre channel events or other protocol events where different protocols are used in the SAN . Additionally the software can register with the HBA so as to receive message event information. For example the functions HBA RegisterForAdapterEvents HBA RegisterForLinkEvents HBA RegisterForAdapterAddEvents HBA RegisterForAdapterPortEvents and HBA RegisterForTargetEvents are used to register callbacks for events. In the case of callbacks the HBA will forward event information directly to the software so registered. HBA RegisterForAdapterEvents causes registration for asynchronous adapter level events. When an adapter level event occurs the callback function is called with a suitable event type and event information. Event delivery can be terminated by a call to HBA RemoveCallback. Similarly HBA RegisterForLinkEvents registers a specified adapter for asynchronous fabric link level events. When a fabric link level event is detected by the adapter the callback function is called and appropriate event information is passed back. Various registrations can occur for callbacks associated with specific events or event types.

A callback interface is useful in many cases because the monitoring software does not have to poll the HBA at periodic or random intervals i.e. the information is pushed to the software once it is received by the HBA. If callback is not supported a polling interface can be used. For example HBA GetEventBuffer can be used to collect the events. If this is the case a suitable timer can be implemented to poll for events at specified intervals e.g. 15 seconds. The interval can be selected to be less than other timeout intervals e.g. SCSI timeouts that are 30 or 60 seconds. Moreover quick identification of SAN topology changes is generally desirable so relatively short intervals may be used.

Thus once initialization is complete operation transitions to where the SAN is monitored. If an HBA supports callback as determined in operation transitions to . Here if the HBA has received an event operation transitions to where event information is extracted. If the HBA does not support event callback or if polling was specifically selected as the information gathering mechanism operation transitions to . Here a determination is made whether the polling interval has been reached. This interval is generally selected to be short enough to catch changes in a timely fashion but long enough so that the software and HBA are not burdened with unnecessary polling requests. If the interval has not been reached operation returns to . If it has been reached operation proceeds to . Here the HBA is queried to determine if it is storing event information. If so that information is extracted and if not the process returns to .

Once the monitoring software receives extracts event related information that information is used to updated the SAN topology data structure . For example if the monitoring software receives and RSCN event from a specific port in the fabric then by looking at the topology data structure the software will determine which paths that are going to be affected. Next some response is taken in light of the topology change . That response can take various forms. In some instances the topology change is not significant enough for proactive steps related to DMP or other software functionality. That is no response will be taken. In other instances the monitoring software will inform DMP functionality so that it can probe the affected paths to update their status. This subsequent probing or the SAN topology information itself can trigger corrective action such as path failover marking certain paths as suspect warning system operators and the like. If the DMP functionality determines that a particular path has failed or should not be used it can in turn extract alternate path information from the SAN topology data structure or request that the monitoring software provide it with alternate path information. For example SAN topology information can be used to determine non overlapping paths i.e. paths that do not include the point of change failure . These paths can be analyzed or used first for possible failover. In general DMP functionality wherever it is implemented can use the acquired SAN topology information to take some corrective action. Once the SAN topology change is responded to operation returns to .

The flow chart of illustrates some of the many operational examples of the SAN topology monitoring techniques disclosed in the present application. Those having ordinary skill in the art will readily recognize that certain steps or operations illustrated in can be eliminated or taken in an alternate order. Moreover the methods described in and many of the modules illustrated in are typically implemented as one or more software programs for a computer system and are encoded in a computer readable medium as instructions executable on one or more processors. The computer readable medium can be one or more of an electronic storage medium a magnetic storage medium an optical storage medium or a communications medium conveying signals encoding the instructions. Separate instances of these programs can be executed on separate computer systems in keeping with the multi process methods described above. Thus although certain steps have been described as being performed by certain devices software programs processes or entities this need not be the case and a variety of alternative implementations will be understood by those having ordinary skill in the art.

Additionally those having ordinary skill in the art will readily recognize that the techniques described above can be utilized in a variety of different storage devices and computing systems with variations in for example the number of nodes the type of operation of the computing system e.g. cluster operation failover parallel etc. the number and type of shared data resources and the number of paths between nodes and shared data resources.

Those having ordinary skill in the art will readily recognize that the techniques and methods discussed below can be implemented in software using a variety of computer languages including for example traditional computer languages such as assembly language Pascal and C object oriented languages such as C C and Java and scripting languages such as Perl and Tcl Tk. Additionally software and can be provided to the computer system via a variety of computer readable media including electronic media e.g. flash memory magnetic storage media e.g. hard disk a floppy disk etc. optical storage media e.g. CD ROM and communications media conveying signals encoding the instructions e.g. via a network coupled to network interface .

Computer system also includes devices such as keyboard mouse SCSI interface network interface graphics display hard disk and CD ROM all of which are coupled to processor by communications bus . It will be apparent to those having ordinary skill in the art that computer system can also include numerous elements not shown in the figure such as additional storage devices communications devices input devices and output devices as illustrated by the ellipsis shown. An example of such an additional computer system device is a fibre channel interface.

Although the present invention has been in some cases described is in terms of providing support for multipath disk arrays the present invention can also be used to support disk arrays having only a single path. Multipath disk arrays are used to illustrate the usefulness of the invention although one of skill in the art will recognize that the invention is not limited to support for multipath disk arrays. In contrast the present invention can be used in conjunction with a variety of different types of storage devices including discrete disks solid state storage devices including flash memory storage appliances and other storage devices.

Loadable module techniques as described herein can also be applied to other remote device access technologies. For example standard protocols e.g. the SCSI protocol can be extended to provide some common functionality in different ways or manufacturers may disagree on some interpretation aspects of a standard and provide different behaviors in their devices. Consequently the loadable module techniques provide extensible polymorphic uniform mechanisms for accessing this non standard but common functionality or for providing specific handling for different behaviors that cannot be efficiently handled by common code. Using simple type specific loadable modules with a set of attributes and functionality handlers allows accommodation of unusual new devices without having to change established products. Moreover one can rely on either a storage device vendors of such a new device or on storage management software vendors to provide the module to handle this new device. This technique generally has broad application and can also be used for example to access extended protocol mechanisms using protocols other than the block access protocols like SCSI.

Although the present invention has been described with respect to a specific preferred embodiment thereof various changes and modifications may be suggested to one skilled in the art and it is intended that the present invention encompass such changes and modifications fall within the scope of the appended claims.


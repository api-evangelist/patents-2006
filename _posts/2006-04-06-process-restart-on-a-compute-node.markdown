---

title: Process restart on a compute node
abstract: Embodiments of the present invention provide a method for reducing load time of a program in a highly-parallelized or distributed computer. In one embodiment, this is accomplished by selectively reusing entries in a page table generated during a previous invocation of the program at a particular compute node of the highly-parallelized or distributed computer system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08429218&OS=08429218&RS=08429218
owner: International Business Machines Corporation
number: 08429218
owner_city: Armonk
owner_country: US
publication_date: 20060406
---
The present invention generally relates to computer systems and development. More specifically the present invention relates to a process for reducing the time required to load a program for execution in a distributed or highly parallelized computer system.

Supercomputer systems continue to be developed to tackle increasingly complex computing problems. These systems have proved to be particularly useful for a broad variety of applications including life sciences financial modeling hydrodynamics quantum chemistry molecular dynamics astronomy weather modeling and prediction and geological modeling. Super computer developers have focused on massively parallel computer architectures to provide ever increasing amounts of computational power to apply to these and other applications.

One family of massively parallel systems has been and continues to be developed by International Business Machines IBM under the name blue gene. The blue gene L system is a scalable that may be configured with a maximum of 65 536 2 compute nodes. Each blue gene L node includes a single application specific integrated circuit ASIC with 2 CPU s and memory. The blue gene architecture has been extremely successful and on Oct. 27 2005 IBM announced that a blue gene L system had reached an operational speed of 280.6 teraflops 280.6 trillion floating point operations per second making it the fastest computer in the world at the time. Further as of June 2005 blue gene L installations at various sites world wide were among 5 out of the 10 top most powerful computers in the world.

IBM is currently developing a successor to the blue gene L system named blue gene P. Blue gene P is expected to be the first computer system to operate at a sustained 1 petaflops 1 quadrillion floating point operations per second . Like the blue gene L system the blue gene P system is a scalable system with a projected maximum of 73 728 compute nodes. Each blue gene P node includes a single application specific integrated circuit ASIC with 4 CPU s and memory. A complete blue gene P system would be housed in 72 racks or cabinets each with 32 node boards with 32 nodes per board .

In addition to the blue gene architecture developed by IBM other distributed computer systems may have a similar overall architecture as a massively parallel computer system. Examples of other distributed systems include clustered systems and grid based systems. For example a Beowulf cluster is a group of computer systems each running a Unix like operating system such as the Linux or BSD operating systems. The computer systems of the collection are connected over high speed networks into a small TCP IP LAN and have libraries and programs installed which allow processing to be shared among the nodes.

In performing many of the applications described above super computer systems are used to solve a variety of problems that often involve performing essentially the same calculations for different data sets. Examples of this type of application include modeling of molecular interactions such as simulating the folding of an individual protein. For these types of applications a relatively small amount of data is used by a program executing on any given node. The program will then make many calculations involving this data. When finished the results of the calculations are returned. Because thousands of nodes are performing the same calculations on different data sets extremely large datasets may be processed in a relatively short period of time.

Given the number of nodes in either a highly parallelized super computer such as a blue gene system or in other distributed systems operations that require even small amounts of overhead for any individual node often translate into large amounts of time for the system as a whole. For example the collective time required to load a program by individual compute nodes can be significant. Thus collectively a substantial amount of time may be expended simply transmitting a program to a compute node. The same phenomenon occurs in distributed systems where datasets programs and the like must be transmitted to processing nodes that are part of the distributed system. Accordingly there is a need in the art for techniques that will reduce the load time of a program in highly parallelized or distributed computer systems.

The present invention generally provides a method for reducing the time required to load a program in a highly parallelized or distributed computer system. One embodiment of the invention provides a method for loading a program on a computing system that includes multiple processing nodes. The method generally includes identifying if available a plurality of processing nodes that previously executed the program wherein the program is associated with a computing job submitted for execution by the computing system. The method generally further includes messaging each identified processing node to retain a page table associated with the previous execution of the program and dispatching the computing job to the plurality of processing nodes.

Another embodiment of the invention includes a computer readable medium containing a program which when executed performs an operation for loading a program on a computing system that includes multiple processing nodes. The operations generally include identifying if available a plurality of processing nodes that previously executed the program wherein the program is associated with a computing job submitted for execution by the computing system. The operations generally further include messaging each identified processing node to retain a page table associated with the previous execution of the program and dispatching the computing job to the plurality of processing nodes.

Still another embodiment of the invention includes a parallel computing system. The system generally includes a plurality of compute nodes and a control system configured to execute a computing job on at least some of the plurality of compute nodes by performing a set of operations. The operations may generally include identifying if available a plurality of processing nodes that previously executed the program wherein the computing job identifies a program to be executed on the parallel computing system. The operations may generally further include messaging at least some of the set of compute nodes to retain a page table associated with the previous execution of the program and dispatching the computing job to at least some of the compute nodes included in the set of compute nodes.

Embodiments of the present invention provide a method for reducing load time of a program in a highly parallelized or distributed computer. This is accomplished by reusing where possible information loaded by a previous invocation of the program at a particular compute node of the highly parallelized or distributed computer system. Since some segments of data transmitted to compute node s when a job is dispatched for execution are the binary instructions of the program which are immutable in memory pages of memory can be saved from previous invocations of the same program. Thus embodiments of the invention avoid page faults that would otherwise occur each time a control system dispatches a job to a particular compute node. At the same time a given compute node need not wait for binary instructions to be paged into memory when instructed to begin executing a particular job.

Embodiments of the invention are described herein with respect to the blue gene computer architecture developed by IBM. Embodiments described herein are particularly advantageous for massively parallel computer systems that include thousands of processing nodes such as a blue gene system. However embodiments of the invention may be adapted for use in a variety of parallel computer systems that employ multiple CPUs arranged to communicate over a network. For example embodiments are may be readily adapted for use in distributed architectures such as clusters or grids in such architectures each processing node may be a computer system communication with others over local regional or global networks.

In the following reference is made to embodiments of the invention. However it should be understood that the invention is not limited to specific described embodiments. Instead any combination of the following features and elements whether related to different embodiments or not is contemplated to implement and practice the invention. Furthermore in various embodiments the invention provides numerous advantages over the prior art. However although embodiments of the invention may achieve advantages over other possible solutions and or over the prior art whether or not a particular advantage is achieved by a given embodiment is not limiting of the invention. Thus the following aspects features embodiments and advantages are merely illustrative and are not considered elements or limitations of the appended claims except where explicitly recited in a claim s . Likewise reference to the invention shall not be construed as a generalization of any inventive subject matter disclosed herein and shall not be considered to be an element or limitation of the appended claims except where explicitly recited in a claim s .

One embodiment of the invention is implemented as a program product for use with a computer system such as for example the computing environment and described below. The program s of the program product defines functions of the embodiments including the methods described herein and can be contained on a variety of signal bearing media. Illustrative signal bearing media include but are not limited to i information permanently stored on non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive ii alterable information stored on writable storage media e.g. floppy disks within a diskette drive or hard disk drive and iii information conveyed to a computer by a communications medium such as through a computer or telephone network including wireless communications. The latter embodiment specifically includes information downloaded from the Internet and other networks. Such signal bearing media when carrying computer readable instructions that direct the functions of the present invention represent embodiments of the present invention.

In general the routines executed to implement the embodiments of the invention may be part of an operating system or a specific application component program module object or sequence of instructions. The computer program of the present invention typically is comprised of a multitude of instructions that will be translated by the native computer into a machine readable format and hence executable instructions. Also programs are comprised of variables and data structures that either reside locally to the program or are found in memory or on storage devices. In addition various programs described hereinafter may be identified based upon the application for which they are implemented in a specific embodiment of the invention. However it should be appreciated that any particular program nomenclature that follows is used merely for convenience and thus the invention should not be limited to use solely in any specific application identified and or implied by such nomenclature.

In one embodiment a compute node may be configured to reuse a page table associated with a program executed on the compute node for multiple invocations of the same program. As is known a page table provides an index of pages and their physical and logical addresses. The reuse of program page tables is described in greater detail below.

The I O nodes may provide a physical interface between the compute nodes and file servers over functional network . In one embodiment the compute nodes and I O nodes communicate with file servers front end nodes and service node over both a control network and or a functional network . In a blue gene system the I O nodes and compute nodes may differ from one another only by which network interfaces are enabled and how the node is used by the system . The I O nodes may also be configured to execute processes that facilitate the control booting job launch and debug of the computing system . This helps simplify an operating system kernel running on each compute node as each compute node need only communicate with a few I O nodes . The front end nodes store compilers and other applications used by users interacting with the system . Typically users access front end nodes submit programs for compiling and submit jobs to the service node .

The service node may include a system database and all the administrative tools provided by the system . Typically the service node includes a computing system configured to handle scheduling and loading of software programs and data on the compute nodes . In one embodiment the service node may be configured to assemble a group of compute nodes referred to as a block and dispatch a job to a block for execution. The service node is typically a computer system that includes an operating system memory storage and control console not shown . For example the blue gene P system uses a computer system running the Linux operating system. The service node communicates with the compute nodes over control network . The control network provides a communication channel for the service node to control aspects of the operation of system .

The blue gene P computer system includes multiple data communication networks. An I O processor located on some node boards process data communication between service node and a group of compute nodes . In one embodiment each I O node manages data communications over functional network for as many as 1024 compute nodes . In a blue gene P system the 73 728 compute nodes and 1024 I O nodes are configured to communicate over both a logical tree network and a torus network. The torus network connects the compute nodes in a lattice like structure that allows each compute node to communicate directly with its six nearest neighbors. Nodes may communicate over the torus network using the well known Message Passing Interface MPI an application programming interface used in developing applications for a highly parallel or cluster computer system e.g. system . Any one compute node may route messages to another compute node over the torus network.

The computer system may be described as a compute node core with an I O node surface where communication to 1024 compute nodes is managed by the same I O node . In a blue gene system the I O nodes are connected to the compute nodes through the tree network and also have functional wide area network capabilities through a gigabit Ethernet network e.g. network .

In one embodiment the hardware controller communicates with the compute nodes using JTAG network . As is known JTAG is a low level communication protocol that may be used to initialize the compute nodes and prepare them to have system images loaded. The JTAG network may also be used to perform a variety of other low level monitoring and support functions. A hardware controller resides in each rack and provides an interface for the control system to configure the compute nodes and load program instructions and data into the memory of a compute node. In the Blue gene P computer system the packaging of each node board and midplane includes an IDo chip. The IDo chips are 25 MHz FPGAs that receive commands from the service node using UDP packets sent over the JTAG network .

Further as shown in the compute core also communicates with file servers and front end nodes over functional network . The functional network may be a gigabit Ethernet. The file servers store data for user applications and system images. Individual compute nodes access the file servers by communicating with an I O node . For example the compute node may have access to the file servers over an NFS share. In one embodiment the control system includes a database configured to hold data tables that specify status information associated with the compute nodes . The mid plane management and control system MMCS may be configured to manage the allocation of hardware in a compute core to different computing jobs. The control system is also connected to functional network . In one embodiment this connection allows the control system to detect when a compute node has completed the boot process after being loaded with a system image and data files. The control system further includes a console for use by users and system administrators.

Again referring to the compute core illustrates the compute nodes housed in racks . In a blue gene system the compute nodes are grouped into processing sets psets . A pset is a group of 64 nodes grouped together in a set for communication with a particular I O node . In one embodiment the compute core may be partitioned into a maximum a number of psets each with one I O node running a Linux operating system kernel and 64 compute nodes running a compute node kernel CNK transmitted as a system image over control network . As described the I O node provides communication over the functional network for the compute nodes in a given pset. In a blue gene P system an I O node may be configured to communicate with between 8 and 128 compute nodes.

Each compute node includes system image and data files stored in the memory of a compute node . The system image files and compute node kernel CNK include system files such as the operating system kernel. The compute node kernel is stored in the memory of each compute node . The system image files are typically loaded and the compute node kernel typically begins executing before the compute node is instructed to perform any user applications.

To execute a computing job on the computer system a request is made to the service node to allocate a collection of compute nodes into a block to run job. As the compute nodes may not include any local persistent storage such as a hard disk drive the compute nodes must be loaded with the proper software in order to run the job. In one embodiment the user specifies the characteristics of a desired partition to execute a job. The scheduler selects a set of compute nodes to form a block used to execute the job. Once selected the compute nodes and corresponding I O nodes selected by the scheduler are configured into a block by the service node using the control network . Once a block is created a job may be dispatched through the control system in communication with the I O nodes over networks and or .

In one embodiment the service node uses database and node status table to maintain a table of what was the last program invoked on each compute node . When a user invokes a job the control system determines whether any compute nodes are currently available that last ran the same program being dispatched. If compute nodes are available then the control system instructs such compute nodes to retain the page table associated with the program identified in a job request. All of page tables except one associated with the program being dispatched are discarded by the compute node. Additionally entries from the retained page table that are marked as invalid or writable may be removed from the retained page table. The memory space of the process last to run the program being dispatched has now been cleansed of any data that the previous execution of the program created or modified. At the same time pages marked as read only or as executable have been retained and will not need to be loaded or cause a page fault a page fault is an exception which is raised by the memory management unit when a needed page is not mapped in physical memory when the job being dispatched is executed by the compute node .

The program is now dispatched to the compute nodes . The control system instructs the compute nodes to use the retained page table instead of creating a new one for the process being dispatched. Any other processes that are part of the computing job start with clean memory spaces and pages may be brought into memory using the functional network and I O nodes using a demand paging process. When the program begins executing it tries to load the first instruction from a location in memory. Normally this would trigger a page fault. This exception is passed on to the operating system which in response loads the required page into physical memory from some secondary storage source. However because information from the retained page table contains a valid mapping the program has ready access to the information binary instruction. This will happen for all of the instructions for the program. Thus the load time of the application associated with the retained page table may be substantially reduced. This is particularly the case for a massively parallel system such as a blue gene system that may dispatch the same job with different data sets thousands millions or even billions of times.

At step the control system may determine whether compute nodes are available which last executed the application program specified in the job description. It not then at step the control system schedules and dispatches the job to the compute core . Additionally once the compute nodes complete processing the job the control system may record which compute nodes were used to perform the job in node status table .

Otherwise if compute nodes are available that last executed an application program specified by the job description then at step the control system identifies a set of compute nodes on which the application program was last executed. At step the control system sends a message to the identified compute nodes to retain the page table for the application program specified in the job description. In one embodiment the control system communicates with the affected compute nodes over control network . described below illustrates actions performed by the compute nodes in response to receiving a message to retain the page table for the application program.

At step the job is dispatched to the compute nodes for execution. Once the compute nodes complete executing the job at step the control system may record which compute nodes were used to perform the job in node status table .

Otherwise at step once all of the entries in the page table have been evaluated the compute node may begin executing the application program using the modified page table from the prior invocation of the same application program. As a result reentrant pages i.e. pages that include only executable instructions remain valid in the page table and do not have to be brought into the physical memory of the compute node in order to load the program.

Embodiments of the invention may be used to reduce the time required to load a program for execution in a distributed or highly parallelized computer system. This is accomplished by reusing certain page table entries created during a previous invocation of the program at a particular compute node. Thus as a whole the number of pages that must be paged into the memory of the compute node is reduced thereby reducing program load time. In both massively parallel systems and distributed or clustered systems where the same job may be executed many times the improvement to overall system performance may be substantial even where the improvement at the individual compute node may be relatively minimal.

While the foregoing is directed to embodiments of the present invention other and further embodiments of the invention may be devised without departing from the basic scope thereof and the scope thereof is determined by the claims that follow.


---

title: Concurrent data restore and background copy operations in storage networks
abstract: Exemplary storage network architectures, data architectures, and methods for data restore operation are disclosed. In one embodiment, a storage device comprises a processor, a memory module communicatively connected to the processor, and logic instructions in the memory module which, when executed by the processor, configure the processor to receive a signal that identifies a source volume and a first target snapshot that represents a point in time copy of the source volume, update metadata to define an restore relationship between the first target snapshot and the source volume, execute a background copy process between the first target snapshot and the source volume, and manage input/output operations during the background copy process to permit input/output operations to the source volume during the background copy process.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07467268&OS=07467268&RS=07467268
owner: Hewlett-Packard Development Company, L.P.
number: 07467268
owner_city: Houston
owner_country: US
publication_date: 20060414
---
The described subject matter relates to electronic computing and more particularly to data restore operations in storage networks.

The ability to duplicate and store the contents of a storage device is an important feature of a storage system. Data may be stored in parallel to safeguard against the failure of a single storage device or medium. Upon a failure of the first storage device or medium the system may then retrieve a copy of the data contained in a second storage device or medium. The ability to duplicate and store the contents of the storage device also facilitates the creation of a fixed record of contents at the time of duplication. This feature allows users to recover a prior version of inadvertently edited or erased data.

There are space and processing costs associated with copying and storing the contents of a storage device. For example some storage devices cannot accept input output I O operations while its contents are being copied. Furthermore the storage space used to keep the copy cannot be used for other storage needs.

Storage systems and storage software products can provide ways to make point in time copies of disk volumes sometimes referred to as snapshots. In some of these systems and products copies may be made quickly without significantly disturbing applications using disk volumes. In other products copies may be made space efficient by sharing storage instead of copying all the disk volume data.

Periodically there may be a need to restore one or more volumes of a storage system to a data state that represents a previous point in time. Users or administrators of storage systems may have varying goals for restore processes. In some instances restore operations that enable contemporaneous access to data may find utility.

In an exemplary implementation a storage device comprises a processor a memory module communicatively connected to the processor and logic instructions in the memory module which when executed by the processor configure the processor to receive a signal that identifies a source volume and a first target snapshot that represents a point in time copy of the source volume update metadata to define an restore relationship between the first target snapshot and the source volume execute a background copy process between the first target snapshot and the source volume and manage input output operations during the background copy process to permit input output operations to the source volume during the background copy process.

Described herein are exemplary storage network architectures data architectures and methods for creating and using difference files in storage networks. The methods described herein may be embodied as logic instructions on a computer readable medium. When executed on a processor the logic instructions cause a general purpose computing device to be programmed as a special purpose machine that implements the described methods. The processor when configured by the logic instructions to execute the methods recited herein constitutes structure for performing the described methods.

The subject matter described herein may be implemented in a storage architecture that provides virtualized data storage at a system level such that virtualization is implemented within a SAN. In the implementations described herein the computing systems that utilize storage are referred to as hosts. In a typical implementation a host is any computing system that consumes data storage resources capacity on its own behalf or on behalf of systems coupled to the host. For example a host may be a supercomputer processing large databases a transaction processing server maintaining transaction records and the like. Alternatively the host may be a file server on a local area network LAN or wide area network WAN that provides storage services for an enterprise.

In a direct attached storage solution such a host may include one or more disk controllers or RAID controllers configured to manage multiple directly attached disk drives. By contrast in a SAN a host connects to the SAN in accordance via a high speed connection technology such as e.g. a fibre channel FC fabric in the particular examples.

A virtualized SAN architecture comprises a group of storage cells where each storage cell comprises a pool of storage devices called a disk group. Each storage cell comprises parallel storage controllers coupled to the disk group. The storage controllers coupled to the storage devices using a fibre channel arbitrated loop connection or through a network such as a fibre channel fabric or the like. The storage controllers may also be coupled to each other through point to point connections to enable them to cooperatively manage the presentation of storage capacity to computers using the storage capacity.

The network architectures described herein represent a distributed computing environment such as an enterprise computing system using a private SAN. However the network architectures may be readily scaled upwardly or downwardly to meet the needs of a particular application.

A plurality of logical disks also called logical units or LUNs may be allocated within storage pool . Each LUN comprises a contiguous range of logical addresses that can be addressed by host devices and by mapping requests from the connection protocol used by the host device to the uniquely identified LUN . A host such as server may provide services to other computing or data processing systems or devices. For example client computer may access storage pool via a host such as server . Server may provide file services to client and may provide other services such as transaction processing services email services etc. Hence client device may or may not directly use the storage consumed by host .

Devices such as wireless device and computers which also may serve as hosts may logically couple directly to LUNs . Hosts may couple to multiple LUNs and LUNs may be shared among multiple hosts. Each of the devices shown in may include memory mass storage and a degree of data processing capability sufficient to manage a network connection.

A LUN such as LUN comprises one or more redundant stores RStore which are a fundamental unit of reliable storage. An RStore comprises an ordered set of physical storage segments PSEGs with associated redundancy properties and is contained entirely within a single redundant store set RSS . By analogy to conventional storage systems PSEGs are analogous to disk drives and each RSS is analogous to a RAID storage set comprising a plurality of drives.

The PSEGs that implements a particular LUN may be spread across any number of physical storage disks. Moreover the physical storage capacity that a particular LUN represents may be configured to implement a variety of storage types offering varying capacity reliability and availability features. For example some LUNs may represent striped mirrored and or parity protected storage. Other LUNs may represent storage capacity that is configured without striping redundancy or parity protection.

In an exemplary implementation an RSS comprises a subset of physical disks in a Logical Device Allocation Domain LDAD and may include from six to eleven physical drives which can change dynamically . The physical drives may be of disparate capacities. Physical drives within an RSS may be assigned indices e.g. 0 1 2 . . . 11 for mapping purposes and may be organized as pairs i.e. adjacent odd and even indices for RAID 1 purposes. One problem with large RAID volumes comprising many disks is that the odds of a disk failure increase significantly as more drives are added. A sixteen drive system for example will be twice as likely to experience a drive failure or more critically two simultaneous drive failures than would an eight drive system. Because data protection is spread within an RSS in accordance with the present invention and not across multiple RSSs a disk failure in one RSS has no effect on the availability of any other RSS. Hence an RSS that implements data protection must suffer two drive failures within the RSS rather than two failures in the entire system. Because of the pairing in RAID 1 implementations not only must two drives fail within a particular RSS but a particular one of the drives within the RSS must be the second to fail i.e. the second to fail drive must be paired with the first to fail drive . This atomization of storage sets into multiple RSSs where each RSS can be managed independently improves the performance reliability and availability of data throughout the system.

A SAN manager appliance is coupled to a management logical disk set MLD which is a metadata container describing the logical structures used to create LUNs LDADs and other logical structures used by the system. A portion of the physical storage capacity available in storage pool is reserved as quorum space and cannot be allocated to LDADs and hence cannot be used to implement LUNs . In a particular example each physical disk that participates in storage pool has a reserved amount of capacity e.g. the first n physical sectors that may be designated as quorum space . MLD is mirrored in this quorum space of multiple physical drives and so can be accessed even if a drive fails. In a particular example at least one physical drive is associated with each LDAD includes a copy of MLD designated a quorum drive . SAN management appliance may wish to associate information such as name strings for LDADs and LUNs and timestamps for object birthdates. To facilitate this behavior the management agent uses MLD to store this information as metadata. MLD is created implicitly upon creation of each LDAD 

Quorum space is used to store information including physical store ID a unique ID for each physical drive version control information type quorum non quorum RSS ID identifies to which RSS this disk belongs RSS Offset identifies this disk s relative position in the RSS Storage Cell ID identifies to which storage cell this disk belongs PSEG size as well as state information indicating whether the disk is a quorum disk for example. This metadata PSEG also contains a PSEG free list for the entire physical store probably in the form of an allocation bitmap. Additionally quorum space contains the PSEG allocation records PSARs for every PSEG on the physical disk. The PSAR comprises a PSAR signature Metadata version PSAR usage and an indication a RSD to which this PSEG belongs.

CSLD is another type of metadata container comprising logical drives that are allocated out of address space within each LDAD but that unlike LUNs may span multiple LDADs . Preferably each LDAD includes space allocated to CSLD . CSLD holds metadata describing the logical structure of a given LDAD including a primary logical disk metadata container PLDMC that contains an array of descriptors called RSDMs that describe every RStore used by each LUN implemented within the LDAD . The CSLD implements metadata that is regularly used for tasks such as disk creation leveling RSS merging RSS splitting and regeneration. This metadata includes state information for each physical disk that indicates whether the physical disk is Normal i.e. operating as expected Missing i.e. unavailable Merging i.e. a missing drive that has reappeared and must be normalized before use Replace i.e. the drive is marked for removal and data must be copied to a distributed spare and Regen i.e. the drive is unavailable and requires regeneration of its data to a distributed spare .

A logical disk directory LDDIR data structure is a directory of all LUNs in any LDAD . An entry in the LDDS comprises a universally unique ID UUID an RSD indicating the location of a Primary Logical Disk Metadata Container PLDMC for that LUN . The RSD is a pointer to the base RSDM or entry point for the corresponding LUN . In this manner metadata specific to a particular LUN can be accessed by indexing into the LDDIR to find the base RSDM of the particular LUN . The metadata within the PLDMC e.g. mapping structures described hereinbelow can be loaded into memory to realize the particular LUN 

Hence the storage pool depicted in implements multiple forms of metadata that can be used for recovery. The CSLD implements metadata that is regularly used for tasks such as disk creation leveling RSS merging RSS splitting and regeneration. The PSAR metadata held in a known location on each disk contains metadata in a more rudimentary form that is not mapped into memory but can be accessed when needed from its known location to regenerate all metadata in the system.

Each of the devices shown in may include memory mass storage and a degree of data processing capability sufficient to manage a network connection. The computer program devices in accordance with the present invention are implemented in the memory of the various devices shown in and enabled by the data processing capability of the devices shown in .

In an exemplary implementation an individual LDAD may correspond to from as few as four disk drives to as many as several thousand disk drives. In particular examples a minimum of eight drives per LDAD is required to support RAID 1 within the LDAD using four paired disks. LUNs defined within an LDAD may represent a few megabytes of storage or less up to 2TByte of storage or more. Hence hundreds or thousands of LUNs may be defined within a given LDAD and thus serve a large number of storage needs. In this manner a large enterprise can be served by a single storage pool providing both individual storage dedicated to each workstation in the enterprise as well as shared storage across the enterprise. Further an enterprise may implement multiple LDADs and or multiple storage pools to provide a virtually limitless storage capability. Logically therefore the virtual storage system in accordance with the present description offers great flexibility in configuration and access.

Client computers may access storage cells through a host such as servers . Clients may be connected to file server directly or via a network such as a Local Area Network LAN or a Wide Area Network WAN . The number of storage cells that can be included in any storage network is limited primarily by the connectivity implemented in the communication network . By way of example a switching fabric comprising a single FC switch can interconnect 256 or more ports providing a possibility of hundreds of storage cells in a single storage network.

Hosts are typically implemented as server computers. is a schematic illustration of an exemplary computing device that can be utilized to implement a host. Computing device includes one or more processors or processing units a system memory and a bus that couples various system components including the system memory to processors . The bus represents one or more of any of several types of bus structures including a memory bus or memory controller a peripheral bus an accelerated graphics port and a processor or local bus using any of a variety of bus architectures. The system memory includes read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computing device such as during start up is stored in ROM .

Computing device further includes a hard disk drive for reading from and writing to a hard disk not shown and may include a magnetic disk drive for reading from and writing to a removable magnetic disk and an optical disk drive for reading from or writing to a removable optical disk such as a CD ROM or other optical media. The hard disk drive magnetic disk drive and optical disk drive are connected to the bus by a SCSI interface or some other appropriate interface. The drives and their associated computer readable media provide nonvolatile storage of computer readable instructions data structures program modules and other data for computing device . Although the exemplary environment described herein employs a hard disk a removable magnetic disk and a removable optical disk other types of computer readable media such as magnetic cassettes flash memory cards digital video disks random access memories RAMs read only memories ROMs and the like may also be used in the exemplary operating environment.

A number of program modules may be stored on the hard disk magnetic disk optical disk ROM or RAM including an operating system one or more application programs other program modules and program data . A user may enter commands and information into computing device through input devices such as a keyboard and a pointing device . Other input devices not shown may include a microphone joystick game pad satellite dish scanner or the like. These and other input devices are connected to the processing unit through an interface that is coupled to the bus . A monitor or other type of display device is also connected to the bus via an interface such as a video adapter .

Computing device may operate in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to computing device although only a memory storage device has been illustrated in . The logical connections depicted in include a LAN and a WAN .

When used in a LAN networking environment computing device is connected to the local network through a network interface or adapter . When used in a WAN networking environment computing device typically includes a modem or other means for establishing communications over the wide area network such as the Internet. The modem which may be internal or external is connected to the bus via a serial port interface . In a networked environment program modules depicted relative to the computing device or portions thereof may be stored in the remote memory storage device. It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

Hosts may include host adapter hardware and software to enable a connection to communication network . The connection to communication network may be through an optical coupling or more conventional conductive cabling depending on the bandwidth requirements. A host adapter may be implemented as a plug in card on computing device . Hosts may implement any number of host adapters to provide as many connections to communication network as the hardware and software support.

Generally the data processors of computing device are programmed by means of instructions stored at different times in the various computer readable storage media of the computer. Programs and operating systems may distributed for example on floppy disks CD ROMs or electronically and are installed or loaded into the secondary memory of a computer. At execution the programs are loaded at least partially into the computer s primary electronic memory.

Each NSC further includes a communication port that enables a communication connection between the NSCs . The communication connection may be implemented as a FC point to point connection or pursuant to any other suitable communication protocol.

In an exemplary implementation NSCs further include a plurality of Fiber Channel Arbitrated Loop FCAL ports that implement an FCAL communication connection with a plurality of storage devices e.g. arrays of disk drives . While the illustrated embodiment implement FCAL connections with the arrays of disk drives it will be understood that the communication connection with arrays of disk drives may be implemented using other communication protocols. For example rather than an FCAL configuration a FC switching fabric or a small computer serial interface SCSI connection may be used.

In operation the storage capacity provided by the arrays of disk drives may be added to the storage pool . When an application requires storage capacity logic instructions on a host computer establish a LUN from storage capacity available on the arrays of disk drives available in one or more storage sites. It will be appreciated that because a LUN is a logical unit not necessarily a physical unit the physical storage space that constitutes the LUN may be distributed across multiple storage cells. Data for the application is stored on one or more LUNs in the storage network. An application that needs to access the data queries a host computer which retrieves the data from the LUN and forwards the data to the application.

One or more of the storage cells in the storage network may implement RAID based storage. RAID Redundant Array of Independent Disks storage systems are disk array systems in which part of the physical storage capacity is used to store redundant data. RAID systems are typically characterized as one of six architectures enumerated under the acronym RAID. A RAID 0 architecture is a disk array system that is configured without any redundancy. Since this architecture is really not a redundant architecture RAID 0 is often omitted from a discussion of RAID systems.

A RAID 1 architecture involves storage disks configured according to mirror redundancy. Original data is stored on one set of disks and a duplicate copy of the data is kept on separate disks. The RAID 2 through RAID 5 architectures all involve parity type redundant storage. Of particular interest a RAID 5 system distributes data and parity information across a plurality of the disks. Typically the disks are divided into equally sized address areas referred to as blocks . A set of blocks from each disk that have the same unit address ranges are referred to as stripes . In RAID 5 each stripe has N blocks of data and one parity block which contains redundant information for the data in the N blocks.

In RAID 5 the parity block is cycled across different disks from stripe to stripe. For example in a RAID 5 system having five disks the parity block for the first stripe might be on the fifth disk the parity block for the second stripe might be on the fourth disk the parity block for the third stripe might be on the third disk and so on. The parity block for succeeding stripes typically precesses around the disk drives in a helical pattern although other patterns are possible . RAID 2 through RAID 4 architectures differ from RAID 5 in how they compute and place the parity block on the disks. The particular RAID class implemented is not important.

The memory representation described herein enables each LUN to implement from 1 Mbyte to 2 TByte in storage capacity. Larger storage capacities per LUN are contemplated. For purposes of illustration a 2Terabyte maximum is used in this description. Further the memory representation enables each LUN to be defined with any type of RAID data protection including multi level RAID protection as well as supporting no redundancy at all. Moreover multiple types of RAID data protection may be implemented within a single LUN such that a first range of logical disk addresses LDAs correspond to unprotected data and a second set of LDAs within the same LUN implement RAID 5 protection. Hence the data structures implementing the memory representation must be flexible to handle this variety yet efficient such that LUNs do not require excessive data structures.

A persistent copy of the memory representation shown in is maintained in the PLDMDC for each LUN described hereinbefore. The memory representation of a particular LUN is realized when the system reads metadata contained in the quorum space to obtain a pointer to the corresponding PLDMDC then retrieves the PLDMDC and loads an level 2 map L2MAP . This is performed for every LUN although in ordinary operation this would occur once when a LUN was created after which the memory representation will live in memory as it is used.

A logical disk mapping layer maps a LDA specified in a request to a specific RStore as well as an offset within the RStore. Referring to the embodiment shown in a LUN may be implemented using an L2MAP an LMAP and a redundancy set descriptor RSD as the primary structures for mapping a logical disk address to physical storage location s represented by an address. The mapping structures shown in are implemented for each LUN . A single L2MAP handles the entire LUN . Each LUN is represented by multiple LMAPs where the particular number of LMAPs depend on the actual address space that is allocated at any given time. RSDs also exist only for allocated storage space. Using this split directory approach a large storage volume that is sparsely populated with allocated storage the structure shown in efficiently represents the allocated storage while minimizing data structures for unallocated storage.

L2MAP includes a plurality of entries where each entry represents 2 Gbyte of address space. For a 2 Tbyte LUN therefore L2MAP includes 1024 entries to cover the entire address space in the particular example. Each entry may include state information corresponding to the corresponding 2 Gbyte of storage and a pointer a corresponding LMAP descriptor . The state information and pointer are only valid when the corresponding 2 Gbyte of address space have been allocated hence some entries in L2MAP will be empty or invalid in many applications.

The address range represented by each entry in LMAP is referred to as the logical disk address allocation unit LDAAU . In the particular implementation the LDAAU is 8 MByte. An entry is created in LMAP for each allocated LDAAU irrespective of the actual utilization of storage within the LDAAU. In other words a LUN can grow or shrink in size in increments of 1 Mbyte. The LDAAU is represents the granularity with which address space within a LUN can be allocated to a particular storage task.

An LMAP exists only for each 2 Gbyte increment of allocated address space. If less than 2 Gbyte of storage are used in a particular LUN only one LMAP is required whereas if 2 Tbyte of storage is used LMAPs will exist. Each LMAP includes a plurality of entries where each entry optionally corresponds to a redundancy segment RSEG . An RSEG is an atomic logical unit that is roughly analogous to a PSEG in the physical domain akin to a logical disk partition of an RStore. In a particular embodiment an RSEG is a logical unit of storage that spans multiple PSEGs and implements a selected type of data protection. Entire RSEGs within an RStore are bound to contiguous LDAs in a preferred implementation. In order to preserve the underlying physical disk performance for sequential transfers it is desirable to adjacently locate all RSEGs from an RStore in order in terms of LDA space so as to maintain physical contiguity. If however physical resources become scarce it may be necessary to spread RSEGs from RStores across disjoint areas of a LUN . The logical disk address specified in a request selects a particular entry within LMAP corresponding to a particular RSEG that in turn corresponds to 1 Mbyte address space allocated to the particular RSEG . Each LMAP entry also includes state information about the particular RSEC and an RSD pointer.

Optionally the RSEG s may be omitted which results in the RStore itself being the smallest atomic logical unit that can be allocated. Omission of the RSEG decreases the size of the LMAP entries and allows the memory representation of a LUN to demand fewer memory resources per MByte of storage. Alternatively the RSEG size can be increased rather than omitting the concept of RSEGs altogether which also decreases demand for memory resources at the expense of decreased granularity of the atomic logical unit of storage. The RSEG size in proportion to the RStore can therefore be changed to meet the needs of a particular application.

The RSD pointer points to a specific RSD that contains metadata describing the RStore in which the corresponding RSEG exists. As shown in the RSD includes a redundancy storage set selector RSSS that includes a redundancy storage set RSS identification a physical member selection and RAID information. The physical member selection is essentially a list of the physical drives used by the RStore. The RAID information or more generically data protection information describes the type of data protection if any that is implemented in the particular RStore.

RSD further includes two fields for bits referred to as share bits a predecessor share bit referred to as Share P S and a successor share bit referred to as Share S S If the data in the RSD is shared with a predecessor snapshot then the predecessor share bit may be set to a first logical value e.g. a binary 1 . By contrast if the data in the RSD is not shared with a predecessor snapshot then the predecessor share bit may be set to a second logical value e.g. a binary zero . Similarly if the data in the RSD is shared with a successor snapshot then the successor share bit may be set to a first logical value e.g. a binary 1. By contrast if the data in the RSD is not shared with a successor snapshot then the predecessor share bit may be set to a second logical value e.g. a binary zero .

Each RSD also includes a number of fields that identify particular PSEG numbers within the drives of the physical member selection that physically implement the corresponding storage capacity. Each listed PSEG corresponds to one of the listed members in the physical member selection list of the RSSS. Any number of PSEGs may be included however in a particular embodiment each RSEG is implemented with between four and eight PSEGs dictated by the RAID type implemented by the RStore.

In operation each request for storage access specifies a LUN and an address. A NSC such as NSC maps the logical drive specified to a particular LUN then loads the L2MAP for that LUN into memory if it is not already present in memory. Preferably all of the LMAPs and RSDs for the LUN are loaded into memory as well. The LDA specified by the request is used to index into L2MAP which in turn points to a specific one of the LMAPs. The address specified in the request is used to determine an offset into the specified LMAP such that a specific RSEG that corresponds to the request specified address is returned. Once the RSEG is known the corresponding RSD is examined to identify specific PSEGs that are members of the redundancy segment and metadata that enables a NSC to generate drive specific commands to access the requested data. In this manner an LDA is readily mapped to a set of PSEGs that must be accessed to implement a given storage request.

The L2MAP consumes 4 Kbytes per LUN regardless of size in an exemplary implementation. In other words the L2MAP includes entries covering the entire 2 Tbyte maximum address range even where only a fraction of that range is actually allocated to a LUN . It is contemplated that variable size L2MAPs may be used however such an implementation would add complexity with little savings in memory. LMAP segments consume 4 bytes per Mbyte of address space while RSDs consume 32 bytes per MB. Unlike the L2MAP LMAP segments and RSDs exist only for allocated address space.

RStores are allocated in their entirety to a specific LUN . RStores may be partitioned into 1 Mbyte segments RSEGs as shown in . Each RSEG in presents only 80 of the physical disk capacity consumed as a result of storing a chunk of parity data in accordance with RAID 5 rules. When configured as a RAID 5 storage set each RStore will comprise data on four PSEGs and parity information on a fifth PSEG not shown similar to RAID4 storage. The fifth PSEG does not contribute to the overall storage capacity of the RStore which appears to have four PSEGs from a capacity standpoint. Across multiple RStores the parity will fall on various of various drives so that RAID 5 protection is provided.

RStores are essentially a fixed quantity 8 MByte in the examples of virtual address space. RStores consume from four to eight PSEGs in their entirety depending on the data protection level. A striped RStore without redundancy consumes 4 PSEGs 4 2048 KByte PSEGs 8 MB an RStore with 4 1 parity consumes 5 PSEGs and a mirrored RStore consumes eight PSEGs to implement the 8 Mbyte of virtual address space.

An RStore is analogous to a RAID disk set differing in that it comprises PSEGs rather than physical disks. An RStore is smaller than conventional RAID storage volumes and so a given LUN will comprise multiple RStores as opposed to a single RAID storage volume in conventional systems.

It is contemplated that drives may be added and removed from an LDAD over time. Adding drives means existing data can be spread out over more drives while removing drives means that existing data must be migrated from the exiting drive to fill capacity on the remaining drives. This migration of data is referred to generally as leveling . Leveling attempts to spread data for a given LUN over as many physical drives as possible. The basic purpose of leveling is to distribute the physical allocation of storage represented by each LUN such that the usage for a given logical disk on a given physical disk is proportional to the contribution of that physical volume to the total amount of physical storage available for allocation to a given logical disk.

Existing RStores can be modified to use the new PSEGs by copying data from one PSEG to another and then changing the data in the appropriate RSD to indicate the new membership. Subsequent RStores that are created in the RSS will use the new members automatically. Similarly PSEGs can be removed by copying data from populated PSEGs to empty PSEGs and changing the data in LMAP to reflect the new PSEG constituents of the RSD. In this manner the relationship between physical storage and logical presentation of the storage can be continuously managed and updated to reflect current storage environment in a manner that is invisible to users.

A data storage system may be configured to implement an instant restore of a data set from one or more point in time copies either snapshots or mirror clones referred to herein as snapshot s . As used herein the term instant restore refers to a restore operation in which a source volume is restored to a previous point in time using a snapshot and in which the data is available for access contemporaneously. In one embodiment referred to herein as a high performance mode instant restore an instant restore operation may invalidate one or more newer snapshots to increase the speed of the restore operation. In another embodiment referred to herein as a normal mode instant restore the restore operation maintains data integrity of newer snapshots during the restore operation. In another embodiment instant restore operations may be implemented in an environment that includes a mirror clone of a source volume.

In one embodiment snapshots may be logically linked e.g. by pointers in one or more memory structures. In the embodiment depicted in snapshot represents a snapshot of the data taken at a first point in time and snapshot represents a snapshot taken at a second point in time later than the first point in time. Snapshots may be linked in a serial fashion. Hence snapshot n represents the most recent point in time copy of source volume . In theory there is no limit to the number of snapshots that may be created of a source volume .

In one embodiment referred to herein as a space inefficient snapshot one or more of snapshots may reserve the space on one or more physical drives necessary to hold a copy of the data in source volume . In an alternate embodiment referred to herein as a space efficient snapshot one or more of snapshots does not reserve space on one or more physical drives to hold a copy of the data in source volume . Rather physical storage space for the snapshot is allocated on demand as the data is written into the snapshot. A space efficient snapshot consumes less storage space than a space inefficient snapshot but creates a possibility of reaching an overcommit state in which the physical storage lacks sufficient storage capacity to hold the snapshot.

Aspects of data mapping associated with instant restore operations will be explained in greater detail with reference to which is a schematic illustration of memory maps for logical volumes such as e.g. a source volume mirror clone or one or more snapshot files such as the snapshot files depicted in . Referring briefly to in one implementation a memory mapping for instant restore operations begins in a logical disk unit table which includes an array of data structures that maps a plurality of logical disk state blocks LDSBs which may be numbered sequentially i.e. LDSB . . . LDSB N. In one embodiment the LDSBs may include a memory mapping for a source volume such as e.g. source volume a snapshot such as snapshot or for a mirrored clone such as e.g. mirrored clone or it s snapshots .

In one embodiment an LDSB includes an L2MAP pointer which points to an L2MAP mapping data structure which as described above ultimately maps to a PSEG or to a disk in a non virtualized system . LDSB may further include a SOURCE LDSB pointer field that may include a pointer to a snapshot that represents the source of a restore operation a DESTINATION LDSB pointer field that may include a pointer to the source volume that is the target of the restore operation a destination IRWrite bitmap pointer that may include a pointer to an IRWrite bitmap a successor LDSB pointer field that may include a pointer to a successor LDSB and a predecessor LDSB pointer field that may include a pointer to a predecessor LDSB. LDSB may further include a restore mode field that may include a flag which indicates whether the instant restore operation is to be executed in a high performance mode or in a normal mode and one or more data fields that includes other state information. LDSBs through N may include similar fields and mappings.

LDSB further includes a data field referred to herein as DESTINATION IRWrite BITMAP PTR. The IRWrite field is used as an indicator of whether the RSEG with which the RSD is associated has been affected by a host write I O operation or a background copy operation during the course of an instant restore operation. In brief the IRWrite field is set to a first logical value during a setup process to prepare for an instant restore operation. If during the instant restore operation a write I O operation affects the RSEC then the IRWrite field is changed to a second logical value different than the first logical value. Similarly if a background copy process executed during the instant restore operation affects the RSEQ then the IRWrite field is changed to a second logical value different from the first logical value.

Thus when the IRWrite bit is set for the source volume it indicates that the data has already been restored to the source volume. No further operations need to be done to maintain data integrity of other snapshots in a snaptree and a host write can just write to the source volume. In addition a set IRWrite indicates that a background copy operation may skip this RSEG since it has already been restored. The IRWrite bit permits host writes reads and the background copy to happen concurrently and to be interleaved.

Accordingly blocks of the flowchart illustrations support combinations of means for performing the specified functions and combinations of steps for performing the specified functions. It will also be understood that each block of the flowchart illustrations and combinations of blocks in the flowchart illustrations can be implemented by special purpose hardware based computer systems which perform the specified functions or steps or combinations of special purpose hardware and computer instructions.

At operation metadata associated with the snapshot is updated to reflect the instant restore relationship between the source volume and the snapshot. As described above in connection with in one embodiment two pointers are added to the LDSB of logical volumes. At operation these pointers are set. In one embodiment establishing an instant restore relationship may include initializing the LDSB of the source volume with the mode of the instant restore operation high performance or normal the IRWrite bits to a first logical value and initializing the source and destination pointers in the LDSB. In addition corresponding metadata that is stored in a non volatile medium i.e. on disk in a part of the PLDMC may be updated.

At operation the source volume is unquiesced which permits I O operations to be directed to the source volume. At operation a background copy process is initiated between the snapshot and the source volume. The background copy process may generate appropriate signaling with one or more host computers that initiated the process. At operation input output I O operations directed to the source volume and or the snapshot are managed to provide data integrity to the source volume and snapshot. At operation when the background copy completes metadata is updated to sever the instant restore relationship between the snapshot and source volume. At operation the snapshot is unquiesced and I O operations to it are allowed. In normal mode I O operations are managed to maintain data integrity to all newer and older snapshots as well. All restores in snaptrees that contain a mirror clone regardless of mode are processed in normal mode and data integrity of the mirror clone and all snapshots in the snaptree is maintained. Various aspects of will be explained in greater detail below.

In one embodiment a data restore operation may be implemented in a high performance mode in which data consistency need not maintained in snapshots between the selected snapshot and the source volume. illustrate operations in a high performance mode data restore operation. is a flowchart illustrating operations in a background copy process in a storage network in accordance with an embodiment. The operations described in may be implemented as a component of the background copy process of operation of . In one embodiment the background copy process traverses the memory map of the selected source volume and copies data from the snapshot selected as a point in time copy from which a source volume is to be restored. In one embodiment only data that differs between the source volume and snapshot is copied.

Referring to at operation intermediate snapshots i.e. snapshots between the snapshot selected for the restore operation and the source volume are set to invalid. Hence these intermediate snapshots are no longer read from or written to as host writes and background copy operations will make their data inconsistent with the point in time at which they were created. At operation the next RSEG in the source volume is selected. In the initial iteration the next RSEG will correspond to the first RSEG in the source volume. If at operation the IRWrite bit is set to a value that indicates that the RSEG on the source volume has been written by the host during a background copy process then control passes back to operation and the next RSEG in the source volume is selected. The particular value to which the IRWrite bit is set to indicate whether the host is not critical. In one embodiment the IRWrite bit may be set to a logic one 1 to indicate that the RSEG on the source volume has been written by the host.

By contrast if at operation the IRWrite bit is not set then control passes to operation . If at operation the data in the RSEG is shared with the snapshot selected for the restore operation then control passes back to operation . In one embodiment determining whether the data in the RSEG is shared with the snapshot may include checking the value of the share bit in the RSD and traversing successor links in the LDSB until reaching the source volume or finding the RSEG is shared with another snapshot see .

By contrast if at operation data in the RSEG is not shared with the snapshot selected for the restore operation then control passes to operation . If at operation there is data in the snapshot then control passes to operation and the data is copied from the snapshot selected for the restore operation to the source volume.

If at operation no data resides in the RSEG of the snapshot selected for the restore operation then control passes to operation and the data is located in the snapshot tree. In one embodiment locating the data in the snapshot tree may include checking the value of the share bit in the RSD and traversing the successor links in the LDSB to find the snapshot in which data is located. Once the data is located the data is copied to the source volume and the snapshot selected for the restore operation operation .

At operation the sharing bits of the snapshot selected for the restore operation are updated to reflect that the RSEG in the snapshot no longer shares data with a successor. The snapshot successor s predecessor sharing bit S is also updated to indicate no sharing with the snapshot. At operation the sharing bits of the source volume are updated to reflect that the source volume no longer shares bits with a predecessor. The predecessor s successor sharing bit S is also cleared. At operation the IRWrite bit in the source volume is set. In one embodiment the IRWrite bit is set to a value that indicates that the data in the RSEG has been restored to the source volume.

If at operation there are more RSEGs in the source volume then control passes back to operation and the next RSEG in the source volume is selected. Thus operations for a loop which when executed traverses the RSEGS of a source volume and restores the RSEGs to the point in time state of the snapshot selected for use in the restore process.

If at operation there are no further RSEGS in the source volume then control passes to operation and metadata is updated to reflect that there is no longer an instant restore operation between the source volume and the snapshot. In one embodiment updating the metadata may include updating the IRWrite pointers SOURCE DESTINATION and RESTORE MODE in the source volume LDSB and the snapshot LDSB to indicate that the instant restore relationship no longer exists. At all snapshots between the snapshot that was the source of the restore and the source volume are deleted. Deletion of these intermediate snapshots does not require any further unsharing or data copying because all the data that needed to be copied was copied to the source volume and snapshot that was the source of the restore in operation .

If at operation the IRWrite bit is set for the RSEG s identified in the write operation then control passes to operation and the data associated with the write operation is written to the source volume. By contrast if at operation the IRWrite bit is not set to a value that indicates that the RSEG on the source volume has been written by the host then control passes to operation .

If at operation the data in the RSEG s identified in the write operation is shared with the snapshot that is the source of the restore then control passes to operation and the data is copied from the source volume RSEG to the snapshot. Control then passes to operation and the share bits in the snapshot selected for the restore operation are updated to reflect that the snapshot no longer shares data with a successor snapshot. The snapshot successor s share bits are also updated to reflect that it no longer shares with its predecessor. Control then passes to operation and the share bits in the source volume are updated to reflect that the source volume no longer shares data with a predecessor snapshot. At operation the IRWrite bit in the source volume is set to a value that indicates that data in the source disk has been restored i.e. that data in the source volume has been restored from the selected snapshot and at operation the data associated with the write operation is written to the source volume.

Referring back to operation if the source volume does not share data with the snapshot that is the source of the restore for the RSEG s identified in the write operation then control passes to operation . If at operation there is data in the snapshot then control passes to operation and the data from the snapshot is copied to the source volume. At operation the share bits in the source volume and its predecessor are updated to reflect they no longer share data for that RSEG. At operation the IRWrite bit in the source volume is set to a value that indicates that data in the source disk has been restored and at operation the data associated with the write operation is written to the source volume.

By contrast if at operation there is no data in the RSEG s of the snapshot identified in the write operation then control passes to operation and the share bit value in the RSD is checked and the successor pointer in the LDSB is traversed until a snapshot that holds data in the RSEG is located. At operation the data is copied to the source volume and the snapshot selected for use in the restore operation. The copy operations implemented in operations and are implemented because copy operations may be executed at a block size that differs from the block size of a host write operation. For example copy operations may be executed at a block size of 1 MB while host write operations may be executed at much smaller block sizes i.e. 4k 8k 16k or 64k.

At operation the share bits in the snapshot selected for the restore operation are updated to reflect that the snapshot no longer shares data with a successor snapshot. Control then passes to operations to which are described above.

If at operation the IRWrite bit in the source volume for the RSEG s identified in the read operation is set to a value that indicates that the RSEG s on the source volume have been written by the host during the restore operation then control passes to operation and the source volume LMAP may be used to locate the data for the read operation. By contrast if at operation the IRWrite bit is not set to a value that indicates that the RSEG on the source volume has been written by the host during the restore operation then control passes to operation . Starting at the snapshot that is the source of the restore the share bit value in the RSD is checked and the successor pointer in the LDSB is traversed until a snapshot that holds data in the RSEG is located. At operation the data is returned to the host.

In another embodiment a data restore operation may be implemented in a normal mode in which data consistency is maintained in snapshots between the selected snapshot and the source volume. illustrate operations in a normal mode data restore operation. is a flowchart illustrating operations in a background copy process in a storage network in accordance with an embodiment. The operations described in may be implemented as a component of the background copy process of operation of . In one embodiment the background copy process traverses the memory map of the selected source volume and copies data from the snapshot selected as a point in time copy from which a source volume is to be restored. For efficiency and performance reasons of starting another restore before this one has completed operations unshare data from the source volume to its predecessor snapshot. This is done to maintain the data integrity of all snapshots in the snaptree and is not required in high performance mode. These operations are done first because they also may need to be done when a subsequent restore is started. Operations restore data from the snapshot that is the source of the restore to the source volume

Referring to at operation the next RSEG in the source volume is selected. In the initial iteration the next RSEG will correspond to the first RSEG in the source volume. If at operation the source volume and the snapshot selected for the restore operation share the data in the RSEQ then control passes back to operation and the next RSEG in the source volume is selected. By contrast if at operation the source volume and the snapshot selected for the restore operation do not share the data in the RSEQ then control passes to operation .

If at operation the source volume does not share the data in the RSEG with a predecessor snapshot then control passes back to operation and the next RSEG in the source volume is selected. By contrast if at operation the source volume shares the data in the RSEG with a predecessor snapshot then control passes to operation and the data in the source volume is copied to the predecessor snapshot. At operation the sharing bits in the RSEG s in the predecessor and source volume are cleared i.e. set to a value which indicates that the data is not shared with the source volume .

If at operation there are more RSEGs in the source volume then control passes back to operation and the next RSEG is selected. Thus operations form a loop which traverses the RSEGs in the source volume and copies any data shared between the source volume and the predecessor snapshot to the predecessor snapshot.

If at operation there are no more RSEGs in the source volume to analyze then control passes to operations in which an analogous process is implemented. At operation the next RSEG in the source volume is selected. In the initial iteration the next RSEG will correspond to the first RSEG in the source volume. If at operation the IRWrite bit in the source volume is set to a value that indicates that a write operation has been directed to the source volume during the data restore operation then control passes back to operation and the next RSEG in the source volume is selected. By contrast if at operation the IRWrite bit is not set then control passes to operation .

If at operation the source volume shares the data in the RSEG with the snapshot selected for the restore operation then control passes back to operation and the next RSEG in the source volume is selected. By contrast if at operation the source volume does not share the data in the RSEG with the snapshot selected for the restore operation then control passes to operation and the data in the snapshot is copied to the source volume. This data may reside in the snapshot itself or it may reside in a newer snapshot. Starting at the snapshot that is the source of the restore the share bit value in the RSD is checked and the successor pointer in the LDSB is traversed until a snapshot that holds the data in the RSEG is located. At operation the IRWrite bit in the RSEG is set to a value that indicates that the RSEG has received a write operation during the instant restore operation.

If at operation there are more RSEGs in the source volume then control passes back to operation and the next RSEG is selected. Thus operations form a loop which traverses the RSEGs in the source volume and copies data shared with the snapshot selected for the restore operation to the source volume.

At operation the metadata is updated to reflect that there is no longer an instant restore operation between the source volume and the snapshot. In one embodiment updating the metadata may include updating the IRWrite pointers in the source volume LDSB and the snapshot LDSB to indicate that the instant restore relationship no longer exists.

If at operation the IRWrite bit is set for the RSEG s identified in the write operation then control passes to operation and the data associated with the write operation is written to the source volume. By contrast if at operation the IRWrite bit is not set to a value that indicates that the RSEG on the source volume has been written by the host then control passes to operation .

If at operation the data in the RSEG s identified in the write operation is shared with the predecessor snapshot then control passes to operation and the data is copied from the source volume RSEG to the predecessor snapshot. Control then passes to operation and the share bits in the source volume and predecessor snapshot RSEG are updated to reflect that the source volume no longer shares data with a predecessor snapshot.

If at operation the snapshot does not share data with the source volume predecessor then control passes to operation and the data from the RSEG in the snapshot selected for the restore operation is copied to the source volume. By contrast if at operation data is not shared then control passes to operation .

At operation the IRWrite bit in the source volume is set to a value that indicates that data in the source disk has been restored and at operation the data associated with the write operation is written to the source volume.

If at operation the IRWrite bit in the source volume for the RSEG s identified in the read operation is set to a value that indicates that the RSEG s on the source volume have been written by the host during the restore operation then control passes to operation and the source volume LMAP may be used to locate the data for the read operation. By contrast if at operation the IRWrite bit is not set to a value that indicates that the RSEG on the source volume has been written by the host during the restore operation then control passes to operation and starting at the snapshot that is the source of the restore the share bit value in the RSD is checked and the successor pointer in the LDSB is traversed until a snapshot that holds data in the RSEG is located. At operation the data is returned to the host.

Thus the operations of permit a storage controller to manage input output operations while restore a source volume to a data state represented by a point in time copy i.e. a snapshot of the source volume. Because the copy operations are implemented in a background copy process and the source volume remains accessible during the background copy process the restore operation appears as a substantially instant data restore operation to a user of the system. Because Instant Restore provides read and write access to all of the snapshot s data through the source volume before the background copy operation has completed it allows host applications the ability to instantly verify the integrity of the restored data. If the data from the snapshot is also corrupt or does not meet the applications needs the user may initiate a restore from another snapshot without waiting for the background copy to complete. This allows the user to try many restore points in a short amount of time to determine which one is best. In normal mode the data integrity of all snapshots mirror clones and source volume in the entire snaptree is maintained throughout.

Although the described arrangements and procedures have been described in language specific to structural features and or methodological operations it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or operations described. Rather the specific features and operations are disclosed as preferred forms of implementing the claimed present subject matter.


---

title: Method and system for improved resource giveback
abstract: A method and system performs a sendhome procedure for giving back resources to a node that had been restored after a takeover of that node's resources is provided. Critical operations that may be running are completed prior to the sendhome process. An ownership module consults information in an ownership table about each resource, e.g. a data container. A data container, such as a root aggregate of the waiting node is identified and sent back first, after which the node is booted. When the node has been successfully booted, the remaining aggregates are sent back one at a time until the full compliment of aggregates has been returned. A veto of the sendhome procedure can be invoked by a subsystem that is performing a critical operation prior to the sendhome of the root aggregate and each individual other aggregate.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07546302&OS=07546302&RS=07546302
owner: NetApp, Inc.
number: 07546302
owner_city: Sunnyvale
owner_country: US
publication_date: 20061130
---
U.S. patent application Ser. No. 11 606 727 filed on even date herewith entitled SYSTEM AND METHOD FOR STORAGE TAKEOVER by Susan M. Coatney et al. which is presently incorporated by reference herein in its entirety and

U.S. patent application Ser. No. 11 606 538 filed on even date herewith entitled SYSTEM AND METHOD FOR MAINTAINING DISK LOCATION VIA HOMENESS by Steven S. Watanabe et al. which is presently incorporated by reference herein in its entirety.

The present invention relates to networked storage systems and more particularly to failover protection in clustered storage systems.

A storage system is a computer that provides storage service relating to the organization of information on writeable persistent storage devices such as memories tapes or disks. The storage system is commonly deployed within a storage area network SAN or a network attached storage NAS environment. When used within a NAS environment the storage system may be embodied as a file server including an operating system that implements a file system to logically organize the information as a hierarchical structure of directories and files on e.g. the disks. Each on disk file may be implemented as a set of data structures e.g. disk blocks configured to store information such as the actual data for the file. A directory on the other hand may be implemented as a specially formatted file in which information about other files and directories are stored.

The file server or filer may be further configured to operate according to a client server model of information delivery to thereby allow many client systems clients to access shared resources such as files stored on the filer. Sharing of files is a hallmark of a NAS system which is enabled because of semantic level of access to files and file systems. Storage of information on a NAS system is typically deployed over a computer network comprising a geographically distributed collection of interconnected communication links such as Ethernet that allow clients to remotely access the information files on the file server. The clients typically communicate with the filer by exchanging discrete is frames or packets of data according to pre defined protocols such as the Transmission Control Protocol Internet Protocol TCP IP .

In the client server model the client may comprise an application executing on a computer that connects to the filer over a computer network such as a point to point link shared local area network wide area network or virtual private network implemented over a public network such as the Internet. NAS systems generally utilize file based access protocols therefore each client may request the services of the filer by issuing file system protocol messages in the form of packets to the file system over the network. By supporting a plurality of file system protocols such as the conventional Common Internet File System CIFS the Network File System NFS and the Direct Access File System DAFS protocols the utility of the filer may be enhanced for networking clients.

A SAN is a high speed network that enables establishment of direct connections between a storage system and its storage devices. The SAN may thus be viewed as an extension to a storage bus and as such an operating system of the storage system enables access to stored information using block based access protocols over the extended bus . In this context the extended bus is typically embodied as Fibre Channel FC or Ethernet media adapted to operate with block access protocols such as Small Computer Systems Interface SCSI protocol encapsulation over FC FCP or TCP IP Ethernet iSCSI . A SAN arrangement or deployment allows decoupling of storage from the storage system such as an application server and some level of storage sharing at the application server level. There are however environments wherein a SAN is dedicated to a single server. When used within a SAN environment the storage system may be embodied as a storage appliance that manages access to information in terms of block addressing on disks using e.g. a logical unit number LUN in accordance with one or more block based protocols such as FCP.

One example of a SAN arrangement including a multi protocol storage appliance suitable for use in the SAN is described in United States Patent Application Publication No. US2004 0030668 A1 filed on Feb. 14 2004 entitled MULTI PROTOCOL STORAGE APPLIANCE THAT PROVIDES INTEGRATED SUPPORT FOR FILE AND BLOCK ACCESS PROTOCOLS by Brian Pawlowski et al.

It is advantageous for the services and data provided by a storage system such as a storage node to be available for access to the greatest degree possible. Accordingly some storage systems provide a plurality of storage system nodes organized as a cluster with a first storage system node coupled to and cooperating with a second storage system node. Each storage system node is configured to takeover serving data access requests for the other storage system node if the other node fails. The storage nodes in the cluster notify one another of continued operation using a heartbeat signal exchanged over a cluster interconnect and a cluster switching fabric. If one of the storage system nodes detects the absence of a heartbeat from the other storage node over both the cluster interconnect and the cluster switching fabric a failure of the other node is assumed and a takeover procedure is initiated. The node failure is also usually confirmed by the surviving storage node using a mailbox mechanism of the other storage node to confirm that in fact a failure of the other storage node has occurred rather than simply a failure of the cluster node coupling.

Specifically the mailbox mechanism includes a set of procedures for determining the most up to date coordinating information through the use of one or more master mailbox disks. Such disks receive messages from the storage node with which they are associated in order to confirm that the node continues to be in communication with the disks and that the node continues to be capable of writing to other disk coupled to that node. Further details on the configuration and operation of the master mailbox disk are provided in commonly owned U.S. patent application Ser. No. 10 378 400 of Larson et al. for a SYSTEM AND METHOD FOR COORDINATING CLUSTER STATE INFORMATION filed on Mar. 3 2003 which is presently incorporated by reference herein in its entirety.

In some storage system architectures each storage node in the cluster is generally organized as a network element N module and a disk element D module . The N module includes functionality that enables the node to connect to clients over a computer network while each D module connects to one or more storage devices such as the disks. The disks are arranged as one or more aggregates containing one or more volumes. A file system architecture of this type is generally described in United States Patent Application Publication No. US 2002 0116593 entitled METHOD AND SYSTEM FOR RESPONDING TO FILE SYSTEM REQUESTS by M. Kazar et al. the contents of which are incorporated herein by reference in entirety .

Extensions to such architectures include the assignment of certain functionality to the D module which may have previously been performed by the N module. For example the N module is generally responsible for network connectivity while the D module performs functions relating to data containers and data access requests to those containers. In such configurations it may be desirable to further configure the D module such that it can perform a recovery procedure including takeover and giveback operations independent of the N module.

Once the failed node has been either replaced or repaired in accordance with the recovery procedure the failed node is typically brought back into service. Data containers such as disks and their associated volumes and or aggregates previously served by that failed node are returned to the now recovered node such that data access requests may once again be served by the recovered node. However returning a full compliment of aggregates and volumes back to the recovered node has a fairly substantial processing performance impact because of the many tasks which are required to be performed during node recovery. For example RAID assimilations for all of the aggregates are required to bring the aggregates online at once so that they may be served by the recovered node. Yet the aggregates are generally not available during performance of these tasks which can result in noticeable downtime to clients since service to data access requests is essentially disabled during the recovery procedure. Furthermore if the recovered node does not reboot after the giveback operation there may be additional downtime while the problem is detected and addressed.

There remains a need therefore for an improved method for giveback of data resources such as aggregates volumes and disks to a previously failed node after recovery of that node that does not have a significant adverse impact in terms of processing performance and noticeable downtime to clients.

The present invention overcomes the disadvantages of the prior art by providing an improved technique for resource give back also referred to herein as sendhome . This occurs after a takeover when the resources are being given back to a previously failed storage system node in the cluster once that node is brought back into service. During the takeover procedure a surviving storage system node i.e. the takeover node asserts ownership over the resources of the failed node and serves data access requests directed to those resources. The term resources as used herein includes disks volumes aggregates or other data containers and portions of disks volumes aggregates or other data containers. When the failed node has been recovered i.e. has been repaired replaced or otherwise brought back into service it is placed in a waiting for sendhome state until its resources are re assigned to it. Thus this node is referred to herein as the waiting node.

When the takeover node receives notice that the waiting node is ready to begin servicing its resources such as disks a failover monitor module of the takeover node triggers a sendhome procedure. In accordance with the invention the sendhome procedure first involves identifying the resources that need to be returned to the waiting node and this is illustratively accomplished by the failover monitor module of the takeover node consulting the RAID module to inter alia identify a root aggregate of the waiting node. The disks of the root aggregate are the first disks to be sent and thus reassigned to the waiting node. The waiting node is thereafter booted using the root aggregate. Prior to sending the root aggregate home one or more subsystems on the takeover node are given an opportunity to veto the sendhome if any long non restartable or other critical operations are in progress for the root aggregate. If such operations are in progress the subsystem performing such an operation can veto the sendhome in order to allow such processes to continue.

If the sendhome is allowed then the disks of the root aggregate associated with the waiting node are returned in parallel until the full compliment of the disks originally assigned to the waiting node are successfully returned and the waiting node can serve data access requests on its own for the root aggregate. Once the waiting node has fully started all services on that node the other non root aggregates are sent home by the takeover node in a similar fashion each subsystem is given the opportunity to veto the sendhome for that particular aggregate and if no subsystem vetoes the sendhome then the sendhome is performed. Other procedures are provided in accordance with the present invention to cover situations in which the waiting node must perform a takeover prior to all of its disks being returned during the sendhome procedure. Additionally an illustrative procedure for an early sendhome is provided in accordance with the invention.

More specifically the waiting node boots from compact flash and signals to the takeover node that it is ready to receive its aggregates back. The takeover node determines the root aggregate for the waiting node and then allows each subsystem to veto a sendhome. If none of the subsystems vetoes the sendhome then the root aggregate is sent back to the waiting node. The waiting node receives the root aggregate and then completely boots up by starting other subsystems as described in further detail herein. When such subsystems are up and running the waiting node signals this fact to the takeover node. The takeover node finds remaining aggregates belonging to the waiting node and sends each one back one at a time. For each aggregate it allows each subsystem to veto the sendhome for that aggregate.

The nodes are also coupled across a cluster interconnect which provides an additional communication path between the nodes. The cluster interconnect may be Fibre Channel FC InfiniBand or another suitable medium. The cluster interconnect may be used to provide heartbeat signals heartbeats between the two nodes. The heartbeats are used to monitor the active state of each node. The cluster heartbeats are also sent across the cluster switching fabric over which communications between an N module and D module are illustratively effected through remote message passing over the cluster switching fabric which is sometimes also referred to as the storage layer. The death failure of a node is indicated by the loss of heartbeat from both the cluster interconnect and the storage layer. The cluster interconnect is sometimes also referred to as the storage takeover interconnect because as described further hereinafter if the heartbeat terminates i.e. times out then a takeover procedure is enabled.

The clients may be general purpose computers configured to interact with the nodes in accordance with a client server model of information delivery. That is each client may request the services of the node and the node may return the results of the services requested by the client by exchanging packets over the network . The client may issue packets including file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively the client may issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks.

During normal cluster operation the node e.g. node that is connected to a set of disks is identified as the home of the disks . That node is also identified as the current owner at initialization and is primarily responsible for servicing data requests directed to blocks on volumes contained on its set of the disks. Further details about this aspect of ownership of a disk are provided in the above cited U.S. patent application Ser. No. 11 606 538 entitled SYSTEM AND METHOD FOR MAINTAINING DISK LOCATION VIA HOMENESS. For example the node is primarily responsible for the volumes of the disk array which are represented as disk . Similarly the node is primarily responsible for the disks in the volumes represented as disk in . The cluster is configured such that either node or can take over data servicing capabilities for the other node in the event of a failure in the manner described further herein. Notably in a multiple node cluster a failed node s disks may not all be claimed by a single takeover node. Instead some of the disks may be claimed by a first takeover node with the remaining disks being claimed by a second takeover node for example. This may be used for load balancing of I O traffic.

Each storage system node is illustratively embodied as a dual processor storage system executing a storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named data containers such as directories files and special types of files called virtual disks hereinafter generally blocks on the disks. However it will be apparent to those of ordinary skill in the art that the node may alternatively comprise a single or more than two processor system. Illustratively one processor executes the functions of the N module on the node while the other processor executes the is functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures associated with the present invention. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node by inter alia invoking storage operations in support of the storage service implemented by the node. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein.

The network adapter comprises a plurality of ports adapted to couple the node to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively the computer network may be embodied as an Ethernet network or a FC network. Each client may communicate with the node over network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the disks . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

Storage of information on each disk array is preferably implemented as one or more storage volumes that comprise a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number vbn space on the volume s . Each logical volume is generally although not necessarily associated with its own file system. The disks within a logical volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID . Most RAID implementations such as a RAID 4 level implementation enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of parity information with respect to the striped data once all of the disks in a given RAID group are assimilated. An illustrative example of a RAID implementation is a RAID 4 level implementation although it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.

To facilitate access to the disks the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by disks . The file system logically organizes the information as a hierarchical structure of named data containers such as directories and files on the disks. Each on disk file may be implemented as a set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as a hierarchical structure of data containers such as blocks on the disks that are exported as named logical unit numbers luns .

In the illustrative embodiment the storage operating system is preferably the NetApp Data ONTAP operating system available from Network Appliance Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term WAFL is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

In addition the storage operating system includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on the disks of the node . As described further herein the software layers of the storage server are illustratively embodied as a D module . The storage server illustratively comprises a file system module . The file system module interacts in cooperating relation with a volume striping module VSM a RAID system module and a disk driver system module . The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations while the disk driver system implements a disk access protocol such as e.g. the Small Computer System Interface SCSI protocol.

The VSM illustratively implements a striped volume set SVS and as such cooperates with the file system to enable storage server to service a volume of the SVS. In particular the VSM implements a Locate function to compute the location of data container content in the SVS volume to thereby ensure consistency of such content served by the cluster.

A disk ownership module cooperates with the disk driver system to identify the appropriate D module for processing data access requests for particular volumes on the disk array . To that end the ownership module consults an associated data structure illustratively implemented as a disk ownership table which contains disk ownership information that is illustratively generated at boot up time and that is updated by various components of the storage operating system to reflect changes in ownership of disks. It should be understood that the data structure implemented as the table may implemented in a configuration other than a table in alternative embodiment of the invention. A disk iterator module and a disk migration module cooperate to identify ownership information in the ownership layer and to change on disk reservations and ownership information in response to a takeover procedure or a sendhome procedure.

In other words the disk ownership module includes program instructions for writing predefined ownership information at a proper location on each disk such a sector on the disk such as the disk platter and which sector is a portion of the media identified schematically by reference character in referred to herein as ownership location . The disk ownership module also includes program instructions for asserting and eliminating SCSI reservation tags in response to commands received and generated by its disk iterator and disk migration module . In a non SCSI environment appropriate software and or firmware can be used to assert ownership on the disks by the disk elements.

A takeover or sendhome procedure is initiated and controlled by a takeover monitor process in accordance with a set of routines stored in an associated data structure which is illustratively implemented as the takeover monitor resource table as described in further detail herein. The routines are summarized illustratively in the above cited U.S. patent application Ser. No. 11 606 727. Prior to initiating the sendhome procedure for each aggregate another data structure is consulted to determine if any subsystem vetoes the sendhome procedure. This data structure is illustratively implemented as the sendhome veto table . The table sets forth the subsystems that are given veto authority over a sendhome procedure. More specifically subsystems may be running long non restartable or critical operations at the time that a sendhome is to be initiated. If a sendhome of that aggregate were to be initiated there could be negative consequences due to the interruption of such critical operations. Thus such subsystems are programmed to follow a sendhome veto process prior to committing to the sendhome process. Further details of this sendhome veto process are provided in the description of .

Initially the disk ownership table is generated upon boot up of the system. More specifically I O services of the disk driver system query all devices e.g. disks attached to the system. This query requests information as to the nature of the attached disks. Upon completion of the query the ownership module instructs the disk driver system to read the ownership information from each disk. In response the disk driver system reads the ownership information from each disk from ownership location and creates the entries in the disk ownership table .

Subsequently the ownership module accesses the disk ownership table to extract the identification of all disks that are owned by the appropriate D module. The ownership module then verifies the SCSI reservations on each disk owned by that D module by reading the ownership information stored on disk. If the SCSI reservations and ownership information do not match the ownership module changes the SCSI reservation to match the on disk ownership information. Once the SCSI reservations and the on disk ownership information match for all disks identified as owned by the D module the ownership module then passes the information to the file system and the RAID module which configure the individual disks into the appropriate RAID groups and volumes for the D module .

Referring again to the takeover monitor process operates in conjunction with a cluster fabric CF interface module to monitor the heartbeats between the node and the one or more other nodes in the cluster. If the absence of a heartbeat is detected the takeover monitor process initiates the takeover procedure. In addition the takeover monitor is also responsive to a storage takeover command by e.g. an administrator. In response to lack of heartbeat or issuance of a storage takeover command the takeover procedure is enabled and takeover processing begins with the takeover monitor process invoking appropriate takeover routines as defined by the takeover monitor resource table . The takeover routines are thereafter executed in the manner described herein.

The file system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The vdisk module enables access by administrative interfaces such as a user interface of a management framework see in response to a user system administrator issuing commands to the node . The SCSI target module is generally disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as blocks.

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks. That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as mirroring and or parity RAID . The file system illustratively implements the WAFL file system hereinafter generally the write anywhere file system having an on disk format representation that is block based using e.g. 4 kilobyte kB blocks and using index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store metadata describing the layout of its file system these metadata files include among others an inode file. A file handle i.e. an identifier that includes an inode number is used to retrieve an inode from disk.

Broadly stated all inodes of the write anywhere file system are organized into the inode file. A file system fs info block specifies the layout of information in the file system and includes an inode of a file that includes all other inodes of the file system. Each logical volume file system has an fsinfo block that is preferably stored at a fixed location within e.g. a RAID group. The inode of the inode file may directly reference point to data blocks of the inode file or may reference indirect blocks of the inode file that in turn reference data blocks of the inode file. Within each data block of the inode file are embedded inodes each of which may reference indirect blocks that in turn reference data blocks of a file.

Operationally a request from the client is forwarded as a packet over the computer network and onto the node where it is received at the network adapter . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write anywhere file system . Here the file system generates operations to load retrieve the requested data from disk if it is not resident in core i.e. in memory . If the information is not in memory the file system indexes into the inode file using the inode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the network .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by node in response to a request issued by client . Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the node. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment and a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

In the illustrative embodiment the storage server is embodied as D module of the storage operating system to service one or more volumes of array . In addition the multi protocol engine is embodied as N module to i perform protocol termination with respect to a client issuing incoming data access request packets over the network as well as ii redirect those data access requests to any storage server of the cluster . Moreover the N module and D module cooperate to provide a highly scalable distributed storage system architecture of the cluster . To that end each blade includes a CF interface module adapted to implement intra cluster communication among the N and D modules including D module to D module communication for data container striping operations.

The protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of the N module function as protocol servers that translate file based and block based data access requests from clients into CF protocol messages used for communication with the D module . That is the N module servers convert the incoming data access requests into file system primitive operations commands that are embedded within CF messages by the CF interface module for transmission to the D modules of the cluster . Notably the CF interface modules cooperate to provide a single file system image across all D modules in the cluster . Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module of the cluster.

Further to the illustrative embodiment the N module and D module are implemented as separately scheduled processes of storage operating system however in an alternate embodiment the modules may be implemented as source code modules within a single operating system process. Communication between an N module and D module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . As noted the cluster switching fabric is also used as a second medium over which heartbeats between the nodes are transmitted and received. A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from Network Appliance Inc. The SpinFS protocol is described in the above referenced U.S. Patent Application Publication No. US 2002 0116593.

The CF interface module implements the CF protocol for communicating file system commands among the modules of cluster . Communication is illustratively effected by the D module exposing the CF API to which an N module or another D module issues calls. To that end the CF interface module is organized as a CF encoder and CF decoder. The CF encoder of e.g. CF interface on N module encapsulates a CF message as i a local procedure call LPC when communicating a file system command to a D module residing on the same node or ii a remote procedure call RPC when communicating the command to a D module residing on a remote node of the cluster . In either case the CF decoder of CF interface on D module de encapsulates the CF message and processes the file system command.

A data container e.g. a file is accessed in the file system using a data container handle. is a schematic block diagram illustrating the format of a data container handle including a SVS ID field an inode number field a unique ifier field a striped flag field and a striping epoch number field . The SVS ID field contains a global identifier within the cluster of the SVS within which the data container resides. The inode number field contains an inode number of an inode within an inode file pertaining to the data container. The unique ifier field contains a monotonically increasing number that uniquely identifies the data container handle . The unique ifier is particularly useful in the case where an inode number has been deleted reused and reassigned to a new data container. The unique ifier distinguishes that reused inode number in a particular data container from a potentially previous use of those fields. The striped flag field is illustratively a Boolean value that identifies whether the data container is striped or not. The striping epoch number field indicates the appropriate striping technique for use with this data container for embodiments where the SVS utilizes differing striping techniques for different data containers.

Whereas the aggregate is analogous to a physical volume of a conventional storage system a flexible volume is analogous to a file within that physical volume. That is the aggregate may include one or more files wherein each file contains a flexible volume and wherein the sum of the storage space consumed by the flexible volumes is physically smaller than or equal to the size of the overall physical volume. The aggregate utilizes a physical pvbn space that defines a storage space of blocks provided by the disks of the physical volume while each embedded flexible volume within a file utilizes a logical vvbn space to organize those blocks e.g. as files. Each vvbn space is an independent set of numbers that corresponds to locations within the file which locations are then translated to dbns on disks. Since the flexible volume is also a logical volume it has its own block allocation structures e.g. active space and summary maps in its vvbn space.

A container file is a file in the aggregate that contains all blocks used by a flexible volume. The container file is an internal to the aggregate feature that supports a flexible volume illustratively there is one container file per flexible volume. Similar to a pure logical volume in a file approach the container file is a hidden file not accessible to a user in the aggregate that holds every block in use by the flexible volume. The aggregate includes an illustrative hidden metadata root directory that contains subdirectories of flexible volumes 

Specifically a physical file system WAFL directory includes a subdirectory for each flexible volume in the aggregate with the name of subdirectory being a file system identifier fsid of the flexible volume. Each fsid subdirectory flexible volume contains at least two files a file system file and a storage label file. The storage label file is illustratively a 4 kB file that contains metadata similar to that stored in a conventional raid label. In other words the storage label file is the analog of a raid label and as such contains information about the state of the flexible volume such as e.g. the name of the flexible volume a universal unique identifier uuid and fsid of the flexible volume whether it is online being created or being destroyed etc.

Each fsinfo block includes a block pointer to an inode file that contains inodes of a plurality of files including an owner map an active map a summary map and a space map as well as other special meta data files. The inode file further includes a root directory and a hidden meta data root directory the latter of which includes a namespace having files related to a flexible volume in which users cannot see the files. The hidden meta data root directory includes the WAFL fsid directory structure that contains file system file and storage label file . Note that root directory in the aggregate is empty all files related to the aggregate are organized within the hidden meta data root directory .

In addition to being embodied as a container file having level 1 blocks organized as a container map the filesystem file includes block pointers that reference various file systems embodied as flexible volumes . The aggregate maintains these flexible volumes at special reserved inode numbers. Each flexible volume also has special reserved inode numbers within its flexible volume space that are used for among other things the block allocation bitmap structures. As noted the block allocation bitmap structures e.g. active map summary map and space map are located in each flexible volume.

Specifically each flexible volume has the same inode file structure content as the aggregate with the exception that there is no owner map and no WAFL fsid filesystem file storage label file directory structure in a hidden meta data root directory . To that end each flexible volume has a volinfo block that points to one or more fsinfo blocks each of which may represent a snapshot along with the active file system of the flexible volume. Each fsinfo block in turn points to an inode file that as noted has the same inode structure content as the aggregate with the exceptions noted above. Each flexible volume has its own inode file and distinct inode space with corresponding inode numbers as well as its own root fsid directory and subdirectories of files that can be exported separately from other flexible volumes.

The storage label file contained within the hidden meta data root directory is of the aggregate is a small file that functions as an analog to a conventional raid label. A raid label includes physical information about the storage system such as the volume name that information is loaded into the storage label file . Illustratively the storage label file includes the name of the associated flexible volume the online offline status of the flexible volume and other identity and state information of the associated flexible volume whether it is in the process of being created or destroyed .

The VLDB is a database process that tracks the locations of various storage components e.g. SVSs flexible volumes aggregates etc. within the cluster to thereby facilitate routing of requests throughout the cluster. In the illustrative embodiment the N module of each node accesses a configuration table that maps the SVS ID of a data container handle to a D module that owns services the data container within the cluster. The VLDB includes a plurality of entries which in turn provide the contents of entries in the configuration table among other things these VLDB entries keep track of the locations of the flexible volumes hereinafter generally volumes and aggregates within the cluster. Examples of such VLDB entries include a VLDB volume entry and a VLDB aggregate entry .

Notably the VLDB illustratively implements a RPC interface e.g. a Sun RPC interface which allows the N module to query the VLDB . When encountering contents of a data container handle that are not stored in its configuration table the N module sends an RPC to the VLDB process. In response the VLDB returns to the N module the appropriate mapping information including an ID of the D module that owns the data container. The N module caches the information in its configuration table and uses the D module ID to forward the incoming request to the appropriate data container. Thus after a takeover or sendhome procedure in accordance with the invention the N module is notified of the newly assigned D module when the VLDB is updated at the D module ID field of the VLDB aggregate entry .

The functions and interactions between the N module and D module are coordinated on a cluster wide basis through the collection of management processes and RDB library user mode applications. To that end the management processes have interfaces to are closely coupled to RDB . The RDB comprises a library that is provides a persistent object store storing of objects for the management of data processed by the management processes. Notably the RDB replicates and synchronizes the management data object store access across all nodes of the cluster to thereby ensure that the RDB database image is identical on all of the nodes . At system startup each node records the status state of its interfaces and IP addresses those IP addresses it owns into the RDB database.

The procedure then continues to step in which the takeover monitor queries the RAID subsystem to identify the root aggregate for the waiting node. In step each subsystem listed in the sendhome veto table is given an opportunity to veto the sendhome for the root aggregate. A query is performed at the takeover node to determine whether any critical operations are occurring on the takeover node such that subsystems running such critical operations have a sendhome veto. More specifically the failover monitor calls a veto routine in each subsystem that has been programmed with a sendhome veto. This routine provides a return code notifying the failover monitor to continue or to abort. As noted the sendhome veto is used to avoid interruption of critical operations. Such critical operations take a comparatively substantial amount of time to complete and thus it would not be efficient to interrupt such processes to begin a sendhome procedure.

If any subsystem vetoes the sendhome of the root aggregate by notifying the failover monitor using a return code for this purpose then the sendhome is aborted and nothing more is done as shown in step . If no subsystem vetoes the sendhome of the root aggregate then continues on as in step . Next in accordance with step the failover monitor notifies the storage operating system and a subsystem such as a RAID subsystem to offline the root aggregate. In accordance with step the failover monitor next iterates the disks comprising the root aggregate and changes the ownership of the disks so that they are indicated as being owned by the waiting node. In step the VLDB is updated to reflect that the takeover node is no longer the assigned D module for that root aggregate.

The takeover node now performs the remainder of the sendhome procedure. This aspect of the procedure is illustrated in which together form a flow chart of a procedure by which the takeover node first allows subsystems to veto the sendhome and in the absence of such a veto completes the sendhome in accordance with an illustrative embodiment of the invention. The procedure begins at step and continues to step in which the takeover monitor queries the RAID subsystem to identify each aggregate that belongs to the waiting node. For each such aggregate each subsystem listed in the sendhome veto table is allowed to veto the sendhome for that aggregate e.g. because of long running operations. If any subsystem vetoes the sendhome of the aggregate step then the sendhome of that specific aggregate is aborted but processing will continue for the remaining aggregates. Thus for those aggregates that are not subject to a veto the procedure continues for each such aggregate in accordance with step in which the failover monitor notifies the storage operating system and the RAID subsystem to offline the respective aggregate.

In step the failover monitor iterates the disks comprising the aggregate and changes the ownership field in the ownership entry in the disk ownership table for each disk to reflect that they are now owned by the waiting node. In accordance with step the VLDB is updated to reflect that the takeover node is no longer the assigned D module for that aggregate. Once all the aggregates have been sent home then an appropriate number of spare disks are returned to the waiting node by changing the ownership of the spare disks as desired in a particular application of the invention. It is noted that after the aggregates are sent home to the waiting node then in the illustrative implementation on the takeover node the space for the returned aggregates is cleaned up.

Now all disks are back to being owned by the waiting node. Each N module interfacing with one or more clients can begin sending data access requests to the waiting node which is now serving its originally owned disks.

Notably during the sendhome procedure and while the new disks are arriving at the waiting node a short delay of illustratively about 5 seconds is provided to allow sufficient time for all the disks to arrive to thereby prevent premature RAID assimilation which could result in a degraded aggregate. In addition it is noted that if the root aggregate cannot be formed or located then the waiting node will remain in its waiting for sendhome state.

In a multiple node cluster a failed node s disks may not all be claimed by a single takeover node. Instead some of the disks may be claimed by a first takeover node with the remaining disks being claimed by a second takeover node for example. This may be used for load balancing of I O traffic. In such a case each takeover node will perform its own sendhome procedure in accordance with the flow charts herein described to return the resources to the waiting node. There may be other instances in which an administrator decides not to return all resources to the waiting node but instead returns less than it originally served and this may be done for load balancing or other reasons in a particular application of the invention.

As noted the procedures to be followed to determine whether a sendhome veto is to be invoked are set forth in the sendhome veto table running on each D module. An appropriate API is utilized to communicate between layers of the operating system to notify the various subsystems of the sendhome process and the sendhome veto process.

Illustratively the performance of takeover and sendhome procedures is reported via a message from the file system of the node performing the procedure to a central administrator. Thus to the extent that takeover or sendhome fails to complete the administrator can issue commands to ensure another D module to service these unassigned disks.

Advantageously the present invention reduces processing impact through an improved method for returning or sending home resources such as aggregates volumes files and disks to a previously failed node after recovery of that node. In addition the technique of sending aggregates back one at a time allows greater access to more of the aggregates for clients and reduces downtime for files volumes and aggregates associated with the nodes involved in the novel procedure.

The foregoing description has been directed to particular embodiments of the invention. It will be apparent however that other variations and modifications may be made to the described embodiments with the attainment of some or all of their advantages. Specifically it should be noted that the principles of the invention may be implemented in a non distributed file system. Furthermore while this description has been written in terms of D and N modules the teachings of the present invention are equally suitable to systems in which the functionality of the N and D modules are implemented in a single system. Alternatively the functions of the N and D modules may be distributed among a number of separate systems wherein each system performs one or more functions. Additionally the features of the present invention have been described with respect to a cluster containing two nodes however it is equally applicable to clusters including a plurality of nodes which allow for an n way failover. Furthermore the procedures processes and or modules described herein may be implemented in hardware software embodied as a computer readable medium having program instructions for one or a combination thereof. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention.


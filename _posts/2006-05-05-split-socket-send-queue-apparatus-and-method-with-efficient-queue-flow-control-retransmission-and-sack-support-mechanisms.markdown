---

title: Split socket send queue apparatus and method with efficient queue flow control, retransmission and sack support mechanisms
abstract: A mechanism for offloading the management of send queues in a split socket stack environment, including efficient split socket queue flow control and TCP/IP retransmission support. An Upper Layer Protocol (ULP) creates send work queue entries (SWQEs) for writing to the send work queue (SWQ). The Internet Protocol Suite Offload Engine (IPSOE) is notified of a new entry to the SWQ and it subsequently reads this entry that contains pointers to the data that is to be transmitted. After the data is transmitted and acknowledgments are received, the IPSOE creates a completion queue entry (CQE) that is written into the completion queue (CQ). The flow control between the ULP and the IPSOE is credit based. The passing of CQ credits is the only explicit mechanism required to manage flow control of both the SWQ and the CQ between the ULP and the IPSOE.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07818362&OS=07818362&RS=07818362
owner: International Business Machines Corporation
number: 07818362
owner_city: Armonk
owner_country: US
publication_date: 20060505
---
This application is a divisional of application Ser. No. 10 235 689 filed Sep. 5 2002 now U.S. Pat. No. 7 519 650 which is herein incorporated by reference.

The present invention relates generally to communication protocols between a host computer and an input output I O device. More specifically the present invention provides a hardware implementation for offloading management of a send queue. In particular the present invention provides a mechanism by which work requests are turned into work queue entries WQEs and are passed from Upper Layer Protocol e.g. sockets software to an Internet Protocol IP Suite Offload Engine IPSOE . The present invention also provides a mechanism by which completed WQEs are passed back to the Upper Layer Protocol ULP software. The present invention also provides a mechanism for supporting Selective Acknowledgments.

In an Internet Protocol IP Network the software provides a message passing mechanism that can be used to communicate with input output devices general purpose computers host and special purpose computers. The message passing mechanism consists of a transport protocol an upper level protocol and an application programming interface. The key standard transport protocols used on IP networks today are the Transmission Control Protocol TCP and the User Datagram Protocol UDP . TCP provides a reliable service and UDP provides an unreliable service. In the future the Stream Control Transmission Protocol SCTP will also be used to provide a reliable service. Processes executing on devices or computers access the IP network through upper level protocols such as Sockets iSCSI and Direct Access File System DAFS .

Unfortunately the TCP IP software consumes a considerable amount of processor and memory resources. This problem has been covered extensively in the literature see J. Kay J. Pasquale Profiling and reducing processing overheads in TCP IP IEEE ACM Transactions on Networking Vol. 4 No. 6 pp. 817 828 December 1996 and D. D. Clark V. Jacobson J. Romkey H. Salwen An analysis of TCP processing overhead IEEE Communications Magazine volume 27 Issue 6 June 1989 pp 23 29 . In the future the network stack will continue to consume excessive resources for several reasons including increased use of networking by applications use of network security protocols and the underlying fabric bandwidths are increasing at a higher rate than microprocessor and memory bandwidths. To address this problem the industry is offloading the network stack processing to an IP Suite Offload Engine IPSOE .

There are two offload approaches being taken in the industry. The first approach uses the existing TCP IP network stack without adding any additional protocols. This approach can offload TCP IP to hardware but unfortunately does not remove the need for receive side copies. As noted in the papers above copies are one of the largest contributors to central processing unit CPU and memory bandwidth utilization. To remove the need for copies the industry is pursuing the second approach that consists of adding Framing Direct Data Placement DDP and Remote Direct Memory Access RDMA over the TCP and SCTP protocols. The IP Suite Offload Engine IPSOE required to support these two approaches is similar the key difference being that in the second approach the hardware must support the additional protocols.

The IPSOE provides a message passing mechanism that can be used by sockets Internet Small Computer System Interface iSCSI Direct Access File Systems DAFS and other Upper Layer Protocols ULPs to communicate between nodes. Processes executing on host computers or devices access the IP network by posting send receive messages to send receive work queues on an IPSOE. These processes also are referred to as consumers .

The send receive work queues WQ are assigned to a consumer as a queue pair QP . The messages can be sent over three different transport types traditional TCP RDMA TCP UDP or SCTP. Consumers retrieve the results of these messages from a completion queue CQ through IPSOE send and receive work completion WC queues. The source IPSOE takes care of segmenting outbound messages and sending them to the destination. The destination IPSOE takes care of reassembling inbound messages and placing the inbound messages in the memory space designated by the destination s consumer. These consumers use IPSOE verbs to access the functions supported by the IPSOE. The software that interprets verbs and directly accesses the IPSOE is known as the IPSO interface IPSOI .

Today the host CPU performs most IP suite processing. IP Suite Offload Engines offer a higher performance interface for communicating to other general purpose computers and I O devices. Data sends or receives through the IPSOE require that the CPU either copy data from one memory location to another or register the memory so that the IPSOE can directly access the memory region. Each of these options requires significant CPU resources with the memory registration option being preferred for large memory transfers however as network speeds increase the amount of CPU resources required will increase. A simple mechanism is needed to implement Send Queue in the IPSOE and perform RDMA DDP framing and TCP IP processing in the IPSOE. The mechanism needs to maintain all RDMA DDP framing TCP IP and Ethernet state in the IPSOE. It must also provide the necessary protection to support out of user space Send Queue operations. The present invention also provides a mechanism for supporting Selective Acknowledgments.

The present invention provides a method computer program product and distributed data processing system for management of a send queue in a split socket stack in order to reduce the processing overhead in host processors. Specifically the present invention is directed to a mechanism for offloading the management of send queues in a split socket stack environment including efficient split socket queue flow control and TCP IP retransmission support.

With the present invention an Upper Layer Protocol ULP creates send work queues and completion queues that are utilized by an Internet Protocol Suite Offload Engine IPSOE and the ULP to transfer information and carry out send operations. As consumers initiate send operations send work queue entries SWQEs are created by the ULP and written to the send work queue SWQ . The ISPOE is notified of a new entry to the SWQ and it subsequently reads this entry that contains pointers to the data that is to be transmitted.

After the data is transmitted and acknowledgments are received the IPSOE creates a completion queue entry CQE that is written into the completion queue CQ . The CQE includes a Work Request ID that associates a given SWQE to a CQE. After the CQE is written the ULP subsequently processes the entry and removes it from the CQE freeing up a space in both the SWQ and CQ. The number of entries available in the SWQ are monitored by the ULP so that it does not overwrite any valid entries. Likewise the IPSOE monitors the number of entries available in the CQ so as not overwrite the CQ.

The flow control between the ULP and the IPSOE is credit based. The communication of credits between the ULP and the IPSOE is partly explicit and partly implicit. This is facilitated by an assumed association between SWQ and CQ updates. The passing of CQE s by the IPSOE to the ULP implies a passing of both SWQ and CQ credits. The passing of work requests by the ULP to the IPSOE implies a passing of SWQE credits from the ULP to the IPSOE. CQ credits are passed explicitly from the ULP to the IPSOE. Thus the passing of CQ credits is the only explicit mechanism required to manage flow control of both the SWQ and the CQ between the ULP and the IPSOE.

The IPSOE also maintains a correlation between the TCP sequence number reflected in TCP acknowledgments and the corresponding byte position in the send buffers. This allows the IPSOE to support TCP retransmission without buffering transmitted data until the corresponding acknowledgments or selective acknowledgments are received. The IPSOE retransmit mechanism also supports the TCP IP Selective Acknowledgment SACK option. Buffering of transmitted data by the IPSOE is avoided both with and without the SACK option enabled.

The present invention provides a distributed computing system having endnodes switches routers and links interconnecting these components. The endnodes can be Internet Protocol Suite Offload Engines or traditional host software based Internet protocol suites. Each endnode uses send and receive queue pairs to transmit and receive messages. The endnodes segment the message into frames and transmit the frames over the links. The switches and routers interconnect the endnodes and route the frames to the appropriate endnode. The endnodes reassemble the frames into a message at the destination.

With reference now to the figures and in particular with reference to a diagram of a distributed computer system is illustrated in accordance with a preferred embodiment of the present invention. The distributed computer system represented in takes the form of an Internet protocol network IP net such as IP net and is provided merely for illustrative purposes and the embodiments of the present invention described below can be implemented on computer systems of numerous other types and configurations. For example computer systems implementing the present invention can range from a small server with one processor and a few input output I O adapters to massively parallel supercomputer systems with hundreds or thousands of processors and thousands of I O adapters. Furthermore the present invention can be implemented in an infrastructure of remote computer systems connected by an Internet or intranet.

IP net is a high bandwidth low latency network interconnecting nodes within the distributed computer system. A node is any component attached to one or more links of a network and forming the origin and or destination of messages within the network. In the depicted example IP net includes nodes in the form of host processor node host processor node and redundant array independent disk RAID subsystem node . The nodes illustrated in are for illustrative purposes only as IP net can connect any number and any type of independent processor nodes storage nodes and special purpose processing nodes. Any one of the nodes can function as an endnode which is herein defined to be a device that originates or finally consumes messages or frames in IP net .

In one embodiment of the present invention an error handling mechanism in distributed computer systems is present in which the error handling mechanism allows for TCP or SCTP communication between endnodes in a distributed computing system such as IP net .

A message as used herein is an application defined unit of data exchange which is a primitive unit of communication between cooperating processes. A frame is one unit of data encapsulated by Internet Protocol Suite headers and or trailers. The headers generally provide control and routing information for directing the frame through IP net . The trailer generally contains control and cyclic redundancy check CRC data for ensuring frames are not delivered with corrupted contents.

Within a distributed computer system IP net contains the communications and management infrastructure supporting various forms of traffic such as storage interprocess communications IPC file access and sockets. IP net shown in includes a switched communications fabric which allows many devices to concurrently transfer data with high bandwidth and low latency in a secure remotely managed environment. Endnodes can communicate over multiple ports and utilize multiple paths through the IP net fabric. The multiple ports and paths through the IP net shown in can be employed for fault tolerance and increased bandwidth data transfers.

The IP net in includes switch switch and router . A switch is a device that connects multiple links together and allows routing of frames from one link to another link using the layer destination address field. When the Ethernet is used as the link the destination field is known as the media access control MAC address. A router is a device that routes frames based on the layer destination address field. When Internet Protocol IP is used as the layer protocol the destination address field is an IP address.

In one embodiment a link is a full duplex channel between any two network fabric elements such as endnodes switches or routers. Example suitable links include but are not limited to copper cables optical cables and printed circuit copper traces on backplanes and printed circuit boards.

For reliable service types TCP and SCTP endnodes such as host processor endnodes and I O adapter endnodes generate request frames and return acknowledgment frames. Switches and routers pass frames along from the source to the destination.

In IP net as illustrated in host processor node host processor node and RAID subsystem node include at least one IPSOE to interface to IP net . In one embodiment each IPSOE is an endpoint that implements the IPSOI in sufficient detail to source or sink frames transmitted on IP net . Host processor node contains IPSOEs in the form of host IPSOE and IPSOE . Host processor node contains IPSOE and IPSOE . Host processor node also includes central processing units and a memory interconnected by bus system . Host processor node similarly includes central processing units and a memory interconnected by a bus system .

IPSOE provides a connection to switch while IPSOE provides a connection to switch and IP Suite Offload Engines and provide a connection to switches and .

In one embodiment an IP Suite Offload Engine is implemented in hardware or a combination of hardware and offload microprocessor s . In this implementation IP suite processing is offloaded to the IPSOE. This implementation also permits multiple concurrent communications over a switched network without the traditional overhead associated with communicating protocols. In one embodiment the IPSOEs and IP net in provide the consumers of the distributed computer system with zero processor copy data transfers without involving the operating system kernel process and employs hardware to provide reliable fault tolerant communications.

As indicated in router is coupled to wide area network WAN and or local area network LAN connections to other hosts or other routers. In this example RAID subsystem node in includes processor memory IP Suite Offload Engine IPSOE and multiple redundant and or striped storage disk unit .

IP net handles data communications for storage interprocessor communications file accesses and sockets. IP net supports high bandwidth scalable and extremely low latency communications. User clients can bypass the operating system kernel process and directly access network communication components such as IPSOEs which enable efficient message passing protocols. IP net is suited to current computing models and is a building block for new forms of storage cluster and general networking communication. Further IP net in allows storage nodes to communicate among themselves or communicate with any or all of the processor nodes in a distributed computer system. With storage attached to IP net the storage node has substantially the same communication capability as any host processor node in IP net .

In one embodiment the IP net shown in supports channel semantics and memory semantics. Channel semantics is sometimes referred to as send receive or push communication operations. Channel semantics are the type of communications employed in a traditional I O channel where a source device pushes data and a destination device determines a final destination of the data. In channel semantics the frame transmitted from a source process specifies a destination processes communication port but does not specify where in the destination processes memory space the frame will be written. Thus in channel semantics the destination process pre allocates where to place the transmitted data.

In memory semantics a source process directly reads or writes the virtual address space of a remote node destination process. The remote destination process need only communicate the location of a buffer for data and does not need to be involved in the transfer of any data. Thus in memory semantics a source process sends a data frame containing the destination buffer memory address of the destination process. In memory semantics the destination process previously grants permission for the source process to access its memory.

Channel semantics and memory semantics are typically both necessary for storage cluster and general networking communications. A typical storage operation employs a combination of channel and memory semantics. In an illustrative example storage operation of the distributed computer system shown in a host processor node such as host processor node initiates a storage operation by using channel semantics to send a disk write command to the RAID subsystem IPSOE . The RAID subsystem examines the command and uses memory semantics to read the data buffer directly from the memory space of the host processor node. After the data buffer is read the RAID subsystem employs channel semantics to push an I O completion message back to the host processor node.

In one exemplary embodiment the distributed computer system shown in performs operations that employ virtual addresses and virtual memory protection mechanisms to ensure correct and proper access to all memory. Applications running in such a distributed computer system are not required to use physical addressing for any operations.

Turning next to a functional block diagram of a host processor node is depicted in accordance with a preferred embodiment of the present invention. Host processor node is an example of a host processor node such as host processor node in . In this example host processor node shown in includes a set of consumers which are processes executing on host processor node . Host processor node also includes IP Suite Offload Engine IPSOE and IPSOE . IPSOE contains ports and while IPSOE contains ports and . Each port connects to a link. The ports can connect to one IP net subnet or multiple IP net subnets such as IP net in .

Consumers transfer messages to the IP net via the verbs interface and message and data service . A verbs interface is essentially an abstract description of the functionality of an IP Suite Offload Engine. An operating system may expose some or all of the verb functionality through its programming interface. Basically this interface defines the behavior of the host. Additionally host processor node includes a message and data service which is a higher level interface than the verb layer and is used to process messages and data received through IPSOE and IPSOE . Message and data service provides an interface to consumers to process messages and other data.

With reference now to a diagram of an IP Suite Offload Engine is depicted in accordance with a preferred embodiment of the present invention. IP Suite Offload Engine A shown in includes a set of queue pairs QPs A A which are used to transfer messages to the IPSOE ports A A. Buffering of data to IPSOE ports A A is channeled using the network layer s quality of service field QOSF for example the Traffic Class field in the IP Version 6 specification A A. Each network layer quality of service field has its own flow control. Internet Engineering Task Force IETF standard network protocols are used to configure the link and network addresses of all IP Suite Offload Engine ports connected to the network. Two such protocols are Address Resolution Protocol ARP and Dynamic Host Configuration Protocol. Memory translation and protection MTP A is a mechanism that translates virtual addresses to physical addresses and validates access rights. Direct memory access DMA A provides for direct memory access operations using memory A with respect to queue pairs A A.

A single IP Suite Offload Engine such as the IPSOE A shown in can support thousands of queue pairs. Each queue pair consists of a send work queue SWQ and a receive work queue RWQ . The send work queue is used to send channel and memory semantic messages. The receive work queue receives channel semantic messages. A consumer calls an operating system specific programming interface which is herein referred to as verbs to place work requests WRs onto a work queue.

Similarly depicts a router C according to a preferred embodiment of the present invention. Router C includes a frame relay C in communication with a number of ports C through network layer quality of service fields such as IP version 4 s Type of Service field C. Like switch B router C will generally be able to route frames from one port to any other port on the same router.

With reference now to a diagram illustrating processing of work requests is depicted in accordance with a preferred embodiment of the present invention. In a receive work queue send work queue and completion queue are present for processing requests from and for consumer . These requests from consumer are eventually sent to hardware . In this example consumer generates work requests and and receives work completion . As shown in work requests placed onto a work queue are referred to as work queue elements WQEs .

Send work queue contains work queue elements WQEs describing data to be transmitted on the IP net fabric. Receive work queue contains work queue elements WQEs describing where to place incoming channel semantic data from the IP net fabric. A work queue element is processed by hardware in the IPSOE.

The verbs also provide a mechanism for retrieving completed work from completion queue . As shown in completion queue contains completion queue elements CQEs . Completion queue elements contain information about previously completed work queue elements. Completion queue is used to create a single point of completion notification for multiple queue pairs. A completion queue element is a data structure on a completion queue. This element describes a completed work queue element. The completion queue element contains sufficient information to determine the queue pair and specific work queue element that completed. A completion queue context is a block of information that contains pointers to length and other information needed to manage the individual completion queues.

Example work requests supported for send work queue shown in are as follows. A send work request is a channel semantic operation to push a set of local data segments to the data segments referenced by a remote node s receive work queue element. For example work queue element contains references to data segment data segment and data segment . Each of the send work request s data segments contains part of a virtually contiguous memory region. The virtual addresses used to reference the local data segments are in the address context of the process that created the local queue pair.

A remote direct memory access RDMA read work request provides a memory semantic operation to read a virtually contiguous memory space on a remote node. A memory space can either be a portion of a memory region or portion of a memory window. A memory region references a previously registered set of virtually contiguous memory addresses defined by a virtual address and length. A memory window references a set of virtually contiguous memory addresses that have been bound to a previously registered region.

The RDMA read work request reads a virtually contiguous memory space on a remote endnode and writes the data to a virtually contiguous local memory space. Similar to the send work request virtual addresses used by the RDMA read work queue element to reference the local data segments are in the address context of the process that created the local queue pair. The remote virtual addresses are in the address context of the process owning the remote queue pair targeted by the RDMA read work queue element.

A RDMA write work queue element provides a memory semantic operation to write a virtually contiguous memory space on a remote node. For example work queue element in receive work queue references data segment data segment and data segment . The RDMA write work queue element contains a scatter list of local virtually contiguous memory spaces and the virtual address of the remote memory space into which the local memory spaces are written.

A RDMA FetchOp work queue element provides a memory semantic operation to perform an atomic operation on a remote word. The RDMA FetchOp work queue element is a combined RDMA Read Modify and RDMA Write operation. The RDMA FetchOp work queue element can support several read modify write operations such as Compare and Swap if equal. The RDMA FetchOp is not included in current RDMA over IP standardization efforts but is described here because it may be used as a value added feature in some implementations.

A bind unbind remote access key R Key work queue element provides a command to the IP Suite Offload Engine hardware to modify destroy a memory window by associating disassociating the memory window to a memory region. The R Key is part of each RDMA access and is used to validate that the remote process has permitted access to the buffer.

In one embodiment receive work queue shown in only supports one type of work queue element which is referred to as a receive work queue element. The receive work queue element provides a channel semantic operation describing a local memory space into which incoming send messages are written. The receive work queue element includes a scatter list describing several virtually contiguous memory spaces. An incoming send message is written to these memory spaces. The virtual addresses are in the address context of the process that created the local queue pair.

For interprocessor communications a user mode software process transfers data through queue pairs directly from where the buffer resides in memory. In one embodiment the transfer through the queue pairs bypasses the operating system and consumes few host instruction cycles. Queue pairs permit zero processor copy data transfer with no operating system kernel involvement. The zero processor copy data transfer provides for efficient support of high bandwidth and low latency communication.

When a queue pair is created the queue pair is set to provide a selected type of transport service. In one embodiment a distributed computer system implementing the present invention supports three types of transport services TCP SCTP and UDP.

TCP and SCTP associate a local queue pair with one and only one remote queue pair. TCP and SCTP require a process to create a queue pair for each process that TCP and SCTP are to communicate with over the IP net fabric. Thus if each of N host processor nodes contains P processes and all P processes on each node wish to communicate with all the processes on all the other nodes each host processor node requires P N 1 queue pairs. Moreover a process can associate a queue pair to another queue pair on the same IPSOE.

A portion of a distributed computer system employing TCP or SCTP to communicate between distributed processes is illustrated generally in . The distributed computer system in includes a host processor node a host processor node and a host processor node . Host processor node includes a process A . Host processor node includes a process C and a process D . Host processor node includes a process E .

Host processor node includes queue pairs and each having a send work queue and receive work queue. Host processor node has a queue pair and host processor node has queue pairs and . The TCP or SCTP of distributed computer system associates a local queue pair with one and only one remote queue pair. Thus the queue pair is used to communicate with queue pair queue pair is used to communicate with queue pair and queue pair is used to communicate with queue pair .

A WQE placed on one send queue in a TCP or SCTP causes data to be written into the receive memory space referenced by a receive WQE of the associated queue pair. RDMA operations operate on the address space of the associated queue pair.

In one embodiment of the present invention the TCP or SCTP is made reliable because hardware maintains sequence numbers and acknowledges all frame transfers. A combination of hardware and IP net driver software retries any failed communications. The process client of the queue pair obtains reliable communications even in the presence of bit errors receive underruns and network congestion. If alternative paths exist in the IP net fabric reliable communications can be maintained even in the presence of failures of fabric switches links or IP Suite Offload Engine ports.

In addition acknowledgments may be employed to deliver data reliably across the IP net fabric. The acknowledgment may or may not be a process level acknowledgment i.e. an acknowledgment that validates that a receiving process has consumed the data. Alternatively the acknowledgment may be one that only indicates that the data has reached its destination.

The User Datagram Protocol is connectionless. The UDP is employed by management applications to discover and integrate new switches routers and endnodes into a given distributed computer system. The UDP does not provide the reliability guarantees of the TCP or SCTP. The UDP accordingly operates with less state information maintained at each endnode.

Turning next to an illustration of a data frame is depicted in accordance with a preferred embodiment of the present invention. A data frame is a unit of information that is routed through the IP net fabric. The data frame is an endnode to endnode construct and is thus created and consumed by endnodes. For frames destined to an IPSOE the data frames are neither generated nor consumed by the switches and routers in the IP net fabric. Instead for data frames that are destined to an IPSOE switches and routers simply move request frames or acknowledgment frames closer to the ultimate destination modifying the link header fields in the process. Routers may modify the frame s network header when the frame crosses a subnet boundary. In traversing a subnet a single frame stays on a single service level.

Message data contains data segment data segment and data segment which are similar to the data segments illustrated in . In this example these data segments form a frame which is placed into frame payload within data frame . Additionally data frame contains cyclic redundancy check CRC which is used for error checking. Additionally routing header and transport header are present in data frame . Routing header is used to identify source and destination ports for data frame . Transport header in this example specifies the sequence number and the source and destination port number for data frame . The sequence number is initialized when communication is established and increments by 1 for each byte of frame header DDP RDMA header data payload and CRC. Frame header in this example specifies the destination queue pair number associated with the frame and the length of the Direct Data Placement and or Remote Direct Memory Access DDP RDMA header plus data payload plus CRC. DDP RDMA header specifies the message identifier and the placement information for the data payload. The message identifier is constant for all frames that are part of a message. Example message identifiers include for example send write RDMA and read RDMA.

In a portion of a distributed computer system is depicted to illustrate an example request and acknowledgment transaction. Distributed computer system in includes a host processor node running process A and a host processor node running process B . Host processor node includes an IPSOE . Host processor node includes an IPSOE . The distributed computer system in includes IP net fabric which includes switch and switch . The IP net fabric includes a link coupling IPSOE to switch a link coupling switch to switch and a link coupling IPSOE to switch .

In the example transactions host processor node includes a client process A. Host processor node includes a client process B. Client process A interacts with host IPSOE through queue pair comprising send queue and receive queue . Client process B interacts with host IPSOE through queue pair comprising send queue and receive queue . Queue pairs and are data structures that include a send work queue and a receive work queue.

Process A initiates a message request by posting work queue elements to the send queue of queue pair . Such a work queue element is illustrated in . The message request of client process A is referenced by a gather list contained in the send work queue element. Each data segment in the gather list points to part of a virtually contiguous local memory region which contains a part of the message such as indicated by data segments and which respectively hold message parts and in .

Hardware in host IPSOE reads the work queue element and segments the message stored in virtual contiguous buffers into data frames such as the data frame illustrated in . Data frames are routed through the IP net fabric and for reliable transfer services are acknowledged by the final destination endnode. If not successfully acknowledged the data frame is retransmitted by the source endnode. Data frames are generated by source endnodes and consumed by destination endnodes.

With reference to a diagram illustrating the network addressing used in a distributed networking system is depicted in accordance with the present invention. A host name provides a logical identification for a host node such as a host processor node or I O adapter node. The host name identifies the endpoint for messages such that messages are destined for processes residing on an endnode specified by the host name. Thus there is one host name per node but a node can have multiple IPSOEs.

A single link layer address e.g. Ethernet Media Access Layer Address is assigned to each port of an endnode component . A component can be an IPSOE switch or router. All IPSOE and router components must have a MAC address. A media access point on a switch is also assigned a MAC address.

One network address e.g. IP Address is assigned to each port of an endnode component . A component can be an IPSOE switch or router. All IPSOE and router components must have a network address. A media access point on a switch is also assigned a MAC address.

Each port of switch does not have a link layer address associated with it. However switch can have a media access port that has a link layer address and a network layer address associated with it.

A portion of a distributed computer system in accordance with a preferred embodiment of the present invention is illustrated in . Distributed computer system includes a subnet and a subnet . Subnet includes host processor nodes and . Subnet includes host processor nodes and . Subnet includes switches and . Subnet includes switches and .

Routers create and connect subnets. For example subnet is connected to subnet with routers and . In one example embodiment a subnet has up to 216 endnodes switches and routers.

A subnet is defined as a group of endnodes and cascaded switches that is managed as a single unit. Typically a subnet occupies a single geographic or functional area. For example a single computer system in one room could be defined as a subnet. In one embodiment the switches in a subnet can perform very fast wormhole or cut through routing for messages.

A switch within a subnet examines the destination link layer address e.g. MAC address that is unique within the subnet to permit the switch to quickly and efficiently route incoming message frames. In one embodiment the switch is a relatively simple circuit and is typically implemented as a single integrated circuit. A subnet can have hundreds to thousands of endnodes formed by cascaded switches.

As illustrated in for expansion too much larger systems subnets are connected with routers such as routers and . The router interprets the destination network layer address e.g. IP address and routes the frame.

An example embodiment of a switch is illustrated generally in . Each I O path on a switch or router has a port. Generally a switch can route frames from one port to any other port on the same switch.

Within a subnet such as subnet or subnet a path from a source port to a destination port is determined by the link layer address e.g. MAC address of the destination host IPSOE port. Between subnets a path is determined by the network layer address IP address of the destination IPSOE port and by the link layer address e.g. MAC address of the router port which will be used to reach the destination s subnet.

In one embodiment the paths used by the request frame and the request frame s corresponding positive acknowledgment ACK frame is not required to be symmetric. In one embodiment employing oblivious routing switches select an output port based on the link layer address e.g. MAC address . In one embodiment a switch uses one set of routing decision criteria for all its input ports in the switch. In one example embodiment the routing decision criteria are contained in one routing table. In an alternative embodiment a switch employs a separate set of criteria for each input port.

A data transaction in the distributed computer system of the present invention is typically composed of several hardware and software steps. A client process data transport service can be a user mode or a kernel mode process. The client process accesses IP Suite Offload Engine hardware through one or more queue pairs such as the queue pairs illustrated in and . The client process calls an operating system specific programming interface which is herein referred to as verbs. The software code implementing verbs posts a work queue element to the given queue pair work queue.

There are many possible methods of posting a work queue element and there are many possible work queue element formats which allow for various cost performance design points but which do not affect interoperability. A user process however must communicate to verbs in a well defined manner and the format and protocols of data transmitted across the IP net fabric must be sufficiently specified to allow devices to interoperate in a heterogeneous vendor environment.

In one embodiment IPSOE hardware detects work queue element postings and accesses the work queue element. In this embodiment the IPSOE hardware translates and validates the work queue element s virtual addresses and accesses the data.

An outgoing message is split into one or more data frames. In one embodiment the IPSOE hardware adds a DDP RDMA header frame header and CRC transport header and a network header to each frame. The transport header includes sequence numbers and other transport information. The network header includes routing information such as the destination IP address and other network routing information. The link header contains the destination link layer address e.g. MAC address or other local routing information.

If a TCP or SCTP is employed when a request data frame reaches its destination endnode acknowledgment data frames are used by the destination endnode to let the request data frame sender know the request data frame was validated and accepted at the destination. Acknowledgment data frames acknowledge one or more valid and accepted request data frames. The requestor can have multiple outstanding request data frames before it receives any acknowledgments. In one embodiment the number of multiple outstanding messages i.e. request data frames is determined when a queue pair is created.

One embodiment of a layered architecture for implementing the present invention is generally illustrated in diagram form in . The layered architecture diagram of shows the various layers of data communication paths and organization of data and control information passed between layers.

IPSOE endnode protocol layers employed by endnode for instance include upper level protocols defined by consumer transport layer network layer link layer and physical layer . Switch layers employed by switch for instance include link layer and physical layer . Router layers employed by router for instance include network layer link layer and physical layer .

Layered architecture generally follows an outline of a classical communication stack in order to complete consumer operations of transferring data between consumers and . With respect to the protocol layers of endnode for example upper layer protocols employs verbs to create messages at transport layer . Transport layer passes messages to network layer . Network layer routes frames between network subnets . Link layer routes frames within a network subnet . Physical layer sends bits or groups of bits to the physical layers of other devices. Each of the layers is unaware of how the upper or lower layers perform their functionality.

Consumers and represent applications or processes that employ the other layers for communicating between endnodes. Transport layer provides end to end message movement. In one embodiment the transport layer provides four types of transport services as described above which are traditional TCP RDMA over TCP SCTP and UDP. Network layer performs frame routing through a subnet or multiple subnets to destination endnodes. Link layer performs flow controlled error checked and prioritized frame delivery across links.

Physical layer performs technology dependent bit transmission. Bits or groups of bits are passed between physical layers via links and . Links can be implemented with printed circuit copper traces copper cable optical cable or with other suitable links.

As discussed above the present invention provides a mechanism for managing a send queue in a split socket stack in order to reduce the processing overhead in host processors. An Upper Layer Protocol e.g. socket library creates Work Queues WQ and Completion Queues CQ that are utilized by an Internet Protocol Suite Offload Engine IPSOE and the Upper Layer Protocol ULP to transfer information and carry out send operations. As consumers initiate send operations Work Queue Entries WQE are created by the ULP and written to the Send Work Queue SWQ . The ISPOE is notified of a new entry to the SWQ and it subsequently reads this entry which contains pointers to the data that is to be sent. After the data is sent and acknowledgments are received the IPSOE creates a Completion Queue Entry CQE that is written to the CQ. The CQE includes a Work Request ID that associates a given WQE to a CQE. After the CQE is written the ULP subsequently processes the entry and removes it from the CQ freeing up a space in both the WQ and CQ. The number of entries available in the SWQ are monitored by the ULP so that it does not overwrite any valid entries. Likewise the IPSOE monitors the number of entries available in the CQ.

The Socket Context includes Flags which contain the state of the QP the IP version being utilized and the port number of the QP. The state of the QP is set by the IPSOE. The IP version and port number of the QP are set by the Consumer. The Path Maximum Transfer Unit PMTU field contains the maximum data payload transfer size. The Retry Flags include the number of times a WQE is retried must be retried by the IPSOE and the current number of retries that have been attempted. The latter is used by the IPSOE to keep track of the number of times the IPSOE has actually retried the WQE. The ACKT Acknowledgment Timeout field is the amount of time the IPSOE will wait for an acknowledgment ACK before marking the TCP Segment associated with the ACK as lost.

The Window Size WS field contains the outbound TCP IP and inbound TCP IP window sizes each in number of bytes for the connection. The Maximum Remote Direct Memory Access MRDMA field is the maximum number of outstanding RDMA Read Requests from the remote socket. The pending receive count PRC field is the number of receive packets that are pending acknowledgments ACKs . The Queue Pair Type QT field describes the type of service associated with the QP e.g. Sockets iSCSI DAFS etc . . . . The Data Segments DS field is the maximum number of data segments per WQE. The protection domain PD field identifies the Protection Domain associated with the ULP in one embodiment it is set to the process ID and is used to ensure that the current operations have authority to access the memory region being read .

The send and receive work queue contexts contain similar data except that they point to different queues. The Send Work Queue Head Pointer Physical Address field points to the head of the circular work queue for send operations. The ULP writes to the head of the SWQ when a send is initiated. The Send CQ Index field is the index into the send completion queue for the associated QP. The Pending SQWE Count is the number of pending SWQE s in the SWQ. The Pending Send Xfer Count field is the number of pending send operations waiting for an acknowledgment from a remote node. The Next SWQE field is a pointer to the next SWQE in this SWQ to be processed. Each SWQE contains a list of Data Segments. For RDMA each of the Data Segments contains a STag Virtual Address and Length. The IPSOE uses the STag to translate the Virtual Address into a Physical Address. The Logical Next Send Transfer Address and Physical Next Send Transfer Address are the virtual and physical addresses respectively immediately following the last outbound transfer as translated via the IPSOE s memory translation and protection table.

The send work queue context also contains fields that support retransmission of lost packets without the need for holding copies of transmitted data until the corresponding acknowledgments are received. The purpose of these fields is to maintain a correlation between send buffer locations as specified in SQWE s and the last acknowledged TCP IP sequence number Last Ack in the TCP context . Whereas the Next Send Transfer Addresses indicate where a transfer on a particular connection left off the retransmission support fields and the Last Ack indicate the oldest sent but not acknowledged byte of source data. There are three such fields in the send work queue context. Together they indicate the byte location in the send buffer that corresponds to the Last Ack indicated in the TCP context. This will be referred to as the Last Ack Byte. The Last SWQE Index indicates the SWQE that describes the send buffer in which the Last Ack Byte is located. The Last Data Segment indicates the data segment in that SWQE in which the Last Ack Byte is located. The Last Segment Offset indicates the byte position in that Last Data Segment in which the Last Ack Byte is located. As described later these three pieces of information together with the SACK table in the TCP Context provide the IPSOE with the information it needs to support retransmission of lost TCP IP packets with or without the TCP IP SACK option enabled .

The receive work queue context has fields that are analogous to those in the send work queue context as represented in as elements .

The TCP Context and IP Context contain information regarding the type of TCP and IP connections that are being utilized as transport mechanisms.

When a send operation is initiated the ULP creates one or more SQWE s that are written to the SWQ pointed to by the SWQ Head Pointer as shown in . The SWQ is a linked list of pages of SWQEs. The last entry in each page is a pointer containing the physical address to the next page of SWQEs.

When the SWQ is created the initial SQW free space is set in a variable maintained by the ULP referred to as the SWQ Credit Count . An SWQ Credit corresponds to one SQWE. As long as there is space in the SWQ i.e. SQW Credit Count is non zero SWQE s can be added to the head of the list pointed to by the SWQ Head Pointer . After some number of SWQE s have been added to the SWQ the ULP notifies the IPSOE that it has done so via an SWQ doorbell and then clears the SQW Credit Count. In a preferred embodiment a doorbell is a memory mapped I O operation MMIO . The number of SWQE s added to the SWQ are indicated to the IPSOE in the SWQ doorbell. The IPSOE adds this count to the Pending SQWE Count to track the number of pending SWQE s in the SWQ.

Returning to an enlarged view of the SWQE is shown in the box having fields . As shown in the SWQE includes a Work Request ID field which is an identifier that is used to associate WQE s with eventual CQE s. The Op Type field is the operation type. Operation types include Send Send with Solicited Event RDMA Write RDMA Read or a Memory e.g. Bind Memory Window Operation. The Flags include information such as Signaled Completion requested Immediate Data present and Fence requested. If the ULP requested Signaled Completion then a Work Completion WC will be returned by the IPSOE when the SWQE completes. If Immediate Data is requested then the SWQE contains data that the IPSOE must send as Immediate Data on the outbound transfer. Finally if the SWQE contains a Fence then the IPSOE must wait for the SWQE to complete before processing the next SWQE. The Fence operation can be used for Memory and RDMA Read Operations.

The Number of Data Segments field is the quantity of Data Segments that are to be transmitted. Each Data Segment contains a STag Virtual Address and Length. The IPSOE s Memory Translation and Protection Table uses these 3 fields to access the data referenced by the data segment. In an iSCSI environment this list of addresses is replaced by a pointer to the iSCSI command which will contain the destination IP address and port number along with a list of physical addresses of data that is to be transmitted.

After the data pointed to by the list of addresses in the SWQEs is transmitted by the IPSOE hardware described previously the IPSOE must notify the consumer that the work is completed. This is carried out through the use of a completion queue CQ . CQ s are created by the ULP for each IPSOE. At the time of creation the size of the CQ is set i.e. number of CQE s the CQ can hold . shows the CQ context along with the detail of the CQE. The Socket Completion Queue Context Table contains Completion Context Entries CCE such as associated with a given WQ. Each CCE contains a CQ Tail Pointer Physical Address field which is a pointer to the address of an entry such as in the circular linked list which is the CQ . This is a linked list of CQE pages where the last entry in each page is a pointer to the next page of the list.

Each CQE contains a pointer to the send operation that completed. This is accomplished by utilizing a WQ Number field and a WQE index field that point to the WQE that has completed. The Send Receive field of the CQE identifies the type of operation Send WQE or Receive WQE that completed and the Completion Status field contains information as to whether the WQE completed successfully or unsuccessfully.

The WQE that is pointed to is shown in . For a CQE that is associated with a Send WQE all the fields contained in the Send WQE are contained in the CQE. Similarly for a CQE that is associated with a Receive WQE all the fields contained in the Receive WQE are contained in the CQE. The Work Request ID field is a 64 bit identifier that is used to associate this completion queue entry back to a specific WQE.

As operations complete the IPSOE writes to the tail of the CQ using the CQ Tail Pointer assuming the CQE Free Space count in the Socket CQ Context indicates that there is available free space. CQE Free Space is a count of the number of free entries in the CQ. If no space is available CQ i.e. CQE Free Space is zero then an interrupt will be issued up to the ULP. At that point the ULP may increase the size of the CQ and notify the IPSOE how many entries have been added to the CQ. The IPSOE responds by adjusting the CQE Free Space count up by the indicated amount. The ULP keeps track of the last CQE that it read and when the next entry becomes valid. The ULP ensures that the operation completed successfully and removes CQE from the head of the CQ by invalidating the entry and advancing a software CQ head pointer that it maintains. The ULP also maintains counts of the number of CQE s it has removed from the CQ both in total and on a per WQ basis. The per WQ counts are maintained in SWQ Credit Count variables as shown in managed by ULP software. SWQ Credit Count conservatively indicates to the ULP how much free space there is in the corresponding WQ. When the ULP issues an SWQ Doorbell to the IPSOE it passes the corresponding SWQ Credit Count to the IPSOE in the doorbell as an SWQE Count and then clears the SWQ Credit Count.

The total number of CQE s the ULP removes from a CQ are maintained in a CQ Credit Count variable as shown in managed by the ULP in software. The ULP indicates to the IPSOE how many CQE s it has removed from a CQ by passing the CQ Credit Count to the IPSOE in a CQ Doorbell . A CQ doorbell is an MMIO like the SWQ Doorbell. However instead of indicating how many SWQE s have been added to a SWQ a CQ doorbell indicates how many CQE s the ULP has removed from a specified CQ. When the ULP issues a CQ Doorbell to the IPSOE it passes the CQ Credit Count to the IPSOE in the doorbell and then clears the CQ Credit Count. The IPSO adds the CQ Credits to the CQ Free Space count of the Socket CQ Context that is also specified in the CQ doorbell . When the IPSOE adds one or more CQE s to the tail of a CQ it decreases the CQ Free Space Count by that amount. Hence the CQ Free Space count conservatively indicates to the IPSOE the amount of free space in the corresponding CQ.

Note the implicit flow of SWQ and CQ Credits between the ULP and the IPSOE. As the ULP consumes CQEs from a CQ it acquires CQ and SWQ Credits. As the ULP issues SWQ Doorbells to the IPSOE it consumes SWQ Credits and implicitly passes them to the IPSOE. The IPSOE in turn implicitly returns SWQ Credits to the ULP as it posts CQE s in a CQ. Conversely the IPSOE implicitly consumes CQ Credits as it posts CQE s to a CQ. The ULP explicitly returns CQ Credits to the IPSOE in CQ Doorbells.

Now turning to which is a flowchart that outlines an exemplary process for creating a queue pair. Step starts the flowchart. The consumer initiates the creation of queue pair by calling the ULP to create a queue pair step . the ULP then allocates and pins memory for the queue pair context or socket context as depicted in Step . Once the QP is created including the setting attributes such as the number of WQEs allowed in the QP then control is returned to the consumer step and the process ends step .

The SWQE is processed by the IPSOE hardware by transmitting the data in the data segments pointed to by the Data Segment Addresses within the SWQE . Once the entire SWQE has been processed then the IPSOE creates a CQE and decrements the Pending SWQE Count. The IPSOE writes the CQE into the CQ at the address pointed to by the CQ Tail Pointer Physical Address updates this pointer to the next available CQE and decrements the CQE Free Space count. The IPSOE then notifies the ULP of a new CQE . This can be done by several methods with one implementation being an interrupt polling mechanism between the IPSOE and the ULP. Once notified the ULP processes the CQE s it removes from the CQ and updates the corresponding CQ and SWQ Credit counts . At this point the entire send operation has ended .

Retransmission support in the IPSOE is invoked on the occurrence of two different event types. One event is the arrival of a TCP IP ACK. When this occurs the IPSOE updates the retransmission support fields in the SWQ Context . The other event is when the TCP timer expires and all the TCP contexts are checked for forward progress.

The above steps maintain a tight correlation between the TCP IP sequence number indicted in the arriving ACK s and the corresponding byte position in the send buffers pointed to by the SWQE s in the SWQ. Hence when the TCP timer expires the IPSOE knows where retransmission should start from. When the TCP timer expires if no ACK s had arrived since the last time the timer expired the IPSOE goes into a retransmit mode. If the SACK option is not enabled the IPSOE retransmits everything on the timed out socket connection starting from the send buffer byte position indicated by the three retransmission support fields in the SWQ Context to the send buffer byte position just prior to the Next Send Transfer Address . If the TCP IP SACK option is enabled the IPSOE skips over data corresponding to the TCP IP sequence number intervals i.e. SACK intervals recorded in the SACK table of the TCP Context see RFC for details on the TCP IP SACK option .

The description of the present invention has been presented for purposes of illustration and description and is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art. The embodiment was chosen and described in order to best explain the principles of the invention the practical application and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.


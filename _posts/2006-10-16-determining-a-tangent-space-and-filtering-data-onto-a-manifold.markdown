---

title: Determining a tangent space and filtering data onto a manifold
abstract: A technique for determining the number of constraints on, or topological dimension of, a set of input data produced by a nonlinear system, such as a pathological vocal or econometric system. The technique characterizes the tangent space about a predetermined base point by identifying a maximal set of non-redundant nonlinear fits to the data. It needs only a few data points and only assumes that the functional form of the true constraints is smooth. Each fit is equivalent to a set of contours, with the data lying along the zero-value contour. For each fit, the gradient at the base point in the uphill direction identifies the constraint direction. The number of linearly independent constraint directions provides the number of constraints near the base point. The remaining unconstrained directions define the tangent space, which has a dimensionality equal to the number of linearly independent unconstrained directions.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07711529&OS=07711529&RS=07711529
owner: Speech Technology and Applied Research Corporation
number: 07711529
owner_city: Bedford
owner_country: US
publication_date: 20061016
---
This application is a continuation of U.S. application Ser. No. 10 662 991 filed Sep. 15 2003 now U.S. Pat. No. 7 124 065 which is a continuation in part of U.S. application Ser. No. 09 425 784 filed Oct. 25 1999 now abandoned which claims the benefit of U.S. Provisional Patent Application No. 60 105 690 filed Oct. 26 1998 the entire teachings of which are incorporated herein by reference.

This invention was supported at least in part by contract number R44 DC 02343 with the United States National Institutes of Health who may have certain rights in this invention.

The present invention relates to analysis of nonlinear dynamical systems and in particular to a technique for determining the number of constraints or equivalently the topological dimension for a set of data.

Certain practical applications of data processing systems relate to fitting models to sets of data. For example in applications such as speech processing signal processing econometric data prediction demographic analysis and the like a set of data points are first collected from a real world process. It is desired then to find a set of mathematical equations which can be used to model the process accurately for example to predict the future behavior of the real world system.

Determining the number of dimensional constraints on the collected data or equivalently the topological dimension d is an important problem in the study of nonlinear system responses. For example three coordinate data may fill a volume lie on a surface be confined to a curve or even degenerate to a point reflecting zero one two or three independent constraints representing a topological dimension d of three two one or zero respectively . In the case of a real world system in which linear responses may be assumed this problem is able to be robustly solved by matrix decomposition techniques such as Singular Value Decomposition SVD or eigenvalue decomposition. These modeling methods assume that linear functions will adequately fit the data. However such linear techniques cannot generally be directly applied to an instance of a nonlinear system with satisfactory results.

The present invention is a significant extension of Singular Value Decomposition SVD and eigenvalue decomposition techniques and robustly determines the constraints on experimental data without prior assumptions about their functional form. This strongly distinguishes it from curve fitting in which the functional form is assumed and the data serve merely to determine some free parameters of the fit to the form. The method s essence is to characterize a tangent space at a base point as a simultaneous collection of all linear fits that best describe the immediate neighborhood of the point. The technique works for both clean and noisy data applies to complicated curving and twisting constraints e.g. abstract shapes or geometries of data and requires only a modest number of data samples as input. It also accommodates prior knowledge either positive or negative about constraints.

More particularly the technique relies on two principal process steps. First an SVD process is used to identify a maximal set of non redundant fits to the data preferably single constraint fits all centered about some convenient base point. Each fit is equivalent to a set of contours lines surfaces etc. with the data themselves all lying along the zero value or perfect fit contour. The gradient of each fit at the base point e.g. the uphill direction across the contours identifies each constrained direction. The number of such directions that are linearly independent thus provides exactly the number of constraints in the neighborhood of the base point. Consequently the directions if any that remain unconstrained also define the tangent space.

The fits found by the SVD process must then pass a statistical significance test i.e. they are suppressed to the extent that even unconstrained data might frequently produce them. However imperfect fits are accepted to the extent that measurement errors in the data could cause them. Care in evaluating the significance of the statistics allows both a fitting shapes that can be as complicated as the data can meaningfully describe yet b accommodating measurement error.

Once the tangent space is found it is a straightforward matter to project additional data points near the base point onto it thus enforcing the constraints. The degree of mismatch i.e. of constraint violation provides a measure of the process noise if this is not initially well known.

The method has been tested empirically on several data sets. It is also applied to one higher level problem to determine dfor strongly spatially varying geometry that can in some places mimic additional false constraints with potentially severe consequences to analyzing e.g. chaotic dynamical systems.

Turning attention now to the drawings more particularly a preferred embodiment of one particular application of the invention to characterization of laryngeal dynamics will be described in greater detail. The system consists of a computer workstation a microphone and display . The workstation includes components typical of a data processing system of the personal computer type including a digital signal input output card consisting of a signal amplifier and analog to digital converter a processor accelerator board such as one including a digital signal processor DSP memory a central processing unit CPU and display interface .

In operation speech signals are first picked up by the microphone and fed to the amplifier and analog to digital converter . A time sequence of the signal samples from the converter is then stored as an array S n in the memory where n is a predetermined number of signal samples. The central processing unit then processes the samples S n either with its own internal arithmetic units or in cooperation with the digital signal processor .

This particular end use is illustrated herein by way of example. It should be understood that the invention can be applied to other applications in which characterization of a non linear dynamical system is desired.

In general the present technique attempts to fit one or more constraints to the data which are assumed not to fit any predefined function perfectly. As shown in the data are acquired in a first step and then filtered in a next step . In step a matrix decomposition technique such as Singular Value Decomposition SVD is then used to identify a set of non redundant fits to the data preferably single constraint fits all centered about a convenient base point. Next in step a tangent space is defined by determining if each fit passes a statistical significance test. Fits which do pass the statistical significance test serve collectively to define the manifold near the base point. They may be used to project additional data points in the neighborhood of the base point effecting a filtering of the additional points onto the manifold in step . In steps and the idea is to find the intrinsic dimensionality of the data S n and the local tangent space at a specified base point.

A detailed program listing of one implementation of the process is included in the attached Appendix.

As mentioned above a first step is to find a base point and N other data points in the neighborhood of the base point. Next an N by N square matrix is constructed by evaluating N linearly independent basis functions for each of the N points found.

For example if 6 points are found in the neighborhood of a base point in a two dimensional xy space then 6 basis functions can be evaluated for each point found. These basis functions could be chosen to be all of the two dimensional polynomials up through order i.e. 

If 10 points are found the basis functions could be chosen to be all of the two dimensional polynomials up through order i.e. what we have already found through order plus the four new functions 

Having constructed an N by N design matrix we now use standard matrix decomposition techniques such as singular value decomposition SVD or eigenvalue decomposition to identify linear combinations of the basis functions that combine to yield something that is essentially 0 when evaluated at each of the N points found. SVD is performed directly on the design matrix D whereas eigenvalue decomposition is performed on DD. Essentially 0 is specified more precisely below. 

Symbolically we say that we want to find vectors of coefficients of the form C C C . . . C such that f P Cf P 0 for the N points P j 1 . . . N found in the neighborhood.

If we find an equation of the form f P Ci fi P 0 j we say that this represents a constraint equation relating the data.

For example if we have 6 xy data points lying on a circle of radius 1 and satisfying the equation x y 1 we can write this as the constraint equation 1 0 1 0 0 1 0 1 0 1 . 1 0 0 1 0 1 0 in matrix form. Here we have found a vector of coefficients 1 0 0 1 0 1 with 1 0 . . . .

Vectors of coefficients singular eigen vectors which are actually unit vectors unlike the example above corresponding to constraint equations are found by first identifying associated singular values or eigenvalues that are essentially 0. These could be identified in various ways e.g. by Monte Carlo or symbolically the following input output analysis is particularly efficient and straightforward.

In general singular or eigen values will not be exactly 0 but if they are relatively small compared to the uncertainties in their values then they can be considered to be effectively 0.

The uncertainties in the singular eigen values are determined by perturbing the data points Pby small amounts consistent with the stated level of noise on the data and by then seeing how this affects the singular eigen values found for the design matrix constructed from the perturbed points. For example if the perturbation is done on an individual point by point and component by component basis a Monte Carlo approach involving simultaneous perturbation of all of the points is another possibility then we can first add the stated level of noise to the first component of P so that f P f P . . . f P in the first row of the matrix is replaced by

This procedure is repeated for a perturbation of the second component of P a perturbation of the first component of P and so on. Finally after the effects of all of these individual perturbations have been determined in isolation the overall uncertainty in the singular eigen values specifically the covariance matrix of the variations is determined in a statistically appropriate manner.

Before qualifying singular eigen values as being essentially 0 relative to their associated uncertainties a final statistical screening has to be applied to compensate for the fact that chance alone will result in a certain number of values being small with this number increasing as the number of data points in the problem i.e. value of N increases. Only after comparing the observed ratios of singular eigen values to their associated uncertainties with a theoretical distribution of values corresponding to the same noise and number of degrees of freedom is it possible to say that observed values are really effectively 0. Given the number of coordinates and points in even the most trivial nonlinear problem 2 coordinates 6 points it is reasonable to approximate the effect of virtually any plausible distribution of measurement errors as producing a Gaussian distribution of eigenvalue errors and a Chi squared distribution of their squares which are used to compute the singular values . Thus very standard statistical significance tests may be used to decide whether a given singular value with its uncertainty as determined above can be identified as effectively zero .

II. Using Single Constraint Manifolds to Construct the Lowest Dimensional Manifold Containing The Data

If eigenvalues and singular values are sorted similarly in decreasing order for example then their associated eigen singular vectors are identical to within a or sign that is essentially irrelevant. For this reason it is sufficient to consider singular vectors only in the following analysis.

When multiplying a matrix times any of the singular vectors obtained from the singular value decomposition of the matrix the result is a vector whose length is the singular value corresponding to the singular vector in question. Each component of the resulting vector is the product of the corresponding row of the original matrix times the singular vector in question.

Accordingly if a singular value from the decomposition of our design matrix discussed earlier is 0 the product of every row of the design matrix times the singular vector in question must be 0 and the singular vector in this case is like the vector C mentioned earlier with components C C . . . C where C f P 0 for all points P j 1 . . . N used to construct the design matrix. In this case we say that the singular vector in question corresponds to a constraint equation f P 0 relating the data.

When a singular value is not 0 but the data from which the design matrix was constructed are noisy and therefore imprecise it is still theoretically possible for the associated singular vector to correspond to a constraint equation for the theoretical noise free underlying data. However as singular values become increasingly large relative to their associated uncertainties i.e. to their associated standard deviations it becomes increasingly unlikely that the singular vector in question really does correspond to a constraint equation.

Constraint equations have geometrical interpretations here and the set of all points satisfying a given constraint equation form a manifold curve surface . . . . For example for three coordinate xyz data a constraint equation of the form ax by cz d 0 represents a plane which is a two dimensional manifold surface and an equation of the form y a z b 0 represents a line which is a one dimensional manifold curve . The plane here corresponds to a single constraint manifold It is not possible to move from one point in the plane to an adjacent point in the plane by proceeding in the direction a b c which is normal to the plane. The line corresponds to a double constraint manifold y a and z b. Both y and z are constrained to particular values here.

For a single constraint manifold it is possible to determine the constrained direction for any point on the manifold by taking the gradient of the constraint function f P . For more details please see the discussion in the Why Single Constraint Manifolds section here. For example the gradient x y z applied to the function ax by cz d does yield the correct constrained direction a b c .

Constrained directions are described as being in the null space of the tangent space of a manifold while allowed directions are in the range of a manifold. For the line example above the y and z directions are in the null space while the x direction is in the range of the line.

Now returning to a description of the process for each of the N singular vectors obtained from the decomposition of our design matrix we then determine the gradient f P where f P is Cf P from before at a specified base point P P.

If associated singular values were essentially 0 as determined by the analysis outlined above we set these values exactly to 0. Larger singular values are not changed.

We then multiply each of the N gradients grad j 1. . . N found above by a probability factor for example exp sval 2 sval where svalis the possibly adjusted jth singular value and svalis the uncertainty standard deviation associated with sval . Multiplying by this factor has the effect of greatly suppressing the contributions from possible solutions that are statistically unlikely.

We now simultaneously consider the N probability weighted gradients just obtained. For a d dimensional coordinate space the gradients are d dimensional vectors so that an N by d matrix can be formed from the N probability weighted gradients. We now perform a singular value or eigenvalue decomposition on this N by d matrix and this yields d singular values.

The values that are clearly non zero correspond to singular vectors that span the space spanned by the weighted gradients. I.e. these singular vectors span the null space of the data set at the base point and the number of these vectors is the dimension of the null space which is equal to the number of constraints on the data set.

The other singular vectors correspond to singular values that are essentially 0. Essentially zero is determined by a two step input output analysis i.e. perturbed data points P vector uncertainties in weighted gradients uncertainties in singular values obtained from decomposition of N by d weighted gradient matrix. This is followed by a statistical screening to compensate for the value of N i.e. the risk of false alarms as outlined earlier . These essentially 0 valued singular values correspond to singular vectors that are orthogonal to the null space of the manifold so these singular vectors represent the tangent space description of the manifold.

A line might be found as the intersection of two distinct planes. Each plane would correspond to a single distinct constraint and the line would represent a one dimensional manifold with two distinct constraints.

Data degenerating to a single point might be described as the intersection of three distinct planes each corresponding to a distinct constraint. The point would then represent a zero dimensional manifold with three distinct constraints.

Points from a circle with x y 1 and z 0 might be found as the intersection of the cylindrical surface with x y 1 z anything and the x y plane with z 0. Each surface here would correspond to a single constraint manifold and the circle would represent a one dimensional manifold with two distinct constraints.

Additional surfaces might also be found passing through the points from the circle above e.g. spherical or conical surfaces . For all of these extra surfaces however the normal constraint information would be found to be redundant with the constraint information already obtained from the plane and cylindrical surface above. Specifically the normal to any one of these extra surfaces would be seen to be a linear combination of the plane and cylinder normals already found and so the final determination of the intrinsic dimensionality and tangent space of the data would be unaffected.

In a three dimensional xyz space consider the line in the x direction with y 2 and z 3. Think of this line as the intersection of two planes described respectively by the constraint equations f and fbelow where f describes the single constraint y 2 and fdescribes the single constraint z 3. 0102 0 0013 0

The two normals are distinct and span a 2 dimensional space that is orthogonal to the line. We say that the dimension of the orthogonal space nullspace is two here so that the dimension of the lowest dimensional manifold containing the points on the line is

If we tried to simultaneously incorporate the two constraints into a single equation we would get something representing a less than D 1 dimensional manifold.

For example to simultaneously require y 2 and z 3 we would need an equation of the form 2 3 0 or 2 3 0 or . . .

If we tried y 2 z 3 0 we would really have y z 5 0 which is not the equation for a line but is the equation for a plane.

Looking at the original f x y z y 2 z 3 0 we see that x y z 0 2 2 2 3 However since y 2 and z 3 everywhere along the line we have f 0 0 0 everywhere along the line and this represents an unrestrictive gradient.

The equation y 2 z 3 0 represents the equation for a D 2 dimensional manifold here two independent constraints but from a gradient analysis point of view the equation here is not very useful.

Now to describe the process more particularly given a base point at without loss of generality 0 for a d dimensional embedding 2 d 10 perhaps where represents the Real numbers we ask if a discrete set of points S Xin its neighborhood are apparently confined to some lower dimensional manifold and if so what their tangent space at this base point is.

In the context of analyzing nonlinear dynamical systems we do this for two reasons. First we wish to ensure that meaningless noise dominated dimensions are associated with Lyapunov exponents that are identified and suppressed to leave only meaningful exponents . Second it is important that meaningful principal axes near the base point of each neighborhood s computation not be mixed by linear combinations with meaningless axes when the displacements happen to evolve with similar exponents. In this case the basis could mix such axes freely because the parts of the basis describing dynamics with equal exponents is a completely arbitrary basis of the subspace an arbitrary linear combination of the true basis vectors i.e. those tracking the actual dynamics and those identifying noise. 

This application analyzing a possibly fractal attractor of a nonlinear dynamical system is the primary focus of the invention. In places it will therefore be convenient to be aggressive in identifying or inferring tangent space directions conservative for their orthogonal complement the constrained i.e. null space directions . That is directions for which the evidence is ambiguous because of the placement or uncertainty of available data may be preferentially identified as part of the tangent space instead of the null space . This can be desirable because inferring a null space direction is equivalent to concluding that the data satisfy a constraint a strong assertion.

A secondary consideration comes from determining the full equation for a tangent space. Under certain common circumstances the additive constant or offset of the tangent space from a given data point provides an estimate of the error in the data point itself at least in the null space directions . Subtracting this estimate corresponds to filtering the point onto the manifold.

Throughout one must be careful in distinguishing between absence of evidence and evidence of absence. In some parts of the following mathematical analysis the first of these will be helpful in others the second.

The practical solution proceeds in stages starting from that for a large if still finite number of noise free data. In this ideal case as mentioned above the solution has two steps 

We begin by considering reasonable constraint equations. First check if all neighborhood data fit practically any continuous functional form even implicitly in terms of a row vector valued function . The function must be a sum of n linearly independent basis functions i.e. principal components of a function space linear representation . . . 0 an equation which is linear in the row vector a where for notational convenience we take the embedding dimension d 3 and the neighborhood data then to be X a set of precisely n elements row vectors . Thus for a given a is a transform of the data albeit one that is nonlinear in the data. In this case if 0 the base point is itself one of the data we must have a 0. It is convenient to index the basis functions fbeginning with j 0. Without loss of generality WLOG then adopt basis functions such that

We choose to filter the data to determine if there is a lower dimensional manifold using principal components analysis PCA or equivalently singular value decomposition SVD . To wit We try to find a set of n coefficients not all zero such that 0 for 1 0 0 . . . 0 1 . . . . . . 1 . . . where separates the rows i.e. 

At this point we write F for F S when convenient and F . for the matrix valued function if necessary we factor F using singular value decomposition F UWV. Wis a diagonal matrix of the singular values which are non negative the columns of U form an orthonormal o.n. basis of the function space and constitute the principal components of F the columns of V are the singular vectors of F also an o.n. basis of the coefficient space in this case. Then we form some pseudo inverse W diagonal for W such that 0 0 and noise on the data 1. Note that for any definition of the pseudo inverse the effective rank of W can be defined as the trace of WW which is often more meaningful than a mere count of the number of its non zero eigenvalues. 

Thus we seek any non zero vector s NS F the null space of F. Since this means F 0 or UWV 0 and since Uis an orthonormal basis recall that F is square we must have . The vector therefore consists of a any linear combination of the columns of Vthat correspond to singular values equal to zero in W. The number of linearly independent solutions for is the number of such columns of V precisely the number of null singular values in W which in turn is just the co rank of F n rank F .

Adopting for now the Moore Penrose pseudo inverse for we have that WWis a diagonal matrix of zeroes for NS F and ones. Conventionally SVD algorithms order the singular values in Win decreasing order. so diag WW consists of all the ones then all the zeros we shall assume such an algorithm here whenever convenient for exposition. Then I WW Vconsists of precisely an o.n. basis of NS F prefixed with some null vectors as many as the number of non zero singular values of F i.e. its rank .

Notice that the first row of V matches the constant function f. That is the constant part of f . is given by UWtimes the transpose of this row. Likewise the next d rows of Vmatch the linear basis functions whose coefficients in a are precisely the derivatives at the origin of f . the functional fit specified by . Now we extract the first derivative columns of I WW V into a matrix A and observe that A consists of length d column vectors the first rank F of which are zero.

Distinct nonlinear fits may have identical or linearly dependent solutions for the linear coefficients although not of course for the complete vector of coefficients . That is rank F may be as large as n but rank A cannot possibly exceed the number of its rows which is only d. Thus linearly independent columns of A describe distinct directions at the origin in which the manifold does not extend the orthogonal complement of the tangent directions.

Now we use SVD again to find those linearly independent columns of A. This determines the rank and the linear equations for the tangent space notice that the rank may equal d if the data really extend in all directions around the base even after fitting to the nonlinear functions but we will still call the result a tangent space albeit a trivial one consisting only of 0 rank rank . The diagonal elements of W in each of A s principal directions inversely determine whether the data really extend in the direction Large diagonal elements correspond to directions that do annihilate are orthogonal to the linear approximation to the data. And the principal directions are precisely the columns of U. Infinitesimal displacements from the origin are d dimensional vectors so notice that they must pre multiply A because A is d n. 

We are thus led to form the projection for the tangent space which clearly has the required form for a projection apart from a rotational similarity i.e. U it is diagonal with eigenvalues of zero and unity or approximations thereto depending on the definition of moreover it preserves the vectors that A annihilates i.e. the tangent space and vice versa. This therefore has just the proper behavior suppressing directions whose data extents are small compared to the noise but leaving unaltered those that are well determined extent much larger than the noise .

For some purposes this projection may be replaced with some other qualitatively similar matrix such as U cos WW 2 U. Most of the discussion below is unaffected by such a replacement.

It is convenient to be able to enforce dimensionality constraints in certain cases. For example a certain direction may be known to lie in the tangent space yet some data sets may favor inferring erroneously that it lies instead in the null space. There are several methods by which this prior information may be enforced. However it will be important to use a method that applies appropriately to noisy data sets or even to slightly uncertain prior information not only to logically certain cases. Accordingly this topic will be deferred until we consider the problem of noisy data below .

If the prior information concerns a null space direction on the other hand it is trivial to form the projection matrix that annihilates this direction and to multiply P by it.

Suppose now that the Xvectors or points contain additive zero mean random noise of some distribution not yet specified. We revisit the formal noise free solution after normalizing the data set. Specifically scale the set by the factor max which produces a normalized data set whose largest magnitude element has a squared norm equal to unity. For some scalar standard deviation 0 let the scaled noisy elements have Gaussian noise N 0 say independent across points and independent equal variance across components as the notation indicates . We assume 

Now we perturb every datum by approximately in every component recompute A or at least W for every such perturbation and note whether any imperfect fits corresponding to initially non zero singular values can become perfect zero . If so then these additional fits are included although discounted according to the magnitude of perturbations relative to needed to produce them.

In principle we could adjust all of the noisy points to minimize the sum of squared residuals or some other convenient functional . The above development provides initial coefficients to achieve such a fit but iteration would no doubt be needed. This would occur in the space of all coefficients jointly with all points errors. Since the latter part alone is a space of nd dimensions and the functional depends nonlinearly on the errors the task would be computationally expensive. Even a single iteration could depending on the iteration algorithm involve numerically differentiating all of the n basis functions with respect to all of the nd scalar errors.

There is an important and inexpensive special case the base point if it is one of the data points. We can then content ourselves with estimating only the base point s error. As mentioned earlier this is related to the offset of the tangent space from the origin Like any linear function the tangent space is described by a slope in each dimension the direction vectors and an intercept the offset. Estimating this offset and applying it to the base point is conceptually trivial and for the chosen basis functions it is even computationally trivial.

Typically we will translate the data to a neighborhood of the origin by subtracting one datum from all of the others. However there is one important refinement. Although the offset s estimate is nominally accurate to second order the base point s error is the one error that appears in every other neighborhood point s error simply because in the common case the base point s position is subtracted from every other point in order to translate the base to the coordinate origin. As a result it may be beneficial to subtract this estimate from the base point and repeat the translation and fit operation. Observe that this unlike the general best fit problem is a low dimensional iteration of inexpensive computations. The errors to be estimated constitute a d dimensional space and the functions are numerically evaluated once per iteration rather than an nd dimensional space with d 1 or more evaluations per iteration.

It may happen that we have prior information of some significant even perfect certainty that the tangent space or the null space includes a particular vector or set of vectors a known subspace. For example if the data are sampled from a dynamical flow then the tangent space at the base point must include the local velocity vector even if the data are sampled so coarsely that the above algorithm cannot initially reproduce this result. It is useful to extend the algorithm to combine this prior information with the initial results for example that the tangent space should include some vector that is approximately equal to an imperfectly estimated or even perfectly known local velocity vector.

There is a mechanism that generalizes easily to the case of uncertain prior information. Suppose for example that the prior information directions Pconsist of null space vectors i.e. constraint directions only. Then one can construct the augmented null space basis matrix . . . where the vare the null space vectors inferred by the above procedure. Next extract an o.n. basis from this matrix construct the corresponding basis matrix by making each column one of the basis vectors and multiply by its transpose to construct the null space projection map with or without considering base point noise . Finally construct the tangent space map by subtraction from the identity matrix.

The solution for the case of prior information about tangent space directions is virtually identical relying instead on augmenting the initial tangent space basis matrix.

Observe that this technique constructing an o.n. basis from the augmented matrices ensures that any redundant information either among the prior directions or between them and the initial data determined directions is automatically removed. It also ensures that the magnitudes of the Pare ignored in constructing the revised null space This is proper because these vectors are only supposed to identify directions.

It remains to show how to incorporate uncertain prior information. At heart this entails using the magnitudes of the prior information vectors compared to the magnitudes of the initial estimates the latter are given by the o.n. basis vectors each multiplied by its corresponding certainty which will be a weight constructed from how close to zero its corresponding singular value is.

It is important that this certainty reflect the statistical significance of the solution zero or near zero singular value given the number of data and number of basis functions.

In this case assume that each direction vector whether prior p or data determined u is scaled by its certainty . Specifically we set its length i.e. norm so that its norm multiplied by its angular error is just equal to . Now form the augmented matrix B as before and perform SVD on the result . In this case however we do not simply extract the principal components in Uthat correspond to non zero singular values in W. Rather in keeping with the earlier uses of SVD when dealing with noise we determine the singular values that are irrelevantly small suppressing the corresponding column vectors in U. We retain only the remaining substantial vectors those that we can confidently assert impose true constraints on the data and thus define the null space.

As before we form the null space projection from the matrix consisting of a row of all these column vectors finally we construct the tangent space matrix by subtraction from the Identity matrix.

Consider the attractor of the Rossler chaotic dynamical system. This system is so strongly dissipative that the manifold attractor is extremely thin 10of its height and width virtually everywhere Although topologically a three dimensional 3 D system its Lyapunov dimension is 2.02. Therefore most neighborhoods are much too thin to permit realistic measurement of the correct topological dimension. However estimation of this system s dynamics would demand the correct dimension since that is precisely the number of Lyapunov exponents of the system The wrong number would be a qualitative error destroying the integrity of the dynamics.

The solution begins by noting that the topological dimension is independent of position on the attractor. Therefore it suffices to find one neighborhood on which the three dimensionality is reliably measured.

Notice that this is actually a generic rule Dissipation can make any attractor appear to be lower dimensional than it truly is but never higher dimensional. Proper accommodation for noise is important however since noise can only increase the apparent dimension over its correct value. The Rossler system like many others though exhibits its chaotic dynamics over the global scale of its attractor. Consequently even a very noisy rendition of the attractor say signal noise 5 is sufficient to show the stretch and fold nature of the dynamics with the consequent Cantor set structure of the attractor.

Specifically where the attractor folds over and collapses onto itself it cannot be represented as 2 D even with high noise but it can be represented as 3 D if the embedding dimension number of coordinates is sufficiently high.

While this invention has been particularly shown and described with references to preferred embodiments thereof it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined by the appended claims.


---

title: Preventing network micro-congestion using send pacing based on end-to-end bandwidth
abstract: A system for preventing network micro-congestion using send pacing based on end-to-end bandwidth. The system includes one or more processors and memory coupled to the processors, wherein the memory contains program instructions executable by the processors to obtain an estimate of end-to-end network bandwidth available to a particular application-level data transfer operation in progress over a network between a first endpoint and a second endpoint. Based at least in part on the estimate of end-to-end bandwidth, the instructions are further executable to introduce a delay between transmissions of two or more successive messages or packets of the particular application-level data transfer operation from the first endpoint.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07616585&OS=07616585&RS=07616585
owner: Symantec Operating Corporation
number: 07616585
owner_city: Cupertino
owner_country: US
publication_date: 20060228
---
This invention relates to computer systems and more particularly to the management of network traffic between computer systems.

In today s enterprise environments more and more applications rely on bulk data transfers to accomplish their functionality. Applications that require large amounts of data to be transferred between one network endpoint and another for a single application level job or task may include for example storage management applications of various types such as backup and restore applications disaster recovery applications etc. media server applications that may be required to transmit movie and audio files telephony e.g. voice over IP and other telecommunications applications scientific analysis and simulation applications geographically distributed software development projects and so on. The amount of data that has to be transferred for a given job or task varies with the specific applications and use cases but can easily reach several tens of megabytes or even gigabytes in some cases. Furthermore the data often has to be transferred over large distances e.g. a disaster recovery application may be configured to replicate data from a primary data center in the United States to another data center in Europe or Asia.

Much of the data for these bulk data transfers is often transferred at least in part over public networks such as the Internet especially as budgetary pressures have reduced the ability of many enterprises to deploy proprietary high speed networks and or non standard protocols. As a result the network paths taken by the data packets corresponding to a given bulk data transfer often include a variety of network devices e.g. switches routers etc. and or links over which the sending endpoint does not have control. In fact the sending endpoint typically relies on a standard communication protocol such as the Transmission Control Protocol Internet Protocol TCP IP for routing packets and has very limited knowledge of the specific paths taken by the data packets on their way to the destination endpoint. The sending application simply hands over data packets as fast as possible to the networking software stack at the sending endpoint and the networking software stack typically transfers the data as fast as possible to a hardware device e.g. an Ethernet network interface card or NIC connected to the network. The data packet sizes supported by the network hardware device are often relatively small compared to bulk data transfer sizes e.g. in a typical implementation where the size of an Ethernet packet is limited to about 1500 bytes a 64 kilobyte chunk of application data may be transferred as a burst of about 40 very closely spaced packets. The burst of packets may be followed by a gap e.g. representing time needed by the application to transfer an additional chunk of data to the networking stack and or to receive an application level acknowledgment from the receiving endpoint followed by another burst and so on until the bulk data transfer is eventually complete. As a result the sending device for a given bulk data transfer often transmits data in a bursty pattern.

Due to the decentralized nature of most common network protocols a given network resource such as a router or switch is often not aware of the application to which a particular data packet received by the device belongs. The network device does not distinguish between the packets of different data transfers it simply sees sequences of incoming packets on each of its input ports determines the next link on each packet s route by examining the packet header and sends the packet on over the next link using an appropriate output port. Often network devices may have limited input and or output buffer space e.g. in some implementations each port s input buffer on a network switch may be limited to buffering 40 packets at a time and each port s output buffer may be limited to 70 packets. Because the network devices can participate in multiple concurrent data transfers the bursty nature of the packet streams emitted by sending endpoints can sometimes temporarily overwhelm the resources of a network device resulting in packets being dropped or lost. For example if two bursts of more than twenty data packets each happen to arrive on the same port at a particular switch that can buffer at most forty packets in a given input buffer some of the packets may be dropped. Similarly if more than seventy packets need to be buffered for output from a given port whose output buffer capacity is limited to seventy packets at a time some of the outgoing packets may be dropped. Such micro congestion even though it may only be a local and transient phenomenon and even though the network as a whole may have a relatively low level of utilization can have potentially far reaching effects on the bulk data transfers since networking protocols such as TCP IP react to data packet loss by automatically throttling the data transfer adjusting parameters such as window sizes and the like.

A number of different approaches to tuning network traffic have been considered. Some such schemes either require changes to standard network software stacks or require custom hardware however such schemes are difficult to implement in environments that rely on standards based and interoperable communication technologies. Techniques that require substantial changes to legacy applications or third party applications are also unlikely to be deployed in most enterprise environments. Other techniques attempt to implement global solutions that cannot adapt rapidly to changes in the current state of a given data flow e.g. some schemes may attempt to statically partition bandwidth between different applications but may still not be able to avoid the transient micro congestion phenomena and the resulting problems described above.

Various embodiments of systems and methods for preventing network micro congestion using send pacing based on end to end bandwidth are disclosed. According to one embodiment a system comprises one or more processors and memory coupled to the processors wherein the memory comprises program instructions executable by the processors to obtain an estimate of end to end network bandwidth available to a particular application level data transfer operation in progress over a network between a first endpoint and a second endpoint. Based at least in part on the estimate of end to end bandwidth the instructions are further executable to introduce a delay between transmissions of two or more successive messages or packets of the particular application level data transfer operation from the first endpoint. The instructions may be packaged for example as a user mode traffic manager e.g. a library and or in one or more kernel mode traffic manager drivers in one embodiment. The delays may help to reduce burstiness in the outgoing traffic from the first endpoint and match the sending transfer rate more closely to the bandwidth that is actually available for the application level data transfer. As a result of the reduction in burstiness of the send traffic the probability of transient congestion events e.g. when a large number of packets arrive at the same port of a network device such as a switch exceeding the available buffer space and potentially leading to dropped packets in the network may be reduced. A reduction in transient congestion events which may be termed micro congestion herein may in turn help to alleviate some of the problems that may otherwise be caused by self adjusting flow control mechanisms used in common networking protocols which may for example severely throttle a data transfer in response to dropped data packets.

The estimates of available bandwidth may be derived on a per connection basis in real time in some embodiments for example using user mode instrumentation that does not require modifications to application code operating system code or networking protocol stack code. The delays between successive packets may be introduced at any of various layers of the software stack at the sending endpoint in various embodiments e.g. in one embodiment delays may be introduced before application data is passed from user space into kernel space while in another embodiment delays may be introduced in kernel space just before the packets are transferred to networking hardware. The duration of the delays may be calculated in some embodiments to set the sending data rate close to the estimated available bandwidth. For example if the available bandwidth is estimated to be 10 Megabits second and the application is ready to send data in bursts of 20 Megabits second the traffic manager may introduce delays to reduce the actual send rate to slightly less than 10 Megabits second.

In some embodiments trends in available bandwidth may be detected by the traffic manager and the delays to be used for subsequent packets of the ongoing data transfer may be adjusted accordingly. In one embodiment different delays may be introduced into concurrent data transfers originating from the same application. Data transfers performed on behalf of a wide variety of applications may be shaped or paced introducing task specific delays in various embodiments including storage management applications telecommunication telephony applications media applications etc. In one embodiment user specified parameters may be used to control various aspects of the data shaping such as how closely the send rate should approach the estimated available bandwidth.

While the invention is susceptible to various modifications and alternative forms specific embodiments are shown by way of example in the drawings and are herein described in detail. It should be understood however that drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed but on the contrary the invention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.

The broken arrows labeled and in illustrate three exemplary bulk data transfers. In the bulk data transfer labeled data is transferred on behalf of a task performed using application A at endpoint A to application B at endpoint B. In the bulk data transfer labeled data is transferred on behalf of a task of application D at endpoint D to application A at endpoint A. Some bulk data transfers such as the transfer labeled may be bidirectional e.g. for a given application task multiple network messages or packets collectively containing large amounts of application data may be sent from endpoint D to endpoint C and from endpoint C back to endpoint D. It is noted that even for data transfers that are largely unidirectional with respect to application data such as a transfer of a 100 Megabyte backup data set from a primary site to a secondary site network packets may be transmitted in both directions e.g. while packets containing the application data may be transmitted only from the sending endpoint the primary site in the backup example to the receiving endpoint the secondary site acknowledgement packets and or other control packets may be transmitted in the reverse direction. For bidirectional bulk data transfers each endpoint may serve as a sending endpoint as well as a receiving endpoint. A given bulk data transfer may use multiple network devices and multiple network links . In some embodiments a single application such as application D may receive and or send data corresponding to multiple data transfers often concurrently. Some of the applications may be distributed across multiple endpoints and bulk data transfers may be performed from one component of a distributed application to another. For example in endpoint A may represent a primary site whose data is to be backed up to a secondary site at endpoint B and applications A and B may respectively represent primary site and secondary site components of a distributed backup application.

Subsets of the endpoints and network devices shown in may be administered or controlled by different organizations in some embodiments. For example in one embodiments endpoint A may be a video server managed by a media company while endpoint B may be a home computer owned by a customer of the media company and the network devices participating in a bulk data transfer of a video file from endpoint A to endpoint B may be owned and controlled by one or telecommunication companies. In general no single entity may have complete control over the endpoints and or the network devices involved in a given data transfer. The sending and receiving endpoints may be unaware of the specific path taken by packets of a bulk data transfer and the paths taken by different packets of a given bulk data transfer may differ.

In one embodiment a traffic manager at an endpoint may be configured to obtain estimates or measurements of the actual end to end bandwidth available for a given bulk data transfer while the transfer is still in progress. The estimates or measurements may be obtained from a variety of sources in different embodiments such as an application participating in the bulk data transfer instrumentation inserted into the packets of the transfer by the traffic manager or by some other entity third party network performance tools etc. The term end to end bandwidth as used herein refers to the rate at which data associated with the bulk data transfer is transmitted from one endpoint to another endpoint taking the entire network path between the endpoints into account. It is noted that the end to end bandwidth as measured at a given endpoint may differ from the advertised or achieved bandwidth at individual components of the network path between endpoints. For example an Ethernet network interface card used to transmit the data from endpoint A may have an advertised bandwidth of 100 Megabits second or 1 Gigabit second various network devices such as switches may offer advertised aggregate bandwidths of several Gigabits second and links may each have their own associated advertised bandwidth capacity. The actual bandwidth achieved from endpoint to endpoint may be substantially less than the advertised bandwidth of even the slowest device or link of the network path for a variety of reasons in different implementations. In some embodiments several of the network devices and or links may be shared among multiple tasks of a given application between multiple applications and or between multiple endpoints . Concurrent use of the devices may result in queuing and or packet collisions at various points along the path depending on the specific implementations of the devices and the networking protocol or protocols in use. Furthermore because of the nature of some of the networking protocols that may be employed in network the loss of even a few packets may result in a substantial slowdown for a given data transfer. Protocols of the TCP IP Transfer Control Protocol Internet Protocol family may for example automatically adjust parameters such as by reducing send window sizes increasing retransmit timeout values etc. in response to dropped packets under the assumption that the loss of a packet is an indicator of congestion or a faulty network. Even if the packet loss is caused by a transient phenomenon rather than a sustained phenomenon as described below in conjunction with the example data traffic illustrated in and the network quickly recovers from the transient phenomenon the adjustment of the protocol parameters may tend to throttle the data transfer at least for a while. While the parameters may eventually be reset to values that allow higher transfer rates if packet loss does not reoccur it often takes a while for the parameters to return to the appropriate settings. As a result the measured end to end throughput is often lower than the advertised throughput for example measured throughput for a given application task on a one Gigabit second network may be as low as 50 Megabits second.

A traffic manager may in one embodiment use real time measurements or estimates of the end to end bandwidth for a data transfer to shape the remaining outbound traffic of the data transfer from an endpoint e.g. by inserting delays between successive packets to reduce the burstiness of the outbound traffic on a per connection level as described below in further detail. The insertion of delays into the outbound packet stream may also be referred to herein as pacing or send pacing . As a result of the introduced delays the likelihood of temporary congestion at one or more network devices may be reduced. The reduction in burstiness and the accompanying reduction in congestion may actually allow the traffic manager to gradually reduce the delays between packets thereby potentially achieving a higher bandwidth than was possible before the delays were introduced leading to a reduction in the total time taken to complete the data transfer and the corresponding application task. At the same time the introduction of delays between packets may also have indirect beneficial effects in some embodiments on other applications that may or may not have their own associated traffic managers. For example the delays introduced in one application s outbound traffic may lead to a reduction in packet loss and or retransmission rates of other data transfers of other applications thus potentially reducing the protocol initiated throttling effects that may affect the other data transfers as well.

In some embodiments the bursty nature of the traffic generated by sending endpoints involved in bulk data transfers may be a significant cause of micro congestion at a network device . illustrates exemplary time distributions of packets sent for three bulk data transfers A B and C and how the combination of the traffic for the three data transfers may result in a buffer overrun at a network device according to one embodiment. Data transfer A includes a transmission of three separate bursts A A and A of data packets separated by time periods in which A does not transmit any data. Similarly data transfer B includes bursts B and B while data transfer C includes bursts C and C. Time i.e. time at which the packets are received at a network device progresses from left to right in and the height of the bursts is indicative of the number of packets in the bursts. The burstiness of the traffic illustrated in may be caused for example by the sending application for each of the data transfers A B and C may simply handing over data packets as fast as possible to the networking software stack at the sending endpoint . The networking software stack typically transfers the data as fast as possible to a network interface card connected to the network . The data packet sizes supported by the network hardware device are often relatively small compared to bulk data transfer sizes e.g. in a typical implementation where the size of an Ethernet packet is limited to about 1500 bytes a 64 kilobyte chunk of application data may be transferred as a burst of about 40 very closely spaced packets. The gaps between successive bursts may for example indicate time needed by the sending application to transfer an additional chunk of data to the networking stack and or to receive an application level acknowledgment from the receiving endpoint .

In the example illustrated in the packets for each of the data transfers A B and C are assumed to be directed to a single port of a network device . The cumulative impact of the bursty traffic of data transfers A B and C at the network device is illustrated in the lower portion of where a dashed horizontal line indicates a buffer limit e.g. a maximum storage capacity of an input buffer for the port at which the data for the three transfers is received. At times T and T the cumulative number of packets for the three data transfers A B and C exceeds the number of packets that can be simultaneously buffered and as a result some of the packets may be dropped potentially leading to a reduction in data transfer throughput as described earlier. A similar impact may occur on output as well e.g. if the output buffer capacity limit is exceeded on a given port one or more packets may be dropped instead of being transmitted from the network device.

The duration of the delays e.g. A B and C may be computed by traffic manager based on the estimate or measurement of end to end throughput in some embodiments. In other embodiments in response to detecting that the end to end throughput is lower than a threshold a default initial delay e.g. 10 milliseconds between every Ethernet packet or a random delay e.g. a delay randomly selected between 1 and n milliseconds where n may be a parameter provided to the traffic manager may be introduced into the data transfer. After delays are introduced into an outbound data transfer a traffic manager may continue to monitor the available end to end bandwidth for the data transfer in one embodiment e.g. to determine trends in the available bandwidth. The traffic manager may increase or decrease the delay durations for future packets of the same data transfer based on the trends. For example if a traffic manager introduced delays that limit the achievable throughput for a given data transfer to 50 Megabits second and the measured throughput remains approximately 50 Megabits second for a specified period after the delays are initially introduced the traffic manager may decrease the delay duration to determine whether conditions in the network may allow a higher throughput. In contrast in one implementation if the maximum achievable throughput after an introduction of a delay is 50 Megabits second and the observed throughput over a period drops down to 20 Megabits second the traffic manager may increase the delay duration. In some embodiments using real time measurements of available bandwidth or achieved throughput the traffic manager may adjust to changing conditions in the network fairly rapidly while reducing the likelihood of micro congestion at the network devices .

It is noted that limited buffer capacity at one or more network devices as illustrated in . may represent just one of many different possible sources of micro congestion. In some embodiments for example micro congestion may be caused by a misconfiguration of a network device by a transient bug in software at a network device and or as a result of a large number of packets that legitimately happen to be directed to a given port or device at about the same time. The adaptive introduction of delays in outbound traffic as described above based on an estimate or measurement of end to end available bandwidth may help to alleviate the negative effects of micro congestion independent of the specific source of the micro congestion in different embodiments.

In some embodiments one or more components of a traffic manager may be implemented at various levels of a software stack at an endpoint . is a block diagram illustrating an abstracted view of a sending endpoint showing various hardware and software components with the software components operating within either a user space or a kernel space according to one embodiment. In the illustrated embodiment the endpoint may be configured to support communications using TCP IP. Alternative embodiments may support other communication protocols in addition to TCP IP or instead of TCP IP.

In the embodiment shown in the user space may include application and a traffic manager library implementing the functionality of traffic manager . For example traffic manager library may be configured to instrument packets of data generated for transmission on network to estimate or measure available end to end bandwidth and or to insert delays between successive packets or successive groups of packets as they are passed from application in user space into the kernel space . Generally speaking while multiple applications and traffic manager libraries may be included in a given endpoint only a single application and library are shown for ease of discussion. In one embodiment traffic manager library may comprise a dynamically linked library DLL that includes code to support network communications. In one embodiment library may be loaded prior to a communication socket library on a per process basis. For example library may provide an Application Programming Interface API for accessing communication services provided by the operating system such as a modified version of a socket API that is invoked in lieu of the original socket API included in the operating system. While the term library is used herein to refer to software component this term is not intended to restrict the traffic manager to a particular type of software component. For example as described above library may be a DLL. However the functions performed by the code within the library may be performed by code within the application itself in a different software component or elsewhere.

Kernel space may include a system interface services and hardware interface . System interface may provide an interface between the operating system services and application code within the user space . Services may include for example protocol stacks drivers and other facilities commonly found in an operating system. Hardware interface may include code that provides an interface between the operating system and hardware components of the endpoint such as NIC . Outbound traffic corresponding to a given task of application originates at the application and proceeds down the layers of components illustrated in on its way to network . Various operations may be performed on the traffic as it flows down the stack. For example traffic manager library may instrument data packets generated by the application and or insert delays between successive application packets prior to transferring the application data to the kernel layer . The data packets may be buffered at various software layers and or at the NIC and packet headers may be added or modified at various layers as well in accordance with the specific networking protocol in use. Software and hardware components similar to those illustrated in may be present at the receiving endpoint of the data transfer where the incoming packets may be handed up the stack until the data reaches a receiving application. The receiving endpoint may also include a traffic manager library . The receiving traffic manager library may be configured to manage outbound bulk data transfers from the receiving endpoint and may also be configured to cooperate with the sending endpoint s traffic manager library e.g. to include timestamps in packets sent back to the sending endpoint to help measure or estimate end to end available bandwidth.

In the embodiment illustrated in the functionality of traffic manager may be accomplished entirely in user space without affecting either the kernel space software stack including a networking software stack or the application . For example available bandwidth estimation may be performed in user space and delays may be introduced between packets in user space without requiring any modifications to the application code or the operating system. Thus the traffic managers may be platform independent in such embodiments for example the sending and receiving endpoints may use different operating systems and or different computer architectures. In alternative embodiments at least some of the functionality of traffic manager may be implemented in kernel space. is a block diagram illustrating various components of a sending endpoint according to an embodiment in which a kernel space traffic manager driver implements at least part of the functionality of traffic manager . Several of the components of endpoint shown in are present in as well such as application system interface services and drivers hardware interface and NIC . In one embodiment traffic manager driver may be configured to receive an estimate of available end to end bandwidth or to compute end to end bandwidth corresponding to a given data transfer in progress and to introduce delays between packets of the data transfer before the packets are transferred to the NIC . As in the case of the embodiment of no code changes may be required in the application or in the operating system or networking stack to implement the functionality of traffic manager driver in some embodiments. A similar traffic manager driver may be included at receiving endpoints in some embodiments. Traffic manager driver of may be configured to transparently shape outbound traffic for a plurality of applications in such embodiments without for example requiring each application to separately invoke traffic manager components at the user level. It is noted that while and illustrate two exemplary layers of an endpoint s software stack at which functionality of traffic manager may be implemented in other embodiments traffic manager functionality may be implemented at any layer or at any combination of various layers of an endpoint software stack. In one embodiment kernel level and user level components of traffic manager may coexist at the same endpoint and the different components may be configured to cooperatively perform the functionality of the traffic manager. For example in one implementation end to end available bandwidth may be measured performed by a user level traffic manager component using user level instrumentation or external network performance tools while delays between successive packets may be introduced by a kernel level traffic manager driver.

In one embodiment a traffic manager may be configured to insert timestamps and or data transfer identifiers into the packets of a data transfer e.g. in order to obtain estimates of the network bandwidth available for the data transfer. is a block diagram illustrating contents of an exemplary packet instrumented by a traffic manager according to one embodiment. As shown the packet may comprise a header and a data body . The header may comprise a number of fields such as a destination address a checksum etc. that may for example be generated by various layers of a networking software stack. The data body may include a portion of the application data corresponding to the data transfer as well as instrumentation metadata comprising one or more timestamps and data transfer identifiers inserted by a traffic manager . In some implementations the instrumentation metadata may be inserted into an otherwise unused portion of the packet such as a padding region used to ensure that the packets have a minimum or standard size according to the networking protocol being used . A data transfer identifier may be used to uniquely identify a given data transfer e.g. to help separately monitor the performance achieved for each data transfer. For example packets corresponding to a data transfer of a video file to a particular client of a media server may include a first data transfer identifier while packets corresponding to a data transfer of another video file to another client may include a second data transfer identifier. In one embodiment a traffic manager at a sending endpoint which may be referred to herein as a sending traffic manager may be configured to generate a new unique identifier for each application task that is initiated and use the generated identifier to identify the data packets corresponding to the application task. In some embodiments the sending traffic manager may insert a timestamp indicating the current time at the sender and or the data transfer identifier into the data body at the time that the data of the packet is transferred from user space to kernel space while in other embodiments the timestamp and or identifier may be inserted at other times during the passage of the packet down the sending endpoint s software stack e.g. just before the packet is transferred to a NIC . In one embodiment a traffic manager at a receiving endpoint referred to herein as a receiving traffic manager may be configured to send a receive timestamp indicating when the packet is received at the receiving endpoint along with the original timestamp and the data transfer identifier back to the sending traffic manager e.g. as part of a data body of an acknowledgement packet sent from the receiving endpoint . Using the receive timestamp the original timestamp and the data transfer identifier the sending traffic manager may be able to compute an estimate of the bandwidth currently available for the outbound data transfer. Using an indication of the current time when the receive timestamp reaches the sending endpoint the sending traffic manager may also be able to estimate the current inbound bandwidth available and use this estimate to compute and introduce delays into the outbound packet stream in accordance with the available bandwidth and or with trends identified in the available bandwidth over time. In some embodiments the traffic manager at a sending endpoint may be configured to synchronize clocks with a traffic manager at a receiving endpoint in order to ensure that the timestamps provided by both sides of the data transfer provide accurate indications of the time taken to transfer a packet.

A number of variations of the basic timestamp based technique described above may be used in various embodiments. For example in one embodiment only a subset of the packets may be instrumented e.g. every fifth packet or every tenth packet may be instrumented by the sending traffic manager and corresponding acknowledgements from the receiving traffic manager may be used to estimate the currently available bandwidth. In another embodiment instead of modifying packets that contain application data the sending side traffic manager may send special diagnostic packets from time to time to the receiving endpoint to help estimate the available bandwidth. The special diagnostic packets may not contain application data in some embodiments instead the data bodies of the diagnostic packets may contain e.g. control information about the data transfer and or padding inserted to make the packet reach a desired size. In one implementation instead of sending a receive timestamp in an acknowledgment the receiving traffic manager may send its estimates of the transmission time for the packet and or its estimates of the available bandwidth back to the sending traffic manager. The instrumentation metadata may in some embodiments include different fields than those shown in e.g. in one implementation additional fields such as sequence numbers may be included in the metadata while in other implementations data transfer identifiers may be omitted and the traffic managers may use information contained in the protocol specific headers to map packets to application tasks.

In some embodiments e.g. in order to ensure that its estimates of available bandwidth are not skewed by outlying data points the sending traffic manager may be configured to generate an average value for the available bandwidth using instrumentation data corresponding to several packets. In one implementation for example the sending traffic manager may be configured to compute a running average of a specified number of packet transmission times and use this average to update its estimate of available bandwidth. A user specified or application specified parameter may for example direct the traffic manager to use at least N data points in computing available bandwidth and or the delays to be inserted into the outgoing packet stream. In one embodiment the sending traffic manager may be configured to store historical performance data e.g. indicating respective trends in packet transmission times estimated bandwidth and introduced inter packet delay durations over time. The historical performance data may be used to adjust delays introduced in the outbound packets to changing network conditions e.g. if the historical performance data indicates that network congestion is generally easing over time the sending traffic manager may decrease the duration of the delays introduced between the outbound packets and monitor the resulting packet transmission times to determine whether a higher throughput is sustainable.

Based at least in part on the estimate of available end to end bandwidth the traffic manager may be configured to determine a delay to be introduced between packet sends block and introduce the delays into the outbound packet sequence corresponding to the application level data transfer block . For example if the estimated available bandwidth is 5 Megabits per second and bursts of data are being generated at the sending application at 25 Megabits per second the traffic manager may compute a delay that effectively reduces the outbound data rate for the data transfer to slightly less than 5 Megabits per second. The delays may be introduced for example to smoothen or reduce spikes in the outgoing data transfer before the data leaves the sending endpoint and enters the network so that the likelihood of micro congestion e.g. buffer overruns at network devices such as routers switches etc. is reduced. The traffic manager thus adapts the traffic flow to current conditions on a per application task basis. In some embodiments the traffic manager may be configured to obtain the estimates and determine and introduce the delays on a per connection basis. For example if a given application task involves data transfer over a particular connection e.g. a socket connection an estimate for the bandwidth available for that particular connection may be obtained and delays specifically computed for that particular connection may be introduced between packets sent on that particular connection. If the same application uses another connection for another data transfer e.g. to a different receiving endpoint a different delay may be used for the other connection based on the current conditions pertaining to the other connection indicated by an estimate of the bandwidth available for the other connection. Thus the parameters used to adjust outgoing traffic flow may differ from one connection to another and or from one application task to another even for the same application at the same endpoint. Concurrent data transfers originating from the same endpoint for the same sending application or for different applications may have different estimated available bandwidths and different introduced delays.

In some embodiments one or more input parameters may be used to control aspects of the operation of a traffic manager . For example an input parameter may be used to specify how closely traffic manager should attempt to match the estimated available bandwidth for a given application s data transfers or for a particular data transfer. If the input parameter indicates that the traffic manager should be aggressive and attempt to maximize throughput as much as possible the duration of the introduced delays may be set such that the sending transfer rate is equal to or very slightly lower than the estimate of available bandwidth. If the input parameter indicates that the traffic manager should not be as aggressive in maximizing throughput longer delays may be introduced at least initially e.g. if the available bandwidth is estimated at 10 Megabits second delays that reduce the outgoing traffic rate to 8 Megabits second or even lower may be introduced. Parameters may also be used to indicate relative priorities of different applications or different tasks which may then be used by the traffic manager to reduce delays for a high priority data transfer more quickly than for a low priority data transfer the minimum durations of monitoring periods to use for estimating available bandwidth etc. in various embodiments. In one embodiment a maximum allowed delay may be specified using an input parameter while in another embodiment input parameters may be used to identify one or more applications for which no delays are to be introduced. Parameters may also be used to control various other aspects of the operation of traffic manager such as whether packets containing application data are to be instrumented or whether diagnostic packets are to be used instead to estimate available bandwidth. Input parameters for traffic manager may be specified via any of a variety of interfaces in various embodiments such as a graphical user interface GUI one or more parameter files environment variables command line options etc.

After delays based on estimated bandwidth availability are introduced between a set of packets the traffic manager may determine if more data remains to be sent for the current data transfer block of . The sending application may in some embodiments provide an indication to the traffic manager when all the data has been transferred successfully e.g. by closing a socket connection that was being used for the transfer. If no more data is to be sent the traffic manager may end the data transfer block . If however data remains to be sent in one embodiment traffic manager may be configured to monitor ongoing performance of the data transfer block e.g. to observe the effect of the delays and to identify trends in available bandwidth. The trends may indicate how available bandwidth varies over a time period e.g. the last second or the last minute in which data points corresponding to different points in time were obtained. For example transfer times for several packets whose sending times were separated by introduced delays may be obtained using the instrumentation techniques described above and may be used to determine how closely the actual achieved transfer rate matches the sending rate the rate at which the data is emitted from the sending endpoint . The results of the monitoring may be used to determine whether the duration of the introduced delays needs to be adjusted block and if so a modified delay duration may be determined block and applied to the a subsequent set of packets block . The adjustments to outgoing traffic pattern of a given data transfer may be performed in real time and may be based on real time measurements on a per connection and or a per data transfer level. Operations corresponding to blocks and may be repeated until the data transfer eventually ends.

In adjusting the delays the traffic manager may generally attempt to maximize throughput in view of the latest results of the monitoring. For example if the traffic manager determines that the throughput achieved for the current data transfer continues is falling despite the introduction of a particular delay between successive packets the duration of the delay may be increased for subsequent packets. If the achieved throughput matches the rate at which the packets are being sent the delay may be shortened to see if the network can now provide a higher bandwidth. Various parameters described above such as relative priorities allowed ranges of inter packet delays etc. may also be used in making the adjustments. It is noted that in various embodiments not all the operations illustrated in may be performed as shown or in the same order as shown. For example in one implementation the actual computation of the delay durations may be performed by another software or hardware module and the durations to be used may be provided to the traffic manager by the other software or hardware module.

In addition to traffic manager memory and or storage devices may also store operating systems software and or software for various applications in various embodiments. In some embodiments part or all of traffic manager may be included an operating system a storage management software product or another software package while in other embodiments traffic manager may be packaged as a standalone product. In some embodiments the component modules of a traffic manager may be distributed across multiple hosts or may be replicated at a plurality of hosts . In one embodiment part or all of the functionality of traffic manager may be implemented via one or more hardware devices e.g. via one or more Field Programmable Gate Array FPGA devices or in firmware. It is noted that in addition to or instead of computer hosts in some embodiments endpoints linked to network may include a variety of other devices configured to implement applications and traffic managers such as television set top boxes mobile phones intelligent stereo devices etc.

Although the embodiments above have been described in considerable detail numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.


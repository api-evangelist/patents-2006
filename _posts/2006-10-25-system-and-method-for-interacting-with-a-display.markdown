---

title: System and method for interacting with a display
abstract: A system and method of interacting with a display. The method comprises recognizing a disturbance in a display zone of a projected image and displaying a selected state in response to the recognized disturbance. The method further includes recognizing a gesture which interrupts a light source and is associated with an action to be taken on or associated with the displayed selected state. An action is executed in response to the recognized gesture. The system includes a server having a database containing data associated with at least one or more predefined gestures, and at least one of a hardware and software component for executing an action based on the at least one or more predefined gestures.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08356254&OS=08356254&RS=08356254
owner: International Business Machines Corporation
number: 08356254
owner_city: Armonk
owner_country: US
publication_date: 20061025
---
The invention generally relates to a system and method for interacting with a projected display and more particularly to a system and method for interacting with a projected display utilizing gestures capable of executing menu driven commands and other complex command structures.

Businesses strive for efficiencies throughout their organization. These efficiencies result in increased productivity of their employees which in turn results in increased profitability for the business and if publicly traded its shareholders. To achieve such efficiencies by way of examples it is not uncommon to hold meetings or make presentations to audiences to discuss new strategies advances in the industry and new technologies etc.

In such meetings presentation boards or so called whiteboards are one way to present material relevant to the presentation or meeting. As is well known a whiteboard allows a presenter to write using special dry erase markers. When the text is no longer needed such material may be erased so that the user can continue with the presentation for example. But unfortunately often the text needs to be saved in order to refer back to the material or place new material in the proper context. In these situations an attendee may save the material by manually copying the text in a notebook before the image is erased by the presenter. A problem with this approach is that it is both time consuming and error prone. Also the use of whiteboards is limited because it is difficult to draw charts or other graphical images and it is not possible to manipulate data.

In another approach it is not uncommon to use large scrolls or tear off pieces of paper to make the presentation. By using this approach the presenter merely removes the paper from the pad or rolls the paper and then continues with the next sheet. This approach though can be cumbersome and although it allows the presenter to refer back to past writings it is not very efficient. Additionally this can result in many different sheets or very large scrolls of one sheet which can become confusing to the audience and even the presenter. Also as with the above approach it is difficult to draw charts or other graphical images and it is not possible to manipulate data.

In a more technology efficient approach the presenter can present charts or other graphical images to an audience by optically projecting these images onto a projection screen or a wall. In known applications an LCD liquid crystal display projector is commonly used as the image source where the charts text or other graphical images are electronically generated by a display computer such as a personal computer PC or a laptop computer. In such display systems the PC provides video outputs but interaction with the output is limited at best.

Also whether the presenter is standing at a lectern or is moving about before the audience there is little direct control over the image being displayed upon the projection screen when using a conventional LCD PC projection display system. For example a conventional system requires the presenter to return to the display computer so as to provide control for the presentation. At the display computer the presenter controls the displayed image by means of keystrokes or by mouse commands with a cursor in the appropriate area of the computer monitor display screen.

In some applications an operator may use a remote control device to wirelessly transmit control signals to a projector sensor. Although the presenter acquires some mobility by means of the remote control device the presenter still cannot interact with the data on the screen itself that is the operator is limited to either advancing or reversing the screen.

Accordingly there exists a need in the art to overcome the deficiencies and limitations described hereinabove.

In a first aspect of the invention a method comprises recognizing a disturbance in a display zone of a projected image and displaying a selected state in response to the recognized disturbance. The method further includes recognizing a gesture which interrupts a light source and is associated with an action to be taken on or associated with the displayed selected state. An action is executed in response to the recognized gesture.

In another aspect of the invention the method comprises projecting an image on a surface using at least a source of light and a processor configured to store and execute application programs associated with the image. The method senses a first action in a display zone of the image and validates the first action. The method displays a selected state in response to the validated first action. The method further senses a gesture interrupting the light source and validates that the gesture is associated with a pre defined command and the displayed selected state. The method executes the pre defined command in response to the validated gesture.

In another aspect of the invention a system comprises a server having a database containing data associated with at least one or more predefined gestures and at least one of a hardware and software component for executing an action based on the at least one or more predefined gestures. The hardware and software compares a first action in an interaction zone to a predefined template of a shape and a second action which interrupts a light source to the at least one or more predefined gestures. The system validates the first action and the second action based on the comparison to the predefined template and the at least one or more predefined gestures. The system executes the action based on the validating of the first action and the second action.

In yet another aspect of the invention a computer program product comprising a computer usable medium having readable program code embodied in the medium includes at least one component to perform the steps of the invention as disclosed and recited herein.

In still another embodiment a method comprises recognizing a first action of a first object and a second action of a second. The method further includes validating a movement comprising a combination of the first action and the second action by comparison to predefined gestures and executing a complex command based on the validating of the combination of the first action and the second action.

In a further aspect of the invention a method for deploying an application for web searching which comprises providing a computer infrastructure. The computer infrastructure is operable to project an image on a surface sense a first action in a predefined interaction zone of the image validate the first action and displaying a selected state sense a gesture validate that the gesture is associated with a pre defined action and execute the pre defined action in response to the validated gesture.

The invention is directed to a system and method for interacting with a projected display and more specifically to a system and method for interacting with a projected display utilizing gestures capable of executing menu driven commands and other complex command structures. The system and method can be implemented using a single computer over any distributed network or stand alone server for example. In embodiments the system and method is configured to be used as an interactive touch screen projected onto any surface and which allows the user to perform and or execute any command on the interactive touch screen surface without the need for a peripheral device such as for example a mouse or keyboard. Accordingly the system and method is configured to provide device free non tethered interaction with a display projected on any number of different surfaces objects and or areas in an environment.

The system and method of the invention projects displays on different surfaces such as for example walls desks presentation boards and the like. In implementations the system and method allows complex commands to be executed such as for example opening a new file using a drag down menu or operations such as cutting copying pasting or other commands that require more than a single command step. It should be understood though that the system and method may also implement and execute single step commands.

In embodiments the commands are executed using gestures which are captured reconciled and executed by a computer. The actions to be executed in one implementation require two distinct actions by the user as implemented by a user s hands pointers of some kind or any combination thereof. Thus the system and method of the invention does not require any special devices to execute the requested commands and accordingly is capable of sensing and supporting forms of interaction such as hand gestures and or motion of objects etc. to perform such complex operations.

In embodiments the system and method can be implemented using for example the Everywhere Display manufactured and sold by International Business Machines Corp. Everywhere Display and IBM are trademarks of IBM Corp. in the United States other countries or both. By way of example the Everywhere Display can provide computer access in public spaces facilitate navigation in buildings localize resources in a physical space bring computational resources to different areas of an environment and facilitate the reconfiguration of the workplace.

In general the processor executes computer program code which is stored in memory A and or storage system B. While executing computer program code the processor can read and or write data from look up tables which are the basis for the execution of the commands to be performed on the computer to from memory A storage system B and or I O interface . The bus provides a communications link between each of the components in the computing device . The I O device can comprise any device that enables an individual to interact with the computing device or any device that enables the computing device to communicate with one or more other computing devices using any type of communications link.

The computing device can comprise any general purpose computing article of manufacture capable of executing computer program code installed thereon e.g. a personal computer server handheld device etc. . However it is understood that the computing device is only representative of various possible equivalent computing devices that may perform the processes described herein. To this extent in embodiments the functionality provided by computing device can be implemented by a computing article of manufacture that includes any combination of general and or specific purpose hardware and or computer program code. In each embodiment the program code and hardware can be created using standard programming and engineering techniques respectively.

Similarly the computer infrastructure is only illustrative of various types of computer infrastructures for implementing the invention. For example in embodiments the computer infrastructure comprises two or more computing devices e.g. a server cluster that communicate over any type of communications link such as a network a shared memory or the like to perform the process described herein. Further while performing the process described herein one or more computing devices in the computer infrastructure can communicate with one or more other computing devices external to computer infrastructure using any type of communications link. The communications link can comprise any combination of wired and or wireless links any combination of one or more types of networks e.g. the Internet a wide area network a local area network a virtual private network etc. and or utilize any combination of transmission techniques and protocols. As discussed herein the management system enables the computer infrastructure to recognize gestures and execute associated commands.

In embodiments the invention provides a business method that performs the process steps of the invention on a subscription advertising and or fee basis. That is a service provider such as a Solution Integrator could offer to perform the processes described herein. In this case the service provider can create maintain and support etc. a computer infrastructure that performs the process steps of the invention for one or more customers. In return the service provider can receive payment from the customer s under a subscription and or fee agreement and or the service provider can receive payment from the sale of advertising content to one or more third parties.

Still referring to a camera is also connected to the computer and is configured to capture gestures or motions of the user and provide such gestures or motions to the computer for reconciliation and execution of commands as discussed in greater detail below . The camera is preferably a CCD based camera which is configured and located to capture motions and the like of the user. The camera and other devices may be connected to the computer via any known networking system as discussed above.

More specifically as shown in the system and method of the invention texture maps the image to be displayed onto a virtual computer graphics 3D surface VS identical minus a scale factor to the actual surface AS . The view from the 3D virtual camera should correspond exactly or substantially exactly to the view of the projector if the projector was the camera when 

In embodiments a standard computer graphics board may be used to render the camera s view of the virtual surface and send the computed view to the projector . If the position and attitude of the virtual surface VS are correct the projection of this view compensates the distortion caused by oblique projection or by the shape of the surface. Of course an appropriate virtual 3D surface can be uniquely used and calibrated for each surface where images are projected. In embodiments the calibration parameters of the virtual 3D surface may be determined manually by projecting a special pattern and interactively adjusting the scale rotation and position of the virtual surface in the 3D world and the lens angle of the 3D virtual camera.

In embodiments the services layer includes six modules . For example a vision interface module vi may be responsible for recognizing gestures and converting this information to the application e.g. program being manipulated by the gestures . A projection module pj may handle the display of visual information via the projector on a specified surface while a camera module sc provides the video input via the camera from the surface of interest to the vision interface vi . The camera as discussed above will send the gestures and other motions of the user. Interaction with the interface by the user comprises orchestrating the vision interface projection module and camera module through a sequence of synchronous and asynchronous commands which are capable of being implemented by those of skill in the art. Other modules present in the services layer include a 3D environment modeling module a user localization module and a geometric reasoning module

The 3D environment modeling module can be a version of standard 3D modeling software. The 3D environment modeling module can support basic geometric objects built out of planar surfaces and cubes and allows importing of more complex models. In embodiments the 3D environment modeling module stores the model in XML format with objects as tags and annotations as attributes. The 3D environment modeling module is also designed to be accessible to the geometric reasoning module as discussed below.

The geometric reasoning module is a geometric reasoning engine that operates on a model created by a modeling toolkit which in embodiments is a version of standard 3D modeling software. The geometric reasoning module enables automatic selection of the appropriate display and interaction zones hotspots based on criteria such as proximity of the zone to the user and non occlusion of the zone by the user or by other objects. In this manner gestures can be used to manipulate and execute program commands and or actions. Applications or other modules can query the geometric reasoning module F through a defined XML interface.

In embodiments the geometric reasoning module receives a user position and a set of criteria specified as desired ranges of display zone properties and returns all display zones which satisfy the specified criteria. The geometric reasoning module may also have a look up table or access thereto for determining gestures of a user which may be used to implement the actions or commands associated with a certain application. The properties for a display zone may include amongst other properties the following 

The user localization module is in embodiments a real time camera based tracking to determine the position of the user in the environment as well as in embodiments gestures of the user. In embodiments the user localization module can be configured to track the user s motion to for example move the display to the user or in further embodiments recognize gestures of the user for implementing actions or commands.

In embodiments the tracking technique is based on motion shape and or flesh tone cues. In embodiments a differencing operation on consecutive frames of the incoming video can be performed. A morphological closing operation then removes noise and fills up small gaps in the detected motion regions. A standard contour tracing algorithm then yields the bounding contours of the segmented regions. The contours are smoothed and the orientation and curvature along the contour is computed. The shape is analyze for each contour to check if it could be a head or other body part or object of interest which is tracked by the system and method of the invention.

In the example of a head the system looks for curvature changes corresponding to a head neck silhouette e.g. concavities at the neck points and convexity at the top of the head . In embodiments sufficient flesh tone color within the detected head region is detected by matching the color of each pixel within the head contour with a model of flesh tone colors in normalized r g space. This technique detects multiple heads in real time. In embodiments multiple cameras with overlapping views to triangulate and estimate the 3D position of the user are possible. This same technique can be used to recognize gestures in order for the user to interact with the display e.g. provide complex commands.

In embodiments the integration layer provides a set of classes that enable a JAVA application to interact with the services. Java and all Java based trademarks are trademarks of Sun Microsystems Inc. in the United States other countries or both. The integration layer in embodiments contains a set of JAVA wrapper objects for all objects and commands along with classes enabling synchronous and asynchronous communication with modules in the services layer . The integration layer in embodiments mediates the interaction among the services layer modules . For example through a single instruction to the interaction manager a JAVA application can start an interaction that sends commands to the vision interface the projection module and the mirror defining instantiating activating and managing a complex interactive display interaction. Similarly the integration layer for example can coordinate the geometric reasoning module and the 3D environment modeler in a manner that returns the current user position along with all occluded surfaces to the application at a specified interval.

In embodiments the application layer comprises a set of classes and tools for defining and running JAVA applications and a repository of reusable interactions. In embodiments each interaction is a reusable class that is available to any application. An application class for example is a container for composing multiple interactions maintaining application state during execution and controlling the sequence of interactions through the help of a sequence manager . Other tools may also be implemented such as for example a calibrator tool that allows a developer to calibrate the vision interface module the projection module and the camera module for a particular application.

In embodiments the user interacts with the projected display by using hand gestures over the projected surface as if the hands for example were a computer mouse. Techniques described above such as for example using the geometric reasoning module or the user localization module can be implemented to recognize such gesturing. By way of non limiting illustration the geometric reasoning module may use an occlusion mask which indicates the parts of a display zone occluded by objects such as for example hand gestures of the user.

More specifically in embodiments the camera may perform three basic steps i detecting when the user is pointing ii tracking where the user is pointing and iii detecting salient events such as a button touch from the pointing trajectory and gestures of the user. This may be performed for example by detecting an occlusion of the projected image over a certain zone such as for example an icon or pull down menu. This information is then provided to the computer which then reconciles such gesture with a look up table for example.

As a further example the invention further contemplates that a complex command can be executed based on a combination of movements by two or more objects such as for example both of the user s hands. In this embodiment the system and method of the invention would attempt to reconcile and or verify a motion gesture of each object e.g. both hands using the look up table of and for example. If both of the motions cannot be independently verified in the look up table for example the system and method would attempt to reconcile and or verify both of the motions using a look up table populated with actions associated with combination motions. By way of one illustration an S motion of both hands which are motions not recognized independently may be a gesture for taking an action such as requesting insertion of a watermark in a word processing application. It should be recognized by those of skill in the art that all actions whether for a single motion or combination of motions etc. may be populated in a single look up table or multiple look up tables without any limitations.

The fingertip template of is kept short in embodiments so that it will match fingertips that extend only slightly beyond their neighbors and will match fingertips within a wider range of angles. As a result the template often matches well at several points in the image. It should be readily understood that other templates may also be used with the invention such as for example pointers and other objects. These templates may also be used for implementing and recognizing the gestures referring back to and

In particular shows a process flow diagram describing a scenario in which a user performs a series of actions using the gesture based user interaction grammar provided herein. At step a user approaches a wall or other surface on which the system of the invention has projected a User Interface UI for a given application. At step the user is recognized by the camera as they disturb the field of vision for the camera. At step the user opts to open a menu from an icon in the UI via a right click action. In embodiments the user selects the icon with dominant hand e.g. left hand . In one example button touches are detected by examining the hand trajectory for several specific patterns that indicate this type of motion.

At step the camera recognizes the disturbance of the hotspot zone associated to the selected icon and calls the system to validate that the shape of the disturbance is identified in the template. At step a determination is made to establish if the shape of the disturbing object is a valid shape in the template. If not then at step no action is taken however as described above in embodiments the system may recognize a second disturbance or gesture at which time the system will make a determination that the combination of the first and second motions e.g. disturbances are a unique valid gesture for an action to be taken.

If a valid shape is found at step then the system displays the selected state of the selected icon at step . In an alternative embodiment the system may recognize two gesture simultaneously at which time the system will make a determination as to whether the combination of gestures is associated with an action. If so an appropriate action will be taken. This same or similar processing may continue with other examples.

At step after successful display of the selected state of the icon at step the user uses the non dominant hand e.g. right hand to articulate the gesture associated to a right click action for example counter clockwise rotation see look up table of and . At step the camera recognizes the articulation of the gesture and at step the system performs lookup to validate that the gesture resides in the system and is associated to an action.

At step a determination is made as to whether the gesture is associated to an action. If there is no associated action the system will revert to step and take no action. If there is an associated action at step the system will execute the action e.g. display open menu . Thus after the system successfully identifies the articulated gesture the system displays the appropriate action e.g. opening a menu associated to the initially selected icon .

At step the user selects from one of X number of possible navigational menu options. At step the camera recognizes the disturbance of the hotspot interaction zone associated to the selected menu item and calls to validate that the shape of the disturbance is identified in the template. At step a determination is made as to whether the shape of the disturbing object is a valid shape in the template. If not recognized then the system reverts back to step and takes no action. If the gesture is valid recognized then at step the system displays the selected state of the selected menu item.

At step after successful display of the selected state of the menu item the user uses the non dominant hand for example to articulate the gesture associated to a single left click action single clockwise rotation see look up table of and . At steps and the camera recognizes the articulation of the gesture and the system performs lookup to validate that the gesture resides in the system and is associated to an action.

At step the system makes a determination if the gesture is associated with an action. If not the system again reverts back to step . If there is an associated action at step the system execute the associated action navigate user to associated screen in the UI in this case . The process then ends at E .

In a more generalized embodiment a user points to a particular zone within the display area e.g. a certain application. The system of the invention would recognize such action by the methods noted above. In embodiments once the system recognizes the user within a zone and verifies that this is the proper zone the system would lock that selection. Once locked the user can then provide a gesture such as for example an e shape to exit the application which will then verified and executed by the system of the invention.

While the invention has been described in terms of embodiments those skilled in the art will recognize that the invention can be practiced with modifications and in the spirit and scope of the appended claims.


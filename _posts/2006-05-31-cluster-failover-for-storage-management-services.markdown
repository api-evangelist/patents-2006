---

title: Cluster failover for storage management services
abstract: A method, system, and computer program product to enable other nodes in a cluster to resume operations of a failed node. These operations include storage management services that allow configuration changes to be made dynamically to storage resources. Resource configuration data are synchronized on a set of nodes in a cluster immediately when a resource configuration change is made. If a node that has made a resource configuration change fails, the resource configuration change is available for use by other nodes in the set, each of which can resume operations of the failed node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07555673&OS=07555673&RS=07555673
owner: Symantec Operating Corporation
number: 07555673
owner_city: Cupertino
owner_country: US
publication_date: 20060531
---
This application is a continuation of U.S. patent application Ser. No. 10 273 213 entitled Cluster Failover for Storage Management Services filed Oct. 17 2002 now U.S. Pat. No. 7 058 846 and naming Rasesh A. Kelkar Swanand A. Vaidya Rupali K. Tathavdekar and Aditya V. Deshpande as inventors.

Portions of this patent application contain materials that are subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office file or records but otherwise reserves all copyright rights whatsoever.

Information drives business. A hardware or software failure affecting a data center can cause days or even weeks of unplanned downtime and data loss that could threaten an organization s productivity. For businesses that increasingly depend on data and information for their day to day operations this unplanned downtime can also hurt their reputations and bottom lines. Businesses are becoming increasingly aware of these costs and are taking measures to plan for and recover from hardware and software failure and disasters affecting entire data centers.

One strategy to recover from failure of hardware and or software is clustering wherein computer systems and storage devices are interconnected typically at high speeds within a local data center. Clustering is used for various purposes including improving reliability availability serviceability and or performance via load balancing. Redundant interconnections between the computer systems are typically included and the collection of computer systems storage devices and redundant interconnections is referred to herein as a cluster. The cluster appears to users as a single highly available system. Different types of clusters may be established to perform independent tasks to manage diverse hardware architectures performing similar tasks or when local and backup computer systems are far apart physically.

Often computer systems within a cluster use a common pool of storage devices and the purpose of the cluster is to provide an alternative processing resource for the data on the shared storage devices in the event of failure of one of the computer systems. In some clustering environments only one of the computer systems in the cluster provides processing resources with respect to a particular software application. The computer system currently providing processing resources in the cluster for a particular software application is referred to herein as the primary node and other computer systems in the cluster are referred to herein as backup or secondary nodes.

Each clustered computer system typically runs special software to coordinate the activities of the computer systems in the cluster. This software is referred to herein as a cluster manager. A cluster manager may monitor the health of sites in a distributed system and restart an application on another node when the node running the application fails. Typically cluster management functions are limited to such clustering operations as monitoring starting and stopping resources. Communication between nodes in a cluster is typically limited to messages to check the heartbeat of other nodes in the cluster and to ensure proper operation of the cluster.

Clustering and storage technologies have grown substantially in recent years and changes in one technology sometimes require changes in the other for interoperability. Most storage devices in use today are not specially adapted to operate in a clustering environment and configuration data about the storage devices are typically maintained by host computer systems acting as servers for the storage devices. In some environments configuration data about storage resources are maintained in files or databases on the host computer system. If a server for a given storage resource fails configuration data about the storage resource can be inaccessible to other nodes in the cluster. A new node resuming operations of the failed node would be unaware of the configuration change and may be unable to communicate properly with the reconfigured storage resource.

What is needed is a system that enables other nodes in a cluster to resume operations of a failed node. These operations should include storage management services that allow configuration changes to be made dynamically to storage resources. Storage configuration information should be made available to some or all nodes in a cluster in as close to real time as possible after making a storage configuration change. The solution should impose minimal or no overhead on operation of the nodes. If a node that has made a resource configuration change fails the resource configuration change should be made available to another node resuming operations of the failed node.

The present invention provides a method system and computer program product to enable other nodes in a cluster to resume operations of a failed node. These operations include storage management services that allow configuration changes to be made dynamically to storage resources. Resource configuration data are synchronized on a set of nodes in a cluster immediately when a resource configuration change is made. If a node that has made a resource configuration change fails the resource configuration change is available for use by other nodes in the set each of which can resume operations of the failed node.

For a thorough understanding of the subject invention refer to the following Detailed Description including the appended Claims in connection with the above described Drawings. Although the present invention is described in connection with several embodiments the invention is not intended to be limited to the specific forms set forth herein. On the contrary it is intended to cover such alternatives modifications and equivalents as can be reasonably included within the scope of the invention as defined by the appended Claims.

In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the invention. It will be apparent however to one skilled in the art that the invention can be practiced without these specific details.

References in the specification to one embodiment or an embodiment means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the invention. The appearances of the phrase in one embodiment in various places in the specification are not necessarily all referring to the same embodiment nor are separate or alternative embodiments mutually exclusive of other embodiments. Moreover various features are described which may be exhibited by some embodiments and not by others. Similarly various requirements are described which may be requirements for some embodiments but not other embodiments.

The present invention provides a method system and computer program product to make resource configuration information available to nodes in a cluster in as close to real time as possible with minimal overhead. Resource configuration data are synchronized at nodes in the cluster thereby enabling a node having synchronized configuration data to resume operations of a failed node. These operations include storage management services that allow configuration changes to be made dynamically to storage resources. Examples of resource configuration changes include adding a new disk to a storage array creating a snapshot of a storage area to back up data at given point in time and so on.

In some configurations all nodes in a cluster may not be included in synchronization of resource configuration data. For example a set of nodes may be designated as failover nodes and other nodes can be set aside to perform other functions. All of the failover nodes synchronize resource configuration data. Each of the failover nodes therefore is capable of resuming operations of any other failover node. Preferably the set of failover nodes includes most of the nodes in the cluster. The set of failover nodes includes at least two nodes.

Resource configuration information is made available to nodes in a cluster immediately when a resource configuration change is made by synchronizing resource configuration data on a set of nodes of the cluster. If a node that has made a resource configuration change fails another node having synchronized resource configuration data can resume operations of the failed node because the resource configuration change has already been provided to that node.

In the environment of the present invention storage resources are defined to include one or more physical storage devices and operations are performed on storage resources. The present invention is discussed with regard to storage configuration information although one of skill in the art will recognize that the present invention applies to other types of information that may be needed to recover from failure of hardware or software. Furthermore although the present invention is discussed in terms of storage resources information about any type of shared and managed resource such as a printer can be communicated using the systems and methods described herein.

Storage access interface A represents a vendor supplied interface for managing a particular storage resource. Storage access interface A is typically installed on a host computer system coupled to the storage resource in this case node A. Storage access interface A may be for example a dynamically linked library including functions that can be called to perform storage operations. Storage access interface A may include an application programming interface a command line interface and or a graphical user interface for managing storage resource .

Storage management service A is a service for managing storage resources such as storage resource in a multi vendor storage environment. In the example shown storage management service A communicates with storage access interface A when a storage resource configuration is to be changed. Storage resource definition D defines attributes of storage resource and is used by storage access interface A to perform storage operations.

In action . an update to a storage resource configuration is provided to storage management service A. Typically an update to a resource configuration is made by an administrator using a command line interface not shown but other ways of making resource configuration changes are also within the scope of the invention. In action . the update to the storage resource configuration is provided to storage access interface A which updates storage resource definition D for storage resource in action .. In action . storage access interface A notifies storage management service A that the configuration update is complete.

One problem with system described above is that storage resource definition D is stored on node A. If node A fails storage resource cannot be used because storage resource definition D is not accessible to other nodes. To make resource configuration available to another node that can resume operation of node A upon failure the invention synchronizes resource configuration data on multiple nodes in a clustering environment.

Nodes A and B are connected via cluster communication channel which includes redundant cluster connections A and B. Node A shares common storage resource with node B. Node A is interconnected with storage resource via interconnection A and node B is interconnected with storage resource via interconnection B.

Storage resource is shown as a local storage resource although storage for a particular cluster may be accessed via a network not shown . For example storage resource may comprise a storage resource managed by a storage server not shown on a storage area network not shown . Storage resource may include intelligent and or non intelligent storage arrays.

In an example embodiment nodes A and B are configured as servers for the same application program. Nodes in a cluster can be connected to shared storage such as fibre channel or Small Computer System Interface SCSI RAID arrays via a separate fibre switch using redundant fibre channel connections or directly using SCSI fibre cables. Cluster communication channel connects nodes A and B of cluster . Redundant interconnections A and B are shown as redundant heartbeat private network connections via crossover cables between redundant network interface cards NICs when two nodes form the cluster. When more than two nodes form the cluster the private network connection can use a hub. Alternatively cluster communication channel may be implemented as a common storage area shared by nodes of cluster . Heartbeat data can be written to the common storage area and read whenever the status of nodes in the cluster is checked.

The private network provided via cluster communication channel enables failover software not shown to recognize when a system or process has failed. Redundant power sources not shown are also included in the event of a power failure. In most environments clusters also have redundant public network connections such as connections and to network to communicate via a public network such as the Internet.

Nodes A and B share resource configuration data and each node has a respective copy respectively labeled resource configuration data A and resource configuration data B. Each of resource configuration data A and B includes respective resource attributes A and B. Resource configuration data A and B including resource attributes A and B are maintained as synchronized copies using cluster communication channel . Resource agents A and B are present on both nodes A and B but only one of resource agents A and B actively manages a given resource online. Other resource agents for that given resource consider the resource to be offline on their respective nodes.

Resource configuration manager A on node A and resource configuration manager B on node B are software interfaces through which resource configuration data can be changed dynamically. Typically these changes are made by an administrator using an application programming interface a command line interface or a graphical user interface.

Maintaining resource configuration data A and B as synchronized copies is shown in the data flow of . In action . a resource configuration is changed via resource configuration manager A. In action . resource configuration manager A updates resource attributes A of resource configuration data A. In action . resource configuration manager A notifies resource agent A for the affected resource. Resource configuration manager A notifies cluster manager A in action .. Cluster manager A reads resource configuration data A and provides configuration data to all nodes in the cluster in action . via cluster communication channel . Configuration data may include all or a portion of resource configuration data A.

In action . cluster manager B receives configuration data indicating that resource attributes A are changed. In action . cluster manager B provides configuration data to resource configuration manager B. In action . resource configuration manager B updates resource attributes B using configuration data to reflect the changes made to resource attributes A. Resource configuration data A and B are now synchronized.

In the embodiment shown in the configuration change is shared by setting attributes. For example resource agent B is responsible for configuring a newly added resource or configuring resources upon startup of node B. When a node fails and node B resumes operations previously performed on the failed node resource agent B reads the values of resource attributes B. If the values of resource attributes B indicate that a storage configuration change has occurred resource agent B creates a new storage resource definition not shown on node B. This new resource definition reflects the configuration change written to resource attributes B and is used to manage the resource. Failure of a node and resumption of operation on another node is discussed further with reference to below.

Log agents A and B can obtain information from a log not shown of activity performed on a node. Recovery agents A and B are used to recover operation of a node for which activity is recorded in the log. In one embodiment respective recovery agents A and B also read respective attributes for log agents A and B. Operation of log agents and recovery agents is discussed further with respect to .

A data flow showing a configuration change is shown in . In action . a resource configuration is changed via resource configuration manager A. In action . resource configuration manager A updates storage resource attributes A log attributes A and recovery attributes A of resource configuration data A. In action . resource configuration manager A notifies resource agents for the affected resource including storage resource agent A log agent A and recovery agent A. Resource configuration manager A also notifies cluster manager A in action .. In action . cluster manager A reads resource configuration data A and provides configuration data to all nodes in the cluster via cluster communication channel .

In action . cluster manager B receives configuration data indicating that storage resource attributes A log attributes A and recovery attributes A of resource configuration data A are changed. In action . cluster manager B provides configuration data to resource configuration manager B. In action . resource configuration manager B updates storage resource attributes B log attributes B and recovery attributes B of resource configuration data B to reflect the changes made to storage resource attributes A log attributes A and recovery attributes A of resource configuration data A. Resource configuration data A and B are now synchronized.

A change in an attribute is therefore made available to other nodes in the cluster via cluster communication channel . One of skill in the art will recognize that other mechanisms may be used to make information about a configuration change available to all nodes in the cluster. More detail about synchronization of configuration data is provided in the discussion of below.

In action . the update to the storage resource configuration is provided to storage access interface A which updates storage resource definition D for storage resource in action .. In action . storage access interface A notifies storage management service A that the configuration update is complete. Actions . through . may occur concurrently with actions . and . and the ordering of the two sets of actions is not significant as long as updates to both storage resource definition D and resource configuration data A occur.

After updating storage resource attributes A log attributes A and recovery attributes A of resource configuration data A in action . resource configuration manager A notifies resource agents for the affected resource. In this embodiment these agents include storage resource agent A log agent A and recovery agent A. In action . resource configuration manager A informs cluster manager A of the attribute changes.

In action . cluster manager A provides updated configuration data to other nodes in the cluster. In action . cluster manager B of node B receives configuration data . In action . cluster manager B synchronizes resource configuration data B with resource configuration data A. Action . is shown as a broken arrow indicating that intermediate steps may take place in performing the synchronization. For example synchronization of resource configuration data A and resource configuration data B was described in more detail with respect to actions . and . of .

Respective cluster managers on each node in this case cluster managers A and B and respective resource configuration managers on each node in this case resource configuration managers A and B work together to accomplish the synchronization of configuration data on a set of nodes in the cluster. The cluster manager and resource configuration manager on a given node can collectively be referred to as a synchronization module. The synchronization module can be considered to include a causing module to cause modification of a value of an attribute of a resource definition and a sending module to send the value of the attribute as the resource configuration data to the set of nodes.

Similarly because the cluster manager and the resource configuration manager work together to resume operation of a resource on a secondary node the cluster manager and resource configuration manager collectively can be referred to as a resuming module. In the example shown the synchronization and the resuming modules both include the cluster manager and the resource configuration manager however it is not necessary that the modules are implemented using the same components. A different configuration of modules may be implemented to provide the synchronizing and resumption of operation of resources functionality described herein.

Because a resource agent working with the resource configuration manager can create a resource definition for the resource from the configuration data on one of the nodes each node may be considered to include a creating module. The node on which the resource definition is created uses the resource definition for managing the resource.

In response to the failover in action . cluster manager B activates resources. In one embodiment cluster manager B calls an online entry point or set of instructions for each agent. Also in action . storage resource agent A log agent B and recovery agent B begin preparation to actively manage resources such as storage resource . In action . each of storage resource agent A log agent B and recovery agent B requests resource configuration manager B for respective attributes. Resource configuration manager B obtains the attributes from storage resource attributes B log attributes B and recovery attributes B of resource configuration data B in action .. Because resource configuration data A and B are synchronized cluster manager B has access to current values for the attributes for storage resource .

In action . resource configuration manager B provides storage resource attributes B log attributes B and recovery attributes B to the respective agents storage resource agent A log agent B and recovery agent B. In action . storage resource agent B uses storage resource attributes B to create storage group definition D on node B.

Using a log disk name obtained from log attributes B log agent B reads a log not shown of activity in action .. From the log log agent B obtains information about the operation that was in progress during failure and any storage resources that were involved in the operation.

In action . recovery agent B starts recovery of operations using data obtained from the log. In action . storage management service B informs storage access interface B that an update to a storage resource configuration is being made. In action . storage access interface B reads and or updates storage group definition D.

As configuration information is made available to nodes in a cluster an additional problem arises. For example assume that an administrator manually switches the node managing a given resource. In such a case a resource definition can reside on each of the two nodes the original managing node and the new managing node. Using these resource definitions it is possible that agents on both nodes may declare the resource online. If both resource agents declare the resource online a concurrency violation has occurred because a given resource can be online on only one node in the cluster.

To avoid this problem an attribute such as one of storage resource attributes B can be used. The attribute can be updated with the name of the host that manages the resource. Every agent can compare the name of the host on which the agent is running with the name of the host in the attribute before declaring a resource to be online on the host on which the agent is running.

Assume that several disks are configured to serve as log disks and that the active log disk name changes depending upon which of the disks is currently being used. An attribute can be used to store the name of the currently active log disk. When a configuration change is being made the attribute containing the name of the currently active log disk is updated. When an operation completes successfully the name of the log disk can be removed from this attribute. A non null value for the log disk attribute during failover indicates that an operation failed and provides the name of the log disk for recovery purposes.

Advantages of the present invention are many. A set of nodes and preferably any node in a cluster can resume operations of a failed node. These operations include storage management services that allow configuration changes to be made dynamically to storage resources. Little overhead is incurred to communicate configuration changes to the nodes in a cluster.

The following section describes an example computer system and network environment in which the present invention may be implemented.

Bus allows data communication between central processor and system memory which may include read only memory ROM or flash memory neither shown and random access memory RAM not shown as previously noted. The RAM is generally the main memory into which the operating system and application programs are loaded and typically affords at least 66 megabytes of memory space. The ROM or flash memory may contain among other code the Basic Input Output system BIOS which controls basic hardware operation such as the interaction with peripheral components. Applications resident with computer system are generally stored on and accessed via a computer readable medium such as a hard disk drive e.g. fixed disk an optical drive e.g. optical drive floppy disk unit or other storage medium. Additionally applications may be in the form of electronic signals modulated in accordance with the application and data communication technology when accessed via network modem or interface .

Storage interface as with the other storage interfaces of computer system may connect to a standard computer readable medium for storage and or retrieval of information such as a fixed disk drive . Fixed disk drive may be a part of computer system or may be separate and accessed through other interface systems. Modem may provide a direct connection to a remote server via a telephone link or to the Internet via an internet service provider ISP . Network interface may provide a direct connection to a remote server via a direct network link to the Internet via a POP point of presence . Network interface may provide such connection using wireless techniques including digital cellular telephone connection Cellular Digital Packet Data CDPD connection digital satellite data connection or the like.

Many other devices or subsystems not shown may be connected in a similar manner e.g. bar code readers document scanners digital cameras and so on . Conversely it is not necessary for all of the devices shown in to be present to practice the present invention. The devices and subsystems may be interconnected in different ways from that shown in . The operation of a computer system such as that shown in is readily known in the art and is not discussed in detail in this application. Code to implement the present invention may be stored in computer readable storage media such as one or more of system memory fixed disk optical disk or floppy disk . Additionally computer system may be any kind of computing device and so includes personal data assistants PDAs network appliance X window terminal or other such computing devices. The operating system provided on computer system may be MS DOS MS WINDOWS OS 2 UNIX Linux or another known operating system. Computer system also supports a number of Internet access tools including for example an HTTP compliant web browser having a JavaScript interpreter such as Netscape Navigator Microsoft Explorer and the like.

Moreover regarding the signals described herein those skilled in the art will recognize that a signal may be directly transmitted from a first block to a second block or a signal may be modified e.g. amplified attenuated delayed latched buffered inverted filtered or otherwise modified between the blocks. Although the signals of the above described embodiment are characterized as transmitted from one block to the next other embodiments of the present invention may include modified signals in place of such directly transmitted signals as long as the informational and or functional aspect of the signal is transmitted between blocks. To some extent a signal input at a second block may be conceptualized as a second signal derived from a first signal output from a first block due to physical limitations of the circuitry involved e.g. there will inevitably be some attenuation and delay . Therefore as used herein a second signal derived from a first signal includes the first signal or any modifications to the first signal whether due to circuit limitations or due to passage through other circuit elements which do not change the informational and or final functional aspect of the first signal.

The foregoing described embodiment wherein the different components are contained within different other components e.g. the various elements shown as components of computer system . It is to be understood that such depicted architectures are merely examples and that in fact many other architectures can be implemented which achieve the same functionality. In an abstract but still definite sense any arrangement of components to achieve the same functionality is effectively associated such that the desired functionality is achieved. Hence any two components herein combined to achieve a particular functionality can be seen as associated with each other such that the desired functionality is achieved irrespective of architectures or intermediate components. Likewise any two components so associated can also be viewed as being operably connected or operably coupled to each other to achieve the desired functionality.

With reference to computer system modem network interface or some other method can be used to provide connectivity from each of client computer systems and to network . Client systems and are able to access information on storage server A or B using for example a web browser or other client software not shown . Such a client allows client systems and to access data hosted by storage server A or B or one of storage devices A N B N N or intelligent storage array . depicts the use of a network such as the Internet for exchanging data but the present invention is not limited to the Internet or any particular network based environment.

The present invention is well adapted to attain the advantages mentioned as well as others inherent therein. While the present invention has been depicted described and is defined by reference to particular embodiments of the invention such references do not imply a limitation on the invention and no such limitation is to be inferred. The invention is capable of considerable modification alteration and equivalents in form and function as will occur to those ordinarily skilled in the pertinent arts. The depicted and described embodiments are examples only and are not exhaustive of the scope of the invention.

The foregoing described embodiments include components contained within other components. It is to be understood that such architectures are merely examples and that in fact many other architectures can be implemented which achieve the same functionality. In an abstract but still definite sense any arrangement of components to achieve the same functionality is effectively associated such that the desired functionality is achieved. Hence any two components herein combined to achieve a particular functionality can be seen as associated with each other such that the desired functionality is achieved irrespective of architectures or intermediate components. Likewise any two components so associated can also be viewed as being operably connected or operably coupled to each other to achieve the desired functionality.

The foregoing detailed description has set forth various embodiments of the present invention via the use of block diagrams flowcharts and examples. It will be understood by those within the art that each block diagram component flowchart step operation and or component illustrated by the use of examples can be implemented individually and or collectively by a wide range of hardware software firmware or any combination thereof.

The present invention has been described in the context of fully functional computer systems however those skilled in the art will appreciate that the present invention is capable of being distributed as a program product in a variety of forms and that the present invention applies equally regardless of the particular type of signal bearing media used to actually carry out the distribution. Examples of signal bearing media include recordable media such as floppy disks and CD ROM transmission type media such as digital and analog communications links as well as media storage and distribution systems developed in the future.

The above discussed embodiments may be implemented by software modules that perform certain tasks. The software modules discussed herein may include script batch or other executable files. The software modules may be stored on a machine readable or computer readable storage medium such as a disk drive. Storage devices used for storing software modules in accordance with an embodiment of the invention may be magnetic floppy disks hard disks or optical discs such as CD ROMs or CD Rs for example. A storage device used for storing firmware or hardware modules in accordance with an embodiment of the invention may also include a semiconductor based memory which may be permanently removably or remotely coupled to a microprocessor memory system. Thus the modules may be stored within a computer system memory to configure the computer system to perform the functions of the module. Other new and various types of computer readable storage media may be used to store the modules discussed herein.

The above description is intended to be illustrative of the invention and should not be taken to be limiting. Other embodiments within the scope of the present invention are possible. Those skilled in the art will readily implement the steps necessary to provide the structures and the methods disclosed herein and will understand that the process parameters and sequence of steps are given by way of example only and can be varied to achieve the desired structure as well as modifications that are within the scope of the invention. Variations and modifications of the embodiments disclosed herein can be made based on the description set forth herein without departing from the scope of the invention. Consequently the invention is intended to be limited only by the scope of the appended claims giving full cognizance to equivalents in all respects.


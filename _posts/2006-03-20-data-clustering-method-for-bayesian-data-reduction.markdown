---

title: Data clustering method for bayesian data reduction
abstract: This invention is a method of training a mean-field Bayesian data reduction algorithm (BDRA) based classifier which includes using an initial training for determining the best number of levels. The Mean-Field BDRA is then retrained for each point in a target data set and training errors are calculated for each training operation. Cluster candidates are identified as those with multiple points having a common training error. Utilizing these cluster candidates and previously identified clusters as the identified target data, the clusters can be confirmed by comparing a newly calculated training error with the previously calculated common training error for the cluster. The method can be repeated until all cluster candidates are identified and tested.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07587374&OS=07587374&RS=07587374
owner: The United States of America as represented by the Secretary of the Navy
number: 07587374
owner_city: Washington
owner_country: US
publication_date: 20060320
---
The invention described herein may be manufactured and used by or for the Government of the United State of America for governmental purpose without payment of any royalties thereon or therefore.

This invention relates to a method for classifying data and more particularly to a training method for a Bayesian Data Reduction Algorithm classifier that enables the identification of data clusters.

Classification systems are a type of artificial intelligence systems that are implemented on digital computers. These systems are implemented using neural networks or statistical measures. Implementation on a neural network involves training the neural network to recognize the given classes. As an example when given an input XI the classification system decides to which class the input X belongs. If known measurable characteristics separate classes the classification decision is straightforward. However for most applications such characteristics are unknown and the classification system must decide which output class does the input X most closely resemble. In such applications the output classes and their characteristics are modeled estimated using statistics for the classes derived from training data belonging to known classes. Thus the standard classification approach is to first estimate the statistics from the given training data belonging to known classes and then to apply a decision rule using these estimated or modeled statistics.

In many real world classification problems the domain of the observed data or features describing each class can be complicated obscure and highly overlapped. The result is that the task of discriminating amongst the classes with standard supervised training techniques can be nearly impossible. However within these difficult domains it can often be the case that the target class of interest e.g. data that produce a desired yield and are thus categorized as the target class contains isolated unknown clusters subgroups of data points where the observations within each cluster have similar statistical properties. In these situations classification performance or the average yield can be significantly improved if one develops a classifier to recognize or mine observations within the clusters as the target class and where all other nonclustered observations i.e. both with and without a desired yield are considered the alternative class the non target class . A benefit of such a classifier is that subsets of target data points producing a consistent desired average yield can be recognized with a minimum probability of error. This is in contrast to a traditional classification approach to this problem i.e. trained in a completely supervised manner that has the potential to produce a much higher probability of error and a lower average yield.

Bayesian networks also known as belief networks are known in the art for use as filtering systems. The belief network is initially learned by the system from data provided by an expert user data and user preference data. The belief network is relearned when additional attributes are identified having an effect. The belief network can then be accessed to predict the effect.

These benefits can be achieved in diverse fields having multi dimensional data. Large quantities of data are available in the securities market and it would be valuable to find groups of securities having predefined characteristics such as a certain yield from the available data. Other fields for using such a classification system are target identification medical diagnosis speech recognition digital communications and quality control systems.

Classification systems are a type of artificial intelligence systems that are implemented on digital computers. These systems are implemented using neural networks or statistical measures. Implementation on a neural network involves training the neural network to recognize the given classes. As an example when given an input X the classification system decides to which class the input X belongs. If known measurable characteristics separate classes the classification decision is straightforward. However for most applications such characteristics are unknown and the classification system must decide which output class does the input X most closely resemble. In such applications the output classes and their characteristics are modeled estimated using statistics for the classes derived from training data belonging to known classes. Thus the standard classification approach is to first estimate the statistics from the given training data belonging to known classes and then to apply a decision rule using these estimated or modeled statistics.

Bayesian networks also known as belief networks are known in the art for use as filtering systems. The belief network is initially learned by the system from data provided by an expert user data and user preference data. The belief network is relearned when additional attributes are identified having an effect. The belief network can then be accessed to predict the effect.

In this case the ordinate that defines the yield of each data point is plotted versus the domain where a yield value of 0.5 is used to separate and define the five hundred samples of the target class i.e. yield 0.5 and the five hundred samples of the non target class yield

It can clearly be seen that the two classes contain many commonly distributed points with respect to the range of the single feature. This case differs from the case shown in in that three clusters of data points A B and C exist within the target class containing actual respective yields of 0.6 0.75 and 0.9. In this example each data cluster was randomly placed to be centered somewhere between the yield values of 0.5 and 1 where as stated previously the focus of the general embodiment of the method is on mining each of these clusters.

Prior art methods for classifying data are provided in U.S. Pat. Nos. 6 397 200 and 6 789 070. These are incorporated by reference herein. U.S. Pat. No. 6 397 200 provides a data reduction method for a classification system using quantized feature vectors for each class with a plurality of features and levels. The method utilizes application of a Bayesian data reduction algorithm to the classification system for developing reduced feature vectors. Test data is then quantified into the reduced feature vectors. The reduced classification system is then tested using the quantized test data. A Bayesian data reduction algorithm is further provided by computing an initial probability of error for the classification system. Adjacent levels are merged for each feature in the quantized feature vectors. Level based probabilities of error are then calculated for these merged levels among the plurality of features. The system then selects and applies the merged adjacent levels having the minimum level based probability of error to create an intermediate classification system. Steps of merging selecting and applying are performed until either the probability of error stops improving or the features and levels are incapable of further reduction.

U.S. Pat. No. 6 789 070 provides an automatic feature selection system for test data with data including the test data and or the training data containing missing values in order to improve classifier performance. The missing features for such data are selected in one of two ways the first approach assumes each missing feature is uniformly distributed over its range of values and the second approach increases the number of discrete levels for each feature by one for the missing features. These two choices modify the Bayesian Data Reduction Algorithm for automatic feature selection.

This method for solving the problem in builds upon and utilizes the previously introduced Mean Field Bayesian Data Reduction Algorithm Mean Field BDRA based classifier. The Mean Field BDRA classifier was developed to mitigate the effects of the curse of dimensionality by eliminating irrelevant feature information in the training data i.e. lowering M while simultaneously dealing with the missing feature information problem. The mean field BDRA was first introduced in R. S. Lynch Jr. and P. K. Willett Adaptive Classification by Maximizing the Class Separability with Respect to the Unlabeled Data Proceedings of the 2003 SPIE Symposium on Security and Defense Orlando Fla. April 2003. This paper discloses a method of Bayesian Data Reduction which assigns an assumed uniform Dirichlet completely non informative prior for the symbol probabilities of each class. In other words the Dirichlet is used to model the situation in which the true probabilistic structure of each class is unknown and has to be inferred from the training data.

The Modified Mean Field BDRA was developed to better deal with problems in which the class labeling feature is the primary missing attribute in the training data. In general this problem greatly complicates the modeling of each class and to deal with it the mean field BDRA was created that encourages dissimilar distributions with respect to all missing value data.

The primary aspect of the Mean Field BDRA that is in addition to its data model that incorporates a class labeling feature that differentiates it from the original BDRA is its method of dealing with the missing features problem. In the Mean Field BDRA the missing feature information is adapted by estimating the missing feature from the available training data. The following model provides further detail. Specifically let z be an N dimensional vector containing the entire collection of training data for all k classes and using the Dirichlet distribution based model this is written as

Equation 1 represents the optimal approach to solving this problem. However when expanded and after integration Equation 1 results in a sum of products whose number of terms depends upon the number of missing features in the data. That is there are

In general under mean field theory the expectation E f x is replaced by f E x . Thus identifying f x as a particular term in the sum of products in Equation 1 meaning a particular configuration of the actual symbols of the symbol uncertain data the expected value of this data is added to the appropriate symbol s total number of observations. To accomplish this the following iterative steps are used these steps will be referred to as the mean field recursion 

Notice that steps i through iii shown above are similar to the recursive steps utilized in the Expectation Maximization EM algorithm. A typical implementation of EM involves using the available data to estimate or plug in the components of a Gaussian mixture density. However the recursive steps above involve estimation of the s for an algorithm that is approximately Bayesian. In any case as the EM algorithm has been shown to converge to a solution it is expected that due to its similar form the Mean Field BDRA will also converge.

In seeking best performance for a given data set the dimensionality reduction steps of the BDRA are used after each application of the mean field recursion described above. That is the Mean Field BDRA alternates between reducing irrelevant feature information and filling in missing feature values. The steps of the basic BDRA have been modified to include a class labeling feature in augmentation to each datum. Recall the algorithm reduces the quantization complexity to the level that minimizes the average conditional probability of error P e X and in its modified form it appears as

Given the above equations dimensionality reduction i.e. feature selection is implemented on the training data using the following iterative steps which are analogous to backward sequential feature selection.

 ii Using the initial training data with quantization complexity M e.g. in the case of all binary valued features M 2 where Nis the number of features Equation 2 is used to compute P e X M .

 iii Beginning with the first feature selection is arbitrary and excluding the class labeling feature reduce this feature by summing or merging i.e. marginalizing the numbers of occurrences of those quantized symbols that correspond to joining adjacent discrete levels of that feature. iv Re apply mean field recursive steps to the data. v Use the newly merged training data it is referred to as X and the new quantization complexity e.g. M 2in the binary feature case and use Equation 2 to compute P e X M . vi Repeat items iii iv and v for all Nfeatures. vii From item vi select the minimum of all computed P e X M in the event of a tie use an arbitrary selection and choose this as the new training data configuration. This corresponds to permanently reducing or removing the associated feature. viii Repeat items iii through vii until the probability of error does not decrease any further or until M 2 at which point the final quantization complexity has been found.

The Mean Field BDRA is modified in this section to improve its performance. Its performance is particularly improved when the adapted training data is missing the class labeling feature. The idea behind the method of the current invention is based on developing a model that encourages dissimilar distributions amongst the classes with respect to all missing feature information. Therefore given the missing feature values the new method is designed to give more likelihood to those feature vectors that have dissimilar values.

The modified Mean Field BDRA is based on the assumptions that the distribution of the true discrete symbol probabilities p for the idiscrete symbol of the kclass are uniformly Dirichlet distributed and that the form of the underlying new distributional model is given by 

The prior art does not disclose a method for training a Mean Field Bayesian Reduction classifier for detecting clusters in unknown data.

Accordingly this invention is a method of training a mean field Bayesian data reduction algorithm BDRA which includes using an initial training for determining the best number of levels. The Mean Field BDRA is then retrained for each point in a target data set and training errors are calculated for each training operation. Cluster candidates are identified as those with multiple points having a common training error. Utilizing these cluster candidates and previously identified clusters as the identified target data the clusters can be confirmed by comparing a newly calculated training error with the previously calculated common training error for the cluster. The method can be repeated until all cluster candidates are identified and tested.

These and other features aspects and advantages of the present invention will become better understood with reference to the following drawings description and claims.

The modified version of the Mean Field BDRA disclosed in the prior art is used as the basis for a new method to solve the problem shown in because of its superior performance with difficult unsupervised training situations. To further develop the new technique a new training method is developed for the modified algorithm that enables it to mine the domain of an unlabeled set of data points for clusters. The new training method utilizes a combination of unsupervised and supervised training and a sequential data search to localize all points within the cluster. In general all results shown for the methods developed here with the Mean Field BDRA will be based on simulated data like that shown in . However this approach is equally applicable to real life data sets.

Typical domain data can have any number of features so that a given cluster may exist across all dimensions of the feature space or across some subset of features within that space. Thus the built in dimensionality reduction aspects of the Mean Field BDRA are useful for isolating the data cluster. Further as the Mean Field BDRA is a discrete classifier it naturally defines threshold points in multi dimensional space that isolate the relative location of the cluster.

The automatic algorithm developed here to locate data clusters strongly relies on the Mean Field BDRA s training metric P e . This is given above as equation 2 above. Using this quantization complexity is reduced to the level that minimizes the average conditional probability of error P e X .

The idea is that because the Mean Field BDRA discretizes all multi dimensional feature data into quantized cells or levels any data points that are common to a cluster will share the same discrete cell which also assumes that appropriately defined quantization thresholds have been determined by the Mean Field BDRA. These are shown as dashed lines in . Therefore given all or most cluster data points can be quantized to share a common discretized cell they will all also share a common probability of error metric P e .

In other words locating cluster data points can be based on developing a searching method that looks for data points sharing a common P e . In this case it is expected that this common P e value for all points within the cluster will be relatively small with respect to that computed for most other data points outside of the cluster. This latter requirement should be satisfied in most situations as data clusters should tend to be distributed differently with respect to data outside of the cluster. As a final step in training the validity of each cluster can be checked by computing the overall average yield for all points within the cluster i.e. any grouped data points producing the largest average yield are chosen as appropriately mined data clusters .

To improve results the steps shown in and described below have been developed for training the Mean Field BDRA that is in such a way that the data cluster is identified with a minimum probability of error. For each of these steps training will proceed in a semi unsupervised manner in that all target data yield 0.5 identified as is utilized without class labels i.e. no class information at all and all non target data yield

Initially a maximum number of levels should be set as in step . A user can provide this maximum based on the available amount of computing resources the time required for completion or by some characteristic of the data. This maximum can also be computed from these attributes. In step using all available training data i.e. with all target points unlabeled and all non target points labeled the Mean Field BDRA is separately trained for each level. The levels are shown for illustrative purposes in by dashed lines . After training the Mean Field BDRA the training error is computed in step . The number of levels is incremented in step . Step continues the process until the maximum number of levels is reached. From the separate training runs the method chooses the initial number of discrete levels to use for each feature as that producing the least training error see Equation 2 above in step . In the next stage of training the Mean Field BDRA is trained for each target data point. In step a target data point is labeled with the correct target label. The remaining data points are unlabeled in step . The Mean Field BDRA is trained in step with this labeling and training error is computed for each point in step . Step proceeds to the next point while step loops through the routine until the Mean Field BDRA has been separately trained for each training data point. Notice that these steps produce a set of Ncomputed training errors equal to the number of target training data points.

The next group of steps is utilized to identify the clusters from the computed training errors. Target data points are sorted by training error and grouped in step . Step chooses all data points that have both the smallest common training error and the most number of data points from the set of Ncomputed training errors. These data points are candidate cluster data points and are accepted or rejected for example with the problem of by checking the commonality of associated yield values in step .

Notice that it is possible that in some problems multiple data clusters can be found in this way. That is if more than one candidate cluster appears to have points with more than one minimum error probability value. In this case data points common to each cluster can be grouped according to accepted yield values.

As a final step in step the training is finished by refining the computed number of levels. In this step all cluster data points found in step are labeled as target and all other target data points are unlabeled. The Mean Field BDRA is then retrained to recognize this data. This step fine tunes the best initial number of discrete levels to use for each feature by the Mean Field BDRA.

To extend the idea described above to finding multiple unknown clusters it is required for the new method to have the ability to intelligently sort through and separate data points having common error probabilities. In this case both the total number of clusters and the number of samples per cluster are assumed to be unknown to the classifier. Therefore with multiple data clusters each error probability value must now be thought of as an indicator to each point within each cluster. Restated it is expected that with multiple clusters all data points within each separate thresholded cluster region will share common error probability values. These common error probability values will be relatively small with respect to those computed for most other data points outside of any clusters. In general the degree to which this latter requirement is satisfied depends on how differently the clusters tend to be distributed with respect to the non clustered data. As data within a cluster becomes distributed more like the data outside of the cluster it becomes less distinguishable. Unknown clusters within a data set will be distinguishable by being distributed differently with respect to all other data points outside of the clusters. Notice that these methods exploit this important assumption.

Therefore a proper data mining algorithm of multiple clusters and one that is based on the Mean Field BDRA will have a higher likelihood of finding leading cluster candidates by focusing on the largest groups of data points that cluster around smaller common error probability values. As the sorting or mining continues in this way any data points associated with small error probabilities and that have few common data points are rejected as cluster members. The algorithm will be designed to automatically stop when all unknown data clusters have been found or when the training error begins to increase. Finally and as in the single cluster case the validity of each cluster with respect to the training data can be checked by computing the overall average yield for all points within the cluster.

The steps shown below have been developed for training the new multiple cluster classifier using the Mean Field BDRA that is in such a way that all unknown data clusters are identified with a minimum probability of error. These steps are detailed in the flow chart given as . For each of these steps training proceeds in a semi unsupervised manner in that all target data yield 0.5 identified as in is utilized without class labels i.e. no class information at all and all non target data yield

Initially a user selects a maximum number of levels for the algorithm in step . This selection depends on type and amount of data and the available computing resources. For the example shown here the maximum level is set as twenty. Typically it is desired to train with as many initial levels as the data will support for best results. As above this can be set by a user or calculated based on preferences. Using all available training data i.e. with all target points unlabeled and all non target points labeled the Mean Field BDRA is trained separately for each level step . For the results shown here all available training data means 50 of the entire data set. After training step computes the training error for that level. The number of levels is incremented in step until the preset maximum level is exceeded step .

From the iterated training runs for each level the initial number of discrete levels to use for each feature is chosen as the number of levels that produces the least training error step . See Equation 2 . Notice that the idea of steps is to find the best initial number of discrete levels to use for each feature prior to looking for individual clusters.

The next steps of the method train the Mean Field BDRA to identify clusters in the data. A first target data point is labeled in step and the remaining points are left unlabeled in step . The Mean Field BDRA is retrained in this manner in step . A cluster training error is computed after training for each target data point in step . This error is computed based on counting the number of wrong decisions made under each hypothesis. The method then proceeds to the next point in step . Step returns back to step until processing of all target data points is complete. Thus step produces a set of Ncomputed training errors equal to the number of target training data points.

In step the set of Ncomputed cluster training errors in steps are sorted and grouped according to those having common error values. The final list of separate cluster training errors should proceed from the smallest error to the largest error. All data points that share each error should be identified. This step helps to reveal those data points that are sharing a similar region in quantized feature space.

Step conducts a cluster search and looks for the first data cluster candidate using the list obtained in step above. In step the first data cluster candidate is chosen as the one having simultaneously the smallest cluster training error and the largest number of common data points. Typically the first error value on the list has both the absolute smallest error and the largest number of common points. However because the algorithm is suboptimal this does not have to always be the case. Optionally the user can set a minimum number of data points for each cluster. Once the cluster is selected the error associated with all points of this first cluster candidate are identified as P e 0 .

After selecting the first cluster candidate in step pre existing cluster candidates and current cluster points are all labeled in step . All points not associated with the current or previous cluster candidates are unlabeled in step . The Mean Field BDRA is then retrained in step . A new cluster training error is computed in step . This error is identified as P e 1 . Steps determine how statistically similar the selected group of training data points are with each other or on the other hand how different this group is with respect to the non target class which now includes all other target data points outside of the cluster .

In step P e 1 and P e 0 from steps and are compared. If P e 1 P e 0 as it should be in most cases containing data clusters one can conclude that the current cluster is a valid data cluster and proceed to process for additional clusters. Otherwise one can conclude that no substantial data clusters exist in step and terminate the algorithm.

When the current cluster is valid this is indicated in step . A search is conducted for the next cluster candidate in step according to the previously stated criteria excluding all points in the first cluster. This new group of points will have simultaneously the next smallest cluster training error and the largest number of common data points. Steps and are then repeated until the current error is greater than the initially computed error as found in step . It is important to note that these steps always utilize and train with all previously determined clusters from the previous steps marked. Upon terminating the algorithm the average yield for each cluster is computed in step and if applicable step is performed selecting those clusters producing the largest overall yield. The training method results in a trained mean field BDRA classifier that is capable of recognizing data clusters in the target region.

Table 1 below shows classification performance results for the Mean Field BDRA i.e. w o a cluster mining algorithm applied with supervised training i.e. data with yields greater than 0.5 are called target and those with yields less than 0.5 are called non target for single cluster data of the type shown in . Appearing in this table is the average probability of error computed on an independent test set 50 training 50 test and the average associated yield shown in parentheses obtained from data classified as the target class. Each entry in the table is shown as a function of the true mean yield value c per dimension of the data cluster where the two entries in braces f g shows the initial number of discrete levels used for each feature by the Mean Field BDRA respectively for one land four l dimensional data spaces. Also appearing in this table is the total number of features n in the data space where the true number of those features relevant to the data cluster nis shown in brackets 

It can be seen in Table 1 that average classification results are poor when all of the training data are labeled correctly and training proceeds in a supervised manner. This is significant as the results in this table were obtained by partitioning the available data into 50 training and 50 test sets which highlights the difficulty of the classification problem shown in . Observe that the exact location of the cluster seems to make very little difference to the overall average probability of error and average yield no matter how many features are contained in the data. Even in the case when three additional irrelevant features are added to the data n 4 the results are very similar for both actual cluster locations.

As a final observation in Table 1 notice that the initial number of discrete levels per feature was chosen to be either eight nine or ten by the Mean Field BDRA for either the one or four dimensional cases. For the supervised training case shown in this table the initial number of discrete levels used for each feature was chosen to be consistent with that used below in obtaining the modified results of Table 2. In all cases when obtaining these results the actual number of initial discrete levels per feature was incrementally varied between two and ten by the Mean Field BDRA. The final values shown were determined by the Mean Field BDRA to be those that produced the smallest training error with the clustering algorithm applied.

Table 2 shows classification performance results for the Mean Field BDRA with the cluster mining algorithm applied and semi supervised training i.e. all cluster data points are labeled as target and all unclustered target data points and all non target data points are unlabeled for single cluster data of the type shown in . Appearing in this table is the average probability of error computed on an independent test set 50 training 50 test and the average associated yield shown in parentheses obtained from data classified as the target class. Each entry in the table is shown as a function of the true mean yield value c per dimension of the data cluster where the two entries in braces f g shows the initial number of discrete levels used for each feature by the Mean Field BDRA respectively for one l and four l dimensional data spaces. Also appearing in this table is the total number of features n in the data space where the true number of those features relevant to the data cluster nis shown in brackets .

In Table 2 it can be seen that when the cluster mining method is applied average classification results have dramatically improved over that shown in Table 1. That is not only have error probabilities been substantially reduced but average yields have also been significantly increased. For example notice in Table 1 that with a true cluster location of 0.6 and for both the one and four dimensional cases the average yield is less than 0.5. However in Table 2 and after the cluster algorithm is applied it can be seen that the average yield has been increased to be much closer to the true value of 0.6. Notice that a similar significant performance improvement occurs for a true cluster location of 0.9.

It is interesting to note that in obtaining the yield results of Table 2 the Mean Field BDRA classifier labeled an average of fifty four data points as target i.e. belonging to the cluster . In Table 1 the Mean Field BDRA called an average of two hundred forty eight points the target. In other words the clustering algorithm was able to significantly increase the average yield of the data with only slightly more than twenty percent of the number of data points. Thus the clustering algorithm is utilizing the data much more efficiently to predict a gain in yield in unlabeled data. However there still are some false alerts with the clustering method as other data points share the exact same feature space as those within the cluster. The fine tuning of threshold locations shown as the last step in the clustering algorithm above helps to reduce these false declarations by more precisely locating the best initial discrete levels to use by the Mean Field BDRA.

In Table 3 classification performance results are illustrated for the Mean Field BDRA i.e. w o a cluster mining algorithm applied with supervised training i.e. data with yields greater than 0.5 are called target and those with yields less than 0.5 are called non target for two and three cluster data of the type shown in . Appearing in this table is the average probability of error computed on an independent test set 50 training 50 test for the respective number of unknown clusters shown. In this case supervised training results appear for both unclustered i.e. the classifier has no knowledge about the data clusters and clustered i.e. the classifier knows all data points in each cluster and these are the only points labeled as target . In producing these results the Mean Field BDRA trains with twenty initial discrete levels of quantization.

Table 3 illustrates the interesting aspects of this data with regard to classifying data that contains isolated clusters. Observe in this table that average classification results are poor when all of the training data are labeled correctly and training proceeds in a supervised manner see the unclustered results column given the classifier has no knowledge about any data clusters. However it can also be seen see the clustered results column that performance improves dramatically when the classifier is given precise knowledge about the location of all points within the data clusters.

The error probabilities in Table 3 indicate that there is only a slight difference in the results if the data contains either two or three clusters such as the data shown in . For example with the unclustered results the three cluster case is slightly better as more clusters are providing information to help discriminate the classes as a comparison to this in Table 1 single cluster results using supervised training produced an error probability of near 0.5 . When the classifier is given knowledge about the points within each cluster the two cluster case appears to perform slightly better. In this situation with three clusters an increasing number of isolated quantized cells also causes more false positive classifications to occur in the regions containing all clusters.

As a final observation in Table 3 notice that the initial number of discrete levels per feature was chosen to be twenty by the Mean Field BDRA. For the supervised training case shown in this table the initial number of discrete levels used for each feature was chosen to be consistent with that used below in obtaining the modified results of Table 4. In all cases when obtaining these results the actual number of initial discrete levels per feature was incrementally varied between two and twenty by the Mean Field BDRA. The final value of ten shown was determined by the Mean Field BDRA to be those that produced the smallest training error with the clustering algorithm applied.

In Table 4 classification performance results appear for the Mean Field BDRA i.e. with a cluster mining algorithm applied and unsupervised training i.e. using the algorithmic steps described above for two and three cluster data of the type shown in . Appearing in this table is the average probability of error computed on an independent test set 50 training 50 test for the respective number of unknown clusters shown. Notice that for comparison the error probabilities are repeated for the supervised clustered case of Table 3. Observe that the utility of the data clustering algorithm developed here can clearly be seen in the results of Table 4. Observe for both the two and three cluster cases that the error probability of the cluster mining algorithm is only about one percent higher than it is for the clustered supervised classifier that knows everything. This is significant because the cluster mining algorithm used here has no prior information at all about the clusters.

Table 5 shows average yield results for the multiple cluster cases of Tables 3 and 4 and for comparison previously obtained single cluster results are also shown. In each of these cases the actual average yield for all data clusters is 0.75. Appearing for two and three clusters are computed average yields for the unsupervised Mean Field BDRA based classifier of Table 3 and the supervised unclustered classifier of Table 1. For the single cluster case yield values are based on averaging the one dimensional results for actual cluster yields of 0.6 and 0.9. From this table it can be seen that the cluster mining algorithm developed here is improving the overall average yield for all numbers of clusters over that of the supervised classifier. This implies that the new algorithm is improving the quality of the decisions in that it is declaring a proportionately larger ratio of high yielding data points as the target. However notice also that as the number of clusters increases yield performance of the supervised classifier improves with respect to that of the unsupervised Mean Field BDRA. Intuitively as more clusters appear in the data classification performance with supervised training should improve as each cluster provides additional information. This implies that in some cases it might be best for an algorithm such as the Unsupervised Mean Field BDRA to mine for clusters individually as opposed to collectively as a group.

In summary this invention provides a new cluster mining algorithm which has been developed for the Mean Field Bayesian Data Reduction Algorithm BDRA . The new method works by utilizing a semi unsupervised method only non target training data points were completely labeled and an iterative sequential search through the target data to locate features that are clustered relative to all other target and non target features within the data set. For the simulated data generated here clustering was typically based on two defined goodness metrics. In particular the clustering was based both on reducing the relative training error and on improving the overall predicted yield of the data clusters were located within target locations where yield values were greater than 0.5 . In all cases classification results revealed that the new clustering algorithm improved performance over the conventional supervised training method by significantly reducing the average predicted probability of error and average predicted yield for independent evaluation data. Typically data outside of a cluster will have a more random distribution of error probability values that will not necessarily associate with a common yield value. 

It should be understood of course that the foregoing relates to preferred embodiments of the invention and that modifications may be made without departing from the spirit and scope of the invention as set forth in the following claims.


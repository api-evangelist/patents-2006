---

title: N-port virtualization driver-based application programming interface and split driver implementation
abstract: An API in an NPIV-compatible SAN is disclosed that includes functions for creating a vlink, replicating driver software for managing the vlink, monitoring resources in an HBA, or removing a target so that resources can be freed up for other vlinks. The API is part of a driver that communicates with a host OS and also communicates with an HBA to establish the vlinks between the host OS and FC devices. To create vlinks, an “add” function in the API is called by the OS. In addition, when a new vlink is created, a single version of an HBA interface function block is maintained in the driver, but a discovery function block, SCSI bus function block, and I/O function block are all duplicated, forming one logical vlink driver for each vlink. To obtain HBA resource information, a resource monitoring functions in the API may be called by the OS.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07921431&OS=07921431&RS=07921431
owner: Emulex Design & Manufacturing Corporation
number: 07921431
owner_city: Costa Mesa
owner_country: US
publication_date: 20060120
---
This invention relates to N Port ID Virtualization NPIV in Storage Area Networks SANs and more particularly to Application Programming Interface API functions that allow an Operating System OS to monitor the resources needed by physical and virtual Fibre Channel FC links create and terminate physical and virtual FC links and replicate driver software to assist in managing the virtual links. This invention also applies to the manner in which NPIV may be implemented within an OS or device driver software.

Conventional SANs. In a conventional FC SAN shown in an Input Output Controller IOC or Host Bus Adapter HBA includes a Node Port N Port that is connected to a FC switch or Just a Bunch Of Disks JBOD via a FC link . During initialization a known FC initialization sequence initiated by a driver in a host OS of host causes the HBA to send a Fabric Login command FLOGI to the switch including a World Wide Port Name WWPN for the N Port . The switch returns a FLOGI response to the N Port including a FC address a virtual IDentifier ID associated with the WWPN for the N Port.

The driver also performs a discovery function in which it communicates with the FC switch via the HBA and FC link and obtains a list of the addresses of all devices in the fabric. The discovery function then goes out to every address logs into the device associated with that address at which time a login context is allocated and determines if the device is a FC Small Computer System Interface SCSI target. If the device is a FC SCSI target the discovery function establishes a connection between the target and the HBA . In addition the physical FC link is exported as a SCSI bus to the OS and the remote port associated with the discovered FC SCSI device thereafter appears as a target on the SCSI bus in typical SCSI fashion.

Conventional FC SANs are limited because only one WWPN and FC address can be assigned to the N Port on a single FC link . In other words this conventional computing model contemplates a single OS per system so the OS explicitly owns the FC port. As such system management tools have been defined such as zoning and selective storage presentation Logical Unit Number LUN masking that are based on the FC port.

NPIV. However FC has extended its feature set to include NPIV a feature that allows a fabric attached N Port to claim multiple FC addresses. Each address appears as a unique entity on the FC fabric. Utilizing NPIV multiple WWPNs and FC addresses recognizable by the FC switch can be assigned to a single physical FC link and N Port. By allowing the physical FC port to now appear as multiple entities to the fabric the conventional computing model can now be extended. A system can now run more than one OS by creating virtual systems or machines and running an OS image in each virtual machine. Instead of owning the physical FC port an OS now uniquely owns one or more FC addresses and their associated WWPNs claimed by the FC port. As the relationship of the virtual machine OS owning the WWPN FC address remains consistent with the conventional computing model legacy FC management functions can continue to be used unchanged. As the FC fabric treats each fabric entity as a unique port including all responses to name server queries etc each fabric entity primarily behaves as if it were an independent FC link.

Each instance of a FC link in the fabric whether physical or virtual will be generally referred to herein as a vlink. In addition the physical FC link will be referred to as the physical vlink and the virtual FC links will be referred to as virtual vlinks. Each vlink has an individual and distinct representation within the FC fabric. Each vlink has its own unique identifiers e.g. port WWPN World Wide Node Name WWNN and its own FC address within the fabric. Each vlink is presented its own view of storage and thus can potentially enumerate different targets and Logical Unit Numbers LUNs logical storage entities . Each vlink must therefore independently register for state change notifications and track its login state with remote ports.

NPIV has been fully adopted. See the FC DA Technical Report clause 4.13 N Port ID Virtualization and the FC FS Standard clause 12.3.2.41 Discover F Port Service Parameters FDISC the contents of which are incorporated herein by reference. Note that there is no specific mention of NPIV in the FC FS Standard but the FDISC description has been modified to allow Address ID assignment per NPIV. See also Fibre Channel Link Services FC LS 2 Rev 1.2 Jun. 7 2005 T11.org INCITS which describes the standard for FC link services and provides definitions for tools used for NPIV and describes N Port requirements for virtual fabric support and Fibre Channel Direct Attach 2 FC DA 2 Rev 1.00 Nov. 18 2004 T11.org INCITS which describes the Standard for FC direct connect link initialization including use of the NPIV feature both of which are incorporated by reference herein.

Although NPIV allows the creation of many virtual vlinks along with the actual physical vlink there is in fact only one HBA that must be shared between the physical and virtual vlinks. The resources of the HBA are finite and different HBAs may have different levels of resources. The limitation of only one HBA places resource constraints on the system such as the number of vlinks that may be present at any time. Several key HBA resources will now be discussed.

RPI resource. For every device that is seen on each vlink an independent resource called a Remote Port Index RPI is consumed within the HBA. is an illustration of the consumption of resources in an NPIV FC SAN . In if one FC device D is installed on the physical vlink and two FC devices D and D are installed on the virtual vlink there will be a total of three RPIs consumed in the HBA see RPIs and . RPIs are context cache memory data structures which in FC may also be referred to as a login context between two ports. RPIs maintain communication state information that has been negotiated between both the local FC port on the HBA and the remote FC port on the installed FC device. Some of the data maintained in an RPI includes the addresses for both the local and remote FC ports the class of service parameters to be used between the local and remote FC ports and the maximum frame size. RPIs are data structures and because there is usually a finite table of RPIs maintained in a finite HBA memory there are a limited number of RPIs available.

XRI resource. For every SCSI I O context that is actively occurring between the local and remote FC ports for a particular login context or RPI another independent resource called an eXchange Resource Index XRI is consumed within the HBA . XRIs are typically bound to a particular I O context and to an associated RPI and store exchange IDs or exchange resources such as sequence counters data offsets relationships for Direct Memory Access DMA maps and the like. For example if host sends a read request to FC device D through port P an XRI is consumed and bound to RPI . As with RPIs the XRIs are data structures stored in the finite HBA memory and are limited in number.

To manage these resources properly there is a need to be able to monitor the finite resources of the HBA create and delete vlinks and remove targets and release certain resources known to be unnecessary back into a free pool of resources so that they can be re used by other entities. In addition there is a need for a firmware implementation that can manage the vlinks and the finite resources of the HBA.

Embodiments of the present invention are directed to an API in an NPIV compatible SAN that includes functions for creating a virtual vlink replicating driver software for managing all vlinks deleting a vlink monitoring resources in an HBA or removing a target so that resources can be freed up for other vlinks. The API is part of a driver that communicates with a host OS and also communicates with an HBA to establish the vlinks between the host OS and FC devices over a FC fabric. It should be understood that an API as defined herein includes any type of program that may be called by the host OS to perform the functions described herein. The API may be stored on storage media.

Prior to creating a new virtual vlink the physical vlink and SCSI bus may first be created as described above with regard to non NPIV implementations. However in alternative embodiments creation of the SCSI bus on the physical vlink is not required. In such an embodiment information discovery would not necessarily be based on the SCSI bus.

After the physical vlink and SCSI bus have been created an information gathering function in the API may be called by the host OS via the SCSI bus. This information gathering function may obtain information about whether the HBA is connected to the FC fabric whether the fabric supports NPIV and the maximum and available resources of the HBA e.g for managing port to port logins and or FC exchanges including but not limited to the RPI and XRI resources described above. This function polls the HBA and return the desired values to the API. The host OS can then estimate how many vlinks can be created and monitor manage and allocate the HBA resource levels to ensure they are not overrun.

After the physical vlink has been created and resource information has been gathered a vlink creation function in API may be called by the host OS and executed by a processor in the host to create or instantiate a virtual vlink. When the host OS calls the vlink creation function in the API the OS indicates the HBA that the virtual vlink is to be created on and supplies minimally the WWPN to be used for the virtual vlink to the function. The function then calls the driver which then instantiates a new virtual SCSI bus associates it with the new virtual vlink and returns a context for the new SCSI bus when it returns from the vlink creation function call.

It should also be noted that after a virtual vlink is created execution of the information gathering function described above may also return information about the virtual vlink such as the FC address WWPN WWNN associated with the virtual vlink the state of the virtual vlink active or not and if the virtual vlink was or is inactive the conditions that caused the virtual vlink to fail.

A delete function in the API may also be called by the OS which cancels a virtual vlink and a virtual SCSI bus. For example a virtual vlink can be deleted by calling a function such as lpfc vlink delete . This function will terminate the virtual vlink and disassociate itself from the FC fabric.

By default an HBA typically consumes one RPI as soon as an installed FC device is present and assumes that because it is present the HBA will eventually communicate with it. The HBA consumes one RPI for each connection between the HBA and a target. Despite the establishment of vlinks between the HBA and target FC devices and the consumption of an RPI for each vlink it may be desirable to subsequently remove a target if it will not be communicated with. To accomplish this a target removal function such as lpfc vlink tgt remove can be used to request that communications with the target cease the target be removed and the RPI associated with the target released back to the HBA free pool so that it can be made available to other virtual vlinks and targets.

The driver in the host of a conventional SAN includes an HBA interface function block that communicates with registers in the HBA and passes data between the HBA and the host and a discovery function block for implementing discovery operations that occur on link up conditions. These discovery operations obtain a FC address on the FC link look for devices on the FC link and perform an instantiation function by logging onto the devices. In particular the discovery function block communicates with the FC switch via the HBA and FC link and obtains a list of all the addresses of the devices in the fabric. The discovery function block then goes out to every address logs into the device associated with that address at which time a login context is allocated and determines if the device is a FC SCSI target. If the device is a FC SCSI target the discovery function block creates a FC login between the FC address of the appropriate HBA port in HBA and the FC address of the remote port of the device.

Thereafter a SCSI bus function block in the driver exports the physical FC link as a SCSI bus to an OS and each of the remote ports associated with the discovered FC SCSI devices thereafter appears as a target on the SCSI bus in typical SCSI fashion. The result is a logical abstraction from the point of view of the OS of the physical FC link and the devices connected to that link. For example if there are a number of disk drives on the FC link each one will look like a target to the OS.

An I O function block also communicates with the rest of the OS to perform I O operations. Through a normal I O path the OS send SCSI commands to those targets in some cases down to the LUN level of the target.

Embodiments of the present invention are also directed to a split driver implementation for supporting NPIV. In the NPIV driver implementation a single version of the HBA interface function block is maintained but the discovery function block SCSI bus function block and I O function block are all duplicated N times forming N instances of a logical vlink driver one for each vlink. Each logical vlink driver is a software instantiation that enables each vlink to interact with an OS and manages each vlink. Note that the term software as used herein refers to machine code executed on a Central Processing Unit CPU or other processing entity. The single HBA interface function block is a multiplexing point through which all of the logical vlink drivers communicate with the FC fabric.

The discovery SCSI bus and I O function blocks are essentially duplicated each time a new virtual link and virtual SCSI bus are created by the API vlink create function described above. In the present invention the code is ported almost verbatim from the original.

Each logical vlink driver must perform its own discovery login to the fabric and present its FC address as a SCSI bus to the OS with a standard I O path relative to that bus. All of the targets in the FC fabric which are now relative to this FC address become unique targets and LUNs for that logical vlink driver. Each logical vlink driver has its own set of contexts for the SCSI bus it represents the system every port it sees on the FC fabric and the state of all login negotiations between the logical vlink driver and each port it sees on the FC fabric.

The split driver implementation allows the replicated logical vlink drivers to execute either within the same OS or as part of separate OSs. If within the same OS the HBA interface function block and all replicated logical vlink drivers will execute as part of the same OS. The single OS will see all SCSI bus abstractions associated with the vlinks. If the vlinks are in separate OSs one OS designated as the primary OS will contain the driver portion that executes the HBA interface function block. The Primary OS may also contain one or more logical vlink drivers. Simultaneously one or more other OSs designated as Secondary OSs may exist which contain one or more logical vlink drivers. The SCSI bus abstraction associated with a vlink will only be known to the OS which contains the logical vlink driver for that vlink. The Primary and Secondary OS s logical vlink drivers will interact with the HBA interface function block through an inter OS API which provides communication channels between the HBA interface function block and the logical vlink drivers.

In the following description of preferred embodiments reference is made to the accompanying drawings which form a part hereof and in which it is shown by way of illustration specific embodiments in which the invention may be practiced. It is to be understood that other embodiments may be utilized and structural changes may be made without departing from the scope of the preferred embodiments of the present invention.

Prior to creating a new virtual vlink the physical vlink and SCSI bus may first be created as described above with regard to non NPIV implementations. However in alternative embodiments creation of the SCSI bus on the physical vlink is not required. In such an embodiment information discovery would not necessarily be based on the SCSI bus .

Information gathering function. After the physical vlink and SCSI bus have been created an information gathering function in the API such as lpfc vlink getinfo may be called by the host OS via the SCSI bus. This information gathering function may obtain information about whether the HBA is connected to the FC fabric whether the fabric supports NPIV and the maximum and available resources of the HBA e.g for managing port to port logins and or FC exchanges .

Note that although NPIV allows the creation of many virtual vlinks along with the physical vlink there is in fact only one HBA that must be shared between the physical and virtual vlinks. The resources of the HBA are finite and different HBAs may have different levels of resources. The limitation of only one HBA places resource constraints on the system. Therefore the information gathering function may obtain for each monitored resource the maximum number of resources available and usage levels how many of those resources are in use at this time including but not limited to the PRI and XRI resources described above. This function polls the HBA and return the desired values to the API .

The host OS can then estimate how many vlinks can be created and monitor manage and allocate the HBA resource levels to ensure they are not overrun. However if an HBA runs out of resources any attempt to create a new virtual vlink will fail with an out of resources indication. For example if HBA runs out of RPIs vlink instantiation and discovery of devices will fail and external storage will not become visible. A more common scenario however is that the HBA may run out of vlink resources e.g. the HBA runs out of Virtual Port Indexes VPIs where there is one VPI associated with each vlink. Note that a VPI s context within the HBA remembers the state of the vlink its address and the like.

Note that in one embodiment of the present invention the HBA resource fields returned by the information gathering function may only be valid when the function call is made on the physical vlink . If a query is performed on a virtual vlink invalid values will be returned.

The functionality of the information gathering function described above can be implemented in a straightforward manner using programming constructs well understood by those skilled in the art.

vlink creation function. After the physical vlink has been created and resource information has been gathered a vlink creation function in API such as lpfc vlink create may be called by the host OS and executed by a processor in host to create or instantiate a virtual vlink . When OS calls the vlink creation function in the API the OS indicates the HBA that the virtual vlink is to be created on and supplies the WWPN WWNN to be used for the virtual vlink to the function as may optionally provide authentication information keys as well. The function then calls the driver which then instantiates a new virtual SCSI bus associates it with the new virtual vlink and returns a pointer to the new SCSI bus when it returns from the lpfc vlink create call. The virtual vlink is created by performing NPIV steps as called out by the NPIV Standards described above and is well understood by those skilled in the art.

The functionality of the vlink creation function described above can be implemented in a straightforward manner using programming constructs well understood by those skilled in the art.

Note that the physical vlink does not have to be up i.e. the link is functional and ready for communications when the vlink create function is called. If it is called while the physical vlink is down i.e. the link is not functional and not ready for communications the driver will create the virtual vlink and the virtual SCSI bus but they will be considered in a virtual link down state. When the physical vlink comes back up the driver will automatically perform the NPIV functions necessary to instantiate the virtual vlink with the fabric. If the physical vlink drops or the fabric terminates the virtual vlink the driver will treat those as virtual link down conditions. The default behavior of the driver upon a link down condition after a virtual vlink has been instantiated with the fabric will be to not re instantiate the virtual vlink on a subsequent physical vlink link up condition. However the driver does provide an option specified at create time that can instruct the driver to automatically reinstate the virtual vlink upon link up.

Note also that the OS sees all FC links as SCSI buses and in the example of the disks or devices D D and D are viewed as targets on the SCSI buses associated with the virtual links. However as the switch fabric limits the view on the vlink the physical vlink may see only target D while virtual vlink would see targets D and D. Because each virtual vlink is presented to the OS as a separate SCSI bus it gives the OS an independent storage view seen on each vlink and maps easily into a method where I O is easily discriminated as to which link it is destined for. Note that although only one virtual vlink and one virtual SCSI bus are shown in many virtual vlinks and virtual SCSI buses may be created.

Embodiments of the present invention may also be extended to include other methods of allowing a port to obtain multiple addresses and appear as independent ports to a FC fabric. Examples include the use of multiple Arbitrated Loop Physical Addresses AL PAs or vSAN headers instead of NPIV.

It should also be noted that after a virtual vlink is created execution of the information gathering function described above may also return information about the virtual vlink such as the FC address WWPN WWNN associated with the virtual vlink the state of the virtual vlink active or not and if the virtual vlink was or is inactive the conditions that caused the virtual vlink to fail. Note that active not active as used herein is essentially interchangeable with up down.

vlink delete function. A vlink delete function in API such as lpfc vlink delete may also be called by the OS which cancels a virtual vlink and a virtual SCSI bus . This function will terminate the virtual vlink and disassociate itself from the FC fabric. The functionality of the delete function described above can be implemented in a straightforward manner using programming constructs well understood by those skilled in the art.

API Target Removal function. Note that RPIs and XRIs are shared resources not partitioned or reserved to a particular vlink and thus may be allocated to any of the physical and virtual vlinks as needed. Thus it would be advantageous to be able to terminate unused vlinks and free up RPIs and XRIs that are not needed so that they can be available for other vlinks.

By default an HBA typically consumes one RPI as soon as an installed FC device is present and assumes that because it is present the HBA will eventually communicate with it. As noted above illustrates an example SAN in which HBA is connected to targets D D and D through a FC switch . The HBA has a physical port P and a virtual port P with corresponding FC addresses FCA and FCA respectively. The HBA consumes one RPI for each connection between the HBA and a target. In the example of an RPI is consumed for the physical vlink from port P to device D an RPI is consumed for the virtual vlink from port P to device D and an RPI is consumed for the virtual vlink from port P to device D. However there is an overall limit on the number of vlinks as established by the RPI limits and other resource limits.

Despite the establishment of vlinks between the HBA and fabric and the consumption of an RPI for each vlink it may be desirable to subsequently remove a target. For example in it may be determined at some point in time that no I O requests will be made to target device D or that target D should be removed so that no further I O requests can be made to that target. In such a scenario it would be advantageous to remove the target and release the RPI resource associated with that target back to a free pool maintained by the system so that it is available for other targets that may be discovered on other vlinks.

To accomplish this a target removal function such as lpfc vlink tgt remove can be used according to embodiments of the present invention to request that communications with the target cease the target be removed and the RPI associated with the target released back to the system free pool so that it can be made available to other virtual vlinks and targets. The functionality of the target removal function described above can be implemented in a straightforward manner using programming constructs well understood by those skilled in the art.

In general the XRI resources are managed by the API based on received I O requests and the subsequent establishment of an I O context. In contrast to RPIs XRIs are short lived and are consumed and released as I O contexts are established and torn down. In the example of if a host issues a read request to FC device D through port P of HBA an I O context is established and an XRI resource is consumed. The XRI is temporarily bound to the RPI associated with the connection between port P and FC device D while the connection is functioning and may be considered a sub element of the RPI. Because XRIs are only needed during the processing of an I O request are allocated by the API based on generating an I O request and are torn down when the I O request is completed there is less of a need for a removal function call to tear down an I O context although such a function call does exist. Note that is most OSs there is an abort I O function that is part of the OS to driver interfaces associated with the SCSI bus. To avoid overconsuming XRI resources the API only need know how many XRIs are available and how many are currently in use. If an I O request is received but no more XRIs are available the HBA recognizes this and kicks the I O request back to the host with an error response.

Split driver code for vlinks. illustrates an example of a conventional SAN with a driver in a host OS which is executed by a processor within a host . The driver communicates with hardware such as an HBA which communicates over a FC link in a FC fabric containing a FC switch and devices in the SAN. The driver includes an HBA interface function block that communicates with registers in the HBA and passes data between the HBA and the host . The driver also includes a discovery function block for implementing discovery operations that occur on link up or remote state change conditions. These discovery operations look for devices on the FC link determines if they are FC SCSI devices and establishes a connection between the devices and the HBA as described above.

In particular the discovery function block communicates with the FC switch via the HBA and FC link and obtains a list of all the addresses of the devices in the fabric. The discovery function block then goes out to every address logs into the device associated with that address at which time a login context is allocated and determines if the device is a FC SCSI target. If the device is a FC SCSI target the discovery function block creates a FC login between the FC address of the appropriate HBA port in HBA and the FC address of the remote port of the device.

Thereafter a SCSI bus function block exports the physical FC link or any other vlink as a SCSI bus to the OS as soon as the vlink is created and each of the remote ports associated with the discovered FC SCSI devices appears as a target on the SCSI bus in typical SCSI fashion. The result is a logical abstraction from the point of view of the OS of the physical FC link and the devices connected to that link. For example if there are a number of disk drives on the FC link each one will look like a target to the OS.

An I O function block also communicates with the rest of the OS to perform I O operations. Through a normal I O path the OS send SCSI commands to those targets or may request the abort of a previous command issued to one of the targets.

Note that the system of is based on the assumption that there is always a single relationship between the SCSI bus that the OS sees and the FC link . Because this single relationship is assumed monolithic design choices are made in the creation of data structures function groupings and the state maintained in the driver for the discovery function block .

The discovery SCSI bus and I O function blocks and are essentially duplicated each time a new virtual link and virtual SCSI bus are created by the vlink create function described above. In the present invention the code is ported almost verbatim from the original. The functionality of the software duplication function described above can be implemented in a straightforward manner using programming constructs well understood by those skilled in the art.

As described above each time a new vlink is created a unique FC address is associated with a unique WWPN WWNN and thus each logical vlink driver when created is associated with a unique FC address WWPN WWNN. The allocation of finite HBA resources as described above occurs globally as each logical vlink driver is created and executes.

Each logical vlink driver must perform its own discovery login to the fabric and present its FC address as a SCSI bus to the OS with a standard I O path relative to that bus as described above. All of the targets in the FC fabric which are now seen in the fabric relative to this FC address become unique targets and LUNs for that logical vlink driver . Each logical vlink driver has its own set of contexts for the SCSI bus it represents the system every port it sees on the FC fabric and the state of all login negotiations between the logical vlink driver and each port it sees on the FC fabric.

Note that there must always be at least one logical vlink driver that maps or binds to the original physical FC link and has a slightly different initialization with the fabric as compared to subsequent instantiations of vlinks i.e. F Port FLOGI instead of an FDISC as described above . In addition if the first instance of the logical vlink driver representing the physical vlink terminates it results in the termination of all virtual vlinks. The vlinks may stay instantiated with respect to the OS but as soon as communications with the FC fabric are restored the physical vlink relationship must be established first and then other virtual vlinks can be created after that.

Logical vlink drivers. The following paragraphs although referencing the embodiment of for purposes of explanation only are equally applicable to the embodiment of . Each logical vlink driver contains functionality for communicating over a vlink once connectivity has been established with the FC fabric i.e. after the logical vlink driver has logged in to the FC fabric via FLOGI or FDISC to a Fabric Port F Port on a switch and the logical vlink driver has received back a FC address to use for further vlink communication. At this point each logical vlink driver is largely indistinguishable from another. However subsequent traffic sent using the FC address determines the port role e.g. an FCP Initiator typical for an HBA FCP Target IP FC interface FICON etc. . The FLOGI FDISC difference described above is the only significant distinction between a physical vlink and a virtual vlink. There are a few other distinctions such as a management application may only be able to reset the HBA or download firmware through the SCSI bus associated with the physical vlink. If this functionality was tried with a virtual vlink it might be rejected.

HBA Interface function block. The common HBA interface function block is tailored to the HBA and physical vlink. All of the functions for the HBA are located here such as the function of connecting the HBA into the OS i.e. a Peripheral Component Interconnect PCI bus attachment interface to load the driver for the HBA into the OS . The HBA interface function block also includes functions which may include but are not limited to initializing and communicating with the HBA s read write registers handling interrupts and communicating with message passing mechanisms in the HBA for sending requests and receiving responses. The HBA interface function block also performs a few native functions for the management of the physical FC link including enabling basic transmission reception or turning on the optics for the port.

Once these physical functions are performed the HBA interface function block typically initiates the first step of NPIV which is establishing one logical vlink driver to perform FLOGI initialization with the F Port and become active on the FC link under the control of either the add function described above or more likely an internal add function that performs much the same functions. The physical vlink is responsible for determining whether NPIV is supported and whether there will ever be another virtual vlink present on the fabric. To determine this the HBA interface function block performs the function of detecting whether the HBA is of a type i.e. has the necessary firmware to support multiple vlinks. The created physical vlink which performed the first FLOGI determines whether the switch supports NPIV.

Additional HBA interface function block functions may include HBA and physical vlink management including the management of HBA information such as serial number firmware version Read Only Memory ROM version HBA model etc. HBA interface function block functions may also include the management of PCI information such as what PCI slot the HBA is located in what handlers the OS gives to access registers on the HBA what handlers the OS gives when doing DMA setup for the HBA registering the interrupt handler with the OS and other basic hardware interactions.

The HBA interface function block has functionality in its code to allow physical interaction with the HBA via the system s I O bus such as initialization of the HBA management of hardware errors and the sending and receiving of commands to from the HBA. Other functionality may include physical vlink information management such as whether the vlink is on or off has transmission been enabled or disabled topology type AL or point to point vlink speed e.g. 2 4 8 or 10 GBit and vlink timeout values. Other physical vlink information managed by the HBA interface function block may include global statistics for all vlinks such as low level link primitive counts frame transmit and receive counts etc capabilities such as whether or not NPIV is supported and NPIV resources e.g. maximum amount amount in use .

The HBA interface function block may also provide hooks to the logical vlink drivers for passing information to the logical vlink drivers . The HBA interface function block includes software to determine which vlink is serving in the physical vlink role and to pass this information along with information related to whether the HBA can support NPIV. The HBA interface function block interacts with the physical vlink to pass physical WWPN values and to determine when the physical vlink has actually completed its first interaction with the fabric in the form of an FLOGI command whether NPIV is supported by the fabric and the HBA and the native address that the physical vlink obtained. The HBA interface function block also maintains a list of vlinks active on HBA.

Logical vlink drivers Discovery function block. The discovery function block in each logical vlink driver performs initialization with the fabric and obtains a FC Address. If the logical vlink driver is representing a physical vlink then FLOGI is used. If the logical vlink driver is representing a virtual vlink then FDISC is used. Once the FC address for the vlink is obtained the discovery function block can send any sequence of requests out over the vlinks and they will be treated as separate unique entities. Viewed from the outside the host HBA each vlink is viewed as a separate logical FC port with a separate conventional HBA because the fabric views them according to their FC address WWPN WWNN.

Another critical role of the discovery function blocks is the discovery of and port login with remote ports. The FC fabric is managed by the WWPN WWNN and remote ports maintain a unique state relative to each FC Address WWPN WWNN. Because the remote port e.g a FC disk array sees each vlink as a unique FC address WWPN WWNN the remote port tracks logins to each vlink on a unique basis. Therefore within each vlink a completely independent view of what is present on the fabric is provided even though all vlinks are sharing the same physical FC link. For example with zoning the switch can be configured such that every time it sees a certain WWPN it only presents remote ports A B and C to it although D E and F may also exist on the fabric. Because each vlink is associated with a unique FC address and WWPN each vlink interacts with fabric services as a unique entity from the fabric s perspective whether registering for RSCN events or requesting a download of the name server directory server management server contents and the like. Each discovery function block also performs and maintains port to port logins e.g. local logical FC port to remote port which are the RPIs referred to above.

The discovery function blocks also perform the function of providing hooks to the physical FC link the HBA interface function block . Each discovery function block communicates in a consistent fashion with the HBA interface function block to send requests to the HBA to send commands e.g. SCSI read commands receive responses receive notification of interrupts pertaining to the vlink and the like. These requests are implemented by code in the discovery function blocks that communicates with a routine in the HBA interface function block that takes care of submitting the request to the HBA and getting back a response.

The discovery function block in the logical vlink driver that supports the physical vlink includes some special roles. One of these special roles is to perform fabric initialization via FLOGI obtain a FC address and determine if the fabric supports NPIV. After initialization of the physical vlink is completed using FLOGI if the fabric and HBA were determined to support NPIV the other virtual vlinks can be initialized using FDISC. Note that if the physical vlink drops for any reason the other virtual vlinks are immediately inactivated. However if the physical vlink comes back e.g. a cable is plugged back in then after an FLOGI is performed all the other virtual vlinks are immediately instantiated or restarted again. The discovery function block in the logical vlink driver that supports the physical vlink is also a conduit to user space applications that may want to manipulate the HBA e.g. update firmware etc . The discovery function block in the logical vlink driver that supports the physical vlink also tracks how many NPIV resources exist in the HBA and how many are in use as reported by the switch.

The discovery function blocks that support the virtual vlinks also include some special roles including performing fabric initialization via FDISC.

The discovery function blocks also track and maintain vlink information such as whether the vlink is in an active or inactive state. An inactive state may occur if a cable is unplugged a switch ran out of resources and denied an initialization or the HBA ran out of resources and denied its initialization etc. Each discovery function block also tracks and maintains statistics such as frame transmit and receive counts of traffic specific to the FC address WWPN WWNN of the vlink as compared to the previously described physical vlink statistics which can be an aggregate of all the vlinks .

Logical vlink drivers SCSI function block. In addition to providing hooks to the physical FC link the HBA interface function block each logical vlink driver also contains a SCSI function block that includes code to register its corresponding vlink with the OS and present the vlink to the OS in a style that the OS expects. Typically the HBA is an FCP initiator and the OS includes a SCSI subsystem so the SCSI function block will register with the SCSI subsystem as a SCSI bus and present targets and possibly LUNs to the SCSI subsystem so that SCSI I O operations and the like can be performed. If the HBA is for communicating IP FC the SCSI function block could register with the networking stack as an IP Network Interface Card NIC . The SCSI function block also contains code to send receive requests and responses from the OS relative to the vlink e.g. send receive SCSI commands IP packets etc. .

Logical vlink drivers I O block. The I O block of each logical vlink driver can call on software for SCSI Bus functionality when the HBA is a FCP initiator. This includes interaction with the SCSI bus data structures of the OS to report remote ports found on the vlink and whether they were FC targets and present them as targets with target IDs. The I O block can also maintain bindings such that the FCP target is always presented with the same target ID on the SCSI bus presented to the OS. In addition in some OSs the I O block can also discover and present SCSI LUNs for the LUNs present on each FC target. Note that if the SCSI block supports IP FC then the I O described here could be the sending reception of network packets and the particular mode of the firmware in the HBA that supports IP FC may be selected.

The I O block can also perform functions such as receiving commands from the OS to send to a target or LUN sending and receiving I O requests queuing I O requests if the HBA is busy performing target or LUN round robin or load balancing when sending commands to the HBA performing queue depth management in response to QUEUE FULL responses from the HBA tracking I O timeouts performing I O request aborts and or retries and performing various types of resets e.g. LUN resets target resets and bus level resets and task management functions.

Although the present invention has been fully described in connection with embodiments thereof with reference to the accompanying drawings it is to be noted that various changes and modifications will become apparent to those skilled in the art. Such changes and modifications are to be understood as being included within the scope of the present invention as defined by the appended claims.


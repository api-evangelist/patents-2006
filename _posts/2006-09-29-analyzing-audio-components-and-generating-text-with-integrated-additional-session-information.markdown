---

title: Analyzing audio components and generating text with integrated additional session information
abstract: Systems and methods for analyzing audio components of communications are provided. In this regard, a representative system incorporates an audio analyzer operative to: receive information corresponding to an audio component of a communication session; generate text from the information; and integrate the text with additional information corresponding to the communication session, the additional information being integrated in a textual format.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07991613&OS=07991613&RS=07991613
owner: Verint Americas Inc.
number: 07991613
owner_city: Roswell
owner_country: US
publication_date: 20060929
---
It is desirable in many situations to record communications such as telephone calls. This is particularly so in a contact center in which many agents may be handling hundreds of telephone calls each every day. Recording of these telephone calls can allow for quality assessment of agents improvement of agent skills and or dispute resolution for example.

In this regard it is becoming more commonplace for recordings of telephone communications to be reduced to transcript form. However the number of individual words within each telephone call is such that storing each word as a record in a relational database is impractical for large contact centers handling millions of calls per annum.

In this regard systems and methods for analyzing audio components of communications are provided. An embodiment of such a system comprises an audio analyzer operative to receive information corresponding to an audio component of a communication session generate text from the information and integrate the text with additional information corresponding to the communication session the additional information being integrated in a textual format.

An embodiment of a method comprises receiving information corresponding to an audio component of a communication session generating text from the information and integrating the text with additional information corresponding to the communication session the additional information being integrated in a textual format.

Other systems methods features and advantages of this disclosure will be or become apparent to one with skill in the art upon examination of the following drawings and detailed description. It is intended that all such additional systems methods features and advantages be included within this description and be within the scope of the present disclosure.

Systems and methods for analyzing audio components of communications are provided. In this regard several exemplary embodiments will be described in which various aspects of audio components of communications are analyzed. By way of example in some embodiments the audio component of a communication e.g. a telephone call is converted to a textual format such as a transcript. Additional information such as amplitude assessments of the communication is associated with the textual format. Notably such additional information also can be textual thereby resulting in a data file that uses less memory than if the audio component were stored as audio and appended with the additional information. Moreover since the data file uses a textual format text based indexing and searching can be readily accommodated.

The textual representation of the dialog and surrounding telephony experience occupies much less space per hour of telephone call than the audio recording of the call itself and hence can be accommodated within a recording system for marginal additional storage cost. The infrastructure of the recording system makes it easy to manage access secure and archive the content along with the audio to which it relates.

In some embodiments this approach allows a single repository and search mechanism to search across both contacts that originated as text e.g. email and web chat and those originating as speech e.g. telephone calls . This potentially enables a user to view their entire customer contact through a single mechanism.

In this regard is a schematic diagram illustrating an embodiment of a system for analyzing audio components of communications. As shown in system incorporates an audio analyzer that is configured to analyze audio components of communications. In the audio component is associated with a communication session that is occurring between a caller and an agent via a communication network . In this embodiment the agent is associated with a contact center that comprises numerous agents for interacting with customers e.g. caller .

One should also note that network can include one or more different networks and or types of networks. As a nonlimiting example communications network can include a Wide Area Network WAN the Internet and or a Local Area Network LAN .

In operation the audio analyzer of performs various functions or method steps as depicted in the flowchart of . As shown in the functions include receiving information corresponding to an audio component of a communication session block . In block text is generated from the information. Then in block the text is integrated with additional information corresponding to the communication session with the additional information being integrated in a textual format. By way of example in some embodiments the text with additional information is stored as a text document.

It should be noted that a communication such as a telephone call may last from a few seconds to a few hours and therefore may include from one to several thousand words and several tens of thousands of phonemes i.e. meaning laden sounds that form spoken words . Thus in some embodiments for each word or phoneme the audio analyzer identifies one or more of the following the time or offset within the communication at which each word phoneme started the time or offset within the communication at which each word phoneme ended and the confidence level with which each word phoneme was identified. In this regard some embodiments can identify not only the best guess word phoneme but the N best guesses.

As shown in the embodiment of audio analyzer incorporates a speech recognition engine a phonetic analyzer an amplitude analyzer and a call flow analyzer . It should be noted that in other embodiments an audio analyzer may incorporate fewer than all of the components and and or all of the corresponding functions.

With respect to the speech recognition engine the speech recognition engine which can be a large vocabulary type generates a textual transcript e.g. transcript of at least a portion of an audio component of a communication session. Once so generated the transcript can be stored as a text document.

In some embodiments such a transcript can incorporate interruptions from the other party e.g. uh huh feedback within the text of the active speaker. Schemes that can be used for implementing such a feature include but are not limited to encapsulation within characters that do not form part of the active speaker text e.g. or uh huh a marker character that indicates the location of the interjection without indicating the actual utterance e.g. the interjection may be inserted within a word or at the next previous word boundary and or the interjection may be surrounded by space or other whitespace arbitrary characters so as not to be considered as concatenated to the previous next word.

The phonetic analyzer generates a phonetic representation of at least a portion of the audio component of the communication session as a text document. This can be accomplished using standard symbols for speech or an alternate mapping of phoneme to character. In some embodiments a space character can be used to indicate a pause. In a refinement the duration of pauses may be indicated by multiple space characters e.g. one space per second.

The amplitude analyzer generates a textual representation of the audio component. In particular the textual representation includes an identification of which party is speaking at any time during the communication and an indication of the amplitude of the speech at each time. By way of example depicts a textual representation of an audio component of a communication. Specifically the embodiment of is a one character per second representation of a call recorded in stereo. This embodiment corresponds to a call in which an agent a greeted the customer for four seconds using normal voice levels designated by the use of the lower case letter . After a one second pause indicated by a single space character the customer c responded at normal levels for three seconds then spoke at a high level e.g. shouted for three seconds designated by the use of the upper case letters . After a four second pause indicated by the use of four space characters the agent responded for 13 seconds at normal levels. Then after a one second pause indicated by a single space character the agent spoke for nine seconds during which the customer interjected twice briefly designated by the b for both speaking at normal levels . After another one second pause an extended verbal exchange between the agent and customer takes place that is generally broke into a twelve second portion and a ten second portion. Notably the lack of pauses in this section and the use of capital letters appears to indicate an argument between the customer and the agent.

During the twelve second portion the customer responded at high levels during which the agent interjected two times designated by the B first for five seconds and then for three seconds. Then during the ten second portion the agent was able to speak for one second at a normal level after which the customer interjected at high levels for two seconds following another one second during which the agent was able to speak at a normal level. After that both the agent and the customer spoke simultaneously for one second during which at least one of them was speaking at a high level presumably the customer and then the customer alone spoke at high levels for five seconds.

An alternative representation which does not use a fixed number of characters per second of audio is depicted in . In the same communication session that was used to generate the text in has been used for generating this text. In particular each component in this representation is a combination of a character for who was talking and a number designating the number of seconds of speaking by that person. By way of example a4 indicates that the agent was speaking at less than a high level for four seconds.

As in the embodiment of a lower case letter designates speaking at less than a high level and a capital letter designates speaking at a high level. Note that one benefit of using upper and lowercase letters for designating various features allows for case sensitive searching. Thus when interested in speech of a high level case sensitive searching for upper case letters can be used. Whereas in contrast if the amplitude level is not relevant to a particular search case insensitive searching can be performed.

Clearly various other characters could be used in addition to or instead of those characters used in the embodiments of . For instance the letter f could be used to indicate feedback in the audio component.

In a further refinement more than the three amplitude levels e.g. silence normal high may be identified with different characters being used to indicate each.

With respect to the call flow analyzer this analyzer generates a textual representation of the communication from a call flow perspective. That is the call flow analyzer generates text corresponding to the events occurring during the communication such as time spent ringing hold times and talking times. By way of example depicts an embodiment of a textual representation of the same communication that was used to generate the text outputs depicted in .

As shown in this representation indicates that the communication involved ringing for 15 seconds R15 talking for 61 seconds T61 on hold for the following 35 seconds H35 and then terminated with caller abandonment A . Clearly various other characters could be used in addition to or instead of those characters used in the embodiment of and or various other events could be represented. For instance the letter X could be used to indicate a transfer and the letter H can be used to indicate that an agent hung up the call.

In those embodiments in which an annotation of time is maintained time approximation techniques such as banding can be used to facilitate easier clustering of the information. For example it may be desirable to summarize the talk listen silence fragments rather than provide a fixed number of characters per second rate. In one implementation of banding for example any silence less than 1 second could be represented as s0 a 1 to 2 second delay as s1 and a 2 to 5 second delay as s3 in increasing bands. Notably the banding for periods of speaking may be different from those of silence and or hold. For example hold of 0 to 15 seconds may be considered insignificant and classified as H0 but when there is speaking there is a potentially significant difference between many 2 second sentences and a flowing 10 second sentence. Hence the breadth of the talk bands could be narrower at the low end but broader at the high end. For instance any continuous period of speaking above 1 minute without letting the customer speak may be considered unacceptable and rare enough that additional banding above 1 minute is not necessary.

Based on the foregoing examples it should be understood that an embodiment of a voice analyzer can be configured to generate text documents that include various formats of information pertaining to communications. In some embodiments such a document may include a combination of one or more of the formats of information indicated above to produce a richly annotated record of a communication. This can be achieved in a number of ways. By way of example such information could be interleaved or can be segmented so that information of like type is grouped together. Notably each such text document can include an attribute that identifies the document type s e.g. transcript phonetic transcript and or talk listen pattern.

Additionally or alternatively at least some of the information can be provided as html tags that are associated with a text document. By way of example the following html tag could be associated with one or more of the document types described above with respect to Thank you for calling Widgets Inc . . . . 

Note also the timestamps embedded in the html above. These could be at the talk listen fragment level individual word level or every second on nearest word boundary so as to allow positioning within the call to the nearest second for example.

In some embodiments the voice analyzer can determine the time offset within the call of one or more words phonemes by the insertion of whitespace characters e.g. tab characters. In such an implementation a tab can be inserted once each second for example between words. This allows the offset of any word and the duration of any pause to be determined to within 1 second accuracy. This is adequate for retrieving the appropriate section of audio from a call as typically a short lead in to the specific word phrase is played to give the user some context within which the word phrase of interest has meaning. It should be noted that the mechanism for determining any time offset may in some embodiments be in addition to any mechanism used for determining segment or event timing.

In some embodiments the generated text documents are provided to indexing and search algorithms. In some embodiments the same indexing and search algorithms can be used across the document types because there is little to no overlap between the tokens in the different categories of documents. Thus a search can be constructed using words phonemes or talc listen patterns or a combination thereof. The results will be largely unaffected by the presence of tokens from other two domains and a single search across a mixed set of text documents can be performed.

In some embodiments the text documents are loaded into a text search engine such as Lucene. Information about Lucene can be located at the World Wide Web address of The Apache Software Foundation which generates indexes that allow for rapid searching for words phrases and patterns of text as would be done on web site content.

In this regard any such indexing process could be modified so that at least some of the information is excluded from the indexing. By way of example if a text document is stored as a composite transcript telephony events and talk listen pattern the latter two may be excluded in the same way that embedded html tags are typically excluded from normal text indexing of web documents. In fact if these subsidiary annotations are embedded as html tags these tags can be excluded automatically through normal operation by a standard html ingestion parser.

Additionally or alternatively an ingestion process can be modified so that rather than offset into the text in characters the offset in seconds of each word is stored. By way of example the offsets can be deduced from the number of tab characters processed to date if these are used to indicate one second intervals as above.

In some embodiments a search engine s handling of proximity searches can be modified such that A within T seconds of B can optionally mean by the same speaker or by different speakers. 

Further in some embodiments modified stemming algorithms can be used to stem phonetic strings by stripping the phonetic equivalents of . . . ing . . . ed etc. rather than the normal English language text stemming algorithm.

Bayesian clustering algorithms can be applied in some embodiments to the text documents to identify phrases of multiple words and or phonemes that occur frequently. These text documents and hence the calls call fragments they represent can be grouped into clusters that show common characteristics.

A further refinement proactively highlights the emergence of new clusters and the common phrases that are being identified within them. The user may then review some examples of these and determine the significance or otherwise of the new clusters.

As a further refinement a text document generated by a voice analyzer can be incorporated along with call attributes e.g. ANI AgentID and Skill . In some of these embodiments the information can be provided in an xml file. This information can be stored archived and or secured alongside the recorded audio or in complementary locations.

As should be noted the aforementioned exemplary embodiments tend to leverage the highly scalable and efficient text indexing and search engines that the have been developed to search the millions of documents on the web. This can allow businesses to search through the content of millions of calls without impacting their existing relational databases that typically hold the metadata associated with these calls.

Further the local interface may include address control and or data connections to enable appropriate communications among the aforementioned components. The processor may be a hardware device for executing software particularly software stored in memory.

The memory can include any one or combination of volatile memory elements e.g. random access memory RAM such as DRAM SRAM SDRAM etc. and nonvolatile memory elements e.g. ROM hard drive tape CDROM etc. . Moreover the memory may incorporate electronic magnetic optical and or other types of storage media. Note that the memory can have a distributed architecture where various components are situated remote from one another but can be accessed by the processor. Additionally the memory includes an operating system as well as instructions associated with a speech recognition engine a phonetic analyzer an amplitude analyzer and a call flow analyzer . Exemplary embodiments of each of which are described above.

It should be noted that embodiments of one or more of the systems described herein could be used to perform an aspect of speech analytics i.e. the analysis of recorded speech or real time speech which can be used to perform a variety of functions such as automated call evaluation call scoring quality monitoring quality assessment and compliance adherence. By way of example speech analytics can be used to compare a recorded interaction to a script e.g. a script that the agent was to use during the interaction . In other words speech analytics can be used to measure how well agents adhere to scripts identify which agents are good sales people and which ones need additional training. As such speech analytics can be used to find agents who do not adhere to scripts. Yet in another example speech analytics can measure script effectiveness identify which scripts are effective and which are not and find for example the section of a script that displeases or upsets customers e.g. based on emotion detection . As another example compliance with various policies can be determined. Such may be in the case of for example the collections industry where it is a highly regulated business and agents must abide by many rules. The speech analytics of the present disclosure may identify when agents are not adhering to their scripts and guidelines. This can potentially improve collection effectiveness and reduce corporate liability and risk.

In this regard various types of recording components can be used to facilitate speech analytics. Specifically such recording components can perform one or more various functions such as receiving capturing intercepting and tapping of data. This can involve the use of active and or passive recording techniques as well as the recording of voice and or screen data.

It should be noted that speech analytics can be used in conjunction with such screen data e.g. screen data captured from an agent s workstation PC for evaluation scoring analysis adherence and compliance purposes for example. Such integrated functionalities improve the effectiveness and efficiency of for example quality assurance programs. For example the integrated function can help companies to locate appropriate calls and related screen interactions for quality monitoring and evaluation. This type of precision monitoring improves the effectiveness and productivity of quality assurance programs.

Another aspect that can be accomplished involves fraud detection. In this regard various manners can be used to determine the identity of a particular speaker. In some embodiments speech analytics can be used independently and or in combination with other techniques for performing fraud detection. Specifically some embodiments can involve identification of a speaker e.g. a customer and correlating this identification with other information to determine whether a fraudulent claim for example is being made. If such potential fraud is identified some embodiments can provide an alert. For example the speech analytics of the present disclosure may identify the emotions of callers. The identified emotions can be used in conjunction with identifying specific concepts to help companies spot either agents or callers customers who are involved in fraudulent activities. Referring back to the collections example outlined above by using emotion and concept detection companies can identify which customers are attempting to mislead collectors into believing that they are going to pay. The earlier the company is aware of a problem account the more recourse options they will have. Thus the speech analytics of the present disclosure can function as an early warning system to reduce losses.

Additionally included in this disclosure are embodiments of integrated workforce optimization platforms as discussed in U.S. application Ser. No. 11 359 356 filed on Feb. 22 2006 entitled Systems and Methods for Workforce Optimization which is hereby incorporated by reference in its entirety. At least one embodiment of an integrated workforce optimization platform integrates 1 Quality Monitoring Call Recording voice of the customer the complete customer experience across multimedia touch points 2 Workforce Management strategic forecasting and scheduling that drives efficiency and adherence aids in planning and helps facilitate optimum staffing and service levels 3 Performance Management key performance indicators KPIs and scorecards that analyze and help identify synergies opportunities and improvement areas 4 e Learning training new information and protocol disseminated to staff leveraging best practice customer interactions and delivering learning to support development and or 5 Analytics deliver insights from customer interactions to drive business performance. By way of example the integrated workforce optimization process and system can include planning and establishing goals from both an enterprise and center perspective to ensure alignment and objectives that complement and support one another. Such planning may be complemented with forecasting and scheduling of the workforce to ensure optimum service levels. Recording and measuring performance may also be utilized leveraging quality monitoring call recording to assess service quality and the customer experience.

One should note that the flowcharts included herein show the architecture functionality and or operation of a possible implementation of software. In this regard each block can be interpreted to represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the blocks may occur out of the order. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved.

One should note that any of the programs listed herein which can include an ordered listing of executable instructions for implementing logical functions such as depicted in the flowcharts can be embodied in any computer readable medium for use by or in connection with an instruction execution system apparatus or device such as a computer based system processor containing system or other system that can fetch the instructions from the instruction execution system apparatus or device and execute the instructions. In the context of this document a computer readable medium can be any means that can contain store communicate propagate or transport the program for use by or in connection with the instruction execution system apparatus or device. The computer readable medium can be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device. More specific examples a nonexhaustive list of the computer readable medium could include an electrical connection electronic having one or more wires a portable computer diskette magnetic a random access memory RAM electronic a read only memory ROM electronic an erasable programmable read only memory EPROM or Flash memory electronic an optical fiber optical and a portable compact disc read only memory CDROM optical . In addition the scope of the certain embodiments of this disclosure can include embodying the functionality described in logic embodied in hardware or software configured mediums.

It should be emphasized that the above described embodiments are merely possible examples of implementations merely set forth for a clear understanding of the principles of this disclosure. Many variations and modifications may be made to the above described embodiment s without departing substantially from the spirit and principles of the disclosure. All such modifications and variations are intended to be included herein within the scope of this disclosure.

